foreign
I am mayank and welcome to this amazing
full course on machine learning expert
by simply learn but before we begin if
you enjoy watching these type of videos
find them interesting subscribe to our
YouTube channel as we bring you the best
videos daily also hit the Bell icon to
never miss any update from Simply Lila
so let's get started so we will be
briefing you with the detailed
introduction of machine learning that
covers the data source and the types of
machine learning supervised and
supervised and dreams for men's learning
and top 10 applications of machine
learning after that we will dive deep
into the understanding of the
fundamental different ml algorithms and
how the ml algorithm works up ahead we
will walk you through some amazing
projects like
fake news detection object detection and
time series these projects will serve
you as the finest portfolios for your
future interviews speaking of interviews
we have covered you along with the most
frequently Asked machine learning
interview question to help you correct
the toughest interviews before we move
on to the what is machine learning
introduction to machine learning if
you're an expiring AIML engineer then
there is no better time to train
yourself in exciting field of machine
learning if you are looking for course
that covers everything from fundamentals
to Advanced Techniques like machine
learning algorithm development and
unsupervised learning look no further
than our Caltech in partnership with IBM
this Ai and machine learning bootcamp in
collaboration with Caltech will help you
advance your career as AIML specialist
this AIML bootcamp include live classes
delivered by industry expert Hands-On
lab industry relevant projects and
master classes by Caltech Professor key
features of this amazing machine
learning course includes earn up to 22
cus from Cadillac ctme online
convocation by Caltech ctm program
director simply learn Courier Service
help you get noticed by hiring companies
on a bootcam certificate from Cadillac
ctme industry relevant cap Zone project
in three domains 25 plus Hands-On
projects across industry verticals with
integrated lab so why we're joined now
seats are failing first find the
coastline from the description box below
so without any further Ado over to our
training expert do you guys know from
predicting the weather condition by
examining Cloud designs in the baby long
time in the 18th century BC to using
electronic messages phones analyzing
satellite images in the 20th century the
process of weather forecasting has
evolved essentially throughout the long
term however in this digitalized period
forecasting climate and recreating
long-term climate patterns have been
accomplished with the assistance of
machine learning models by examining
volumes of information by computer
so the question arises is it possible
for a machine to learn from past
experiences just like we humans do and
the answer to this question is yes and
this became possible just because of
machine learning machine learning is a
support of artificial intelligence which
is broadly defined as capability of a
machine to imitate intelligent human
behavior
like humans learn from their past and
acquire knowledge through experience
either directly or shared by others
machine acquired knowledge through
experience shared in the form of data
let's understand with the basic idea of
how machine Learning Works
firstly you have to input the pass data
or training data into the machine
learning model
after that machine learning model learns
from the data and creates logical models
in the end when you input new data to a
machine learning model it predict output
according to the logical models
after understanding what machine
learning is equation arises why we need
machine learning
even though machine learning is
constantly developing with countless new
technologies it is still used in
different organizations machine learning
is important because it aims with a
perspective on patterns in user Behavior
operation design and support the
improvements of new products also as the
volume of data over performs people move
in the direction of computerized systems
that store data and machine learning
models can predict proper solution this
will reduce human efforts time and
increase work efficiency
like Facebook Google Uber top
organization use make machine learning
as their master store
machine learning has demonstrated
history in almost all the sector like
marketing e-commerce Banking and many
others as of now we are well aware of
how machine learning is important let's
have a walkthrough of the advantages and
disadvantages of machine learning
before diving deep into any technology
it is very important to understand its
advantage and disadvantage here are some
advantages of machine learning first one
is low error rate if coded properly
machine learning models could have a
lower rate as compared to humans machine
learning models have incredible Clarity
accuracy and speed second was means
automation
machine learning reduces the need for
human resources at every step as it
automatically learns and predicts the
output third one is wide range
application almost every sector takes
advantage of machine learning to reduce
human effort and get quick results
talking about Healthcare machine
learning is used for screening or to
predict healthy risk in healthcare
machine learning is used to give each
student an individual educational
experience or in banking enables more
accurate reporting by automating
creditors for both Banks and customers
the fourth one is Trends and patterns
identification we can identify the
numerous amount of Trends and patterns
for example the stock market uses
machine learning prediction model to
predict Trends and patterns for
companies talk and other Financial
assets
and the last one is continuous
Improvement using machine learning you
can see continuous Improvement in your
results machine learning is capable of
learning from the information we provide
them
as every coin has two sides machine
learning has advantages as well as
disadvantage let's have a look at some
disadvantages now
the first one is costly machine learning
models are expensive and time consuming
to build rebuild and repair the robotic
repair can reduce the time and Manpower
required to fix machine learning models
but again that is too pricey second one
is limited time and quality resources
machine learning needs a lot of data but
it also needs good quality data to learn
efficiently the third one is storage the
backup and servers are continuous at
work and are responsible for recording
the acquired information this keeps on
building up the course for a machine to
learn the potential information is a
continuous activity thus there is a need
to store the information different
storages and cloud services are not
sufficient to store this amount of large
data and the last one is unemployment
machine replacing human jobs can lead to
several unemployment now that we have an
understanding of the advantages and
disadvantages let's move forward and
discuss different types of machine
learning
machine learning is classified into
first one is supervised learning second
one is unsupervised learning third one
is reinforcement learning fourth one is
semi-supervised learning let's
Understand Each one by one in detail
in supervised learning the machine is
trained with the help of label data with
external Guidance the machine learning
model is given a small preparation data
set to work with let's understand this
by example suppose we input data as an
image having known label data of dogs in
this way machine will learn and create
some logical models when we input a
known data the Machine model will
categorize them as per their
similarities and patterns that is we can
easily sort the unknown input as a dog
supervised machine learning can be
further classified into two types first
one is classification second is
regression and the next one is
unsupervised learning
in unsupervised learning machine Act
without any guidance in this technique
machine act upon the data that is
neither classified nor labeled without
any external guidance for example
suppose we input data as an image having
both mango and banana which the machine
has never seen in this way the machine
has no clue about the characteristic of
mango and banana so we can't categorize
them as mango and banana yet it can
categorize them as their similarities
pattern and contrast that is we can
easily sort the above picture into two
parts
unsupervised learning can be further
classified into two types which are
clustering Association now let's have a
look at reinforcement learning
reinforcement learning is hit and trial
method in the absence of any data the
machine is bound to learn from its
experience only
let's understand with an example suppose
we input the image of a square machine
learning model will respond using the
past experience suppose it gives output
as a circle
then feedback is provided as wrong it's
an image of a square now the machine
learning model will learn from this
feedback and it will predict the current
output when we import the same image
next time
reinforcement learning can be further
classified into two types positive
reinforcement learning negative
reinforcement learning
and the last one is semi-supervised
learning
semi-supervised learning is a sort of
machine learning algorithm that lies
between supervised and unsupervised
machine learning or it takes label and
unlabeled data to predict the outcomes
to beat the disadvantage of supervised
learning and unsupervised learning
algorithm the idea of semi-supervised
learning was presented
the principal point of semi-supervised
learning is to utilize accessible
information successfully after
discussing what machine learning is
importance of machine learning the
advantages and disadvantages of machine
learning and its types let's move
forward to see commonly used algorithm
and machine learning in computer
programming terms an algorithm is a set
of well-defined instruction to solve a
particular problem
it takes a bunch of information sources
and delivers the ideal result in this
modern time there are lots of machine
learning algorithm used today in this
video we will put highlight the most
widely used machine learning algorithm
the first one is support Vector machine
algorithm also known as svm sum
algorithm falls under the supervised
machine learning technique next is the K
nearest neighbor algorithm it falls
under the unsupervised machine learning
technique further we have hierarchical
clusting algorithm which uses an
unsupervised machine learning technique
the last one is decision tree algorithm
which is most widely used algorithm in
the machine learning area the decision
tree algorithm falls under the
supervised machine learning technique
after this let's move forward to see
some tools used to train the models in
machine learning
the current market is flooded with an
array of machine learning tools like
tensorflow purchasinga spark Amazon
machine learning Python and many more
which help in training the models and
the system let's have a look at each of
them first we will start with tensorflow
tensorflow is a free and open source
programming library for machine learning
key features of tensorflow are fast
debugging scalable and many more Google
uses tensorflow to power machine
learning implementation in products like
search Gmail and translate and the next
we have Apache singer
top level project it is used for
developing an open source machine
learning library key features of Apache
singer are distributive learning memory
optimization time profiling and many
more
Apache singer is currently in use at
garnier's Technologies City group and
many others
in the next one we have Apache Spa
Apache spark is an open source bound
together analytics engine Sparks come
with the features like pre-processing
data transformation and many more spark
machine learning whose power the Toyota
customer 360 inside platform and social
media Intelligence Center
and the next we have Amazon machine
learning tool the most promising
strength of AWS machine learning tool is
that they are based on a highly
comprehensive Cloud platform
Amazon machine learning comes with the
features like accurate prediction
reducing operation override improving
customer experience and many other many
pigments like Netflix and Facebook use
AWS as their machine learning service
and the last one we have python
pytots is an open source machine
learning framework in view of torch
Library
features like distributive training a
robust ecosystem Cloud support and many
more
companies commonly using pythons are IBM
Qualcomm Walmart and many more
let's move forward and cover
applications of machine learning
knowingly or unknowingly machine
learning is playing a major role in our
day-to-day life here I have some
application of machine learning which
will improve how machine learning is
helping everyone all over the globe
we all use YouTube Netflix and Facebook
in our day-to-day life the machine
learning algorithm figures out the
patterns of what type of video movie we
watch and thus provide a solution
accordingly Tesla is the one of the best
companies which manufacture self-driving
cars using Electric solar and integrated
Renewable Energy Solutions caused by
Tesla are already making their way to
the market and ready to show the world
of the power Ai and machine learning
another application of machine learning
is speech recognization
we all aware of voice assistance like CD
by Apple Cortana by Microsoft and Alexa
by Amazon these assistant translate is
speech to text and use machine learning
to provide result
machine learning is also used in medical
sector to determine various diseases
with the help of machine learning the
pattern and the symptoms of disease can
easily be diagnosed
and the last one is forecasting weather
forecasting and financial forecasting
are application of machine learning in
which future outcomes are predicted
using historical data it involves the
use of statical modeling data mining
technique and machine learning for
prediction so as we have seen all the
major aspects of machine learning we
will finally have a look at the future
of machine learning the commercial
impacts of machine learning can really
hold a huge potential to generate
significantly High productivity growth
machine learning is one of the quickest
developing field on this planet at the
present time machine learning Engineers
are in high demand the average salary of
machine learning engineers in India is 9
lakh per Anna and the annual base pay
for U.S machine learning Engineers is
one like thirty thousand dollar all the
ventures as of now have countless usage
in machine learning which is the
fundamental inspiration behind why there
is an interest for occupation in that
field
the growth in technology has that in
increasing demand for professionals who
can understand and make use of the vast
amount of data present companies like
Facebook Google Tesla and many more are
constantly hiring machine learning
engineers
the future of machine learning looks
bright and will pave the path to many
more inventions and with
now let's look into the types of machine
learning machine learning is primarily
of three types first one is supervised
machine learning as the name suggests
you have to supervise your machine
learning while you train it to work on
its own it requires labeled training
data next up is unsupervised learning
wherein there will be training data but
it won't be labeled
finally there is reinforcement learning
wherein the system learns on its own
let's talk about all these types in
detail let's try to understand how
supervised Learning Works look at the
pictures very very carefully the monitor
depicts the model or the system that we
are going to train
this is how the training is done we
provide a data set that contains
pictures of a kind of a fruit say an
apple
then we provide another data set which
lets the model know that these pictures
where that of a fruit called Apple
this ends the training phase now what we
will do is we provide a new set of data
which only contains pictures of Apple
now here comes the fun part the system
can actually tell you what fruit it is
and it will remember this and apply this
knowledge in future as well
that's how supervised Learning Works you
are training the model to do a certain
kind of an operation on its own
this kind of a model is generally used
into filtering spam mails from your
email accounts as well yes surprise
aren't you
so let's move on to unsupervised
learning now let's say we have a data
set which is cluttered in this case we
have a collection of pictures of
different fruits we feed these data to
the model and the model analyzes the
data to figure out patterns in it in the
end it categorizes the photos into three
types as you can see in the image based
on their similarities
so you provide the data to the system
and let the system do the rest of the
work simple isn't it this kind of a
model is used by Flipkart to figure out
the products that are well suited for
you
honestly speaking this is my favorite
type of machine learning out of all the
three and this type has been widely
shown in most of the Sci-Fi movies
lately let's find out how it works
imagine a newborn baby you put a burning
candle in front of the baby the baby
does not know that if it touches the
flame its fingers might get burned so it
does that anyway and gets hurt
the next time you put that candle in
front of the baby it will remember what
happened the last time and would not
repeat what it did that's exactly how
reinforcement learning works
we provide the machine with a data set
wherein we ask it to identify a
particular kind of a fruit in this case
an apple
so what it does as a response it tells
us that it's a mango but as we all know
it's a completely wrong answer so as a
feedback we tell the system that it's
wrong it's not a mango it's an apple
what it does it learns from the feedback
and keeps that in mind
when the next time when we ask a same
question it gives us the right answer it
is able to tell us that it's actually an
apple that is a reinforced response so
that's how reinforcement learning works
it learns from his mistakes and
experiences this model is used in games
like Prince of Persia or Assassin's
Creed or FIFA wherein the level of
difficulty increases as you get better
with the games
just to make it more clear for you let's
look at a comparison between supervised
and unsupervised learning firstly the
data involved in case of supervised
learning is labeled as we mentioned in
the examples previously
we provide the system with a photo of an
apple and let the system know that this
is actually an apple
that is called label data so the system
learns from the label data and makes
future predictions
now unsupervised learning does not
require any kind of label data because
its work is to look for patterns in the
input data and organize it
the next point is that you get a
feedback in case of supervised learning
that is once you get the output the
system tends to remember that and uses
it for the next operation
that does not happen for unsupervised
learning and the last point is that
supervised learning is mostly used to
predict data whereas unsupervised
learning is used to find out hidden
patterns or structures in data
I think this would have made a lot of
things clear for you regarding
supervised Gene learning is the art of
making computers learn and act like
humans by fitting data and which focuses
on utilization of information and
imitating the way that people learn step
by step working on its accuracy
machine learning is playing a major role
in our day-to-day life machine learning
is used in Trends and pattern
identification automation weather
forecasting and many more machine
learning is classified into supervised
machine learning unsupervised machine
learning reinforcement learning
semi-supervised learning
let's understand supervised machine
learning in detail
provides machine learning machines are
trained using label data also known as
training data to predict the results
here labeled data refers to a group of
data that have been tagged with one or
more names and data is already known to
the machine
supervised machine learning can be used
for image and object recognition
Predictive Analytics customer sentiment
analysis spam detection and many more
after understanding what supervised
machine learning is let's move forward
and see how supervised machine Learning
Works
suppose we have a data set that includes
dogs and cats now the first step is that
we need to train the model for each dogs
and cats as per the factors like
similarity pattern shapes and contrast
now after training we test our model
using new data set which is unknown to
the model and the task of the model is
to identify the new input data the
machine is already trained on all types
of similarity patterns shapes and
contrast
when it finds new data it classifies it
on the basis of simulatory patterns
shapes and contracts and predict the
output
as of now we are well aware of how
supervised machine Learning Works let's
have a walkthrough of the advantages and
disadvantages of supervised machine
learning
before diving deep into any technology
it is very important to understand its
advantages and disadvantages here are
some advantages of supervised machine
learning first one is real world
competition problems help you to solve
various types of real world completion
problems like spam detection image and
objects recognition and many more the
second one is optimization using past
experience supervised machine learning
can optimize performance criteria
the third one is prediction
in supervised machine learning models
can predict the output based on the past
experience as well fourth one is
reusable data we can reuse the training
data unless there is any feature change
supervised machine learning has
advantages as well as disadvantages
let's have a look at some disadvantages
now
computation time is also known as
running time it's very vast for the
supervised machine learning supervised
machine learning models always need
updates
pre-processing of data a big challenge
for predicting the output anyone can
overlap supervised machine learning
easily it occurs when a statical model
is exactly against its straining data
now that we have an understanding of
advantage and disadvantages let's move
forward and discuss different types of
supervised machine learning
supervised machine learning can be
further classified into two problems
which are classification integration
let's see them one by one in detail
classification is a process of in which
new observation are recognized and
separated in such a way that they can be
categorized assuming you have a group of
things for examples vegetables you can
categorize them based on their
properties that they have like you can
arrange the potatoes in class A tomatoes
in class B capsicum in class C
these are some popular classification
algorithms that come under supervised
machine learning first one is random
Forest second decision trees third one
is logistic regression fourth one is
support Vector machines
moving forward let's have a look on
regression
a regression algorithm is used to figure
out the connection between dependent and
independent variables dependent
variables are nothing but a variables
which we are trying to predict or
forecast and independent variables means
that factors that influence the analysis
it is usually used to make projection
for example we have variable one as
humidity and variable towards
temperature where humidity will play the
role of dependent variable and in
temperature will be the independent
variable humidity and temperatures are
correlated in such a way that as
temperature increases commodity will
decrease and vice versa many
organization utilize regression models
to predict how stocks will act from here
on this is done by breaking down the
past information on stock costs and
Trends to recognize pattern these are
some popular regression algorithms that
comes under supervised machine learning
first one is linear regression second
one is digression trees third one is
non-linear regression fourth one is
Bayesian linear regression after
discussing the advantages and
disadvantages of supervised machine
learning and its types let's move
forward to see algorithms in supervised
machine learning
in computer programming terms an
algorithm is a set of well-defined
instruction to solve a particular
problem it takes a bunch of information
sources and delivers the ideal result
the first provides machine learning
algorithm is linear regression linear
regression is utilized to recognize the
connection between dependent variable
and at least one independent variable
and is commonly utilized to make
forecasts about future results company
frequently use linear regression models
to predict future sales this can be
useful for things like planning and
arranging algorithms like Amazon product
to product collaborative filtering are
utilized to predict what clients will
purchase later on given their past
purchase history
a regression tree is worked through a
cycle known as binary recursive
partitioning which is an alternative
interaction that divides the information
into segments of branches and afterwards
keep splitting each data into smaller
groups as the technique claims each
branch these days are used for dependent
variables with continuous value for
example a regression tree name food
which divides into segments which and
non-width further it keeps splitting
into smaller groups
non-linear regression is a type of
regression examination wherein
information is fit to model and
afterward communicated as a numerical
function simple linear regression
relates to factors with a straight line
while non-linear regression relates to
two factors in a non-linear relationship
non-linear regression can be used to
predict population growth over time or
the relationship between GDP and the
time of a country
this algorithm is a way to deal with
linear regression in which this statical
examination is attempted inside the
settings of bcn inference linear
equation and patient regression can
generate the same prediction and with
the help of patient processing we can
retrieve the complete variety of
inferential solution instead of Point
estimate
random Forest is one of the most
adaptable supervised machine learning
algorithm utilized for both
classification and regression purposes
random Forest is involved at work by
researchers in numerous Ventures
including banking stock exchanging
mitigation it is utilized to predict the
things which assist these businesses
with running productively like client
activity patient history and safety
a distant tree is a very specific sort
of tree that empowers you to conclude
some kind of process used for dependent
variables with discrete values for
example we want to pick between
manufacturing product a on product b or
putting resources into Decision One
decision two or decision three decision
trees are an outstanding method for
managing these kinds of complex choices
Logistics regression is calculated
logically and chosen Bank dependent
variable is category meaning they have
double results for example logistic
reaction can be used to
weather a political nominee will win or
lose a political race or regardless of
whether a secondary school student will
be admitted up to a specific School
a support Vector machine is a well-known
supervised machine learning model it is
utilized for the two information
classification and regression all things
considered it is regularly utilized for
grouping issues like weekend involvement
different Life Care Systems we can
involve in typically happy and sad look
arrangement we can involve in filters if
you make specific looks it would add the
particular filter according to the
expression the scope of articulation
lies between happy and sad after
understanding supervised machine
learning algorithms let's move forward
and cover applications of supervised
machine learning choose from over 300
in-demand skills and get access to 1 000
Plus hours of video content for free
visit skillup by simply learn click on
the link in the description to know more
supervised machine learning models can
be used to build an advanced several
businesses application including the
following the first one is image and
object recognition supervised machine
learning models can be utilized to find
isolate and assault objects out of
recordings of pictures making them
valuable when applied to different
computer design strategies and imagery
analysis the second one is predictive
analysis abroad use case for wise
machine learning models is in making
Predictive Analytics systems to give
propound experience into different
business data of Interest this permits
to expect specific outcomes because of a
given result variable assisting Business
Leaders to justify choices or turn to
serve the association
the third one is customer sentiment
analysis utilizing supervised machine
learning models Association can extract
and arrange significant pieces of data
from huge volumes of information
including context emotion and purpose
with very little human intervention
this can be incredibly valuable while
acquiring a superior understanding of
blinded collaboration and can be
utilized to further develop brand
engagement efforts the fourth one is
Spam detection spam identification is
another application of supervised
machine learning model utilizing
classification calculation companies can
prepare data sets to recognize patterns
or anomalies and new information to sort
out spam and non-spam related data
unsupervised learning is a machine
learning technique used to train the
machine learning algorithm using data
that is either unclassified or unlabeled
and allows the algorithm to act on that
data without guidance unlabeled data is
a designation for pieces of data that
have not been tagged with labels
identified by characteristics Properties
or classifications
so the flow of unsupervised learning
starts with training data that has no
labels and depends on the feature Vector
the machine learning model defines the
predictive model this is tested with an
individual subset of data with its own
feature Vector here the predictive model
defines the likelihood or cluster ID or
a better representation of unlabeled
data
let's look at the difference between
unsupervised and supervised learning
supervised learning technique deals with
labeled data where the output data
patterns are known to the system
unsupervised learning works with
unlabeled data in which the output is
just based on the collection of
perceptions
supervised learning method is less
complex the unsupervised learning method
is more complex supervised learning
conducts offline analysis unsupervised
learning performs real-time analysis
the outcome of the supervised learning
technique is comparatively more accurate
and reliable
unsupervised learning generates
moderately acute but reliable results
while classification and regression are
the types of problems solved under the
supervised learning method
unsupervised learning includes
clustering and associative rule mining
problems
example and application of unsupervised
learning let's understand unsupervised
learning through an example
consider a scenario where a child had no
learning phase and is shown images
without the labels now if the child is
asked to identify if any range is a bird
or an animal he will lack the
information that can help him do so the
best he can do is come up with the
following groups based on common
patterns wings and legs for example this
explains how unsupervised Learning Works
we show a lot of data to our algorithm
and ask it to find patterns in the data
by itself
let's look at the application of
unsupervised learning
unsupervised learning can be used for
anomaly detection as well as clustering
to understand clustering let's look at a
simple real life example a mother asks
her two children to arrange the pieces
of plain blocks the children come up
with two different groups as shown with
different similarities in the blocks
this is clustering each of her children
came up with a different type of
grouping one child grouped them based on
the shape whereas the other grouped them
based on the color there is no right or
wrong way
then how can you pick one set of
clusters over the others
this will depend on the similarity
measure used by the mother in this case
the arrangement of child one is better
than child two if the similarity measure
chosen by the mother was that blocks
should have the same shape however the
arrangement by Child 2 is better if the
similarity measure chosen by the mother
was that blocks should have the same
color therefore defining the similarity
measure is important when performing
clustering there may be different ways
in which data can be arranged in
different groups based on size shape
color texture and other complex features
anomaly detection is a clustering
technique used to identify unusual
patterns that do not conform to expected
Behavior anomaly detection has many
applications in business such as
intrusion detection system Health
monitoring and fraud detection
clustering
the method of grouping similar entities
together is called clustering the goal
of this unsupervised machine learning
method is to seek out similarities
within the data points and a cluster
similar data points together
need for clustering let's look at the
need for clustering grouping similar
entities together helps to merge the
attributes of different clusters in
other words this gives us insight into
underlying patterns of different groups
there are a lot of applications of
grouping unlabeled data for example in
order to maximize the revenue you can
identify different groups or clusters of
customers and Market to each group in a
different way another example is
grouping books together that belong to
similar topics clustering is needed to
determine the intrinsic grouping in a
set of unlabeled data
organize data into clusters that show
internal structure of the data partition
the data points understand and extract
value from large sets of structured and
unstructured data types of clustering
there are two types of clustering
hierarchical clustering and partitional
clustering hierarchical clustering can
be agglomerative and divisive whereas
partitional clustering can be K means
and fuzzy c means a distinction among
different types of clustering is whether
the set of clusters is nested or
unnested a partitional clustering is
just a division of the set of data
objects into non-overlapping sets or
clusters such that every data object is
in just one subset a hierarchical
clustering is a tree structure that has
a set of nested clusters
hierarchical clustering the output of
hierarchical clustering is a hierarchy
how does they hire archical clustering
form a hierarchy assume you are going to
create a three-layer hierarchy from six
different data nodes so first combine a
and based on similarity and also combine
d and e based on similarity combination
of A and B is combined with C in the
similar way combination of d and e is
combined with f now combine C and F
inside one cluster when you look at the
final tree it contains all clusters
combined into a single cluster
let's understand the working of
hierarchical clustering it works in four
steps step one assign each item to its
own cluster such that if you have n
number of items you will have n number
of clusters
Step 2 merge two clusters into a single
cluster by finding the closest pair of
clusters now you will have one cluster
less step three compute distances
between the new cluster and all old
clusters step 4 repeat steps two and
three until all items are clustered into
a single cluster of size n
let's understand the distance measure in
hierarchical clustering let's look at
the different kinds of linkage in
clustering complete linkage clustering
it finds the maximum distance between
points belonging to two different
clusters
single linkage clustering it finds the
minimum possible distance between points
belonging to two different clusters mean
linkage clustering it finds all possible
pairwise distances for points belonging
to two different clusters and then
calculates the average
centroid linkage clustering it finds the
centroid of each cluster and calculates
the distance between them
what is dendrogram
it is a tree diagram frequently used to
illustrate the arrangement of the
Clusters produced by hierarchical
clustering it shows the hierarchical
relationship between objects it is most
commonly created as an output of
hierarchical clustering the main use of
a dendrogram is to work out the best way
to allocate objects to clusters the
dendrogram also shows the hierarchical
clustering of five observations and the
relationship between each of them
hierarchical clustering sample
let's understand hierarchical clustering
through an example in the given example
hierarchical clustering is used to find
the distances between the different
cities in kilometers
the following Matrix traces a
hierarchical clustering of distances in
miles between different cities the
method of clustering is single link here
as you can see from the given distance
Matrix the nearest pair of objects is t
o and MI
Mi and t0 are merged into a single
cluster called Mito as Mi column has
lower values than to column Mito
consists of Mi column values
Mito column has one index with zero
value this is because there is no
distance between cluster Mito and Mito
to get a new distance Matrix we compute
the distance from this new cluster to
all other clusters
now the nearest pair of objects is n a
and RM these are combined into a single
cluster called narm to get a new
distance Matrix we compute the distance
from this new cluster to all other
clusters in the similar way the nearest
pair of objects is ba and narm these are
combined into a single cluster called ba
n a r m to get a new distance Matrix we
compute the distance from the new
cluster to all other clusters
similarly now the nearest pair of
objects is ba narm and f i these
combined into a single cluster called b
a n a rmfi
to get a new distance Matrix we compute
the distance from this new cluster to
all other clusters
finally we merge the last two clusters
this process is summarized by the
clustering diagram on the right and the
final distance Matrix on the left why
reinforcement learning training a
machine learning model requires a lot of
data which might not always be available
to us further the data provided might
not be reliable learning from a small
subset of actions will not help expand
the vast realm of solutions that may
work for a particular problem
and you can see here we have the robot
learning to walk
very complicated setup when you're
learning how to walk and you'll start
asking questions like if I'm taking one
step forward and left what happens if I
pick up a 50 pound object how does that
change how a robot would walk
these things are very difficult to
program because there's no actual
information on it until it's actually
tried out learning from a small subset
of actions will not help expand the vast
realm of solutions that may work for a
particular problem
and we'll see here it learned how to
walk this is going to slow the growth
that technology is capable of machines
need to learn to perform actions by
themselves and not just learn off humans
and you see the objective climb a
mountain a real interesting point here
is that as human beings we can go into a
very unknown environment and we can
adjust for it and kind of explore and
play with it
most of the models the non-reinforcement
models in computer machine learning
aren't able to do that very well there's
a couple of them that can be used or
integrated see how it goes is what we're
talking about with reinforcement
learning so what is reinforcement
learning
reinforcement learning is a sub-branch
of machine learning that trains the
model to return an Optimum solution for
a problem by taking a sequence of
decisions by itself
consider a robot learning to go from one
place to another
the robot is given a scenario must
arrive at a solution by itself the robot
can take different paths to reach the
destination
it will know the best path by the time
taken on each path it might even come up
with a unique solution all by itself
and that's really important as we're
looking for Unique Solutions we want the
best solution but you can't find it
unless you try it so we're looking at
our different systems or different model
we have supervised versus unsupervised
versus reinforcement learning and with
the supervised learning that is probably
the most controlled environment we have
a lot of different supervised learning
models whether it's linear regression
neural networks there's all kinds of
things in between decision trees the
data provided is labeled data with
output values specified and this is
important because we talk about
supervised learning you already know the
answer for all this information you
already know the picture has a
motorcycle in it so your supervised
learning you already know that the
outcome for tomorrow for you know going
back a week you're looking at stock you
can already have like the graph of what
the next day looks like so you have an
answer for it
and you have labeled data which is used
you have an external supervision and
solves Problems by mapping labeled input
to known output
so very controlled
unsupervised learning and the entire
learning is really interesting because
it's now taking part in many other
models they start with an you can
actually insert an unsupervised learning
model
in almost either supervised or
reinforcement learning as part of the
system which is really cool
data provided is unlabeled data the
outputs are not specified machine makes
its own predictions used to solve
association with clustering problems
unlabeled data is used no supervision
solves Problems by understanding
patterns and discovering output
so you can look at this and you can
think some of these things go with each
other they belong together so it's
looking for what connects in different
ways and there's a lot of different
algorithms that look at this when you
start getting into those there's some
really cool images that come up of what
unsupervised learning is how it can pick
out say the area of a donut one model
will see the area of the donut and the
other one will divide it into three
sections based on this location versus
what's next to it so there's a lot of
stuff that goes in with unsupervised
learning
and then we're looking at reinforcement
learning probably the biggest industry
in today's market in machine learning or
growing Market is very it's very infant
stage as far as how it works and what
it's going to be capable of the machine
learns from its environment using
rewards and errors used to solve reward
based problems no predefined data is
used no supervision follows Trail and
error problem solving approach so again
we have a random at first you start with
a random I try this it works and this is
my reward doesn't work very well maybe
or maybe it doesn't even get you where
you're trying to get it to do and you
get your reward back and then it looks
at that and says well let's try
something else and it starts to play
with these different things finding the
best route
so let's take a look at important terms
in today's reinforcement model
and this has become pretty standardized
over the last few years so these are
really good to know we have the agent
agent is the model that is being trained
via reinforcement learning so this is
your actual entity that has however
you're doing it whether using a neural
network or a
cue table or whatever combination
thereof this is the actual agent that
you're using this is the model
and you have your environment the
training situation that the model must
optimize to is called its environment
and you can see here I guess we have a
robot who's trying to get uh chest full
of gyms or whatever and that's the
output and then you have your action
this is all possible steps that can be
taken by the model and it picks one
action and you can see here that's
picked three different routes to get to
the chest of diamonds and gems
we have a state the current position
condition returned by the model
and you could look at this if you're
playing like a video game this is the
screen you're looking at so when you go
back here the environment is a whole
game board so if you're playing one of
those Mobius games
you might have the whole game board
going on but then you have your current
position where are you on that game
board what's around that what's around
you if you were talking about a robot
the environment might be moving around
the yard where it is in the yard and
what it can see what input it has in
that location that would be the current
position condition returned by the model
and then the reward to help the model
move in the right direction it is
rewarded points are given to it to
appraise some kind of action so yeah you
did good or if I didn't do as good
trying to maximize the reward and have
the best reward possible
and then policy policy determines how an
agent will behave at any time it acts as
a mapping between action and present
State this is part of the model what is
your action that you're you're going to
take what's the policy you're using to
have an output from your agent one of
the reasons they separate policy as its
own entity
is that you usually have a prediction
of a different options and then the
policy well how am I going to pick the
best based on those predictions I'm
going to guess at different options
and we'll actually weigh those options
in and find the best option we think
will work so it's a little tricky but
the policy thing is actually pretty cool
how it works let's go ahead and take a
look at a reinforcement learning example
and just in looking at this we're going
to take a look consider what a dog that
we want to train so the dog would be
like the agent so you have your your
puppy or whatever and then your
environment is going to be the whole
house or whatever it is or you're
training them and then you have an
action we want to teach the dog to fetch
so action equals fasting
uh and then we have a little biscuits so
we can get the dog to perform various
actions by offering incentives such as a
dog biscuit as a reward
the dog will follow a policy to maximize
this reward and hence we'll follow every
command and might even learn new actions
like begging by itself so yeah you know
so we started off with fetching it goes
oh I get a biscuit for that it tries
something else and you get a handshake
or begging or something like that and it
goes oh this is also reward based and so
it kind of explores things to find out
what will bring it is biscuit
and that's very much like how reinforced
model goes is it looks for different
rewards how do I find can I try
different things and find a reward that
works
the dog also will want to run around and
play an explore its environment this
quality of model is called exploration
so there's a little Randomness going on
in Exploration
and explores new parts of the house
climbing on the sofa doesn't get a
reward in fact it usually gets kicked
off the sofa
so let's talk a little bit about
markov's decision process
markov's decision process
is a reinforcement learning policy used
to map a current state to an action
where the agent continuously interacts
with the environment to produce new
Solutions and receive rewards and you'll
see here's all of our different uh
vocabulary we just went over we have a
reward our state our agent our
environment interaction and so even
though the environment kind of contains
everything
that you really when you're actually
writing the program your environment's
going to put out a reward and state that
goes into the agent
the agent then looks at this state or it
looks at the reward usually um first and
it says okay I got rewarded for whatever
I just did or it didn't get rewarded and
then looks at the state then it comes
back and if you remember from policy the
policy comes in and then we have a
reward the policy is that part that's
connected at the bottom
and so it looks at that policy and it
says hey what's a good action that will
probably be similar to what I did or
sometimes they're completely random but
what's a good action that's going to
bring me a different reward
so taking the time to just understand
these different pieces as they go
is pretty important in most of the
models today and so a lot of them
actually have templates based on this
you can pull in and start using
um pretty straightforward as far as once
you start seeing how it works you can
see your environment sends it says hey
this is what the agent did this if
you're a character in a game this
happened and it shoots out a reward in a
state the agent looks at the reward
looks at the new state and then takes a
little guess and says I'm going to try
this action and then that action goes
back into the environment it affects the
environment the environment then changes
depending on what the action was and
then it has a new state and a new reward
that goes back to the agent so in the
diagram shown we need to find the
shortest path between node A and D each
path has a reward associated with it and
the path with a maximum reward is what
we want to choose
the node ABCD denote the nodes to travel
from node A to B is an action reward is
the cost of each path and policy is each
path taken
and you can see here a can go to B
or a can go to C right off the bat or it
can go right to D and if explored all
three of these you would find that a
going to D was a zero reward a going to
C and D would generate a different
reward
or you could go a c b d there's a lot of
options here and so when we start
looking at this diagram you start to
realize
that even though today's reinforced
learning models do really good at
finding an answer they end up trying
almost all the different directions you
see and so they take up a lot of work or
a lot of processing time for
reinforcement learning they're right now
in their infant stage and they're really
good at solving simple problems and
we'll take a look at one of those in
just a minute in a tic-tac-toe game but
you can see here uh once it's gone
through these and it's explored it's
going to find the ACD is the best reward
it gets a full 30 points for it if
you're unexpiting AIML engineer then
there is no better time to train
yourself in exciting field of machine
learning if you are looking for a course
that covers everything from fundamental
to Advanced Techniques like machine
learning algorithm development and
unsupervised learning look no further
than our Caltech in partnership with IBM
so why we're join now see it's a feeling
first find the course link from the
description box below machine learning
has improved our lives in a number of
wonderful ways today let's talk about
some of these I'm Rahul from Simply
learn and these are the top 10
applications of machine learning first
let's talk about virtual personal
assistance Google Assistant Alexa
Cortana and Siri now we've all used one
of these at least at some point in our
lives now these help improve our lives
in a great number of ways for example
you could tell them to call someone you
could tell them to play some music you
could tell them to even schedule an
appointment so how do these things
actually work first they record whatever
you're saying send it over to a server
which is usually in a cloud decoded with
the help of machine learning and neural
networks and then provide you with an
output so if you've ever noticed that
these systems don't work very well
without the internet that's because the
server couldn't be contacted next let's
talk about traffic predictions now say I
wanted to travel from Buckingham Palace
to large cricket ground the first thing
I'd probably do is to get on Google Maps
so search it
and let's put it here
so here we have the path you should take
to get to large cricket ground now here
the map is a combination of red yellow
and blue now the blue regions signify a
clear road that is you won't encounter
traffic there the yellow indicates that
they are slightly congested and red
means they're heavily congested so let's
look at the map a different version of
the same map and here as I told you
before red means heavily congested
yellow means slow moving and blue means
clear
so how exactly is Google able to tell
you that the traffic is clear slow
moving or heavily congested so this is
the help of machine learning and with
the help of two important measures first
is the average time that's taken on
specific days at specific times on that
route the second one is the real-time
location data of vehicles from Google
Maps and with the help of sensors some
of the other popular map services are
Bing Maps maps.me and here we go next up
we have social media personalization so
say I want to buy a drone and I'm on
Amazon and I want to buy a DJI mavic Pro
the thing is it's close to one lap so I
don't want to buy it right now but the
next time I'm on Facebook I'll see an
advertisement for the product next time
I'm on YouTube I'll see an advertisement
even on Instagram I'll see an
advertisement so here with the help of
machine learning Google has understood
that I'm interested in this particular
product hence it's targeting me with
these advertisements this is also with
the help of machine learning let's talk
about email spam filtering now this is a
spam that's in my inbox now how does
Gmail know what spam and what's not spam
so Gmail has an entire collection of
emails which have already been labeled
as spam or not spam so after analyzing
this data Gmail is able to find some
characteristics like the word lottery or
winner from then on any new email that
comes to your inbox goes through a few
spam filters to decide whether it's spam
or not now some of the popular spam
filters that Gmail uses is content
filters header filters General Blacklist
filters and so on next we have online
fraud detection now there are several
ways that online fraud can take place
for example there's identity theft where
they steal your identity fake accounts
where these accounts only last for how
long the transaction takes place and
stop existing after that and man in the
middle attacks where they steal your
money while the transaction is taking
place the feed forward neural network
helps determine whether a transaction is
genuine or fraudulent so what happens
with feed forward neural networks are
that the outputs are converted into hash
values and these values become the
inputs for the next round so for every
real transaction that takes place
there's a specific pattern a fraudulent
transaction would stand out because of
the significant changes that it would
cause with the hash values Stock Market
trading machine learning is used
extensively when it comes to Stock
Market trading now you have stock market
indices like nikai they use long
short-term memory neural networks now
these are used to classify process and
predict data when there are time lags
the one size and duration now this is
used to predict stock market trends
assistive Medical Technology now medical
technology has been innovated with the
help of machine learning diagnosing
diseases has been easier from which we
can create 3D models that can predict
where exactly there are lesions in the
brain it works just as well for brain
tumors and Ice chemic stock lesions they
can also be used in fetal Imaging and
cardiac analysis now some of the medical
fields that machine learning will help
assist in is disease identification
personalized treatment drug Discovery
clinical research and radiology and
finally we have automatic translation
now say you're in a foreign country and
you see Billboards and signs that you
don't understand that's where automatic
translation comes of help now how does
automatic translation actually work the
technology behind it is the same as the
sequence of sequence learning which is
the same thing that's used with chatbots
here the image recognition happens using
convolutional neural networks and the
text is identified using optical
character recognition furthermore the
sequence to sequence algorithm is also
used to translate the text from one
language to the other and knows the
algorithm is a step-by-step process to
approach a particular problem
there are numerous examples of algorithm
from figuring out sets of number to
finding Roots through maps to showing
data on a screen let's understand this
by using an example
every algorithm is built on inputs and
outputs Google search algorithm is no
different the input is the search field
and the output is the page of its result
that appears when you enter a particular
phrase or keyword also known as serp or
search engine result page
Google has a search algorithm so it can
sort results from various website and
provides the users the best result
when you start a search you will see the
search box we'll attempt to guess what
you are looking for in order to better
understand what the user is looking for
the algorithm is trying to gather as
many as suggestions from them as
possible
the results from the search field that
best matches the query will be ranked
the choose fit website will Rank and in
what position using more than 200
ranking variables now let's take an
example of coding program and see how
the algorithm works
here we will use a case of computer
program wherein we want to print the
multiplication table of any number let's
take two the algorithm start here and
then it assign a value to a variable the
variable I is having an initial value of
1. the system will read the number the
number in case is 2. now the system has
a condition a condition can now either
be true or false if the value of I
reaches 11 then the loop will end
otherwise value of I will multiply by
the number
the initial value of I is 1 so for the
first time the system output will be 2.
now value of I will be increased by 1.
according to the loop condition
the system will then move back and check
for the condition again the new value of
I is 2. which is still less than 11. the
system will again print 2 into Y which
is 2 into 2 on this screen
the new output result will be 4. the
system will keep following the same
procedure repeatedly until the value of
I becomes 11. once the value of I
becomes 11 then only the algorithm will
terminate after discussing how an
algorithm work let's move forward and
see some popular machine learning
algorithms
some popular machine learning algorithms
are
first one is linear regression algorithm
second one is logistic regression
algorithm
and the third one is decision tree
and the fourth one is support Vector
machine algorithm svm
and the fifth one is KNN K nearest
neighbor algorithms
and the sixth one is K means clustering
algorithms the and the seventh one is
random Forest algorithms and the last
but not the least algorithm is
proprietary algorithms let's go through
them in detail one by one
linear depression is one of the most
famous and straightforward machine
learning algorithms utilized for
predictive analysis linear regression
show the linear connection between the
dependent and the independent factors
the equation of line is y equals to MX
plus b here y stand for this response
variable or a dependent variable whereas
X is for the Predator variable or an
independent variable it attempts best to
fit line between the dependent and
independent variables and this best bit
line is known as line of regression or
regression line
let's take a real application example in
predicting consumer Behavior businesses
use the linear regression to forecast
things like how much a client is likely
to spend
things like targeted marketing and
product development May benefit from
this Walmart for instance use linear
regression to forecast which good would
be in high demand Across the Nation
moving forward let's see types of linear
regression
there are two types of linear regression
algorithm
the first one is simple linear
regression and the second one is
multiple linear regression
in simple linear regression if an
independent variable is utilized to
focus the worth of a mathematical
dependent variable then at that point
such a linear regression algorithm is
called Simple linear regression the
equation of line will be y equals to a0
plus a 1 X
and the second one is multiple linear
regression if the dependent variables
declines on the Y and the independent
variable on the X
then such a relationship is known as
negative linear relationship the line of
equation will be minus of a 0 plus a 1 X
moving forward let's see logistic linear
regression
logistic regression is the supervised
machine learning algorithm which
utilized to anticipate all the
categorical factors or discrete values
it could be very well used for the
grouping issues in machine learning and
the result of the logistic regression
can be either yes or no 0 or 1 men or
women and so on it gives the values
which lies between 0 and 1. for example
a credit card business is interested in
knowing whether the transaction amount
and the credit score have an impact on
the probability that a particular
transaction would be fraudulent the
business can use logistic reduction to
determine how these two Predator values
can relate the probability that a
transaction is fraudulent this response
variable in the model has two possible
outcomes
first one is the transaction is
shortland and the second one is the
transaction is not fraudulent
in logistic regression rather than
fitting a regression line we fit an S
form logistic capability which predicts
two greatest value 0 or 1. the logistic
regression equation can be calculated
from linear regression equation the
steps to get logistic regression
equations are the equation of straight
line can be written as y equals to b0
plus B1 X1 plus b 2 x 2 till b n x n in
logitative regression y can be between 0
and 1 only so for that let's divide the
above equation by 1 minus y then the
equation will be y upon 1 minus y that
is 0 for y 0 and Infinity for y equals
to 1 but range between minus infinity to
plus infinity then we have to take the
logarithm of equation and now it will
become log of Y upon 1 minus y equals to
b0 plus B1 X1 plus b 2 x 2 so on till B
and x n let's move forward and see types
of logistic regression
there are three types of logistic
regression that can be classified first
one is binomial in binomial logistic
regression there can be only two
possible types of dependent variables
like yes or no password man woman and
many more and the second one is
multinomial in multinomial logistic
regression there can be three or more
possible unordered ways of dependent
variable such as horse cow and sheep and
the last one is ordinal in ordinal
logistic regression there can be three
or more possible ordered ways of
dependent variable such as small medium
or large moving forward let's see
decision trees in detail
a decision tree is a tree structured
classifier that could be used for
classification and regression a decision
tree is a tree in which each non-leaf
node is assigned to an attribute
additionally each are contains one of
the available values for its parent node
which is associated with each Leaf node
that is the node from where the arc is
directed
let's see some decision tree terminology
first one is root that contains the
entire data set the next one is node a
test for the data of a certain attribute
and the third one is branch which
connects the node to internal node or
the internal node to Leaf node and the
fourth one is leaf node the terminal
node that predicts the outcome let's
move forward and see decision tree
algorithms the first one is select the
best attribute to use the current node
in the tree the second one is for each
possible values select the attributes
the third one is partition the examples
using the possible values of this
attribute and assign these disjoint
subset of the example to the appropriate
child node recursively generate each
child node until ideally all examples
for a node have the same label like
class moving forward let's understand
the decision tree for building a
decision tree Step One is Select an
attribute then split the data into its
children in a tree continue splitting
with available attributes
and keep splitting until late node are
pure like only one class remains a
maximum depth is
metric is achieved let's move forward
and see svm algorithm support Vector
machine algorithms
a support Vector machine is a well-known
supervised machine learning model it is
utilized for both information
classification and regression
it is regularly utilized for the
grouping issues we can involve it in
different life care system and we can
involve it in typically happy or sad
look Arrangements we can involve
retained filters if we make specific
Loops it would add the particular filter
according to the expression the scope of
articulation lies between happy and sad
support Vector machine helps them to
recognize and return characters use yd
like checks continue to reach a
significant part of the majority of
non-cash transaction and are frequently
written by the pupil
the current check processing system in
many developing nations involves a bank
employee to read and manually enter the
information in a check while also
verifying the data like signature and
date
a handwritten tax recognition system can
reduce expenses and labor hours because
a bank must handle several checks each
day
moving forward let's see the algorithm
of svm
the objective of support Vector machine
is to make the best line or Choice limit
that can isolate and dimensional space
into classes so we can undoubtedly put
the new data of interest in the right
category later on this batch decision
boundary is known as a hyperplate let's
move forward and see types of support
Vector machine support Vector machine
can be of two types first one is linear
svm second one is non-linear SPM let's
move forward and see linear sphere
linear svm is utilized for linearly
detachable information
which implies if a data set can be
ordered into two classes by utilizes a
straight line then such information are
named linearly separable information and
the classifier is utilized called linear
SPM classifier
moving forward let's see non-linear svm
non-linear svm is utilized for
non-dietly isolated information and that
implies in the event that a data set
can't be categorized by utilizing a
straight line
such information is named non-directed
information
and the classifier utilize is called a
non-linear svm classifier moving forward
let's see k n algorithm in detail
KNN is a supervised learning technique
Canon classifies new data into our
targeted classes depending on the
features of its neighboring points and
also we use for the regression problems
it is an instance based learning
algorithm and a bit lazy learning
algorithm k n calculation stores every
one of its accessible information and
orders another information Point based
on the likeliness this means that when
new data information appears it usually
tends to the successfully categorized
into a good suit classes using the k n
algorithm let's imagine we have an image
of animal that resembles a cow or Ox
however here we are not sure if it is a
cow or Ox as k n method is based on a
likeness Matrix it will identify the
properties of new data that are related
to the image of coworox and based on
those quality it will classify the data
is belonging to either cow or Ox group
moving forward let's see how does k n
work
the steps to implement k n algorithms
are step one decide on the neighbor's K
numbers Step 2 calculate the euclidean
distance between K Neighbors in step 2.
third one is based on the determined
nuclear distance select the K closest
neighbors step 4 is count the numbers of
data points in each category between
these scan networks step 5 assign the
fresh data points in the category where
the highest numbers count and then k n
model is ready
Let's see we need to add a new data
point to the vital category at first we
will decide on the numbers of neighbors
therefore we will pick k equals to 5.
then the euclidean distance between the
data points and then can be determined
the distance between two points known as
the euclidean distance can be determined
by under root of X2 minus X1 whole
square plus Y2 minus y 1 whole Square
then we determine the closest neighbors
by calculating the euclide distance
there are three closest Neighbors in
category a and two closest neighbor in
category B
this new data point Must Fall with
category a because as we can see its
three closest neighbors are also from
group a after understanding k n
algorithm let's move forward and see
k-means algorithms in detail
the K means is the cluster false under
that is announced supervised learning
algorithm it is used to address machine
learning clustering problems and utilize
to tackle the grouping issues in machine
learning
it permits us to Bunch the information
into various Gatherings it is a helpful
method for finding the classification of
groups in the unlabeled data set without
the requirement of any training this
k-means algorithm groups the data into
similar classes let's see some
application of k-means clustering let's
see some applications of k-means
clustering diagnostic system the medical
profession uses k-means cursing in
creating smarter medical decision
support system especially in the
treatment of liver alignments the second
one is search engines clustering forms a
backbone of search engine when a search
engine is performed the search result
need to be grouped and the search
engines very open use clustering to do
this moving forward let's see how
k-means algorithm works
his steps to implement k-means
algorithms are step one select the
number K to set the number of clusters
Step 2 select a random K points or
centroid step 3 assign each data point
the closest centroid data forms to
predefined K cluster step 4 determine
the variance enter new Gravity points
for each cluster step 5 repeat the third
step this means G allocating each data
point to the new closest centroid
cluster step 6 if I reassignment occurs
go to step 4 otherwise go to exit step 7
the model is ready to use
so now we have a clear understanding of
how k-means algorithm work let's move
forward to see the graphical
representation of k-means algorithm
consider that there are two variables M1
and M2 this is a scatter plot of these
two variables along the X and Y axis we
should accept the number of K of punches
that is k equals to 2 to recognize the
data set and to place them into various
groups it implies here we will attempt
to Bunch these data set into two unique
groups people really want to pick an
irregular key point or centroid to frame
this group
these centroids can be either the focus
of the data set or some of other points
thus here we are choosing under two
points as K points which are not the
piece of our data set
we will assign every data of interest in
this scatter plot to the nearest K point
or centroid
we will register it by applying some
math that we consider to find the
distance between two points that is
euclidean distance thus we will draw a
median between both the centroids from
the graph the left half of the line are
close points to the right and the K1
centroid green is near the orange
centroid we should variety them as green
and orange for Clear representation as
the need might arise to track down the
nearest group we will repeat the cycle
by picking another centroid
to pick the new centroid we will figure
out the center point of gravity of the
centroid and will track down new
centroids then we will reassign every
piece of information to highlight new
centroid for this we will repeat a
similar course of tracking down a middle
line the middle will be like as seen in
the picture one orange point is on the
left half of the line and two green
points are on the right thus these three
points will be appointed to the new
centroids as reassignment has occurred
we will again go to step 4 tracking down
new centroids or k points we will repeat
the cycle by tracking down the center
point of gravity of centroid so the new
centroids will be displayed as like this
we now have new centroid so once more
defined the middle boundary and reassign
the data of Interest by this graph there
are no unique pieces of information on
one of the other side of the line
implying our model is shaped by the
previous graph there are no unique
pieces of information on one or the
other line
implying our model is shaped as our
k-means model is ready and the two plus
groups will be displayed as like these
now we have a clear understanding of how
k-means clustering algorithm works now
let's move forward to understand random
Forest algorithm
random Forest is an adaptable simple to
utilize machine learning algorithm that
produces even without the hyper boundary
tuning and extraordinary outcome more
often that Norm it is likewise quite
possibly the most utilized algorithm
because of its effortness and variety
like its tend to be utilized for both
grouping and classification tasks
random Forest is a classifier that
contains various Choice trees on
different subsets of the given data set
and takes the normal to work on the
present exactness of the data set
instead of depending on the choice tree
the random Forest takes the forecast
from each tree and in light of the
larger part of boards of expectation it
predicts the final result now let's move
forward and see how does random Forest
work
we should see the random forest in order
since the arrangement is now and again
thought to be the structured block of
machine learning this is what a random
Forest would look like with two trees
the random Forest has a similar
hypermeter to their decision tree or a
paging classifier luckily there is a
compelling reason need to consolidate a
decision tree with a paging classifier
since you can undoubtedly utilize the
classifier classes of random forests
with random Forest you can likewise
manage tasks using the algorithm
regression random forests add extra
bitness to the model while tableving the
trees rather than looking for the man
element while parting a node
it looks to the best component among an
irregular subset of highlights these
outcomes in a wide variety often result
in a superior model
subsequently in a random Forest just a
random subset of the element is thought
about the algorithms you might make
trees more random by involving random
edges for each component instead of
looking for the most ideal limits like a
typical Choice tree does
let's move forward and see some
application of random Forest algorithms
random Forest is involved at work by
researchers in numerous Ventures
including banking stock exchanging
mitigation and many more it is utilized
to predict things which assist these
businesses with running productively
like client activity patient history and
safety in banking random Forest is used
to identify clients who are more likely
to pay back their debts on schedule
additionally it is utilized to forecast
who will make more frequent use of Bank
Services even fraud detection uses it
the robin node of algorithms indeed
random Forest is a tool used by stock
Trader to forecast future stock Behavior
retail businesses utilize it to make
product recommendation and forecast
client satisfaction random forage can be
used in healthcare industry to examine a
patient medical history and detect
disorders random Forest is a tool used
by pharmaceutical expert to determine
the idle mix of ingredients in treatment
or to forecast drug sensitivity by
seeing application of random Forest
algorithm let's move forward and see
some difference between decision trees
and random Forest
let's see the difference between random
forest and decision tree the first one
is
while building a random Forest the
number of rows is selected randomly in
decision trees it builds several
decision trees and find out the output
the second one is it combines two or
more decision trees together in decision
trees whereas the decision is a
collection of variables or data sets or
attributes the third one is it gives
accurate results whereas it gets less
accurate results the fourth one is by
using random Forest it reduces the
chance of overlifting whereas decision
trees it has the possibility of
overlifting the fifth one is random
Forest is more complicated to
interpreters whereas the decision tree
is simple it is easy to read and
understand
after seeing what is random Forest how
it works let's move forward to see a
primary algorithm in detail
the upper ID algorithm utilize standard
item sets to create affiliation rules
and is intended to chip away the
information basis containing exchanges
with the assistance of these application
rules it decide how firmly or feebly two
objects are associated
this algorithm utilize a breakfast
search and history to work out the items
Act association effectively
it is the iterative interaction for
finding out the successive item set from
the huge data set let's move forward and
see steps for a priority algorithms the
steps for a priority algorithms are Step
One is establish minimal support and
confidence for item set in the
transactional database the second one is
take all transaction supports with a
greater support value the minimum or
chosen support value in Step 2 the third
one is track down all the rules in these
subsets with confidence value greater
than the threshold value the fourth one
is arrange the rules to lower the lift
at last we will see some advantages and
disadvantages of a priority algorithm
the advantages of a priority algorithms
are easy to understand an algorithm and
the second one is the join and prune
steps of the algorithms can be easily
implemented on the large data set the
disadvantages of a priority algorithms
are their priority algorithms work
slowly as compared to the other
algorithm and the second one is the
priority algorithms times and space
complexity are o of 2D which is very low
compared to the other ones let's move
forward and see some Hands-On lab demo
so we will see a handsole lab demo on
linear regression as we know linear
regression is a way to find or to
predict the relationship between two
variables generally we use X and Y so
first we will open command prompt to
write command to open Jupiter notebook
so we will write Jupiter
notebook
and then press enter
so this is the landing page of jupyter
notebook and select open
new python file
so this is how jupyter notebook UI looks
like so at first we will import some
major libraries of python which will
help us in mathematical functioning so
the first one is numpy
Nampa is a python Library used for
working with arrays it also has
functions for working in domains like
linear algebra and matrices it is in
open source project and you can use it
freely
numpy stand for numerical python
so we will write like import
numpy
S and B here NP is used for denoting
numpy so we will import the next
libraries pandas pandas is a software
Library written for Python programming
for like data manipulation and Analysis
in particular it offers data structures
and operation for manipulating numerical
labels and Time series
so we will write import pandas
as PD
here PD is used for denoting pandas so
our next library is matplotlib matlode
is a graph plotting library in Python
that serves as a visualization utility
is open source so we can use it freely
and it is most written in Python a few
segments in C and JavaScript for
platform compatibility
for importing Metro lip you have to
write import matplotlib
dot lip
dot Pi plot
as PLT
so after importing the libraries we will
move ahead and import data set so for
importing data set we have to write data
set
equals to PD Dot rate underscore CSV and
here we have to give the path of the
file
data set
dot CSV
here PD is for pandas library and read
is used for reading the data set from
the machine and CSV CSC is used for the
type of file which you want to read
so after reading let's see our data so
we will write data set
dot head
and press enter
here head is used for retrieving the
first five lines from the data so our
data is coming properly so moving ahead
now let's first Define X and Y axis for
X we have to write x equals to data set
Dot ilock
bracket colon comma colon again minus 1
dot values for exercise and for y axis
we have to write y equals to
data set
dot I log
bracket colon comma 1
dot values
so if we will use dataset.ilock for -1
values for x axis it will select till
the second last column of the data frame
instead of the last column
and I know this is as the second last
column value and the last column value
for the row is different
whereas if you will use for the y-axis
values return a series Vector a vector
does not have a column size
so moving ahead let's see the value for
x and y axis first we will see the value
for x like just you have to write X and
press enter so
these are the arrays value for x and for
y axis you have to type Y and then enter
so these are the arrays value for y axis
so after this now let's split the data
set into training and testing separating
data into training and testing set is an
important part for evaluating data
mining models typically when you
separate a data set into a training set
and a testing set most of the data set
is used for training and a similar
portion of data set is used for testing
so we will split it into 70 and 30 ratio
so for splitting we need to import some
more libraries for this process so we
will write from
SK learn
dot model
underscore selection
selection
import
train
underscore test
underscore split
is most useful and robust library for
machine learning in Python it provides a
selection of efficient tools for machine
learning and statical modeling including
classification regression and clustering
so after importing left side code for
the splitting data so we will write here
X underscore
train
comma X underscore test
comma y underscore train
comma y underscore test
equals to
Attain
underscore test
underscore split
in Brackets we have to write X comma y
comma
test
underscores
size
equals to
0.3
comma
random
underscore state
equals to zero
random basically
which random State equals to 0 we get
the same Trend and test sets across
different execution so after this let's
see the values together
so we will write X underscore train
comma X underscore test
comma y underscore
chain
comma y underscore
test
so
these are the values of array X and
array y together moving ahead now let's
work with regression first we need to
import the library for regression so we
will use from
SQ learn
dot linear
underscore
model
in both
linear
the regression
we already discussed sqlan is used for
machine learning and linear regression
is a major part of machine learning so
after this let's make a function for
regression as RSG for easy use so we
will write Reg
equals to
linear
regression
and brackets first we try to train and
then test and compare so we will write
here Reg
Dot
bit
X underscore brain
comma y underscore
change
now let's predict values but prediction
values are always different so we will
predict values for y first so we will
write y
underscore predict
equals to red
dot predict
sorry predict
for X underscore test
so like y underscore
reading
and enter
this is when the predict functions come
into the picture python predict function
enables us to predict the labels of the
data values on the basis of training
model the predict function except only a
single argument which is X underscore
test here usually the data to be tested
so when you will check the value for y
test you will see the different values
like
y
underscore
test
so you can see there are totally
different values from x axis now let's
display on graph for training data set
so we have to write PLP dot scatter
X underscore drain
comma y underscore train
comma color
equals to
you can choose by yourself I will choose
red
then PLT
Dot Plot
X underscore train
comma integration point for predicting
values
or X underscore
train
comma color
equals to
blue
so this color is for the regression line
and the PLT dot title
Indiana
regression
salary
versus experience
and I will prefer this size would be
edit
let's set the
labels for x and y axis so
X label
pair of employee
common size will be
available
then for y-axis we will write
here
right salary is
and I will prefer this same size 15.
let's show the plot by writing PLT dot
show
then
hopefully we are fortunate everything is
going to be fine
perfect so you can see we have linear
regression line fitting through our data
set so this is how the linear regression
work for training data set let's see how
it will work for testing data set
now let's predict what test data set you
can copy for the same and like you can
change it
so we can copy here
so we can paste here
so we have to just change here then to
test data set
here then and again
test here
test here
everything everything we will
reduce some size to
16 let's see then it will use 12 and it
will depend so
perfect so you can see we have linear
regression line putting through our data
set so this is how the linear regression
works for testing data set if you are an
expiring AIML engineer then there is no
better time to train yourself in
exciting field of machine learning if
you are looking for a course that covers
everything from fundamentals to Advanced
Techniques like machine learning
algorithm development and unsupervised
learning look no further than a Caltech
in partnership with IBM so why we're
join now seats are failing first find
the course link from the description box
below so what is the site kit learn it's
simple and efficient tool for data
mining and data analysis it's built on
numpy scipy and matplot Library so it
interfaces very well with these other
modules and it's an open source
commercially usable BSD license BSD
originally stood for Berkeley software
distribution license but it means it's
open source with very few restrictions
as far as what you can do with it
another reason to really like the site
kit learn setup so you don't have to pay
for it as a commercial license versus
many other copyrighted platforms out
there what we can achieve using the
scikit learn we use class the two main
things are classification and regression
models classification identifying which
category an object belongs to for one
application very commonly used is Spam
detection so is it a Spam or is it not a
Spam yes no in banking it might be is
this a good loan bad loan today we'll be
looking at wine is it going to be a good
wine or a bad wine and regression is
predicting an attribute associated with
an object one exam is stock prices
prediction what is going to be the next
value if the stock today sold for 23
dollars and five cents a share what do
you think it's going to sell for
tomorrow and the next day and the next
day so that'd be a regression model same
thing with weather weather forecasting
any of these are regression models we're
looking at one specific prediction I
want a tribute today we will be doing
classification like I said we're gonna
be looking at whether a wine is good or
bad but certainly the regression model
which is in many cases more useful
because you're looking for an actual
value is also a little harder to follow
sometimes so classification is a really
good place to start we can also do
clustering and model selection
clustering is taking an automatic
grouping of similar objects into sets
customer segmentation is an example so
we have these customers like this
they'll probably also like this or if
you like this particular kind of
features on your objects maybe you like
these other objects so it's a referral
is a good one especially on Amazon.com
or any of your shopping networks model
selection comparing validating and
choosing parameters and models now this
is actually a little bit deeper as far
as a site kit learned we're looking at
different models for predicting the
right course or the best course or
what's the best solution today like I
said we're looking at Wines it's going
to be how do you get the best wine out
of this so we can compare different
models and we'll look a little bit at
that and improve the model's accuracy
via different parameters and fine-tuning
now this is only part one so we're not
going to do too much tuning on the
models we're looking at but I'll Point
them out as we go two other features
dimensionality reduction and
pre-processing dimensionality reduction
is we're reducing the number of random
variables to consider this increases the
model efficiency we won't touch that in
today's tutorial but be aware if you
have you know thousands of columns of
data coming in thousands of features
some of those are going to be duplicated
or some of them you can combine to form
a new column and by reducing all those
different features into a smaller amount
you can have a you can increase the
efficiency of your model it can process
faster and in some cases you'll be less
biased because if you're weighing it on
the same feature over and over again
it's going to be biased to that feature
and pre-processing these are both
pre-processing but pre-processing is
feature extraction and normalization so
we're going to be transforming input
data such as text for use with machine
learning algorithms we'll be doing a
simple scaling in this one for our
pre-processing and I'll point that out
when we get to that and we can discuss
pre-processing at that point with that
let's go ahead and roll up our sleeves
and dive in and see what we got here now
I like to use the Jupiter notebook and I
use it out of the Anaconda Navigator so
if you install the Anaconda Navigator by
default it will come with the jupyter
notebook or you can install the jupyter
notebook by itself this code will work
in any of your python setups I believe
I'm running an environment of 3.7 setup
on there I'd have to go in here and
environments and look it up for the
python setup but it's one of the three
x's and we go ahead and launch this and
this will open it up in a web browser so
it's kind of nice it keeps everything
separate and in this and conda you can
actually have different environments
different versions of python different
modules installed in each environment so
it's a very powerful tool if you're
doing a lot of development and jupyter
notebook is just a wonderful visual
display certainly you can use I know
spider is another one which is installed
with the Anaconda I actually use a
simple notepad plus plus when I'm doing
some of my python script any of your
Ides will work fine jupyter notebook is
iron python because it's designed for
the interface but it's good to be aware
of these different tools and when I
launched the Jupiter notebook I'll open
up like I said a web page in here and
we'll go over here to New and create a
new python setup like I said I believe
this is python37 but any of the three
this the site kit learn works with any
of the three X's there's even two seven
versions so it's been around a long time
so it's very big on the development side
and then the guys in the back guys and
gals developed they went ahead and put
this together for me and let's go ahead
and import our different packages now if
you've been reading some of our other
tutorials you'll recognize pandas as PD
pandas library is pretty widely used
it's a data frame setup so it's just
like columns in rows and a spreadsheet
with a lot of different features for
looking stuff up the Seaborn sits on top
of matplot libraries this is for a
graphing and we'll see that how quick it
is to throw a graph out there to view in
the Jupiter notebook for demos and
showing people what's going on and then
we're going to use the random Forest the
SVC or support vector classifier and
also the neural network so we're going
to look at this we're actually going to
go through and look at three different
classifiers that are most common some of
the most common classifiers and let's
show how those work in the scikit learn
setup and how they're different and then
if you're going to do your setup on here
you'll want to go ahead and import some
metrics so the sklearn.metrix on here
and we're going to use the confusion
metrics and the classification report
out of that and then we're going to use
from the sklearn pre-processing the
standard scalar and label encoder
standards scalar is probably the most
commonly used pre-processing there's a
lot of different pre-processing packages
in the sklearn and then model selection
for splitting our data up it's one of
the many ways we can split data into
different sections and the last line
here is our percentage matplot library
in line some of the Seaborn and matplot
Library will go ahead and display
perfectly in line without this and some
won't it's good to always include this
when you're in the Jupiter notebook this
is Jupiter notebook so if you you're an
IDE when you run this it will actually
open up a new window and display the
graphics that way so you only need this
if you're running it in a editor like
this one with the specifically jupyter
notebook I'm not even familiar with
other editors that are like this but I'm
sure they're out there I'm sure there's
a Firefox version or something Jupiter
notebook just happens to be the most
widely used out there and we can go
ahead and hit the Run button and this
now has saved all this underneath the
packages so my packages are now all
loaded I've run them whether you read it
on top we run it to the left and all the
packages are up there so we now have
them all available to us for our project
we're working on and I'm just going to
make a little side note on that when
you're playing with these and you delete
something out and add something in even
if I went back and deleted this cell and
just hit the scissors up here these are
still loaded in this kernel so until I
go under kernel and restart or restart
and clear or restart and run all I'll
still have access to pandas important to
know because I've done that before I've
loaded up maybe not a module here but
I've loaded up my own code and then
changed my mind and wondering why does
it keep putting out the wrong output and
then I realize it's still loaded in the
kernel and you have to restart the
kernel just a quick side note for
working with a Jupiter notebook and one
of the troubleshooting things that comes
up and we're going to go and load up our
data set we're using the pandas so if
you haven't yet go look at our pandas
tutorial a simple read the CSV with the
separation on here let me go ahead and
run that and that's now loaded into the
variable wine and let's take a quick
look at the actual file I always like to
look at the actual data I'm working with
in this case we have wine quality Dash
red I'll just open that up I have it in
my open Office setup separated by
semicolons that's important to notice
and we open that up you'll see we have
go all the way down here it looks like
1600 lines of data minus the first one
so 15
1599 lines and we have a number of
features going across the last one is
quality and right off the bat we see the
quality is uh has different numbers in
it five six seven it's not really I'm
not sure how how high of a level it goes
but I don't see anything over a seven so
it's kind of five through seven is what
I see here five six and seven four five
six and seven looking to see if there's
any other values in there looking
through the demo to begin with I didn't
realize the setup on this so you can see
there's a different quality values in
there alcohol sulfates pH density total
sulfur dioxide and so on those are all
the features we're going to be looking
at and since this is a pandas we'll just
do wine head and that prints the first
five rolls rows of data that's of course
a panda's command and we can see that
looks very similar to what we're looking
at before we have everything across here
it's automatically assigned an index on
the left that's what pandas does if you
don't give it an index and for the
column names it has assigned the first
row so we have our first row of data
pulled off the our comma separated
variable file in this case semicolon
separated and it shows the different
features going across and we have what
one two three four five six seven eight
nine ten eleven features 12 including
quality but that's the one we want to
work on and understand and then because
we're in Panda's data frame we can also
do wine dot info and let's go ahead and
run that this tells us a lot about our
variables we're working with you'll see
here that there is
1599 that's what I said from the
spreadsheet so that looks correct
non-null float 64. this is a very
important information especially the
non-nullness there's no null values in
here that can really trip us up in
pre-processing and there's a number of
ways to process non-null values one is
just to delete that data out of there so
if you have enough data in there you
might just delete your non-null values I
know another one is to fill that
information in with like the average or
the most common values or other such
means but we're not going to have to
worry about that but we'll look at
another way because we can also do wine
is null and sum it up and this will give
us a similar it won't tell us that these
are float values but it will give us a
summation oops there we go let me run
the
run they'll give us a summation on here
how many null values in each one so if
you wanted to you know from here you
would be able to say okay this is a null
value but she doesn't tell you how many
are null values this one would clearly
tell you that you have maybe five null
values here two null values here and you
might just if he had only seven null
values and all that different data you'd
probably just delete them out where if
ninety percent of the data was null
values you might rethink you there are
different data collection setup or find
a different way to deal with the null
values we'll talk about that just a
little bit in the models too because the
models themselves have some built-in
features especially the forest model
which we're going to look at at this
point we need to make a choice and to
keep it simple we're going to do a
little pre-processing of the data and
we're going to create some bins and bins
we're going to do is 2 comma 6.5 comma
8. what this means is that we're going
to take those values if you remember up
here let me just scroll back up here we
add our quality the quality comes out
between two and eight basically or one
and eight we have five five five six you
can see just in the just in the first
five lines of variation in quality we're
going to separate that into just two
bins of quality and so we've decided to
create two bins and we have bad and good
it's going to be the labels on those two
bins we have a spread of 6.5 and an
exact index of eight the exact index is
because we're doing zero to eight on
there the 6.5 we can change we could
actually make this smaller or greater
but we're only looking for the really
good wind we're not looking for the zero
one two three four five six we're
looking for wines with seven or eight on
them so high quality you know like this
is what I want to put on my dinner table
at night
I'm going to taste the good wine not the
semi-good wine or mediocre wine and then
this is a panda so PD remember stands
for pandas Panda's cut means we're
cutting out the wine quality and we're
replacing it and then we have our bins
equals bins that's the command bins is
the actual command and then our variable
bends to comma 6.58 so two different
bins and our labels bad and good and we
can also do let me just do it this way
wine quality since that's what we're
working on and let's look at unique
another pandas command and we'll run
this and I get this lovely error why did
I get an error well because I replaced
wine quality and I did this cut here
which changes things on here I'm sorry I
literally altered one of the variables
is saved in the memory so we'll go up
here to the kernel restart and run all
it starts it from the very beginning and
we can see here that that fixes the
error because I'm not cutting something
that's already been cut we have our wine
quality unique and the wine quality
unique is a bad or good so we have two
qualities objects bad is less than good
meaning add is going to be zero and good
is going to be one and to make that
happen we need to actually encode it so
we'll use the label quality equals label
encoder and the label encoder let me
just go back there so this is part of
sklearn that was one of the things we
imported was a label encoder you can see
that right here from the sklearn dot
processing import standard scalar which
we're going to use in a minute and label
encoder and that's what tells it to use
that equals 0 and good equals one and
we'll go ahead and run that and then we
need to apply it to the data and when we
do that we take our wine quality that we
had before and we're going to set that
equal to label quality which is our
encoder and let's look at this line
right here we have dot fit transform and
you'll see this in the pre-processing
these are the most common used is fit
transform and fit transform because
they're so often that you're also
transferring the data when you fit it
they just combine them into one command
and we're just going to take the wine
quality feed it back into there and put
that back in our wine quality setup and
run that and now when we do the wine and
the head of the first five values and we
go ahead and run this you can see right
here underneath quality zero zero zero I
have to go down a little further to look
at the better wines
let's see if we have some that are ones
yeah there we go there's some ones down
here so when you look at 10 of them you
can see all the way down to zero or one
that's our quality and again we're
looking at high quality we're looking at
the seven and the eights or six point
five and up and let's go ahead and grab
our or was it here we go wine quality
and let's take another look at what else
more information about the wine quality
itself and we can do a simple pandas
thing value counts hopefully I type that
in there correctly and we can see that
we only have 217 of our wines which are
going to be the higher quality so 217
and the rest of them fall into the bad
bucket the zero which is uh 1382 so
again we're just looking for the top
percentage of these the top what is that
it's probably about a little a little
under 20 percent on there so we're
looking for our top wines our seven and
eights and let's use our let's plot this
on a graph so we take a look at this and
the SNS if you remember correctly that
is let me just back at the top that's
our Seaborn Seaborn sits on top of
matplot Library it has a lot of added
features plus all the features of the
matplot library and also makes it quick
and easy to put out a graph we'll do a
simple bar graph and they actually call
it count plot and then we want to just
do count plot the wine quality so let's
put our wine quality in there and let's
go ahead and run this and see what that
looks like and nice in line remember
this is why we did the inline so make
sure it appears in here and you can see
the blue space or the first space
represents low quality wine and our
second bar is a high quality line and
you can see that we're just looking at
the top quality wine here Mimosa wine we
want to just give it away to the
neighbors now maybe if you don't like
your neighbors maybe give them the good
quality wine and I don't know what you
do with the bed quality wine I guess use
it for cooking there we go but you can
see here it forms a nice little graph
for us with the Seaborn on there and you
can see our setup on that so now we've
looked we've done some pre-processing
we've described our data a little bit we
have a picture of how much of the wine
what we expect it to be high quality low
quality checked out the fact that
there's none we don't have any null
values to contend with or any odd values
some of the other things you sometimes
look at these is if you have like some
values that are just way off the chart
so the measurement might be off or
miscalibrated equipment fear in the
scientific field so the next step we
want to go ahead and do is we want to go
ahead and separate our data set or
reformat our data set and we usually use
capital x and that denotes the features
we're working with and we usually use a
lowercase y that denotes what in this
case quality what we're looking for and
we can take this we can go wine it's
going to be our full thing of wine
dropping what are we dropping we're
dropping the quality so these are all
the features minus quality I'll make
sure we have our axes equals one if you
left it out it would still come out
correctly just because of the way it
processes on the defaults and then our y
if we're going to remove quality for our
X that's just going to be 1 and it is
just the quality that we're looking at
for y so we put that in there and we'll
go ahead and run this so now we've
separated the features that we want to
use to predict the quality of the wine
and the quality itself the next step is
if you're going to create a data set in
a model we got to know how good our
model is so we're going to split the
data train and test splitting data and
this is one of the packages we imported
from sklearn and the actual package was
train test split and we're going to do X
Y test size 0.2 random State 42 and this
returns four variables and most common
you'll see is capital x train so we're
going to train our set with capital X
test that's the data we're going to keep
on the side to test it with why train
why remember stands for the quality or
the answer we're looking for so when we
train it we're going to use x train and
Y train and then why test to see how
good our X test does and the train test
split let me just go back up to the top
that was part of the sklearn model
selection import train test split there
is a lot of ways to split data up this
is when you're first starting you do
your first model you probably start with
the basics on here you have one test for
training one for test our test size is
0.2 or 20 percent and random State just
means we just start with a it's like a
random seed number so it's not too
important back there we're randomly
selecting which ones we're going to use
since this is the most common way this
is what we're going to use today there
is and it's not even an sklearn package
yet so someone's still putting it in
there one of the new things they do is
they split the data into thirds and then
they'll run the model on each of they
combine each of those thirds into
two-thirds for training and one for
testing so you actually go through all
the data and you come up with three
different test results from it which is
pretty cool that's a pretty cool way of
doing it you could actually do that with
this by just splitting this into thirds
and then or you have a test site one
test set third and then split the
training set also into thirds and also
do that and get three different data
sets this works fine for most projects
especially when you're starting out it
works great so we have our X train our X
test our y train and our y test and then
we need to go ahead and do the scalar
and let's talk about this because this
is really important some models do not
need to have scaling going on most
models do and so we create our scalar
variable we'll call it SC standard
scalar and if you remember correctly we
imported that here wrong with the label
encoder the standard scalar setup so
there's our scalar and this is going to
convert the values instead of having
some values that go from zero if you
remember up here we had some values are
54 60 40 59 102 so our total sulfur
dioxide would have these huge values
coming into our model and some models
would look at that and they'd become
very biased to sulfur dioxide it'd have
the hugest impact and then a value that
had
0.076.098 our chlorides would have very
little impact because it's such a small
number so when we take the scalar we
kind of level the playing field and
depending on our scalar it sets it up
between 0 and 1 a lot of times is what
it does let's go ahead and take a look
at that and we'll go ahead and start
with our X train and our X train equals
SC fit transform we talked about that
earlier that's an sklearn setup it's
going to both fit and transform our X
train into our X train variable and if
we have an X train we also need to do
that to our test and this is important
because you need to note that you don't
want to refit the data we want to use
the same fit we used on the training as
on the testing otherwise you get
different results and so we'll do just
oops not fit
transform we're only going to transform
the test side of the data so here's our
X test that we want to transform and
let's go ahead and run that and just so
we have an idea let's go ahead and take
and just print out our X train oh let's
do first 10 variables very similar to
the way you do with the head on a data
frame you can see here our variables are
now much more uniform and they've scaled
them to the same scale so they're
between certain numbers and with the
basic scalar you can fine tune it I just
let it do its defaults on this and
that's fine for what we're doing in most
cases you don't really need to mess with
it too much it does look like it goes
between like minus probably minus two to
two or something like that that's just
looking at the train variable I'm going
to cut that one out of there so before
we actually build the models and start
discussing the SK learn models we're
going to use we covered a lot of ground
here most of when you're working with
these models you put a lot of work into
pre prepping the data so we look at the
data notice that it's separated loaded
it up we went in there we found out
there's no null values that's hard to
say no no values we have uh there's none
there's none nobody I can't say it
and of course we sum it up if you had a
lot of null values this would be really
important coming in here so is there a
null summary we looked at pre-processing
the data as far as the quality and we're
looking at the bins so this would be
something you might start playing with
maybe you don't want super fine wine you
don't want the seven and eights maybe
you want to split this differently so
certainly you can play with the bins and
get different values and make the bins
smaller or lean more towards the lower
quality so you then have like medium to
high quality and we went ahead and gave
it the labels again this is all pandas
we're doing in here setting up with
unique labels and group names bad good
bad is less than good that can be so
important you don't know how many times
people go through these models and they
have them reversed or something and then
they go back and they're like why is
this data not looking correct so it's
important to remember what you're doing
up here and double check it and we used
our label encoder so that was to set
that up is equality zero one good in
this case we have bad good zero one and
we just double check that to make sure
that's what came up in the quality there
and then we threw it into a graph
because people like to see graphs I
don't know about you but you start
looking at all these numbers and all
this text and you get down here and you
say oh yes you know here this is how
much of the wine we're going to label as
subpar not good this is how much we're
going to label as good and then we got
down here to finally separating out our
data so it's ready to go into the models
and the models take X and A Y in this
case X is all of our features minus the
one we're looking for and then Y is the
features we're looking for so in this
case we dropped quality and in the Y
case we added quality and then because
we need to have a training set and a
test set so we can see how good our
models do we went ahead and split the
models up X train X test y train y test
and that's using the train test split
which is part of the SK learn package
and we did as far as our testing size
0.2 or 20 percent the default is 25 so
if you leave that out it'll default
setup and we did a random State equals
42. if you leave that out it'll use a
random State I believe it's default one
I'd have to look that back up and then
finally we scaled the data this is so
important to scale the data going back
up to here if you have something that's
coming out as a hundred it's going to
really outweigh something that's 0.071
that's not in all the models different
models handle it differently and as we
look at the different models I'll talk a
little bit about that we're going to
only look at three models today three of
the top models used for this and see how
they compare and how the numbers come
out between them so we're going to look
at three different setups change my cell
here to mark down there we go and we're
going to start with the random Forest
classifier so the three steps we're
looking at is the random Forest
classifier support vector classifier and
then a neural network now we start with
the random Forest classifier because it
has the least amount of Parts moving
parts to fine tune and let's go ahead
and put this in here so we're going to
call it RFC for random force classifier
and if you remember we imported that so
let me go back up here to the top real
quick and we did an import of the random
fourth classifier from SK learn Ensemble
and then we'll all we also let me just
point this out here's our svm or we
inputted our support Vector classifier
so svm is support Vector model support
vector classifier and then we also have
our neural network and we're going to
from there the multi-layered precipitron
classifier kind of a mouthful for the P
precipitron don't worry too much about
that name it's just it's a neural
Network there's a lot of different
options on there and setups which is
where they came up with the precipitron
but so we have our three different
models we're going to go through one
here and then we're going to weigh them
here's our metric so we're going to use
a confusion metrics also from the SK
learn package to see how good our model
does with our split so let's go back
down there and take a look at that and
we have our RFS equals random forest
classifier and we have n estimators
equals 200. this is the only value you
play with with a random Force classifier
how many forests do you need or how many
trees in the forest and so how many
models are in here that makes it pretty
good as a startup model because you're
only playing with one number and it's
pretty clear what it is and you can
lower this number or raise it usually
start up with a higher number and then
bring it down to see if it keeps the
same value so you have less you know the
smaller the model the better the fit and
it's easier to send out to somebody else
if you're going to distribute it now the
random 4 is classified fire everything I
read says it's used for kind of a
medium-sized data set so you can run it
in on Big Data you can run it on smaller
data obviously but tends to work best in
the mid-range and we'll go ahead and
take our RFC and I just copied this from
the other side dot fit X train comma y
train so we're sending it our features
and then the quality in the Y train what
we want to predict in there and we just
do a simple fit now remember this is SK
learn so everything is fit or transform
another one is predict which we'll do in
just a second here if I can do that now
predict our FC equals and it's our RFC
model predict and what are we predicting
on well we trained it with our train
values so now we need our test our X
test so this has done it this is going
to do this is the three lanes of code we
need to create our random Forest
variable fit our training data to it so
we're programming it to fit in this case
it's got 200 different trees it's going
to build and then we're going to predict
on here let me go ahead and just run
that and we can actually do something
like oh let's do predict
RFC just real quick we'll look at the
first 20 variables of it let's go ahead
and run that and in our first 20
variables we have three ones that make
the cut and the other 17 don't so the
other 17 are bad quality and three of
them are good quality in our predicted
values and if you can remember correctly
we'll go ahead and take this out of here
this is based on our test so these are
the first 20 values in our test and this
has as you can see all the different
features listed in there and they've
been scaled so when you look at these
they're a little bit confusing to look
at and hard to read but we have there's
a minus 01 so this is 0.36 minus so one
so 0.164 minus 0.09 or no it's still
minus one so minus 0.9 all between 0 and
1 on here I think I was confused earlier
and I said 0 between 2 negative two but
it's between minus one and one which is
what it should be in the scale and we'll
go ahead and just cut that out of there
run this we have our setup button here
so now that we've run the prediction and
we have predicted values well one you
could publish them but what do we do
with them well we want to do with them
is we want to see how well our model
model performed that's the whole reason
for splitting it between a training and
testing model and for that remember we
imported the classification report
that was again from the SK learn there's
our confusion Matrix and classification
report and the classification report
actually sits on the confusion Matrix so
it uses that information and our
classification report we want to know
how good our y test that's the actual
values versus our predicted RFC so we'll
go ahead and print this report out and
let's take a look and we can see here we
have a Precision out of the zero we had
about 0.92 that were labeled as a bad
that were actually bad and out of
precision for the quality wines we're
running about 78 percent so you kind of
give us an overall 90 percent and you
can see our F1 score our support setup
on there our recall you could also do
the confusion Matrix on here which gives
you a little bit more information but
for this this is going to be good enough
for right now we're just going to look
at how good this model was because we
want to compare the random force
classifier with the other two models and
you know what let's go ahead and put in
the confusion Matrix just so you can see
see that on there with Y test and
prediction RFC so in the confusion
Matrix we can see here that we had
266 correct and seven wrong these are
the missed labels for bad wine and we
had a lot of mislabels for good wine so
our quality labels aren't that good
we're good at predicting bad wine not so
good at predicting whether it's a good
quality wine important to note on there
so that is our basic random forest
classifier and let me go ahead of cell
change cell type to mark down and run
that so we have a nice label let's look
at our svm classifier our support Vector
model and this should look familiar we
have our clf we're going to create
what's it we'll call it just like we
call this an RFC and then we'll have our
clf DOT fit and this should be identical
to up above X
train comma y train and just like we did
before let's go ahead and do the
prediction and here is our clf predict
and it's going to equal the clf dot
predict and we want to go ahead and use
x underscore test and right about now
you can realize that you can create
these different models and actually just
create a loop to go through your
different models and put the data in and
that's how they designed it they
designed it to have that ability let's
go ahead and run this and then let's go
ahead and do our classification report
and I'm just going to copy this right
off of here
they say you shouldn't copy and paste
your code and the reason is is when you
go in here and edit it
you invariably will miss something we
only have two lines so I think I'm safe
to do it today and let's go ahead and
run this
and let's take a look how the svm
classifier came out so up here we had a
90 percent and down here we're running
about an 86 percent so it's not doing as
good now remember we randomly split the
data so if I run this a bunch of times
you'll see some changes down here so
these numbers this size of data if I
read it 100 times it would probably be
within plus or minus three or four on
here in fact if I ran this 100 times
you'd probably see these come out almost
the same as far as how well they do in
classification and then on the confusion
Matrix let's take a look at this one
this had 22 by 25 this one has 35 by 12.
so it's it's doing not quite as good
that shows up here 71 percent versus 78
percent and then if we're going to do a
svm classifier we also want to show you
one more and before I do that it kind of
tease you a little bit here before we
jump into neural networks the big save
all deep learning because everything
else must be shallow learning that's a
joke let's just talk a little bit about
the svm versus the random Forest
classifier the svm tends to work better
on smaller numbers it also works really
good on a lot of times you convert
things into numbers and bins and things
like that the random Forest tends to do
better with those at least that's my
brief experience with it or if you have
just a lot of raw data coming in the svm
is usually the fastest and easiest to
apply model on there so they each have
their own benefits you'll find though
again that when you run these like a
hundred times difference between these
two on a data set like this is going to
just go away there's Randomness involved
depending on which data we took and how
they classify them the big one is the
neural networks and this is what makes
the neural networks nice is they can do
they can look into huge amounts of data
so for a project like this you probably
don't need a neural network on this but
it's important to see how they work
differently and how they come up
differently so you can work with huge
amounts of data you can also many
respects they work really good with text
analysis especially if it's time
sensitive more and more you have an
order of text and they've just come out
with different ways of feeding that data
in where the series and the Order of the
words is really important same thing
with starting to predict in the stock
market if you have tons of data coming
in from different sources the neural
network can really process that in a
powerful way to pull up things that
aren't seen before when I say lots of
data coming in I'm not talking about
just the high lows that you can run an
svm on real easily I'm talking about the
data that comes in where you have maybe
you pulled off the Twitter feeds and
have word counts going on and you've
pulled off the the different news feeds
the business are looking at and the
different releases when they release the
different reports so you have all this
different data coming in and the neural
network does really good with that
pictures picture processing Now is
really moving heavily into the neural
network if you have a pixel 2 or pixel 3
phone put out by Google it has a neural
network for doing it's kind of goofy but
you can put Little Star Wars Androids
dancing around your pictures and things
things like that that's all done with
the neural network so it has a lot of
different uses but it's also requires a
lot of data and is a little heavy-handed
for something like this and this should
now look familiar because we've done it
twice before we have our multi-layered
precipitron classifier we'll call it an
mlpc and it's this is what we imported
mlpc classifier there's a lot of
settings in here the first one is the
hidden layers you have to have the
hidden layers in there we're going to do
three layers of 11 each so that's how
many nodes are in each layer as it comes
in and that was based on the fact we
have 11 features coming in then I went
ahead and just did three layers probably
get by with a lot less on this but yeah
I didn't want to sit and play with it
all afternoon again this is one of those
things you play with a lot because the
more hidden layers you have the more
resources you're using you can also run
into problems with over fitting with too
many layers and you also have to run
higher iterations the max iteration we
have is set to 500. the default's 200
because I use three layers of 11 each
which is by the way kind of a default I
use I realized that usually have about
three layers going down and the number
of features going across you'll see that
it's pretty common for the first
classifier when you're working in neural
networks but it also means you have to
do higher iterations so we up the
iterations to 500 so that means it's
going through the data 500 times to
program those different layers and
carefully adjust them and we do have a
full tutorials you can go look up on
neural networks and understand the
neural network settings a lot more and
of course we have you're looking over
here where we had our previous model
where we fit it same thing here mlpc fit
X train y train and then we're going to
create our prediction so let's do our
predict and mlpc and it's going to equal
the mlpc and we'll just take the same
thing here predict X test
put that down here dot predict a test
and if I run that we've now programmed
it we now have our prediction here same
as before and we'll go ahead and do the
copy print again always be careful with
the copy paste not because you always
run the chance of missing one of these
variables so if you're doing a lot of
coding you might want to skip that copy
and paste and just type it in and let's
go ahead and run this and see what that
looks like and we came up with an 88
percent we're going to compare that with
the 86 from our tree our svm classifier
and our 90 from the random forest
classifier and keep in mind random
Forest classifiers they do good on
mid-size data the svm on smaller amounts
of data although to be honest I don't
think that's initially the split between
the two and these things will actually
come together if you random number of
times and we can see down here the noun
of good wines mislabeled with the setup
on there it's on par with our random
Forest so it had 22 25 should be a
surprise it's identical it just didn't
do as good with the bad wines labeling
what's the bad one and what's not so
yeah because they had 266 and seven we
head down here 260 and 13. so mislabeled
a couple of the bad wines as good wines
so we've explored three of these basic
classifiers these are probably the three
most widely used right now I might even
throw in the random tree if we open up
their website we go under supervised
learning there's a linear model we
didn't do that almost most of the data
usually just start with a linear model
because it's going to process the
quickest I'm going to use the least
amount of resources but you can see they
have linear quadratic they have kernel
Ridge there's our support Vector of
stochastic gradient nearest neighbors
nearest neighbors is another common one
that's used a lot very similar to the
svm gaussian process cross decomposition
naive Bayes this is more of an
intellectual one that I don't see used a
lot but it's like the basis of a lot of
other things decision tree there's
another one that's used a lot Ensemble
methods not as much multi-class and
multi-label algorithms feature selection
neural networks that's the other one we
use down here and of course the forest
so you can see there's a in sklearn
there are so many different options and
they've just developed them over the
years we covered three of the most
commonly used ones in here and went over
a little bit over why they're different
neural network just because it's fun to
work in deep learning and not in Shallow
learning as I told you that doesn't mean
that the svm is actually shallow it does
a lot of it covers a lot of things and
same thing with the decision for the
random forest classifier and we notice
that there's a number of other different
classifier options in there these are
just the three most common ones and I'd
probably throw the nearest neighbor in
there and the decision tree which is
usually part of the decision for us
depending on what the back end you're
using and since as human beings if I was
in the shareholders office I wouldn't
want to leave them with a confusion
Matrix they need that information for
making decisions but we want to give
them just one particular score and so I
would go ahead and we have our klearn
metrics we're going to import the
accuracy score and I'm just going to do
this on the random 4 since that was our
best model and we have our CM accuracy
score and I forgot to print it if you
remember in jupyter Notebook we can just
do the last variable we leave out there
it'll print and so our CM accurate score
we get is 90 percent and that matches up
here we should already see that up here
in Precision so you can either quote
that but a lot of times people like to
see it highlighted at the very end this
is our Precision on this model and then
the final stage is we would like to use
this for future so let's go ahead and
take our wine if you remember correctly
we'll do one head of 10. we'll run that
remember our original data set we've
gone through so many steps then we're
going to go back to the original data
and we can see here we have our top 10
our top 10 on the list only two of them
make it as having high enough quality
wine for us to be interested in them and
then let's go ahead and create some data
here we'll call it X Nu equals and this
is important this data has to be we just
kind of randomly selected some some data
looks an awful lot like some of the
other numbers on here which is what it
should look like and so we have our X Nu
equals
7.3.58 and so on and then it is so
important this is where people forget
this step X Nu equals SC remember SC
that was our standard scalar variable we
created if we go right back up here
before we did anything else we created
an sc we fit it and we transformed it
and then we need to do what transform
the data we're going to feed in
so we're going to go back down here and
we're going to transform our X new and
then we're going to go ahead and use the
where are we at here we go our random
forest and if you remember all it is is
our RFC predict model right there let's
go ahead and just grab that down here
and so our y Nu equals here's our RFC
predict and we're going to do our X new
in and then it's kind of nice to know
what it actually puts out so according
to this it should print out what our
prediction is for this wine and oh it's
a bad wine okay so we didn't pick out a
good wine for our X new and that should
be expected most of wine if you remember
correctly only a small percentage of the
wine met our quality requirements so we
can look at this and say oh we'll have
to try another wine out which is fine by
me because I like to try out new wines
and I certainly have a collection of old
wine bottles and very few of them match
but you can see here we've gone through
the whole process just a quick rehash we
had our Imports we touched a lot on the
SK learn our random Forest our svm and
our MLP classifier so we had our support
vector classifier we had our random
forests and we have our neural network
three of the top used classifiers in the
sklearn system and we also have our
confusion Matrix and our classification
report which we used our standard scalar
for scaling it and our label encoder and
of course we needed to go ahead and
split our data up in our implot line
train and we explored the data in here
for null values we set up our quality
into bins we took a look at the data and
what we actually have and put a nice
little plot to show our quality what
we're looking at and then we went
through our three different models and
it's always interesting because you
spend so much time getting to these
models and then you kind of go through
the models and play with them until you
get the best training on there without
becoming biased that's always a
challenge is to not over train your data
to the point where you're training it to
fit the test value and finally we went
ahead and actually used it and applied
it to a new wine which unfortunately
didn't make the cut it's going to be the
one that we drink a glass out of and
save the rest for cooking of course
that's according to the random Forest on
there because we use the best model that
it came up with if you're an expiring
AIML engineer then there is no better
time to train yourself in exciting field
of machine learning if you are looking
for a course that covers everything from
fundamental to Advanced Techniques like
machine learning algorithm development
and unsupervised learning look no
further than a Caltech in partnership
with IBM so why we're join now see it's
a feeling first find the coast link from
the description box below Eve Bay's
classifier have you ever wondered how
your mail provider implements spam
filtering or how online news channels
perform news text classification or how
companies perform sentimental analysis
of Their audience on social media all of
this and more is done through a machine
learning algorithm called naive Bayes
classifier welcome to naive Bayes
tutorial my name is Richard kirschner
I'm with the simply learned team that's
www.simplylearn.com get certified get
ahead what's in it for you we'll start
with what is naive Bayes a basic
overview of how it works we'll get into
naive Bayes and machine learning where
it fits in with our other machine
learning tools why do we need naive
Bayes and understanding naive Bayes
classifier a much more in depth of how
the math works in the background finally
we'll get into the advantages of the
Nave Bayes classifier in the machine
learning setup and then we'll roll up
our sleeves and do my favorite part
we'll actually do some python coding and
do some text classification using the
naive Bays what is naive Bayes let's
start with a basic introduction to the
Bayes theorem named after Thomas Bayes
from the 1700s who first coined this in
the western literature naive Bayes
classifier works on the principle of
conditional probability as given by the
Bayes theorem before we move ahead let
us go through some of the simple
Concepts and the probability that we
will be using let us consider the
following example of tossing two coins
here we have two quarters and if we look
at all the different possibilities of
what they can come up as we get that
they can come up as head had heads they
come up his head tell tail head and
Telltale when doing the math on
probability we usually denote
probability as a p a capital P so the
probability of getting two heads equals
one-fourth you can see in our data set
we have two heads and this occurs once
out of the four possibilities and then
the probability of at least one tail
occurs three quarters of the time you'll
see in three of the twin tosses we have
tails in them and out of four that's
three fourths and then the probability
of the second coin being head given the
first coin is tell is one half and the
probability of getting two heads given
the first coin is a head is one half
we'll demonstrate that in just a minute
and show you how that math works now
when we're doing it with two coins it's
easy to see but when you have something
more complex you can see where these Pro
these formulas really come in and work
so the Bayes theorem gives us a
conditional probability of an event a
given another event B has occurred in
this case the first coin toss will be B
and the second coin toss a this could
could be confusing because we've
actually reversed the order of them and
go from B to a instead of a to B you'll
see this a lot when you work in
probabilities the reason is we're
looking for event a we want to know what
that is so we're going to label that a
since that's our focus and then given
another event B has occurred in the
Bayes theorem as you can see on the left
the probability of a occurring given B
has occurred equals the probability of B
occurring given a has occurred times the
probability of a over the probability of
B this simple formula can be moved
around just like any algebra formula and
we could do the probability of a after
of given B times probability of b equals
the probability of B given a times
probability of a you can easily move
that around and multiply it and divide
it out let us apply a Bayes theorem to
our example here we have our two
quarters and we'll notice that the first
two probabilities of getting two heads
and at least one tail we compute
directly off the data so you can easily
see that we have one example a HH out of
4 1 4 and we have three with tails in
them giving us three quarters or
three-fourths seventy-five percent the
second condition the second uh set three
and four we're gonna explore a little
bit more in detail now we stick to a
simple example with two coins because
you can easily understand the math the
probability of throwing a tail doesn't
matter what comes before it and the same
with the head so still going to be fifty
percent or one half but when that com
when that probability gets more
complicated let's say you have a D6 dice
or some other instance then this formula
really comes in handy but let's stick to
the simple example for now in this
sample space let a be the event that the
second coin is head and B be the event
that the first coin is tells again we
reversed it because we want to know what
the second event is going to be so we're
going to be focusing on a and we write
that out as a probability of a given B
and we know this from our formula that
that equals the probability of B given a
times the probability of a over the
probability of B and when we plug that
in we plug in the probability of the
first coin being tells given the second
coin is heads and the probability of the
second coin being heads given the first
coin being over the probability of the
first coin being Tails when we plug that
data in and we have the probability of
the first coin being Tails given the
second coin is heads times the
probability of the second coin being
heads over the probability of the first
coin being tails you can see it's a
simple formula to calculate we have one
half times one half over one-half or
one-half equals 0.5 or 1 4. so the Bayes
theorem basically calculates the
conditional probability of the
occurrence of an event based on prior
knowledge of conditions that might be
related to the event we will explore
this in detail when we take up an
example of online shopping further in
this tutorial understand standing naive
bees and machine learning like with any
of our other machine learning tools it's
important to understand where the naive
Bayes fits in the hierarchy so under the
machine learning we have supervised
learning and there is other things like
unsupervised learning there's also a
reward system This falls under the
supervised learning and then under the
supervisors learning there's
classification there's also a regression
but we're going to be in the
classification side and then under
classification is your naive Bayes let's
go ahead and glance into where is naive
Bayes used let's look at some of the use
scenarios for it as a classifier we use
it in face recognition is this Cindy or
is it not Cindy or whoever or it might
be used to identify parts of the face
that they then feed into another part of
the face recognition program this is the
eye this is the nose this is the mouth
weather prediction is it going to be
rainy or sunny medical recognition news
prediction it's also used in medical
diagnosis we might diagnose somebody as
either as high risk or not as high risk
for cancer or heart disease keys or
other elements and news classification
you look at the Google news and it says
well is this political or is this world
news or a lot of that's all done with a
naive Bayes understanding naive Bayes
classifier now we already went through a
basic understanding with the coins and
the two heads and two tells and head
tail tail heads Etc we're going to do
just a quick review on that and remind
you that the naive Bayes classifier is
based on the Bayes theorem which gives a
conditional probability of event a given
event B and that's where the probability
of a given b equals the probability of B
given a times probability of a over
probability of B remember this is an
algebraic function so we can move these
different entities around we could
multiply by the probability of B so it
goes to the left hand side and then we
could divide by the probability of a
given B and just as easy come up with a
new formula for the probability of B to
me staring at these algebraic functions
kind of gives me a slight headache as
it's a lot better to see if we can
actually understand how this data fits
together in a table and let's go ahead
and start applying it to some actual
data so you can see what that looks like
so we're going to start with the
shopping demo problem statement and
remember we're going to solve this first
in table form so you can see what the
math looks like and then we're going to
solve it in Python and in here we want
to predict whether the person will
purchase a product are they going to buy
or don't buy very important if you're
running a business you want to know how
to maximize your profits or at least
maximize the purchase of the people
coming into your store and we're going
to look at a specific combination of
different variables in this case we're
going to look at the day the discount
and the free delivery and you can see
here under the day we want to know
whether it's on the weekday you know
somebody's working they come in after
work or maybe they don't work weekend
you can see the bright colors coming
down there celebrating not being in work
or holiday and did we offer a discount
that day yes or no did we offer free
delivery that day yes or no and from
this we want to know whether the person
is going to buy based on these traits so
we can maximize them and find out the
best system for getting somebody to come
in and purchase our goods and products
from our store now having a nice visual
is great but we do need to dig into the
data so let's go ahead and take a look
at the data set we have a small sample
data set of 30 rows we're showing you
the first 15 of those rows for this demo
now the actual data file you can request
just type in below under the comments on
the YouTube video and we'll send you
some more information and send you that
file as you can see here the file is
very simple columns and rows we have the
day the discount the free delivery and
did the person purchase or not and then
we have under the day whether it was a
weekday a holiday was it the weekend
this is a pretty simple set of data and
long before computers people used to
look at this data and calculate this all
by hand so let's go ahead and walk
through this and see what that looks
like when we put that into tables also
note in today's world we're not usually
looking at three different variables in
30 rows nowadays because we're able to
collect data so much we're usually
looking at 27 30 variables across
hundreds of rows the first thing we want
to do is we're going to take this data
and based on the data set containing our
three inputs Day discount and free
delivery we're going to go ahead and
populate that to frequency tables for
each attribute so we want to know if
they had a discount how many people buy
and did not buy did they have a discount
yes or no do we have a free delivery yes
or no on those dates how many people
made a purchase how many people didn't
and the same with the three days of the
week was it a weekday a weekend a
holiday and did they buy yes or no as we
dig in deeper to this table for our
Bayes theorem let the event buy be a now
remember we looked at the coins I said
we really want to know what the outcome
is did the person buy or not and that's
usually event a is what you're looking
for and the independent variables
discount free delivery and day BB so
we'll call that probability of B now let
us calculate the likelihood table for
one of the variables let's start with
day which includes weekday weekend and
holiday and let us start by summing all
of our our rows so we have the weekday
row and out of the weekdays there's nine
plus two so it's 11 weekdays there's
eight weekend days and eleven holidays
that's a lot of holidays and then we
want to sum up the total number of days
so we're looking at a total of 30 days
let's start pulling some information
from our chart and see where that takes
us and when we fill in the chart on the
right you can see that 9 out of 24
purchases are made on the weekday 7 out
of 24 purchases on the weekend and 8 out
of 24 purchases on a holiday and out of
all the people who come in 24 out of 30
purchase you can also see how many
people do not purchase on the weekdays
two out of six didn't purchase and so on
and so on we can also look at the totals
and you'll see on the right we put
together some of the formulas the
probability of making a purchase on the
weekend comes out at 11 out of 30. so
out of the 30 people who came into the
store throughout the weekend weekday and
holiday 11 of those purchases were made
on the weekday and then you can also see
the probability of them not making a
purchase and this is done for doesn't
matter which day of the week so we call
that probability of no buy would be 6
over 30 or 0.2 so there's a twenty
percent chance that they're not going to
make a purchase no matter what day of
the week it is and finally we look at
the probability of B of A in this case
we're going to look at the probability
of the weekday and not buying two of the
no buys were done out of the weekend out
of the six people who did not make
purchases so when we look at that
probability of the week day without a
purchase is going to be 0.33 or 33
percent let's take a look at the this at
different probabilities and based on
this likelihood table let's go ahead and
calculate conditional probabilities as
below the first three we just did the
probability of making a purchase on the
weekday is 11 out of 30 or roughly 36 or
37 percent 0.367 the probability of not
making a purchase at all doesn't matter
what day of the week is roughly 0.2 or
20 percent and the probability of a
weekday no purchase is roughly two out
of six so two out of six of our no
purchases were made on the weekday and
then finally we take our P of a b if you
look we've kept the symbols up there we
got P of probability of B probability of
a probability of B if a we should
remember that the probability of a if B
is equal to the first one times the
probability of no buys over the
probability of the weekday so we could
calculate it both off the table we
created we can also calculate this by
the formula and we get the 0.367 which
equals or 0.33 times 0.2 over 0.367
which equals 0.179 or roughly uh 17 to
18 percent and that'd be the probability
of no purchase done on the weekday and
this is important because we can look at
this and say as the probability of
buying on the weekday is more than the
probability of not buying on the weekday
we can conclude that customers will most
likely buy the product on a weekday now
we've kept our chart simple and we're
only looking at one aspect so you should
be able to look at the table and come up
with the same information or the same
conclusion that should be kind of
intuitive at this point next we can take
the same setup we have the frequency
tables of all three independent
variables now we can construct the
likelihood tables for all three of the
variables we're working with we can take
our day like we did before we have
weekday weekend and holiday and we
filled in this table and then we can
come in and also do that for the
discount yes or no did they buy yes or
no and we fill in that full table so now
we we have are probabilities for a
discount and whether the discount leads
to a purchase or not and the probability
for free delivery does that lead to a
purchase or not and this is where it
starts getting really exciting let us
use these three likelihood tables to
calculate whether a customer will
purchase a product on a specific
combination of Day discount and free
delivery or not purchase here let us
take a combination of these factors day
equals holiday discount equals yes free
delivery equals yes let's dig deeper
into the math and actually see what this
looks like and we're going to start with
looking for the probability of them not
purchasing on the following combinations
of days we're actually looking for the
probability of a equal no buy no
purchase and our probability of B we're
going to set equal to is it a holiday do
they get a discount yes and was it a
free delivery yes before we go further
let's look at the original equation the
probability of a if B equals the
probability of B given the condition a
and the probability times the
probability of a over the probability of
B occurring now this is basic algebra so
we can multiply this information
together so when you see the probability
of a given B in this case the condition
is b c and d or the three different
variables we're looking at and when you
see the probability of B that would be
the conditions we're actually going to
multiply those three separate conditions
out probability of you'll see that in a
second in the formula times the full
probability of a over the full
probability of B so here we are back to
this and we're going to have let a equal
no purchase and we're looking for the
probability of B on the condition a
where a sets for three different things
remember that equals the probability of
a given the condition B and in this case
we just multiply those three different
variables together so we have the
probability of the discount times the
probability of free delivery times the
probability is the day equal a holiday
those are our three variables of the
probability of a if B and then that is
going to be multiplied by the
probability of them not making a
purchase and then we want to divide that
by the total probabilities and they're
multiplied together so we have the
probability of a discount the
probability of a free delivery and the
probability of it being on a holiday
when we plug those numbers in we see
that one out of six were no purchase on
a discounted day two out of six or a no
purchase on a free delivery day and
three out of six or a no purchase on a
holiday those are our three
probabilities of a of B multiplied out
and then that has to be multiplied by
the probability of a no purchase and
remember the probability of a no buy is
across all the data so that's where we
get the 6 out of 30. we divide that out
by the probability of each category over
the total number so we get the 20 out of
30 had a discount 23 out of 30 30 had a
yes for free delivery and 11 out of 30
were on a holiday we plug all those
numbers in we get
0.178 so in our probability math we have
a 0.178 if it's a no buy for a holiday a
discount and a free delivery let's turn
that around and see what that looks like
if we have a purchase I promise this is
the last page of math before we dig into
the python script so here we're
calculating the probability of the
purchase using the same math we did to
find out if they didn't buy now we want
to know if they did buy and again we're
going to go by the day equals a holiday
discount equals yes free delivery equals
yes and let a equal buy now right about
now you might be asking why are we doing
both calculations why why would we want
to know the no buys and buys for the
same data going in well we're going to
show you that in just a moment but we
have to have both of those pieces of
information so that we can figure it out
as a percentage as opposed to a
probability equation and we'll get to
that normalization here in just a moment
let's go ahead and walk through this
calculation and as you can see here the
probability of a on the condition of b b
being all three categories did we have a
discount with a purchase do we have a
free delivery with a purchase and did we
is a day equal to Holiday and when we
plug this all into that formula and
multiply it all out we get our
probability of a discount probability of
a free delivery probability of the day
being a holiday times the overall
probability of it being a purchase
divided by again multiplying the three
variables out the full probability of
there being a discount the full
probability of being a free delivery and
the full probability of there being a
day equal holiday and that's where we
get this 19 over 24 times 21 over 24
times 8 over 24 times the P of a 24 over
30 divided by the probability of the
discount the free delivery times a day
or 20 over 30 23 over 30 times 11 over
30 and that gives us our
0.986 so what are we going to do with
these two pieces of data we just
generated well let's go ahead and go
over them we have a probability of
purchase equals 0.986 we have a
probability of no purchase equals 0.178
so finally we have a conditional
probabilities of purchase on this day
let us take that we're going to
normalize it and we're going to take
these probabilities and turn them into
percentages this is simply done by
taking these sum of probabilities which
equals
0.98686 plus 0.178 and that equals the
1.164 if we divide each probability by
the sum we get the percentage and so the
likelihood of a purchase is 84.71
percent and the likelihood of no
purchase is 15.29 percent given these
three different variables so it's if
it's on a holiday if it's a with a
discount and has free delivery then
there's an 84.71 percent chance that the
customer is going to come in and make a
purchase hooray they purchased our stuff
we're making money if you're owning a
shop that's like is the bottom line is
you want to make some money so you can
keep your shop open and have a living
now I promised you that we were going to
be finishing up the math here with a few
pages so we're going to move on and
we're going to do two steps the first
step is I want you to understand why you
went under why you want to use the naive
Bayes what are the advantages of naive
bays and then once we understand those
advantages we just look at that briefly
then we're going to dive in and do some
python coding advantages of naive Bayes
classifier so let's take a look at the
six advantages of the naive Bayes
classifier and we're going to walk
around this lovely wheel looks like an
origami folded paper the first one is
very simple and easy to implement
certainly you could walk through the
tables and do this by hand you got to be
a little careful because the notations
can get confusing you have all these
different probabilities and I certainly
mess those up as I put them on you know
is it on the top of the bottom got to
really pay close attention to that when
you put it into python it's really nice
because you don't have to worry about
any of that you let the python handle
that the python mod but understanding it
you can put it on a table and you can
easily see how it works and it's a
simple algebraic function it needs less
training data so if you have smaller
amounts of data this is great powerful
tool for that handles both continuous
and discrete data it's highly scalable
with number of predictors and data
points so as you can see just keep
multiplying different probabilities in
there and you can cover not just three
different variables or sets you can now
expand this to even more categories
number five it's fast it can be used in
real time predictions this is so
important this is why it's used in a lot
of our predictions on online shopping
carts referrals spam filters is because
there's no time delay as it has to go
through and figure out a neural network
or one of the other mini setups where
you're doing classification and
certainly there's a lot of other tools
out there in the machine learning that
can handle these but most of them are
not as fast as a naive Bayes and then
finally it's not sensitive to irrelevant
features so it picks up on your
different probability and if you're
short on date on one probability you can
kind of it automatically adjust for that
those formulas are very automatic and so
you can still get a very solid
predictability even if you're missing
data or you have overlapping data for
two completely different areas we see
that a lot in doing census and studying
of people and habits where they might
have one study that covers one aspect
another one that overlaps and because
the two overlap they can then predict
the unknowns for the group that they
haven't done the second study on or vice
versa so it's very powerful in that it
is not sensitive to the irrelevant
features and in fact you can use it to
help predict features that aren't even
in there so now we're down to my
favorite part we're going to roll up our
sleeves and do some actual programming
we're going to do the use case text
classification now I would challenge you
to go back and send us a note on the
notes below underneath the video and
request the data for the shopping cart
so you can plug that into python code
and do that on your own time so you can
watch walk through it since we walk
through all the information on it but
we're going to do a python code doing
text classification very popular for
doing the naive Bayes so we're going to
use our new tool to perform a text
classification of news headlines and
classify news into different topics for
a News website as you can see here we
have a nice image of the Google news and
then related on the right subgroups I'm
not sure where they actually pulled the
actual data we're going to use from it's
one of the standard sets but certainly
this can be used on any of our news
headlines and classification so let's
see how it can be done using the naive
Bayes classifier now we're at my
favorite part we're actually going to
write some python script roll up our
sleeves and we're going to start by
doing our Imports these are very basic
Imports including our news group and
we'll take a quick glance at the Target
names then we're going to go ahead and
start training our data set and putting
it together we'll put together a nice
graph because it's always good to have a
graph to show what's going on and once
we've traded it and we've shown you a
graph of what's going on then we're
going to explore how to use it and see
what that looks like now I'm going to
open up my favorite editor or inline
editor for python you don't have to use
this you can use whatever your editor
that you like whatever interface IDE you
want this just happens to be the
Anaconda Jupiter notebook and I'm going
to paste that first piece of code in
here so we can walk through it let's
make it a little bigger on the screen so
you have a nice view of what's going on
and we're using Python 3 in this case
3.5 so this would work in any of your 3x
if you have it set up correctly should
also work in a lot of the 2x you just
have to make sure all of the versions of
the modules match your python version
and in here you'll notice the first line
is your percentage matplot library in
line now three of these lines of code
are all about plotting the graph this
one lets the notebook notes and this is
an inline setup that we want the graphs
to show up on this page without it in a
notebook like this which is an Explorer
interface it won't show up now a lot of
Ides don't require that a lot of them
like on if I'm working on one of my
other setups it just has a pop-up and
the graph pops up on there so you have a
that set up also but for this we want
the matplot library in line and then
we're going to import numpy as NP that's
number python which has a lot of
different formulas in it that we use for
both of our sklearn module and we also
use it for any of the upper math
functions in Python and it's very common
to see that as NP numpy is NP the next
two lines are all about our graphing
remember I said three of these were
about graphing well we need our matplot
library.pi plot as PLT and you'll see
that PLT is a very common setup as is
the SNS and just like the NP and we're
going to import Seaborn as S and S and
we're going to do the sns.set now
Seaborn sits on top of Pi plot and it
just makes a really nice heat map it's
really good for heat maps and if you're
not familiar with heat maps that just
means we give it a color scale the term
comes from from the brighter red it is
the hotter it is in some form of data
and you can set it to whatever you want
and we'll see that later on so those
you'll see that those three lines of
code here are just importing the graph
function so we can graph it and as a
data science test you always want to
graph your data and have some kind of
visual it's really hard just to shove
numbers in front of people and they look
at it and it doesn't mean anything and
then from the
sklearn.datasets we're going to import
the fetch 20 news groups very common one
for analyzing tokenizing words and
setting them up and exploring how the
words work and how do you categorize
different things when you're dealing
with documents and then we set our data
equal to fetch 20 news groups so our
data variable will have the data in it
and we're going to go ahead and just
print the target names data.target names
and let's see what that looks like and
you'll see here we have alt atheism comp
Graphics comp OS Ms windows.iscellaneous
and it goes all the way down to talk
politics.miscellaneous talk
religion.miscellaneous these are the
categories they've already assigned to
this news group and it's called fetch20
because you'll see there's I believe
there's 20 different topics in here or
20 different categories as we scroll
down now we've gone through the 20
different categories and we're going to
go ahead and start defining all the
categories and set up our data so we're
actually going to here going to go ahead
and get it get the data all set up and
take a look at our data and let's move
this over to our Jupiter notebook and
let's see what this code does
first we're going to set our categories
now if you noticed up here I could have
just as easily set this equal to
data.target underscore names because
it's the same thing but we want to kind
of spell it out for you so you can see
the different categories it kind of
makes it more visual so you can see what
your data is looking like in the
background once we've created the
categories
we're going to open up a train set so
this training set of data is going to go
into fetch 20 news groups and it's a
subset in there called train and
categories equals categories so we're
pulling out those categories that match
and then if you have a train set you
should also have the testing set we have
test equals fetch 20 News Group subset
equals test and categories equals
categories let's go down one side so it
all fits on my screen there we go and
just so we can really see what's going
on let's see what happens when we print
out one part of that data so it creates
train and under train it creates
train.data and we're just going to look
at data piece number five and let's go
ahead and run that and see what that
looks like and you can see when I print
train dot data number five under train
it prints out one of the Articles this
is article number five you can go
through and read it on there and we can
also go in here and change this to test
which should look identical because it's
splitting the data up into different
groups train and test and we'll see test
number 5 is a different article but it's
another article in here and maybe you're
curious and you want to see just how
many articles are in here we could do
Links of train dot data and if we run
that you'll see that the training data
has 11
314 articles so we're not going to go
through all those articles that's a lot
of articles but we can look at one of
them just you can see what kind of
information is coming out of it and what
we're looking at and we'll just look at
number five for today and here we have
it rewarding the Second Amendment IDs
vtt line 58 lines 58 in article
Etc I'm going to scroll all the way down
and see all the different parts to there
now we've looked at it and that's pretty
complicated when you look at one of
these articles to try to figure out how
do you weight this if you look down here
we have different words and maybe the
word from well from is probably in all
the Articles so it's not going to have a
lot of meaning as far as trying to
figure out whether this article fits one
of the categories or not so trying to
figure out which category it fits in
based on these words is where the
challenge comes in now that we've viewed
our data we're going to dive in and do
the actual predictions this is the
actual naive Bayes and we're going to
throw another model at you or another
module at you here in just a second we
can't go into too much detail but a deal
specifically working with words and text
and what they call tokenizing those
words so let's take this code and let's
skip on over to our Jupiter notebook and
walk through it and here we are in our
jupyter notebook let's paste that in
there and I can run this code right off
the bat it's not actually going to
display anything yet but it has a lot
going on in here so the top we have the
print module from the earlier one I
didn't know why that was in there so
we're going to start by importing our
necessary packages and from the sklearn
features extraction dot text we're going
to import TF IDF vectorizer I told you
we're going to throw a module at you we
can't go too much into the math behind
this or how it works you can look it up
the notation for the math is usually
tf.idf
and that's just a way of weighing the
words and it weighs the words based on
how many times are used in a document
how many times or how many documents are
used in and it's a well used formula
it's been around for a while it's a
little confusing to put this in here but
let's let them know that it just goes in
there and waits the different words in
the document for us that way we don't
have to wait and if you put a weight on
it if you remember I was talking about
that up here earlier if these are all
emails they probably all have the word
from in them from probably has a very
low weight it has very little value in
telling you what this document's about
same with words like in an article in
articles in cost of on maybe cost might
or where words like criminal weapons
destruction these might have a heavier
weight because we describe a little bit
more what the article's doing well how
do you figure out all those weights in
the different articles that's what this
module does that's what the TF IDF
vectorizer is going to do for us and
then we're going to import our
sklearn.naive Bays and that's our
multinomial in B multinomial naive base
pretty easy to understand that where
that comes from and then finally we have
the skylearn pipeline import make
pipeline now the make pipeline is just a
cool piece of code because we're going
to take the information we get from the
TF IDF vectorizer and we're going to
pump that into the multinomial in B so a
pipeline is just a way of organizing how
things flow it's used commonly you
probably already guess what it is if
you've done any businesses they talk
about the sales pipeline if you're on a
work crew or project manager you have
your pipeline of information that's
going through or your projects and what
has to be done in what order that's all
this pipeline is we're going to take the
tfid vectorizer and then we're going to
push that into the multinomial NB now
we've designated that as the variable
model we have our pipeline model and
we're going to take that model and this
is just so elegant this is done in just
a couple lines of code model dot fit and
we're going to fit the data and first
the train data and then the train Target
now the train data has the different
articles in it you can see the one we
were just looking at and the train dot
Target is what category they already
categorized that that particular article
is and what's Happening Here is the
trained data is going into the tfid
vectorizer so when you have one of these
articles it goes in there it waits all
the words in there so there's thousands
of words with different weights on them
I remember once running a model on this
and I literally had 2.4 million tokens
go into this so when you're dealing like
large document bases you can have a huge
number of different words it then takes
those words gives them a weight and then
based on that weight based on the words
and the weights and then puts that into
the multinomial in B and once we go into
our naive Bayes we want to put the train
Target in there so the train data that's
been mapped to the tfid vectorizer is
now going through the multinomial in B
and then we're telling it well these are
the answers these are the answers to the
different documents so this document
that has all these words with these
different weights from the first part is
going to be whatever category it comes
out of maybe it's the talk show or the
article on religion miscellaneous once
we fit that model we can then take
labels and we're going to set that equal
to model.predict most of the sklearn use
the term dot predict to let us know that
we've now trained the model and now we
want to get some answers and we're going
to put our test data in there because
our test data is the stuff we held off
to the side we didn't train it on there
and we don't know what's going to come
up out of it and we just want to find
out how good our labels are do they
match what they should be now I've
already read this through there's no
actual output to it to show this is just
setting it all up this is just training
our model creating the labels so we can
see how good it is and then we move on
to the next step to find out what
happened to do this we're going to go
ahead and create a confusion Matrix and
a heat map so the confusion Matrix which
is confusing just by its very name is
basically going to ask how confused is
our answer did it get it correct or did
it Miss some things in there or have
some missed labels and then we're going
to put that on a heat map so we'll have
some nice colors to look at to see how
that plots out let's go ahead and take
this code and see how that take a walk
through it and see what that looks like
so back to our Jupiter notebook I'm
going to put the code in there and let's
go ahead and run that code take it just
a moment and remember we had the inline
that way my graph shows up on the inline
here and let's walk through the code and
then we'll look at this and see what
that means so make it a little bit
bigger there we go no reason not to use
the whole screen too big so we have here
from sklearnmetrics import confusion
Matrix
and that's just going to generate a set
of data that says I the prediction was
such the actual truth was either agreed
with it or is something different and
it's going to add up those numbers so we
can take a look and just see how well it
worked and we're going to set a variable
matte equal to confusion Matrix we have
our test Target our test data that was
not part of the training very important
in data science we always keep our test
data separate otherwise it's not a valid
model if we can't properly test it with
new data and this is the labels we
created from that test data these are
the ones that we predict it's going to
be so we go in and we create our SN heat
map the SNS is our Seaborn which sits on
top of the pi plot so we created
sns.heat map we take our confusion
Matrix and it's going to be
met.t and do we have other variables
that go into the sns.heat map we're not
going to go into detail what all the
variables mean The annotation equals
true that's what tells it to put the
numbers here so you have the 166 the one
the zero zero zero one format d and c
bar equals false have to do with the
format if you take those out you'll see
that some things disappear and then the
X tick labels and the Y tick labels
those are our Target names and you can
see right here that's the alt atheism
comp Graphics comp osms
windows.miscellaneous and then finally
we have our plt.x label remember the SNS
or the Seaborn sits on top of our
matplot library our PLT and so we want
to just tell that X label equals a true
is is true the labels are true and then
the Y label is prediction label so when
we say a true this is what it actually
is and the prediction is what we
predicted and let's look at this graph
because that's probably a little
confusing the way we rattled through it
and what I'm going to do is I'm going to
go ahead and flip back to the slides
because they have a black background
they put in there that helps it shine a
little bit better so you can see the
graph a little bit easier so in reading
this graph what we want to look at is
how the color scheme has come out and
you'll see a line right down the middle
diagonally from upper left to bottom
right what that is is if you look at the
labels we have our predicted label on
the left and our true label on the right
those are the numbers where the
prediction and the true come together
and this is what we want to see is we
want to see those lit up that's what
that heat map does is you can see that
it did a good job of finding those data
and you'll notice that there's a couple
of red spots on there where I missed you
know it's a little confused we talk
about talk religion miscellaneous versus
talk politics miscellaneous social
religion Christian versus Alt atheism it
mislabeled some of those and those are
very similar topics you could understand
why it might mislabel them but overall
it did a pretty good job if we're going
to create these models we want to go
ahead and be able to use them so let's
see what that looks like to do this
let's go ahead and create a definition a
function to run and we're going to call
this phone function let me just expand
that just a notch here there we go I
like mine in big letters predict
categories we want to predict the
category we're going to send it as a
string and then we're sending it train
equals train we have our training model
and then we had our pipeline model
equals model this way we don't have to
resend these variables each time the
definition knows that because I said
train equals train and I put the equal
for model and then we're going to set
the prediction equal to the model dot
predict s so it's going to send whatever
string we send to it it's going to push
that string through the pipeline the
model pipeline it's going to go through
and tokenize it and put it through the
TF IDF convert that into numbers and
weights for all the different documents
and words and then I'll put that through
our naive Bayes and from it we'll go
ahead and get our prediction we're going
to predict what value it is and so we're
going to return
train.target namespredict of zero and
remember that the train.target good
names that's just categories I could
have just as easily put categories in
there dot predict of zero so we're
taking the prediction which is a number
and we're converting it to an actual
category we're converting it from I
don't know what the actual numbers are
but let's say 0 equals alt atheism so
we're going to convert that 0 to the
word or one maybe it equals comp
Graphics so we're going to convert
number one into comp Graphics that's all
that is and then we got to go ahead and
and then we need to go ahead and run
this so I load that up and then once I
run that we can start doing some
predictions I'm going to go ahead and
type in predict category and let's just
do the predict category Jesus Christ and
it comes back and says it's social
religion Christian that's pretty good
now note I didn't put print on this one
of the nice things about the Jupiter
notebook editor and a lot of inline
editors is if you just put the name of
the variable out it's returning the
variable train.target underscore names
it'll automatically print up for you in
your own IDE you might have to put in
print let's see where else we can take
this and maybe you're a space science
buff so how about sending load to
International
Space Station
and if we run that we get science space
or maybe you're a automobile buff and
let's do um oh they were going to tell
me Audi is better than BMW but I'm going
to do BMW is better than an Audi so
maybe you're a car buff and we run that
and you'll see it says recreational I'm
assuming that's what Rec stands for
Autos so I did a pretty good job
labeling that one how about uh if we
have something like a caption running
through there president of India and if
we run that it comes up and says talk
politics miscellaneous if you're
unexpiting AIML engineer then there is
no better time to train yourself in
exciting field of machine learning if
you are looking for a course that covers
everything from fundamentals to Advanced
Techniques like machine learning
algorithm development and unsupervised
learning look no further than a Caltech
in partnership with IBM so why we're
join now see it's a feeling first find
the course link from the description box
below
in this lesson you are going to
understand the concept of text mining
by the end of this lesson you will be
able to explain text mining execute text
processing tasks
so let's go ahead and understand text
mining in detail let's first understand
what text mining is text mining is the
technique of exploring large amounts of
unstructured Text data and analyzing it
in order to extract patterns from the
text to data it is aided by software
that can identify Concepts patterns
topics keywords and other attributes in
the data it utilizes computational
techniques to extract and summarize the
high quality information from
unstructured textual resources let's
understand the flow of text mining there
are five techniques used in text mining
system information extraction or text
pre-processing this is used to examine
the unstructured text by searching out
the important words and finding the
relationships between them
categorization or text transformation
attribute generation categorization
technique it labels the text document
under one or more categories
classification of Text data is done
based on input output examples with
categorization
colostrain or attribute selection
clustering method is used to group text
documents that have similar content
clusters are the partitions and each
cluster will have a number of documents
with similar content clustering makes
sure that no document will be omitted
from the search and it derives all the
documents that have similar content
visualization technique the process of
finding relevant information is
simplified by visualization technique
this technique uses text Flags to
represent a group of documents or a
single document and compactness is
indicated using colors visualization
technique helps to display textual
information in a more attractive way
summarization or interpretation or
evaluation summarization technique will
help to reduce the length of the
document and summarize the details of
the documents it makes the document easy
to read for users and understand the
content at the moment
let's understand the significance of
text mining
document clustering document clustering
is an important part of text mining it
has many applications in Knowledge
Management and information retrieval
clustering makes it easy to group
similar documents into meaningful groups
such as in newspapers where sections are
often grouped as business Sports
politics and so on pattern
identification text mining is the
process of automatically searching large
amount of text for text patterns and
recognition of features features such as
telephone numbers and email addresses
can be extracted using pattern matches
product insights text mining helps to
extract large amounts of text for
example customer reviews about the
products mining consumer reviews can
reveal insights like most loved feature
most hated feature improvements required
and reviews of competitors products
security monitoring text mining helps in
monitoring and extracting information
from news articles and reports for
national security purposes
text mining make sure to use all of your
available information it is a more
effective and productive knowledge
discovery that allows you to make better
informed decisions automate information
intensive processes gather business
critical insights and mitigate
operational risk
let's look at the applications of text
mining
speech recognition speech recognition is
the recognition and translation of
spoken language into text and vice versa
speech often provides valuable
information about the topics subjects
and concepts of multimedia content
information extraction from speech is
less complicated yet more accurate and
precise than multimedia content this
fact motivates content-based speech
analysis for multimedia Data Mining and
retrieval where audio and speech
processing is a key enabling technology
spam filtering spam detection is an
important method in which textual
information contained in an email is
extracted and used for discrimination
text mining is useful in automatic
detection of spam emails based on the
filtering content using text Mining and
email service providers such as Gmail or
Yahoo mail checks the content of an
email and if some malicious text is
found in the mail then that email is
marked as spam and sent to the spam
folder
sentiment analysis it is done in order
to determine if a given sentence
expresses positive neutral or negative
sentiments sentiment analysis is one of
the most popular applications of text
analytics the primary aspect of
sentiment analysis includes data
analysis of the body of the text for
understanding the opinion expressed by
it and other key factors comprising
modality and mood usually the process of
sentiment analysis works best on text
that has a subjective context then on
that with only an objective context
e-commerce personalization
text mining is used to suggest products
that fit into a user's profile text
mining is increasingly being used by
e-commerce retailers to learn more about
the consumers as it is the process of
analyzing textual information in order
to identify patterns and gain insights
e-commerce retailers can Target specific
individuals or segments with
personalized offers and discounts to
boost sales and increase Customer
Loyalty by identifying customer purchase
patterns and opinions on particular
products
let's look at natural language toolkit
library in detail
natural language toolkit is a set of
Open Source python models that are used
to apply statistical natural language
processing on human language data
let's see how you can do environment
setup of nltk
go to Windows start and launch python
interpreter from Anaconda prompt and
enter the following commands enter
command python to check the version of
python installed on your system
enter import nltk to link you to the
nltk library available to download then
enter
nltk.download function that will open
the nltk download window check the
download directory select all packages
and click on download this will download
nltk onto your python once you have
downloaded the nltk you must check the
working and functionality of it in order
to test the setup enter the following
command in Python idle
from
nltk.corpus import brown brown dot word
parenthesis parenthesis
the brown is an nltk Corpus that shows
the systematic difference between
different genres available words
function will give you the list
available words in the genre the given
output shows that we have successfully
tested the nltk installed on python
let's Now understand how you can read a
specific module from nltk corpora if you
want to import an entire module from
nltk corpora use asterisk symbol with
that module name import command enter
the command from
nltk.book import asterisk it will load
all the items available in nltk's book
module now in order to explore Brown
Corpus enter the command nltk dot Corpus
import Brown this will import Brown
Corpus on the python enter brown dot
categories function to load the
different genres available
select a genre and assign that genre to
a variable using the following syntax
variable name is equal to brown.words
categories is equal to genre name now in
order to see the available words inside
the selected genre just enter the
defined variable name as a command
let's understand text extraction and
pre-processing in detail
so let's first understand the concept of
tokenization tokenization is the process
of removing sensitive data and placing
unique symbols of identification in that
place in order to retain all the
essential information concerned with the
data by its security it is a process of
breaking running streams of text into
words and sentences it works by
segregating words using punctuation and
spaces
text extraction and pre-processing
engrams
now let's look at what n gram is and how
it is helpful in text mining engram is
the simplest model that assigns these
probabilities to sequences of words or
sentences engrams are combinations of
adjacent words or letters of length and
in the source text so engram is very
helpful in text mining when it is
required to extract patterns from the
text as in the given example this is a
sentence all of these words are
considered individual words and thus
represent unigrams a 2 gram or bigram is
a two-word sequence of words like this
is is a or a sentence and a three gram
or trigram is a three-word sequence of
words like this is a or is a sentence
let's Now understand what stop words are
and how you can remove them
stop words are natural language words
that have negligible meanings such as a
and and or the and other similar words
these words also will take up space in
the database or increase the processing
time so it is better to remove such
words by storing a list of stop words
you can find the list of stop words in
the nltk data directory that is stored
in 16 different languages use the
following command to list the stop words
of English language defined in nltk
Corpus importing nltk will import the
nltk Corpus for that instance enter from
nltk.corpus import Stop words will
import Stop words from nltk Corpus Now
set the language as English so use set
function as set under braces stop words
dot words set genre as English
stop words are filtered out before
processing of natural language data as
they don't reveal much information
so as you can see in the given example
before filtering the sentence the
tokenization of stop word is processed
in order to remove these stop words and
the filtering is applied in order to
filter the sentence based on some
criteria
text extraction and pre-processing
stemming
stemming is used to reduce a word to
stem or base word by removing suffixes
such as helps helping helped and helper
to the root word help
the stemming process or algorithm is
generally called a stemmer there are
various stemming algorithms such as
Porter stemmer Lancaster stemmer
snowball stemmer Etc use any of the
stemmers defined under nltk stem Corpus
in order to perform stemming as shown in
the example here we have used Porter
stemmer When You observe the output you
will see that all of the words given
have been reduced to their root word or
stem
text extraction and pre-processing
limitization
limitization is the method of grouping
the various inflected types of a word in
order that they can be analyzed as one
item it uses vocabulary list or a
morphological analysis to get the root
word it uses wordnet database that has
English words linked together by their
semantic relationship as you can observe
the given example the different words
have been extracted to their relevant
morphological word using limitization
text extraction and pre-processing POS
tagging
let's now look at different part of
speech tags available in the national
language toolkit Library
a POS tag is a special label assigned to
each token or word in a Text corpus to
indicate the part of speech and often
also other grammatical categories such
as tense number either plural or
singular case Etc POS tags are used in
text analysis tools and algorithms and
also in Corpus searches so look at the
given example here Alice wrote a program
is the source text given the POS tags
given are Alice is a noun wrote is a
verb a is an article and program is an
adjective look at the given example to
understand how POS tags are defined so
the given sentence or paragraph contains
different words that represent different
parts of speech we will first use
tokenization and removal of stop words
and then allocate the different POS tags
these are shown with different words in
the given sentence
POS tags are useful for limitization in
building named entity recognition and
extracting relationships between words
text extraction and pre-processing named
entity recognition now let's understand
what named entity recognition is all
about Nar seeks to extract a real world
entity from the text and sorts it into
predefined categories such as names of
people organizations locations Etc many
real world questions can be answered
with the help of name entity recognition
where specified products mentioned in
complaints or reviews
does the Tweet contain the name of a
person does the Tweet contain the
person's address
as you can see in the given example
Google America Larry Page Etc are the
names of a person place or an
organization so these are considered
named entities and have different tags
such as person organization
gpe or geopolitical entity Etc
NLP process workflow
now you have an understanding of all
nltk tools so now let's understand the
natural language processing workflow
Step 1 tokenization it splits text into
pieces tokens or words and removes
punctuation Step 2 stop word removal it
removes commonly used words such as the
is are Etc which are not relevant to the
analysis step 3 stemming and
limitization it reduces words to base
form in order to be analyzed as a single
item step 4 POS tagging it tags words to
be part of speech such as noun verb
adjective Etc based on the definition
and context step 5 information retrieval
it extracts relevant information from
the source
mo1 a brown Corpus problem statement the
Brown University standard Corpus of
present-day American English also known
popularly as brown Corpus was compiled
in the 1960s as a general Corpus in the
field of Corpus Linguistics it contains
500 samples of English language text
totaling roughly 1 million words
compiled from Works published in the
United States in 1961. we will be
working on one of the subset data set
and perform text processing tasks
let us import the nltk library and read
the ca underscore 10 Corpus import nltk
we will have to make sure that there are
no slashes in between hence we will use
the replace function within pandas for
the same
let's have a look at the data once
tokenization after performing sentence
tokenization on the data we obtain
similarly after applying sentence
tokenizer the resulting output shows all
individual words tokens
stop word removal let's import the stop
word library from
nltk.corpus import Stop words
we also need to ensure that the text is
in the same case nltk has its own list
of stop words we can check the list of
stop words using
stopwords.words and English inside the
parenthesis
map the lowercase string with our list
of word tokens
let's remove the stop words using the
English stop words list in nltk we will
be using set checking as it is faster in
Python than a list
by removing all stop words from the text
we obtain
often we want to remove the punctuations
from the documents too since python
comes with batteries included we have a
string dot punctuation
from string import punctuation
combining the punctuation with the stop
words from nltk
removing stop words with punctuation
stemming and limitization we will be
using stemming and limitization to
reduce words to their root form for
example walks walking walked will be
reduced to their root word walk
importing Porter stemmer as the stemming
library from nltk.stem import Porter
stemmer
printing the stem words
import the wordnet limitizer from
nltk.stem
printing the root words
we also need to evaluate the POS tags
for each token
create a new word list and store the
list of word tokens against each of the
sentence tokens in data 2.
for I in tokenized
also we will check if there were any
stop words in the recently created word
list
we will now tag the word tokens
accordingly using the POS tags and print
the tagged output
for our final text processing task we
will be applying named entity
recognition to classify named entities
in text into predefined categories such
as the names of persons organizations
locations expressions of times
quantities monetary values percentages
Etc
now press the tagged sentences under the
chunk parser if we set the parameter
binary equals true then named entities
are just tagged as any otherwise the
classifier adds category labels such as
person organization and gpe
create a function named as extract
entity names along with an empty list
named as entity names
we will now extract named entities from
a nltk chunked expression and store them
in the empty created above
again we will set the entity names list
as an empty list and we'll extract The
Entity names by iterating over each tree
in chunked sentences
great we have seen how to explore and
examine the Corpus using text processing
techniques let's quickly recap the steps
we've covered so far One Import the nltk
library to perform tokenization three
perform stemming and limitization four
remove stop words five perform named
entity recognition
structuring sentences syntax
let's first understand what syntax is
syntax is the grammatical structure of
sentences in the given example this can
be interpreted as syntax and it is
similar to the ones you use while
writing codes knowing a language
includes the power to construct phrases
and sentences out of morphemes and words
the part of the grammar that represents
a speaker's knowledge of these
structures and their formation is called
syntax
phrase structure rules are rules that
determine what goes into a phrase that
is constituents of a phrase and how the
constituents are ordered constituent is
a word or group of words that operate as
a unit and can be used to frame larger
grammatical units the given diagram
represents that a noun phrase is
determined when a noun is combined with
a determiner and the determiner can be
optional a sentence is determined when a
noun phrase is combined with a verb
phrase a verb phrase is determined when
a verb is combined optionally with the
noun phrase and prepositional phrase and
a prepositional phrase is determined
when a preposition is combined with a
noun phrase
a tree is a representation of syntactics
structure of formulation of sentences or
strings consider the given sentence the
factory employs 12.8 percent of Bradford
County
what can be the Syntax for pairing the
statement let's understand this a tree
is produced that might help you
understand that the subject of the
sentence is the factory the predicate is
employees and the target is 12.8 percent
which in turn is modified by Bradford
County
syntax parses are often a first step
toward deep information extraction or
semantic understanding of text
rendering syntax trees
download the
corresponding.exe file to install the
ghost script rendering engine based on
your system configuration in order to
render syntax trees in your notebook
let's understand how you can set up the
environment variable
once you have downloaded and installed
the file go to the folder where it is
installed and copy the path of the file
now go to system properties and under
Advanced properties you will find the
environment variable button click on
that to open the pop-up box tab of the
environment now open the bin folder and
add the path to the bin folder in your
environment variables
now you will have to modify the path of
the environment variable use the given
code to test the working of syntax tree
after the setup is successfully
installed
structuring sentences chunking and chunk
parsing
the process of extraction of phrases
from unstructured text is called
chunking instead of using just simple
tokens which may not represent the
actual meaning of the text it is
advisable to use phrases such as Indian
team as a single word instead of Indian
and team as separate words the chunking
segmentation refers to identifying
tokens and labeling refers to
identifying the correct tag these chunks
correspond to mixed patterns in some way
to extract patterns from chunks we need
chunk parsing the chunk parsing segment
refers to identifying strings of tokens
and labeling refers to identifying the
correct chunk type
let's look at the given example you can
see here that yellow is an adjective dog
is a noun and the is the determiner
which are chunked together into a noun
phrase similarly chunk parsing is used
to extract patterns and to process such
patterns from moldable chunks while
using different parsers
let's take an example and try to
understand how chunking is performed in
Python let's consider the sentence the
little mouse ate the fresh cheese
assigned to a variable named scent using
the word tokenize function under nltk
corpora you can find out the different
tags associated with a sentence provided
so as you can see in the output
different tags have been allocated
against each of the words from the given
sentence using chunking
NP chunk and parser
you will now create grammar from a noun
phrase and we'll mention the tags you
want in your chunk phrase within the
function here you have created a regular
expression matching the string the given
regular expression indicates optional
determiner followed by optional number
of adjectives followed by a noun you
will now have to parse the chunk
therefore you will create a chunk parser
and pass your noun phrase string to it
the parser is now ready you will use the
parse parenthesis parenthesis within
your chunk parser to parse your sentence
the sentence provided is the little
mouse ate the fresh cheese this sentence
has been parsed and the tokens that
match the regular expressions are
chunked together into noun phrases NP
create a verb phrase chunk using regular
Expressions the regular expression has
been defined as optional personal
pronoun followed by zero or more verbs
with any of its type followed by any
type of adverb you'll now create another
chunk parser and pass the verb phrase
string to it create another sentence and
tokenize it add POS tags to it so the
new sentence is she is walking quickly
to the mall and the POS tag has been
allocated from nltk corpora now use the
new verb phrase parser to parse the
tokens and run the results you can look
at the given tree diagram which shows a
verb parser where a pronoun followed by
two verbs and an adverb are chunked
together into a verb parse
structuring sentences chinking
chinking is the process of removing a
sequence of tokens from a chunk how does
chunking work the whole chunk is removed
when the sequence of tokens spans an
entire chunk if the sequence is at the
start or the end of the chunk the tokens
are removed from the start and end and a
smaller chunk is retained if the
sequence of tokens appears in the middle
of the chunk these tokens are removed
leaving two chunks where there was only
one before
consider you create a chinking grammar
string containing three things chunk
name the regular expression sequence of
a chunk the regular expression sequence
of your
here in the given code we have the chunk
regular expression as optional personal
pronoun followed by zero or more
occurrences of any type of the verb type
followed by zero or more occurrences of
any of the adverb types the
regular expression says that it needs to
check for the adverb in the extracted
chunk and remove it from the chunk
inside the chinking block with open
curly braces and closing curly braces
you have created one or more adverbs
you will now create a parser from
nltk.reg exp parser and pass the
grammar to it now use the new
parser to parse the tokens sent three
and run the results
as you can see the parse tree is
generated while comparing the syntax
tree of the parser with that of
the original chunk you can see that the
token is quickly adverb chinked out of
the chunk
let's understand how to use context-free
grammar
a context-free grammar is a four Tuple
sum ntrs where sum is an alphabet and
each character in sum is called a
terminal and T is a set and each element
in NT is called a non-terminal r the set
of rules is a subset of NT times the set
of sum U and t s the start symbol is one
of the symbols in NT
a context-free grammar generates a
language L capturing constituency and
ordering in CFG the start symbol is used
to derive the string you can derive the
string by repeatedly replacing a
non-terminal on the right hand side of
the production until all non-terminals
have been replaced by terminal symbols
let's understand the representation of
context-free grammar through an example
in context-free grammar a sentence can
be represented as a noun phrase followed
by a verb phrase noun phrase can be a
determiner nominal a nominal can be a
noun VP represents the verb phrase a can
be called a determiner flight can be
called a noun
consider the string below where you have
certain rules when you look at the given
context-free grammar a sentence should
have a noun phrase followed by a verb
phrase a verb phrase is a verb followed
by a noun a verb can either be Saul or
met noun phrases can either be John or
Jim and a noun can either be a dog or a
cat check the possible list of sentences
that can be generated using the rules
use the join function to create the
possible list of sentences you can check
the different rules of grammar for
sentence formation using the production
function it will show you the different
tags used and the defined context-free
grammar for the given sentence
demo 2 structuring sentences problem
statement a company wants to perform
text analysis for one of its data sets
you are provided with this data set
named
tweets.csv which has tweets of six U.S
airlines along with their sentiments
positive negative and neutral the tweets
are present in the text column and
sentiments in Airline underscore
sentiment column
we will be retrieving all tags starting
with at the rate in the data set and
save the output in a file called
references.txt let us first import the
pandas library and read the tweets data
set
extract the features text and Airline
sentiment
we will iterate through the data set
using reg X find the relevant tweets
now we will import the enter tools
module it returns efficient iterators
the result is stored in a file named
references.txt
let's extract all noun phrases and save
them in a file named noun phrases for
left carrot Airline underscore sentiment
right carrotreview.txt
here left carrot Airline underscore
sentiment right carrot has three
different values positive negative and
neutral so three files will be created
now we will iterate all the leaf nodes
and assign them to noun phrases variable
this means that the functions in itter
tools operate on iterators to produce
more complex iterators
using the map function we will get all
the noun phrases from the text
putting it into list
creating a file name in the name of
review.txt
great we have now seen how to explore
and examine the Corpus using text
processing techniques the ideal way to
display your machine learning skill is
in the form of portfolio of data science
and machine learning projects a solid
portfolio of projects will illustrate
that you can utilize those machine
learning skills in your profile as well
projects like movie determination system
fake news detection and many more are
the best way to improve your early
programming skills you may have the
knowledge but putting it to use what is
keep you competitive here are 10 machine
learning projects that can increase your
portfolio and enable you to acquire a
job as much learning engineer at number
10 we have loan approval prediction
system in this machine learning project
we will analyze and make prediction
about the loan approval process of any
person this is a classification problem
in which we must determine whether or
not the loan will be approved a
classification problem is a predictive
modeling problem that predict a class
label for a given example of input data
some classification problem include spam
email cancer detection sentiment
analysis and many more you can check the
project link from the description box
below to understand classification
problem and how to build a loan
prediction system at number 9 we have
fake news detection system do you
believe in everything you read in social
media isn't it true that not all news is
true but how will you recognize fake
news ml is the answer you will able to
tell the difference between real and
fake news by practicing this project of
detecting face new this ml projects for
detecting fake news is concerned with
the fake news and the true news on our
data set we create our tfid vectorizer
with ascaler the model is then fitted
using a passive aggressive classifier
that has been initialized finally the
accuracy score and the confusion Matrix
indicate how well our model performs the
link for the projects in the description
box below at number 8 we have
personality prediction system the idea
is based on determining an individual
personality using machine learning
techniques a person personality
influences both his personal and
professional life nowadays many
companies are shortlisting applicants
based on their personality which
increases job efficiency because the
person is working on what he is good at
rather than what is compelled to do in
our study we are tempted to combine
personality prediction system using
machine learning techniques such as SVD
name based and Logistics regression to
predict a personal personality and
talent prediction using phrase frequency
method this model or method allows users
to recognize their personality and
Technical abilities easily to learn
about more this project check the link
in the description box below at number
seven we have Parkinson disease system
Parkinson disease is a progressive
central nervous system element that
affects movement and cause tremors and
stiffness it comprises five stages and a
more than 1 million worldwide each other
in this machine learning project we will
develop an svm model using python
modules scikit-learn numpy and pandas
and svm we will import the data extract
the features and label and scale the
features split the data set design and
space model and calculate the model
accuracy and at the end we will check
the Parkinson's disease for the
individual to learn about more this
project check the link in the
description box below at number six we
have text to speech converter
application the machine learning domain
of audio is undoubtedly cutting as right
now the majority of the application
available today are the commercial the
community is building several audio
specific open source framework and
algorithm other text-to-speech apis are
available for this project we will
utilize pytt sx3 pydt access 3 is a
python text to speech conversion
liability it operates offline unlike
other libraries and is compatible with
python 2 and python 3. before API
various pre-trained models were
accessible in Python but changing the
voice of volume was often difficult it
also needed additional computational
power to learn more about this project
check the link in the description box
below at number five we have a speech
recognition system speech recognition
often known as speech two types is the
capacity of a machine or programmed to
recognize and transfer word is spoken
allowed into readable text mlsp's
recognition uses algorithm that models
speech in terms of both language and
sound to extract the more important
parts of the speech such as words
sentences and acoustic modeling is used
to identify the finance and the
phonetics on the speech for this project
we will utilize pyts X3 pyts X3 is a
python text to speech conversion Library
it operates offline unlike other
libraries and is compatible with python
2 and python 3. to learn more about this
project check the link in the
description box below at number 4 we
have sentiment analysis sentiment
analysis also known as opinion mining is
a state forward process of determining
the author's feeling about attacks what
was the user intention when he or she
wrote something to determine what could
be personal information we employ a
variety of natural language processing
and text analysis technology we must
detect extract and quantify such
information from the text to enable
classification and data management
station in this project we will use the
Amazon customer review data set for the
sentiment analysis check the link in the
description box below at number 3 we
have image classification using CNN deep
learning is a booming field currently
most projects and problem statement use
deep learning is and any sort of work
many of you like myself would choose a
conventional neural network as a deep
learning technique for answering any
computer vision problem statement in
this project we will use CNN to develop
an image processing project and learn
about its capabilities and why it has
become so popular we will go over each
stage of creating our CNN model and our
first spectacular project we will use
the CFI 10 data set for image
conspiration in this project to learn
more about this project check the link
in the description box below at number 2
we have face recognition system
currently technology absolutely amazes
people with Incredible invention that
makes life easier and more comfortable
face recognition has shown to be the
least intrusive and fastest form of the
biometric verification over time this
project will use opencv and face
recognition libraries to create a phase
detection system opencv provides a
real-time computer vision tool library
and Hardware we can create amazing
real-time projects using opencv to learn
how to create face recognition system
for you check the link in the
description box below and last but not
the least we have movie recommendation
system almost everyone today use
technology to stream movies and
television show while figuring out what
to stream next can be Discerning
recommendations are often made based on
a viewer history and preferences this is
done through a machine learning and can
be a fun and the easy project for the
beginners new programmers can practice
by coding in either python or R and with
the data from the movie lens dataset
generated by the more than 6000 users to
learn how to create movie recommendation
system for yourself or for your loved
ones check the project in the
description box below if you are an
expiring AIML engineer then there is no
better time to train yourself in
exciting field of machine learning if
you are looking for course that covers
everything from fundamentals to Advanced
Techniques like machine learning
algorithm development and supervised
learning look no further than our
Caltech in partnership with IBM so why
we're join now seats are failing fast
find the course link from the
description box below
so first we will import some major
libraries of python so here I will write
import
pandas as PD
and import
numpy
as NP
then
import
c bond
that's SNS
and import
SK learn
dot model selection
port
train underscore
test the score is split
before that
I will import
matplotlib
dot Pi plot
as PLT
okay then
I will write here from s k learn
Dot
Matrix
import
accuracy
for
than from
Escalon
dot matrix
import
classification
to report
and port
Ari
then import
string
okay
then press enter
so it is saying
okay
here I have to write from
everything seems good
loading let's see
okay till then number is a python
Library used for working with arrays
which also has function for working with
domain of lineal algebra and matrices
it is an open source project and you can
use it freely
number stand for numerical python
pandas so panda is a software Library
written for Python programming language
for data manipulation and Analysis in
particular it offers data structure and
operation for manipulating numerical
tables and Time series
then C bone
an open source python Library based on
matplotlib is called c-bone it is
utilized for data exploration and data
visualization with data frames and the
pandas Library c-bond functions with
ease
than matplotlib for Python and its
numerical extension numpy
matplotlib is a cross platform for the
data visualization and graphical
charting package
as a result it presents a strong open
source suitable for Matlab
the apis for matplotlib allow
programmers to incorporate graphs into
GUI applications then this train test
split we may build our training data and
the test data with the aid of SQL and
train test split function
this is so because the original data set
often serves as both the training data
and the test data starting with a single
data set we divide it into two data sets
to obtain the information needed to
create a model like hone and test
accuracy score the accuracy score is
used to Gorge the modest Effectiveness
by calculating the ratio of total true
positive to Total to negative across all
the model prediction
this re regular expression
the function in the model allow you to
determine whether a given text fits a
given regular occupation or not which is
known as re
okay then string a collection of letters
words or other character is called a
string it is one of the basic data
structure that serves as the foundation
of manipulating data
the Str class is a built-in string class
in Python because python strings are
immutable they cannot be modified after
they have been formed
okay so now let's import the data set
we will be going to import two data set
one for the fake news and one for the
True News or you can say not fake news
okay
so I will write here
EF underscore
big
was to
PD Dot
read underscore CSV
or what can I say dear fake okay
it underscore fake
okay
then
pick
dot CSV
you can download this data set from the
description box below
then data Dot
true
plus so PD dot read
underscore CSV
sorry CSC
plan
fake news sorry true
dot CHP
then press enter
so these are the two data set
you can download these data sets from
the description box below so let's see
the board data set okay
then I will write here data underscore
fake
dot head
so this is the fake data okay then
data underscore true
Dot
and this is the two data
okay this is not fake
so if you want to see your top five rows
of the particular data set you can use
head
and if you want to see the last five
rows of the data set you can use tail
instead of head
okay
so let me give some space for the better
visual
so now we will insert column class as a
Target feature okay then I will write
here data
let's go fake
Plus
equals to zero
then
Theta underscore true
and
Plus
versus one
okay
then
I will write here data underscore fake
dot shape
and
data underscore true
dot ship
okay then press enter
so the shape method return the shape of
an array the shape is a tuple of
integers these number represent the
length of the corresponding array
dimension in other words a tuple
containing the quantities of entries on
each axis is an array shape dimension
so what's the meaning of shape
in the fake word
in this data set we have two three four
eight one rows and five columns
and in this data set true
we have two one four one seven rows and
five column okay so these are the rows
column rows column for the particular
data set
so now let's move
and let's remove the last 10 rows for
the manual testing okay
then I will write here data underscore
fake
let's go manual
testing
question
data underscore fake
dot tail
for the last 10 rows I have to write
here 10.
okay so for I
in range
two three four
eight one
sorry zero
comma 2 3
4 7 0
comma minus 1.
okay
and
TF underscore not DF data
underscore fake
dot drop
one
here instead of one I can write here I
comma
this equals to zero
it plays
equals to true
then
data
not here
data underscore
same I will write for I will copy from
here
and I will paste it here and I will make
the particular changes
so here I can write true
that I can write true
okay
then I have to change a number
two one
six
right
2 1 4 0 6
-1
same
so press enter
X is equal to zero
alert since X maybe you mean double zero
or of this
okay we will put here double quiz
where I am putting this
a DOT drop i x is equal to zero okay in
place okay
and also write equals to an equation
yeah
so
okay axis is not defined
now it's working so
let me see
now
did the
underscore
fake dot shape
okay
and data dot true
and
data underscore true
dot shape
as you can see
10 rows are deleted from each data set
yeah
so I will write here data underscore
fake underscore manual
testing
class
equals to zero
and data underscore
true
let's go
manual underscore testing
class equals to
one
okay
just ignore this warning
and
let's see
data underscore
fake underscore
manual
testing dot hat
as you can see we have this
and then data dot sorry underscore true
underscore
manual
testing
dot at
10.
this is this is done true data set
so here I will merge data
underscore merge
question
PD Dot
concat
concat is used for the concatenation
data underscore fake
data underscore
comma
X is
equals to zero
then data underscore merge
dot head
the top 10 rows
yeah
as you can see the data is merged
here
first it will come for the fake news and
then with that for the True News
and let's merge true and fake data
frames
okay
we did this and
let's merge the column then data dot
merge
Dot columns or let's see the columns
it is not defined whatever data
underscore much
these are the column same title text
subject date class okay
now
let's remove those columns which are not
required for the further process
so here I will write data underscore
or request to
data underscore merge
crop
title I don't need
that
object
we don't need then
so one
so let's check some null values
it's giving here
because of this
that's good then data
dot is null
sum
Center
so no null values
okay then
let's do the random shuffling of the
data frames okay
for that we have to write here data
equals to
data dot sample
one
then
data
okay
data
dot hat
now you can see here the random
shuffling is done
and one for the
true data reset and zero for the fake
news one okay
then
let me write here data Dot
reset
underscore index
place
because you
true
a dot dot drop
comma X is
equals to 1
then comma in place
true true
okay
then let me see columns now data Dot
columns
so here we have two columns only rest we
have deleted
okay
see data Dot at
yeah
everything seems good
let's proceed further and let's create a
function to process the text okay
for that I will write here
but
okay
you can use any name
text
and text equal to
text Dot lower
okay
and text
for the substring
remove these things
from the
datas
okay
so for that I'm writing here
comma
thank you
then text equals to
re Dot substring
comma
comma text
okay
then I have to write text equals to
r e Dot substring
to www
Dot
S Plus
comma
from our text
okay then text equals to
re Dot substring
then
oh
comma
okay
then
text equals to
re Dot substring
and
percentage
as
again percentage or
RG dot SK function
right here string
dot punctuation
comma
then comma then text
right
then text equals to re Dot substring
and and
comma
x equals to
re Dot
substring
right here
and again d
then again
then comma
then again
texture
okay then at the end after right here
return text
so everything like this these type of
special character will be removed from
the data set
okay let's run this let's see
yeah so here I will add DF sorry not DF
data
data
then
next
was so
data
dot apply
to the function name wordpot what opt
okay
press enter
yeah so now let's define the dependent
and independent variables okay x equals
to
data
text
and Y equals U
data
class
okay
then splitting training and testing data
okay sorry
so here I will write X underscore train
comma X underscore test
uh then y underscore train
comma y underscore test equals to
train underscore test underscore split
then X comma y
comma test
let's go size equals to
0.25
okay press enter
so now let's convert text to vectors
for that I have to write here
that it's X
so here I will write from sqlarn
Dot teacher
extraction
Dot text import
t
vectorizer
okay
then vectorization
plus 2 tfid
factorizer
okay
then
three
underscore
train
equals to
vectorization
refactorization dot fit
that transform
X underscore train
okay
then XV underscore test equals to
factorization
Dot transform
X underscore test
okay then press enter
[Music]
foreign
logistic regression
so here I will write
from
sqln Dot
linear underscore model
okay import
logistic
regression
then a lot
goes to
logistic
regression
and
after that LR Dot
fit
then XV
Dot
dot so dot train
comma
x v underscore test
okay
press enter
dot XV dot train
okay here I have to write y train
and press enter
will work so here I will write
prediction
underscore
linear regression
l r dot predict
XV underscore test
okay let's see the accuracy score
for that I have to write l r dot score
then XV underscore test
comma y underscore test
okay
let's see the accuracy so here as you
can see accuracy is quite good 98
percent
now let's print
the classification
port
by underscore test comma
prediction of linear regression
okay
so this is you can see Precision score
then F1 is code then support value
accuracy
okay
so now we will do this same for the
decision free gradient boosting
classifier random Forest classifier okay
then we will do model testing then we
will predict this score
so now for the decision tree
classification so for that I have to
import from SK learn
Dot 3
import
decision
three
classifier
than at the short form I will write here
I will copy it from here
then
okay
then I have to write the same as this so
I will copy it from here
and
let's change linear regression
to
season 3 classification
okay
then I will write here same
let's go DT
question
DT dot predict
XV underscore
test
e
still loading it's it will take time
okay
till then let me write here for the
accuracy
DT DOT score
V underscore test
comma y
let's wait okay
let's run
the accuracy
so as you can see accuracy is good than
this linear regression
okay logistic regression
okay so let me
show you the
let me predict
trend
okay
so this is the accuracy score this is
the all the report
yeah
so now let's move for the gradient
boosting classifier
okay for that I've read from
sqlarn
dot ensemble
port
gradient
boosting
classifier
pacifier
I will write here GB
equals to let me copy it from here
I will give here random
let's go state
equals to zero
wait wait wait wait so I will write here
g b Dot
fit
underscore train
comma
y underscore chain okay then press enter
so here I will write predict
underscore GB
was who
GB Dot
wait sorry
Reddit
three
Dot test
dot dot underscore test
till then it's loading so I will write
here uh it's for the score then I will
add GB
DOT score
then
three underscore test
comma
y underscore test
okay
so let's wait it is running this part
till then let me write for the
printing this
okay it's taking time
taking time still taking time
but if I will run this
it's not coming because of this
yeah it's done now so you can see the
accuracies
not good then
decision tree but yeah it is also good
99
.4 something okay so
now let's check for the last one random
Forest
first I will do
for the random for us we have to write
from sqlarn Dot
symbol
import
random
Forest
classifier
okay
and here I will write RF
equals to
right I will copy it from here
then
random
date
equals to
zero
and
RF Dot
fit
three underscore train
comma y underscore train
okay then press enter
and predict
underscore
RC
or F
equals to
RF dot predict
three underscore test
okay
till then I will write here still
loading it will take time
so till then I will write for this score
score accuracy score
XV underscore test comma y underscore
test
okay
then I will write here till then print
classification
port
and Y underscore test
comma
it will take time little bit then
so
it run the accuracy score is 99 it is
also good
so now I will add the code for the model
testing so I will get back to you
but after writing the code so
so I have made two functions one for the
output label and one for the manual
testing okay
so it will predict
the all the from the all models from the
repeat so it will predict
the the news is fake or not from all the
models okay
so for that
let me write
here news
also string
what
okay
then I will let him manual underscore
testing
so
here I will you can add any news from
the you can copy it from the Internet or
whatever from wherever you want
so I'm just copying from the internet
okay from the Google
the news
which is not fake okay I'm adding which
is not fake because I already know I
searched on Google so I'm entering this
so just run it let's see what is showing
okay
string input object is not callable okay
let me check this first
a I have to give here Str only
yeah let's check
okay I have to add here again the script
yeah
manual testing is not defined
let me see manual testing
okay I have to edit something
it is just GB and it is just RF
in GPS is not defined okay okay
so what I have to do
I have to remove this
this
everything seems sorted
now
as I said to you
I just copied this news from the
internet I already know the news is not
fake so it is showing not a fake news
okay so now what I will do I will copy
one fake news from the internet
and let's see it is detecting it or not
okay
so let me run this
and let me add the news for this
so
all the models are predicting right it
is a fake news
or you can add your own script like this
is the fake news okay
I hope you guys understand
till here
so I hope you guys must have understand
how to detect a fake news using machine
learning you can you can copy it any
news from the internet and you can check
what is time series forecasting
making scientific projection based on
the data with historical timestamps is
known as time series forecasting it
entails creating model through
historical study using them to draw a
conclusion and guide strategic decision
making in the future
the fact that the future result is
wholly unknown at the time of the task
and can only be anticipated through
analysis and evidence-based priors is an
essential decision in forecasting give
yourself a chance to Simply against
professional certificate program in Ai
and machine learning which comes with
completion certificate and in-depth
knowledge of AI and machine learning
check this course detail from the
description box below
so here is one question for you guys I
will give you exactly one minute for
this you can comment or you can give
your answer in the chat section so I can
see if the answers given by you are
right or wrong
okay
so the question is which type of
programming does python support
I'm repeating again which type of
programming does python support
option a object oriented programming
option b structured programming option C
functional programming and option D all
of the above
so let us know in your answer in the
chat section or in the comment section
so I am starting a timer of one minute
just type your answer in the comment
section or in the chat section
do let me know your answers please
so I am starting the timer of one minute
this type of programming does python
support object oriented structured
programming functional or all of the
above
do let me know your answers please
you can comment or you can give your
answer in the chat section so I can see
if the answer is given by you are right
or wrong
which type of programming does python
support object oriented structured
functional or all of the above
30 seconds meaning
this type of programming does python
support object oriented structured
function or all of the above
let us know your answers in the chat
section or in the comment section below
and seconds more
which type of programming does python
support
five seconds more
so the allotted time is over we will
give a reply to those who gave the
correct answer
and for those who didn't give the
correct answer we will give you reply
with the correct answer okay
now let's move to our programming part
so we will open command prompt to write
a command to open Jupiter notebook so
here I will write
Jupiter
notebook
that's okay Jupiter
notebook press enter
will take time
it's open
so this is the landing page of Jupiter
number 2 and here I will select new
python kernel file
so this is how Jupiter kernel look likes
so
here
what we will do we will import some
major libraries of python which will
help us in analyzing the data okay
import numpy
as NP
okay then import
pandas
as PD
then import
C bone
as SNS
the fourth one is from
matplotlib
port
as PLT
okay
then
we will import some model libraries so
here I will add from
stats
models
dot TSA
dot API
import
potential
nothing
and comma then
simple
XP smoothing
and one more
hold
we will write here import sorry from
dot linear
underscore model
import
linear
regression
okay
then import
ance
earnings
then we will already have warnings
Dot
filter warnings
should be ignore
yes there will be no error
or it's still loading yeah
so numpy numpy is a python Library used
for working with arrays it also has a
function for working with the domain of
linear algebra and matrices
it is an open source project and you can
use it freely numpy stands for numerical
python
second is Panda
pandas is a software Library written for
the Python programming language for data
manipulation and Analysis in particular
it offers data structure and operations
for manipulating numerical data and Time
series
than Seaborn an open source python
Library based on matplotlib is called
c-bone
it is utilized for data exploration and
data visualization
with data frames and the pandas
liability c bond function with ease
matplotlib for Python and its numerical
extension numpy matte broadlib is a
cross platform data visualization and
graphical charting package
as a result it presents a strong open
source substitute for matpler
the apis for matplotlib allow
programmers to incorporate graphs into
GUI applications
linear regression the machine learning
method regression built on linear
supervised learning
analysis regression is done
regression creates a value for the aim
prediction using independent variables
as inputs
its main goal is to investigate the
relationship between factors and
forecasting
exponential smoothing
the exponential Windows function is a
general method for smoothing time series
data known as exponential smoothing
it contrasts to the ordinary moving
average which weights previous data
quality
exponential function use weights that
decrease exponentially with time
and there is one more simple exponential
smoothing
the simple exponential smoothing classes
use Simple exponential smoothing models
to give very simple time serial analysis
a weight average of the most recent
value and the preceding smooth value
constitute the predicted value
the contribution of older value degrade
exponentially as a result of the
smoothing parameter
okay so after importing libraries let's
import data set
for this we I will write here DF
or you can write data frame PD Dot
read
underscore CSV
here I will write
monthly
underscore CSV
dot CSV
okay this is my file name
you can download this file from the
description box below
and for seeing the data I will write
here DF Dot
head
then press enter
yeah so here PD is for pandas Library
read is used for reading the data set
from the machine and CSV is used for the
type of file which you are using okay or
whichever you want to read
and if you want to see the top five rows
of your data set you can use head and if
you want to see the last five rows of
your data set you can use tail instead
of head
okay this one you can write your tail
so moving forward let's see how many
rows and columns are present in our data
set for that I have to write DF dot
shape
okay
then press enter
okay it will give error why because
this yeah
so here you can see 847 rows and two
columns date and price only
so moving forward let's do some Eda
exploratory data analysis okay for that
I have to write here print
data
date range
of
of gold prices
available from
and here I have to give curly brackets
and DF dot location Loc
and colon
comma
date
okay D is capital here so I have to
write
date
and from location 0
to
again same thing DF Dot
Loc
ate
to the length of
EF
minus one
before everything seems good yeah
then press enter
okay
we have date range of gold prices
available from this 1950 to 2020. okay
in our data set
and here I will write date
equals to PD Dot
date
range
start from
slash one slash 1950.
comma
and it goes to
it slash
one slash 2020.
comma frequency
virtual media
okay then
then here I will write date
then press enter
yeah
so here you can see date time index
okay
from starting to land these dot dots
and here I will write
DF
and
was to
date
we have dot drop
date
comma axis is 1
comma in place
pursue true
T should be capital
okay then DF equals to DF
dot set
index
and month
and DF
Dot
head
press enter
so instead of
this state
we have adjusted this month
okay for particular value
so moving forward let's see the graph
different different graphs okay
TF Dot Plot
and figure set should be
equals to
20 comma 8.
then plot
dot title
is
like
gold
prices
monthly
tense
1950
and onwards
okay title should be this
then X level should be PLT dot X label
will be months
then PLT dot y labels
should be
price
and I will write a PLT Dot
grid
okay
press enter
okay title spelling my battery okay then
press enter
oh why label it is sorry
here you can see
gold prices monthly since 1950 and
onwards
okay
till from 1950 and from till 2020 okay
this is the price
then moving forward let's see another
graph so I will write here for that
round
DF Dot describe
comma 3.
and here you can see the count variable
and the average is 416.557 standard
deviation that minimum price value is
this 20 this and the maximum value is
this okay
so
so the average gold price in last 70
years is this Force 16.557 okay
only 25 percent of the time the gold
price is above 447.
the highest gold price ever does this
this one 1840 807.
so we will do visual analysis so here I
will write
ax equals to
PLT Dot subplots
sub plots
okay then figure size
pursue
25
comma r
okay then SNS dot box plot
X
equals to
DF dot index
dot ear
comma y
DF dot values
colon comma 0
comma x equals to X
okay
then same PLT dot title
price
monthly
and in 50.
in words
same graph will come but in the
different format okay
let me remove this
the nplt DOT
X label
level must be here
LT dot y level
ice
and
TLT Dot
X ticks
will be rotation
okay I will give rotation 90.
PLT dot grid
you can write here instead of grid I can
write here direct issue so grid is with
this format this box format okay
so press enter
it's loading yeah
so here you can see from 1950 every year
is here till 2020 so how the gold prices
are decreasing and increasing okay
let's see the another graph
I will write here from
ads
models
models
Dot
graphics
Dot
TSA plots
port
month plot
okay
then I will write a figure comma ax
equals to
then PLT Dot
subplots
bigger size
equals to
22 comma 8.
okay
then month plot
TF comma y label
pursue
gold
price
okay comma
then ax equals to ax
and I will give the title PLT Dot
title
so I will copy from here
and PLT Dot X label
month
and PLT dot y label
the price
PLT dot grid
so here you can see gold prices monthly
since 1950. like for every month like
January February March April so on till
December okay
we will cover one more graph
okay and many many more graphs
so
so we will
go with the next graph
for that I will add here
let's go
comma ax
equals to
PLT Dot subplots
and figure size
was to
22 comma 8
10 same SNS
dot box plot
x equals to
DF dot index
dot month name
okay
then y equals to
DF dot values
comma 0
comma ax
equals to ax
okay then PLT dot title
will put same title so I can copy from
it here
let's copy from here and paste it here
okay
and PLT Dot
X level
and PLT dot y label
price
and PLT dot grid
okay
dot show
this time let's not use grid
okay
okay something videos month name
yes
you can see
for every month
this is another type of graph
okay box plot graph
so why we are creating so much graphs
because we are doing Eda exploratory
data analysis in this we have to see the
multiple different and different
different types of
graph
okay so moving forward let's see average
gold price per year like Trend since
1950.
so for that I have to write here DF
let's go yearly
let's go some
question
DF Dot resample
sample
then
a
dot mean
DF underscore
yearly
underscore sum
Dot Plot
PLT Dot title
here I will write average
gold price
yearly
since
1950
or you can write onwards from onwards
1950. so here I will write PLT dot X
label
here
and PLT dot y label
ice
okay
then PLT dot grid this time we'll use
grid
so here you can see the average gold
price yearly since 1950. okay this is
the chart till 2020 like sometimes up
sometimes down okay
and we will see now like average gold
price per quarter
like trends like since 1950
so here I will write DF Dot
not DOT TF dot quarterly
okay
underscore sum equals to
DF Dot resample
order queue
dot mean
yeah
oh okay
move this
then DF underscore
quarterly
scores sum
Dot Plot
and the same PLT dot title
from here
here I will write average good price
waterly
quarterly
okay since 1950
then PLT dot X label
label is now here quarter
PLT dot y label
this price
now PLT dot shown
it this time
okay
EF underscore quarterly
okay quarterly
yeah
here you can see the price prediction
okay let me set to the grid only
it's not this
visuals are not good so great
yeah
so here you can see the quarterly
process prediction
okay average gold price quarterly
prediction
so like moving forward
we will see now average gold price per
decade like per 10 years okay
so from 1950 only so here I will write
TF underscore decade
underscore sum
was to DF Dot resample
every 10 years okay 10 year
dot mean
we are writing mean because we are like
putting out average
okay
so DF underscore decade
underscore sum
Dot Plot
PLT dot title
average
goal
price
per decade
since
1950 okay
since 1950 yeah yeah perfect
so here I will get PLT Dot
X label
and
decade decade is off like every 10 year
PLT dot y label
PLT dot grid
yeah
so here you can see the average gold
price per decade like from 1950 to 1960
like this straight then against it then
up then sometimes down then up and down
okay every 10 years you can see a 1990
2000 2010 and 2020.
so moving forward let's do like analysis
in coefficient of variation the
coefficient of variation CV is a
statical measure of the relative
dispersion of data points in a data
series around the mean
and in finance the coefficient of
variation allows investor to determine
how much mortality
or risk is assumed in comparison of the
amount
like amount to the return expected from
investors
okay the lower the ratio of the standard
deviation to mean return the better risk
return trade-off
okay let's like let us look now the CV
values for each year in gold prices
so CV means coefficient of variation in
prices okay
so here I will write DF underscore one
equals to DF Dot
Group by
F dot index
dot ER
dot mean
dot name
dot columns
equals to
private price
and
mean
then again DF underscore one equals to
DF underscore one
dot range
or what we can do instead of Range View
Android merge
DF dot Group by
School DF dot index
dot EO
in the deviation
rename
columns
equals to
price
standard deviation
comma left
index
was so true true
then comma right index
was so true
okay
and here DF underscore one
it's cool
first two
EF underscore one
standard deviation
slash
DF underscore one
two
hundred
like Dot
round figure should be like 2 after
decimal how much
like numbers you want to see
then TF underscore one
dot hat
press enter yeah
so here you can see for every year I
have mean
standard deviation
and this coefficient of variation
okay
for every year
so
like moving forward let's see the
average gold price per year again
so for that figure
underscore
figure Dot
comma ax
goes to PLT
Dot subplots
bigger size
15 comma
10.
and
I will write here DF underscore one
is
COV underscore
Dot Plot
PLT dot title
average goal
price
yearly
since
1950.
okay then PLT dot X label
is here
or PLT
dot y level
by label
coefficient of variation I'm writing CV
in percent
okay
then PLT dot show
okay DF underscore invalid syntax
HTT The Dot Plot
K codf underscore one DF underscore one
it here
valid syntax
okay
so here you can see average gold price
since 1950.
okay
this is like percentage CV in percent
okay
like
good
you can say the chart
so the CV value reached its highest in
1978 like somewhere here like 1980 1978
okay
like near to 25 percent which could have
made the asset as a highly risky but in
2020 the CE value is closer to five
percent which makes the asset variable
via good investment okay
so
now what we will do we will do time
series forecasting
okay we will train model we will build
model different different model we will
train and test split to build time
series forecasting model
so for that let me do like this first
yeah
so here I will write train
was to DF
TF dot index
dot per year
okay
equals to
2015
and for the testing we will write TF
DF dot index
dot ER
2015.
okay for training we are taking till
2015 for and for the testing we are
taking till 2020 from 2015. okay
then I will write here how many columns
present in train or test
so for that I will add print train
dot shape
print
test dot ship
press enter
792 rows and one column in train
training for training the model and 55
to test the model okay
so now let's see the training data and
testing data
so train
a
like square brackets so
price
Dot Plot
then figure size
2.
13 comma
5.
and
font size should be
also 15.
ice
Dot Plot
bigger size
bigger size
let me give
same 13 comma five
font size
should be
okay then PLT dot let me add grid
LT dot create
then PLT
Dot agent
training
data
comma
test data
okay then PLT
or
so I will tell you what the legend is
okay
so here you can see the training data in
blue and the testing data okay this is
known as the legend this this portion
and
yeah
so here you can see month wise
okay till 2020.
and from 1950
right these are the prices
and this is the chart so moving forward
let's do model formation now okay
we will do two models lineal
linear regression and the name base one
okay so first we are
first we will go from
linear regression
for that I will add train underscore
time
assume I plus 1
1.
four
I in range
brain
test underscore time equals to
I Plus
length like train
one for I in
range then length should be test
okay
so for length
training time
or my length should be tested
press enter
okay this is the training and this for
the testing 792 rows and here 55 rows in
testing
so
LR underscore
alarm means linear regression let me
make it capital
foreign
equals to
make a copy
underscore
test equals to test copy
dot copy
then LR
okay
so LR
train
time
question train time
and LR
underscore test
time
equals to test time
okay
underscore is there
so here I will write LR equals to linear
regression
dot fit to the model
train
and for the time
plot
large
underscore train
ice
values
linear regression is not defined okay my
bad
L should be Capital yeah
so now see the graph
so test
underscore
prediction
let's go model
one
question
Allah Dot
predict
La underscore test
time
test
question
test underscore predictions
score model
one
okay
let's create the graph figure
size should be
let's do
14 comma 6
.
PLT Dot Plot
print
price comma
label
should be
trained
okay then plot
PLT Dot Plot
test
price
label
equals to test
PLT Dot Plot
then LR train or tests
podcast
label
equals to
regression on time okay
regression on time
then again PLT dot Legend
best
PLT dot get it
okay
here I've tried
loading
one more error
price success price
okay
is the price
so here you can see regression on time
test data is this green one and this
training Returns the testing data
OK let's find the map now so for this
we'll write here def
meep
actual comma prediction
then return
Ed
P dot mean
abs
actual
section
Jewel
I
100
comma 2.
okay
forgot to give the
yeah
so for for getting the map you have to
write a map underscore
order
let's go test equals to mape
test
price
Dot values
comma
test underscore
model one
okay
then print
is
percent three
or three f
then here every percent
model
on underscore test
come on
percentage
press enter
test
okay
jewel is not defined
web test
real model one test
May test
values
the test
fictions
model one
okay here a is small
so
map here you can see 29.76 0.
so you are a bit confused like what is
Me Maybe is a measure of prediction of
accuracy of a forecasting method in
statical model OKAY is a measure of
prediction accuracy of forecasting
method in statical model
and now
results
equals to PD Dot
data frame
test
map
in percent
score
model 1
comma index
plus two
regression
on time
okay then I will print the results
okay D should be capital
real pandas has no attribute data frame
okay
model one is not defined
let's go test
when it's in there perhaps you gotta
forget a comma
colon
model one okay
here you can see the test map regression
on time
so let's do with the name now we have to
perform the same pattern so what I will
do what I will write the code and get
back to you okay
so I'm done with the code so here you
can see I have same pattern train and
test copy and this is the name forecast
on the test data so this is the line
and this is the training and this orange
one is testing
the same we have got me like 19.380
okay
so and regression is this
the name model
so what we'll do we will create now
final model of ours okay
and we will forecast final forecasting
will do
for that I have to write final
model
question
exponential
smoothing
okay DF
comma
Trend equals to
iterative
comma seasonal
pursue additive
comma fit
smoothing
level
goes to 0.4
comma
smoothing
trend
0.3
comma smoothing
seasonal
0.6
okay press enter
exponential exponential
so map
go final underscore model is equal to
map
like DF
dice
dot values
comma final
let's go
model
Dot
fitted values
then print
if
from a map
oh
final underscore model
okay
so map is 17.24 which is quite good for
the final model
okay
so getting the prediction for the same
number of time stamps at the present
time in the test data so I will write
here predictions
equals to final
underscore model Dot
forecast
steps
equals to length DOT test
Center
now we will compute 95 5 percent of
confidence interval for the predicted
value so
said tkf equals to PD Dot
data frames
then
lower
CI
prediction
1.96
into
NP dot standard deviation of final
model
Dot
one
okay
and comma
right here prediction
okay then upper CI
action
Plus
1.96
into
dot standard deviation
the same final model
but
so one
okay
then prediction
underscore DF dot add
so this is lowerci prediction and the
upper CI okay how much it will forecast
or
now
at the end at the final State what we
will do we will plot a graph okay
forecast graph along with the confidence
band
so for that X is equals to DF Dot Plot
label
pursue
actual
comma figure size
16
comma 9
addiction DF
and
Dot Plot
x equals to
this
comma label
underscore DF
it's lower CI
what I will do
instead of this
label equals to
forecast
comma Alpha
equals to 0.5
okay
this
dot bill
between
chin
underscore DF
dot index
comma prediction underscore DF
1 underscore CI
comma prediction
underscore DF
this CI
then
color
question
um
comma
Alpha
versus
Point fifteen
okay
then X is
dot set
underscore
X label
year month
axis
dot set
y label
it's
PLT dot Legend
should be location equals to best
then PLT dot grid
PLT dot show
okay this is PLT only
then press enter
okay 16 comma 9 position argument
follows keyword argument
15.
okay okay I have to give you
Oppo CI
alright upper CI
so here you can see our final model
forecasting so till 2028 is showing like
normal and after that till 2030
okay this will be the forecast as per
the data
okay
so
here you can see
as you can see we have the map is
17.24
okay then here I did the prediction for
the testing data
and here I've created the data frames
these are the data frames lower CI
prediction and upper CI and then I have
created
the and then I have created the final
graph of the forecasting
okay
if you're an expiring AIML engineer then
there is no better time to train
yourself in exciting field of machine
learning if you are looking for a course
that covers everything from fundamental
to Advanced Techniques like machine
learning algorithm development and
unsupervised learning look no further
than our Caltech in partnership with IBM
so why we're join now seats are failing
fast find the coastline from the
description box below
opencv open source computer vision
library is an open source computer
vision and machine learning software
Library it is written in C plus plus but
has a binding for various programming
languages such as python Java Matlab
opencv was designed with a goal of
providing a common infrastructure for
computer vision applications and to
accelerate the use of machine learning
perception in commercial product opencv
is widely used in a variety of indices
including robotics automotive and
Healthcare it's supported by a large
community of developers researchers and
users who contribute to its development
and provide supports to its users it is
supported by a large community of
developers researchers and users who
contribute to its development and
provide supports to its users so now
let's see what is object detection
object detection is a computer vision
technology that involves identifying and
localizing the object of interest within
an image or a video it is a challenging
task as it involves not only recognizing
the presence of an object but also
detecting its precise location and size
within the image or video object
detection algorithm typically used deep
learning techniques such as CNN to
analyze the image or video for
identifying the objects these algorithm
can also determine the boundaries of the
object by drawing a bounding box around
them so after understanding what is
object detection now let's move on to
the programming part so this is a kernel
python kernel here we will change this
name
so here I will write object
detection
demo
okay
so
first we will import some major Library
like opencv so for that we will write
import CB2 and the next one is import
matplotlab
dot Pi plot
as PLT
so why we are writing PLT because we
can't write again and again matplotlib
dot pipelot okay it's a long one so we
can write a short form PLT
so yeah
so let's run this
so what is opencv opencv is an open
source software library for computer
vision and machine learning the opencv
full form is open source computer vision
Library it was created to provide a
shared infrastructure for application
for computer vision and to speed up the
use of machine learning perception in
consumer products opencv has a PSD
license software make it simple for
companies to use and change the code
okay so there are some predefined
packages and libraries that make our
life simple and opencv is one of them
second one is matte broadly matte
bloodlib is a easy to use and an amazing
visualize library in Python it is built
on numpy array and designed to work with
broader sci-fi stack and consists of
several plots like line bar scatter
histogram and many others okay so moving
forward we will import our file okay so
here I will write config
file
equals to this is our file name SSD
underscore
mobilenet
to V3
large
Coco
22.
14 dot b b
okay
so you can find this file in the
description box below
Frozen
order
question
I'll explain you
every single thing
inference
graph
dot PB okay
so let me run it first
mobilenet as a name applied the mobile
net model is designed to use in mobile
application and its tensorflow first
mobile computer vision model mobilenet
use depth wise separable convolutions it
significantly reduces the numbers of
parameter when compared to the network
with regular convolutions with the same
depth in their heads this result in the
lightweight of the deep neural network
so mobilenet is a class of CNN that was
open source by Google and therefore this
gives us an excellent starting point for
training our classifiers that are
insanely small and insanely fast okay so
what is this large Coco this is a data
set Coco data set like with applications
such as object detection segmentation
and captioning the Coco data set is
widely understood by the state of the
art of neural network its versatility as
a multi-purpose scene variations of best
to train a computer vision model and
Benchmark its performance okay so what
is Coco the common object and context is
one of the most popular large scale
labeled images data available for public
use it represents a handful of objects
we encounter on a daily basis and
contains Imaging notations in 80
categories I will show you the
categories I have with over 1.5 million
object instances okay so modern day AI
driven solution are still not capable of
producing absolute accuracy and result
which comes down to the fact that Coco
dataset is a major Benchmark for CV to
train test and polish refine models for
faster scaling of The annotation
Pipeline on the top of that the Coco
data set is a supplement to transfer
learning where the data used for one
model serves as a starting point for the
another so what is frozen Insurance
graph like freezing is the process to
identify and save all the required
graphs like weights and many others in a
single file that you can usually use a
typical tensorflow model contains four
file and this com contains a complete
graph okay so forward let's create one
model here I will add modern
DNN
model
chosen
and
then
config file
so here I am giving the parameters two
parameters like frozen model and config
file
score here yeah you run it first
okay there is another
okay so either is cv2.dnn detection
model return is result
with an exception set
the question comes what is the meaning
of detection model or DNN detection
model
so this class represents a high level
API for object detection networks
detection models allows to set
parameters for pre-processing input
image detection model creates net from
file and with trained weights and config
sets in processing input runs forward
pass and return the result deductions
okay moving forward let's set the class
labels
class
labels
file name
txt
I will put this file on the description
box below you can download from there
open
file name
pass labels
the strip
so here I created one array of name
class labels so this is the file name
what I am doing I am putting this label
file into this class label okay so here
if I will print
class labels
so these are the 80
categories in the Coco data set
okay this person bicycle car motorbike
airplane bus train these all are the 80
categories
I will put this file label.txt in the
description box below you can download
from them
okay fine
so let's print the length of the Coco
data set or you can see class labels
this is 80 as you can see I have already
told you
the length will be 80.
so here let's
set this some model input size scaling
mean and all so I will write here model
Dot
set
put
eyes
320 comma 20.
set
input
scale
1.0
slash 127
0.5
okay I will explain you don't worry
model dot set
input
mean
127.5
comma 127.5
comma 127.5
okay and then model
dot set
to web
B
will be
what is set input size
okay
so set input size is a size of new frame
the shape of the new blob Less Than Zero
okay so this is the size of the new
frame the second one is set input scale
the set input scale is a scale factor of
the value for the frame
or you can say the perimeter will be the
multiplier of the frame values or you
can say multiplier for the frame values
okay so at input mean so it set the mean
value for the frame the frame in which
the photo will come the video will come
or my webcam will come so it set the
mean value for the frame or the four
parameters mean scalar with the mean
values which are subtracted from the
channels you can say and the last one is
set input swap RB so it set the flag
swap RB for the every frame we don't
have to put every time a single frame
for a particular image it will be set
the true for the all the images okay so
parameters will be swipe I'll be flagged
which indicates the swipe first and the
last channels so moving forward we will
put one image
M read
y dot jpg
Dot
I am sure
so this is the size of 320 by 320 okay
so first thing is
you can download this the random picture
from the Google I took from Google
itself so now what we will do
we will set the class index
the confidence value
box is the boundary box which I will
create for that particular person cycle
motorbike and the car
okay
because to model
that
the confidence threshold
threshold is used for
if my model will confirm it's the
particular image which is detecting is
correct it will print the name
okay
so let me print
pass
class index is coming one two three four
okay so one means person
two means bicycle
three means car and four means motorbike
this is the class index index for
particular level I will do I will print
the boxes
font
here
equals to 3
and the font equals to CV2 Dot
font Hershey
again
for
class
index and then
confidence and the boxes
and
x dot
pattern
confidence
pattern
box the boundary box
okay
then I will write here CB2 dot rectangle
make the rectangle
set image
and boxes
I've
comma 0 comma zero
this is the color of the box and this
will be the thickness
okay
then I will write CV2 dot put
text
image then
first labels
I will write class
index minus 1
because always index start with 0.
that's fine and the boxes
zero
and
column boxes
and
body
okay
I want comma
scale
assume font
scale
color
was to
this will be the text color
0 comma 255
comma 0
and the thickness
two three
let me run it
no error okay now PLT dot I am sure
then CV2 dot CBT
color
then if
my CV t two
dot color
me
hello and BGR
to prg
that is why we wrote
swap RB equals to true because every
time we will convert BGR to brg
sorry
GB
RGB so we don't have to write again and
again it will convert all the files into
RGB okay run it
okay as you can see the motorbike is
coming
bicycle is coming the person is coming
the card will come the car is coming
okay so it's detecting the right
for the image now we will do this for
the video and for the webcam
we are done with this image one and then
now I will write here
okay
so this is will do for the video
for the video I will write here cap
equal to capture
you can write any name so CV2 Dot
video
capture
so you can take any random video I took
this pixels
George
you can comment down
share the link
for
app dot is open
so here I will add cap
equals to
be 2
sorry CV2
dot video
capture
0.
and if not
cap
dot is open
then
and this
input output
error
and
open the video
and open the video
here everything will be the same font
scale
equals to 3
okay I want
equals to CV2 Dot
font
e
in
okay
so here I will write y true
from a frame
equal to cap dot sheet
this for the reading of the file
the same I will write class index
comma confidence
from a boundary box
question model
dot detect
game
and the confidence
threshold
0.55
okay everything is the same
we did before so here I will print
class
index
okay
so here I will write if
and
the class index
does not equals to zero
and what to perform is here I have to
add 4
class index
comma confidence
got my boxes
and zip
us
tax Dot
flatten
flatten is the layers
okay
go on for this
one
flatten
e-box
and if
class
index is greater than equals to 80.
then
what to do
here
okay the same thing I have to write here
so here I will write CV2 Dot
I am sure
this will be the return in the frame
object detection
by simply learn
and frame
so if CB2 Dot
weight key
to
and 0
equal to f of x
equals to
Ord
Q
okay
then
here I will write Brink
will you break when
it gets into two the weight key will be
2.
okay I will tell you what is the weight
key
so here I will write cap dot release
and CV2 Dot
destroy
or Windows
okay
so now let me done
it's here there will be better okay
Python programming language modules
let me run it again
[Music]
okay
so video is here
the video is here as you can see see
bicycle the person the person the bus
car traffic light the person person so
it our object detection for the video is
coming right
okay person
okay person traffic light bus
this is how you can do for the video
okay
so now let's we will do for the webcam
live
so this is for the video so
if
we want to do for the webcam
with
okay
so we need to just change
one one thing only we have to change
instead of giving the file we have to
write one here
okay the rest will be the same
but it so I have to just shut down my
webcam
so
let me shut down the webcam and get back
to you
thank you
so as you can see
this is a 320 by 320 box so
so this is coming right okay so I if I
will show this the mobile phone is
coming right now okay so this is how you
can do the correct object detection okay
users of Microsoft Excel May format
arrange and compute data in a
spreadsheet data analyst and other
analysts can make information easier to
examine as data is added or altered by
our organizing data using tools like
Excel the boxes and excels are referred
to as cell and they are arranged in a
row and column the key features of
excels are they spreadsheet document in
Ms Excel can have headers and Footers
and user can protect their data by
giving it password protection
and the second one is filtering is
supported allowing you to locate the
necessary data in your spreadsheet and
replace it with the appropriate value
after discussing what is Excel let's go
through and see what is python so python
is the one of the most popular
programming language available today it
is widely used in various sector of
business such as programming web
development machine learning and data
science
given its spreadsheet use it is not
surprising that python has surpassed
Java as a top programming language
python frequently used to create
software website and to perform data
analysis and many more because python is
a journal purple language it may use to
develop a wide range of programs and
isn't alert for any particular issues so
the key features of python are it is
open source and free
python can be downloaded from the
company official website it is simple to
download and install
python is open source because allowing
users with solid technical background to
modify the code to suit certain business
use case and product requirements since
python is a language for beginners most
anyone with the understanding of
programming can quick pick it up and
begin coding
so after seeing what is python let's
move forward and see what is automating
Excel with python
we all know python is dominant
everywhere and we also know that
compared to other language python is
beginner friendly and simpler to use
automation is the one of the coolest
thing you can do with python so how to
automate an Excel sheet in Python
imagine that you are asked to create
accounts for 30 000 employees on a
website
what would you think you will
undoubtedly become frustrated carrying
out the stocks manually and repeatedly
additionally it will take too much time
which is not the wise choice
so just try to picture that what it is
like for employers who work in data
entry the responsibility is to extract
the data from tables like Excel or
Google sheet and insert it in the
another location they read various
magazines and website get the data there
and they are entered it into the
database additionally they must perform
the calculation for the entries
in general this job performance
determines how much money is made
greater entry volume more pay of course
everyone wants a higher salary in their
job so however don't you find doing the
same things over and over boring
the question is now
how can I accomplish it quickly
and how to automate my work spend an r
in coding and automating these kind of
course to make your life simpler rather
than performing these kinds of things by
hand by just writing fewer lines of
python code you can automate your
laborious activity in simpler
so overall python Excel automation is a
creative method that allows you to build
visual reports on python in a smoother
manner similar to how you would on Excel
businesses can use Python Excel
automation to streamline their operation
in accordance with their requirements
so here is one question for you guys I
will give you one minute for this you
can comment or you can give answer in
chat section so I can see the answers
are given by you are correct or not
I'm repeating again here is one question
for you guys I will give you one minute
for this you can comment or you can give
your answer in chat section so I can see
if the answers given by you are right or
wrong
so
the question is which type of
programming does python support
object oriented programming structured
programming functional programming and
all of the mention
so I am repeating again which type of
programming does python support
option a object oriented programming
option b structured programming
functional programming and all of the
mention
so I am starting the timer of one minute
just type your answers in comment
section or in chat section do let me
know your answers guys so like please I
want that everyone should participate in
this so I am starting the timer
so
so your time starts now
so I want that everyone should
participate in this guys please to let
me know your answer in chat section or
in comment section
so I can see if the answers given by you
are right or wrong so
like 36 seconds or more
these guys I want that everyone should
participate in this so
let me know your answers in chat section
or comment section
so you can just type the answers or a b
c d
your wish totally your wish so
10 seconds are left guys hurry up please
I want that everyone should participate
in this five second more
okay so time is over we will give reply
those who gave correct answers and those
who didn't give correct answer we will
give you a reply with the correct answer
okay so no worries
so now let's move to our programming
part to perform Excel automation using
python so first we will open command
prompt to write command to open Jupiter
notebook feature
notebook
yeah
so this is the landing page of Jupiter
notebook and select open
select here new open new file
so this is how the Jupiter notebook UI
looks like so at first we will import
some major libraries of python which
will help us in importing our workbook
or worksheet so the first one is let me
first rename this
we will write Auto meeting
so using python
yeah sounds good
okay
so the first one is Open PI Excel open
py Excel okay I have to write import
yeah perfect
so a python package called open pyxl can
read and write Excel 2010 xlx format
Excel SM format xltx and xltm files
it was created since there was no
library that could read and write office
open XML files natively from python
since open py Excel was initially based
on PHP Excel all praise to the PHP Excel
team so open py Excel does not by
default provide protection from your
quadratic blow up or billion laughter
XML assault installed diffused XML to
protect yourself from these assault so
we will write open py Excel and then
like
yeah so
then we will write for important our
workbook so we can write here from open
p y Excel
port
workbook
underscore workbook
is capital here
so
and this is for code for the importing
our workbook so let me write in command
like you can import workbook like this
so I can write you can import
workbook
this
okay perfect
yeah
so
here
let me import workbook then
so I have to write WB for Workbook
question
load
underscore workbook
a destination of the workbook so uh the
destination is in like desktop so I have
to write C
users
lp09
375
stop
Dash automation
that
relearn
Dot
cell SX format
okay cool
users SLP this one desktop automation
simply learn DOT xlx fine
so our book is loaded to Jupiter
notebook
we can see yeah perfect no error so
let's activate this workbook for the use
so we have to write WS for worksheet so
worksheet equals to WB DOT active
active
like import from openmpi Excel about
book there must be always been one
worksheet in our workbook it is
accessible through the workbook dot
property active so that's why we have
write WB DOT active so after making
workbook active so we will print our
workbook like
we can print
so print
WS
so here
so here you can see the sheet1 so it is
showing worksheet sheet1 why sheet1
because if you open your workbook and
see here is your worksheet name like
like so this is my automation
file
this is simply learn
so here you can see
the sheet1
that is why it is showing sheet1 there
so uh okay so let me just
finish it
so moving forward
let's change the worksheet name so
I will write here import
open p y Excel
so like it's your wish that you want to
write this import file or like this
thing this is not necessary at all so
now I will import the workbook so I will
write here
WB equals to open py Excel
dot load underscore
workbook
when pyx L it is
my bad sorry guys
so
you have to give the path so I can copy
path from here
yeah perfect
okay sounds good
yeah here WB underscore sheet
equals to
WB and square brackets then you have to
write the sheet name so there we have
sheet 1.
sheet 1.
so sheet1 okay correct
so I can change with WB underscore sheet
dot title
sheet
dot title
equals to
code with
simply learn
effect
yeah same scored let me save this
WD dot save
and the same
copy
here paste
yeah so when a sheet is generated it
immediately receives a name like they
have sequential numbers of names like
sheet sheet1 sheet 2 and so on with the
worksheet you can modify this name
whenever you want to like using the
title property so
here let me save this
and yeah
so again go to Simply learn
yeah you can see code waste simply learn
first it was sheet1 and now it is
changed to codebase Simply learn so let
me cut this again
okay
so if you want to change the color of
your sheet name tab whereas written code
will simply learn so you can write like
here
you can write
WD
or okay let me make this new one
okay right I will write here so
underscore sheet
dot sheet
underscore properties
color
was to
here you have to give hex
values so four zeros okay
so let me save this okay it is saying
sheet1 why sheet1 because we already
changed sheet1 to code which simply
learn so what you have to do is
here we have to write code with
simply learn
relearn
okay same same simply learn simply learn
same code code yeah so what I will do
here I will just
make W Capital so yeah
now it will work fine yeah
so
if you save or run your code like like
this let me
let me show you one thing uh
like
color is already changed wait for one
minute guys I will show you something
crazy so it will see like this w
okay
so here I will small
so
if you save or run your code not
actually save but run your code while
your Excel sheet is open
so my Excel sheet is open code will
simply learn
yeah
so my Excel sheet is open so it will
give you error like Vita
repeat like whatever
this okay let me do one thing this is
one okay
it will give you
simply learn one does not exist let me
check this is simply learn code with
simply learn
one
code with okw's capital there
okay see this is a
permission denied then the location
it is showing permission denied
so this is something important to
remember that Excel files should be
closed while running the code and
repeating again so like this is
something important to remember that
Excel file should be closed while
running the code
so first what I have to do is close the
Excel file
then I have to run it again so no error
now so here you can see
what we simply learn that red color came
yeah perfect
so moving forward let's create new
workbook till we were working on the
same worksheet which was present me in
my like desktop so let's create new
workbook so for that I have to write
import
cell as X writer
so I will write here in command to
create
new
WB equals to XL SX writer
Dot
workbook
and where you want to
save the workbook so I will give
the same
path
but with different name
so this time I will give only like
coders simply
simply coders
then I will write w d close close
okay I will run it yeah
so the primary class export by the XLS
writer module in the workbook class and
that is only class you will need to
instantiate directly so the workbook
class represents both the whole
spreadsheet as it appears in the action
and the Excel file as it is stored on
the disk internally so you can see now
yeah you can see now
the workbook which is created in the
desktop with the same name is like
simply coders
so this is done
let's move forward and let's retrieve
cell values so right now we are not
having any data inside the workbook so
let's put some random values like names
and all so uh
I will put some random values
we'll put here
names
okay
uh mayank
question
okay
well
like jelly
it
Louis
so let's save it
let's save it
and let's move to our coding part so
here what I have to write to retrieve
cells here I just write a equals to A is
variable here so I am assigning this WS
that
A1 value
the value which is presented at A1
so here I will write in comments
like
2
retrieve cells
okay
so here I will print
a
okay okay
what I have to do is like we have
created this one file so it's it is not
giving
the uh this is giving sheet1 dot A1 why
because it is taking from the simply
coders one so what I can do is I can
import this file because we have saved
data in this file
so it bases here and make it active
detective yeah so yeah you know I
haven't write here value
so that is why it is giving the cell
name and that A1 name cell name A1 only
so now it will print
names why why names
because at the A1
at the A1 position we have names so if
you want to like print Anjali so what
you have to do is okay let me check this
B3 okay
if you want to print Anjali you have to
just write here just copy paste this
code
okay just to write here the location
B3
you can see Angel is printed
so this is how you can retrieve cells
and we can do the same for the others as
to
so there is one thing like retrieving
we can change the sales values too
so just we have to write here so let me
give first enter internet for the better
visual
yeah
so here what we can write is WS
like I want to change the name of uh
name of name of
uh
I will take this A4 Smith
let me close this
okay
I will give here A4
dot value
equals to here you can give the value so
here you can write Smith you can write
John
here as a replaced value
so I will write here in as a command
cell
value
okay
so what you have to do is just save it
WB dot safe
you have to give the path for the same
yeah
fine
so let me save this and
let me save this first and run this
first okay no error and let's open our
workbooks to see the changes
so here you can see
the changes has been implemented in our
workout
first there it was like Smith and now
John so like this you can change the
cell values and let's move forward and
see how we can merge the cells in a
worksheet
so like just close this and yeah
so we are doing for the merging cells so
here I will write from
open pyxl
dot styles
styles
import
alignment
so here I will write
merge cells
for your better understanding
okay
so here I have to write WB equals to
workbook
okay let me active this workbook
WB DOT active
merge
the cell
which one uh let's take for a while A1
and V2
okay A and B should be in capital
letters
so after that we have to save this
we will save in this only
so I can copy I can paste it here
okay fine
like using merge underscore cells and
after giving cell position you can
easily merge the cell so let's see the
output so let me open this simply learn
file
you can see
they are merged together and like but
what if you want the values at the same
times
so you can write here I have to cut this
so you can write here like
from open p y Excel
dot styles
quote
alignments
alignment a should be capital
alignment
okay
so the same thing you have to write wdb
equals to
workbook
WS equals to
WB DOT active
you don't have to give parenthesis
w s dot merge
underscore cells
cells
A1
B2
so here I have to give the cell value
equals to WS dot cell
which row which column so I can write
row equals to 1
column
equals to 1.
so cell
dot value
equals to
here you can give the value which you
want to print at the cell so here I will
write code with simply learn
find
so what we can do this we can give the
alignment
alignment equals to
alignment
or horizontal horizontal
equals to Center
and from vertical
and from vertical the same
Center
okay let me save this file WB dot save
at the same location so I can copy the
path from here
copy here and paste here so I will
that seems good let me save this
white is giving error a cell object has
no attribute alignment okay alignment
spelling is wrong
sorry my bad
gnme and T
eight seems good now
oh good to go
so
so here you can see
like by giving rho equals to one column
equals to one basically position and
after giving the value to what to print
on that cell and one more thing is
alignment like it is not necessary to
give alignments like I just wrote it to
cover it uh like in video so you can
give alignments like uh right left this
time I gave the center to print in the
center
so let's uh move and see the results
so you can see the value has been
printed exactly at the center of the
cell
well I will make it like this you can
see
the value is printed exactly at the
center of the cell because I gave the
alignment center so let's do one more
for like
to see what is the default place if you
don't give the alignments okay if you
don't give the center alignments what is
the default value so let me first remove
this okay save no issues
okay yeah
so
let's align
alignments
or here what we can do is like
from
UI Excel dot styles
import
alignment
is not necessary to light import
alignments
okay this is without
okay I can write this default
live
seems good
okay everything we have to cover from
here so I can just copy from here and
paste it here
so what we will do here we will change
the cell value we will use like C
six and B
it
sorry guys
so
uh here I have to give row
equals to 6
and column must be
eight so let make it be CS6 B6
okay let's call them should be six
so code was simply learn coders of
Simply learn
photos of Simply learn like we are
giving the value so at the same time we
have to just save it WB dot save
copy
paste here
okay
so this time we are not giving the
alignments you can see here we have gave
alignments not now so let's run this and
after that we will open our workbook
so this is saying merge cells
okay 2 must be greater than 3.
okay so here what is happening
okay I can write here D6
right
so now it will run properly I guess yeah
it's working
so
let me open this
so you can see
this is the default value of like
it so here you can see the value is
printed at its default location
not in centered or not in right not in
left it is default location
so till now we know how to merge cell so
I repeat I hope you guys understand till
here if you have any questions or any
query regarding any code or question
just put as in comments our team will
shortly provide you the correct solution
so
moving forward
until now we know how to merge cells so
let's learn how to unmerce cell okay so
let me open the code
yeah
let me open the code and write for you
so let me quickly write the code how we
can myself so we can write open
okay from
open
e y
Excel
dot styles
you just I can copy this from here
I have to write here import alignments
alignment only
so what we are doing here we will not
give any value or any this
so here I will remove like B6
actually we can give
like a
six
P6 as a value so
let's see the cells are merged or Not by
running it
okay see permission denied because our
Excel was open so let me save this and
close it
and run it again no error
so
here you can see
let me Zoom if I can yeah so as you can
see A6 and B6 are merged so what should
I do to unmerge I have to write
yes you can write here like
where you can write WS Dot unmerge
underscore cells
okay cells
got it and just give this same value
A6 underscore B6
okay colon B6
let me save this again no error let me
open the file
so you can see here
A6 and B6 now unmerged
so by just using unmerge cells you can
easily unmerge the cells in your
workbook
so let's move forward and see how can
you set dimensions for a particular cell
or column
so let me close this and go to the
workbook again
this is something frustrating by like by
regularly doing the same thing going
next next next
so like you need a particular column
height should be something and which
should should be something so that is
what I'm like meant to be Dimensions so
let's write code for it you will see and
understand by seeing so
import
open p y Excel
#to set
dimensions
of a particular
cell
okay
WB equals to
open p y Excel
dot workbook
worksheet equals to
workbook DOT active
ws.row underscore dimensions
yeah one I will explain you don't worry
height
equals to
let me give 70 for a while
a w s dot column
underscore
dimension
Waits I will explain you don't worry
don't worry
it
so let me save the file WB dot save
and let me copy the path from here
okay
everything seems
good
okay
y column okay column name is like okay
no column Dimension okay
dimensions
either so like one height 70 and B which
is it
color one let me open the workbook
so here you can see
by giving row Dimension and column
Dimension and by adding height and width
then we can set the dimension for a
particular cell y1 and B for this
you can see here one for the column like
2 for the column 3 for the column let me
Zoom it
you can see now one for the all the one
column Row 2 for the two Row 3 for the
three row like this
for you can say I use one for the
particular row or you can say one for
denoting row particular Row one two
three four five six like this
chicken I repeat
and particular alphabet to denote the
column see a for this column B for this
column C for this column D for this
column like this okay
so here you can see every cell of Row 1
has the same height this this this this
this this this this
and every cell of
bit and every cell of column B has same
width
BS same width
so
like we can do one more thing we can
print
value though in a particular cells like
okay let me
close this
and open the this one
so what can I do is I can write
here
like we can write at wd's cell
rho equals to 1
comma column
equals to 1.
okay
dot value
equals to
hello
okay and WS dot cell
here I will write rho equals to 2
comma column
equals to 2
dot value
about value
equals to
coders
so I have to write it here not below
this yeah
okay cool let me give this space line
okay so let me save this
and
let me check
so here you can see
hello is for the row one and coder at
the column B
okay fine
so let me close this and
okay next I repeat next let's see how we
can move data to a particular row or
column or you can say a particular cell
here we need data to make it move so
let's fill some random data
on it
so what I can do is
I can put some random data
of like
fruit
vegetables
and some names like Maya
jelly
above
partial
oh no
and we can give like
anything is myth
John
and Chloe
Sarah
and I all right Sachin
okay
I think this much is enough so let me
save this
and yeah
so data part is done and let's move to
the this one and write import
open pyxl
yeah
so let me write here move
cell
you can say move or jump whatever you
want to say so from open p y Excel
import
workbook
Come on load
underscore workbook
will be equals to
load
can give the path like this yeah
C and we
okay let's make it active WS equals to
WB DOT active
of X seems good okay
so I have to move WS
dot move
score range
this is I can give okay let me see
the data first
okay I will make an verb here
and
blank here
or here
okay
so let's see again I forgot it is B
5 and A4
B5 A4
okay
so what we can do it like
oh
e
five
comma like row
plus two two
and call
V rho is equal to and calls equals to
so let me save this w dot safe
and let me
so here what I'm doing is changing the
position of the value which is present
in the location
ah A4 and B5 so and moving them by you
can say the value will jump by two rows
and two columns because I gave here two
rows rows equals to 12 columns equals to
2.
so let me save this run this okay
and
so here you can see
so let me run this
and what you have to do is open this
file
yeah
so you can see A2 means 1 2
it comes fruit comes here and coders
come here so this is how you can move
your
coding part
I repeat so this is how you can move
these cell values so let's move forward
and
see moving forward let's see how we can
insert a row in our workbook okay so
first what I will do is
I will add some data like
name here like fruits
and something like car
and here I can like bike
okay
something here coding
okay all the data is filled and all rows
are filled by some values so here we
have no like no empty row present here
till now so what we will do is
we will insert a row
okay so what we have to write is import
open p y Excel
hashtag
insert
pose
okay
from open p
p
so
import
book
comma load
let's go
load
all right
let me copy the path
see
so let me make it active
so I want to insert row
deposition to
one more row
at the position five
so let me save this I will copy from
here
okay
I will make like this so let me save
this no error
let's move to the workbook
so you can see here Row 2 is created and
Row 5 is created with no values
so
as you can see rows are added to the
particular position or you can say the
indexes
so we have seen how we can insert the
rows let's see the deletion of the rows
so what we can write is let me open the
code
so what we can write is let me copy to
save our time
okay just delete this
so we can write WS
dot delete
underscore
rows and which row
2 5 I will delete row number two
for a while
so let me save this
and run this
so you now you can see empty Row 2 is
deleted
so by using insert rows and giving them
index you can insert the row at the
index position you give and meanwhile
you can use delete underscore rows
function to delete the particular row so
moving forward let's quickly write code
for the insert column
so for inserting column
I will copy from here
yeah insert row
column
and this one is for
e-litting
row
okay
so I have to write here
WS
Dot
active not active sorry insert
uh calls at the position second
okay at the position 1 and
okay I will make it
insert
calls at the position three
okay
let me save this file by copying this
path
okay saving let me do one thing
yeah perfect
oh let me open the workbook
so here what I've done is insert call at
second position here we can't give
alphabet we have to give a number for
denoting a position so
here you can see the column is added at
the position one
and three
so by using insert underscore calls and
by giving index you can insert column in
your workbook
so let's do one thing guys
okay first let me close this and yeah
so what I was saying that let's do one
thing guys
you will tell me how can I delete a
particular column
okay let me write here
the question
comments like
how
can
you
delete a
particular
particular
column
okay
uh I'm giving you one minute for this
just comment the code for this column
deletion or else you can reply on chat
I will suggest you to write a comment
because after this session our team can
easily check who gave the correct answer
and who didn't give the correct answer
don't worry I will provide you the
answers at the end of the session or our
team will reply at your comment at the
same time
so uh like for those who are watching
this video like we watch normal video
means you are not watching the live
session so what you can do is just pause
the video right now and put the comment
how you can delete the column
I am repeating again I will suggest you
to write a comment because after this
session our team can easily check who
get the correct answer and who didn't
give the correct answer
don't worry I will provide you the
answer at the same time
or our team will reply at your comment
at the same time
and those who are watching the video
like we watched normal video
so that you can do is just you can pause
this video and put the comment how you
can delete the column
so I guess time is up guys I hope most
of you gave the correct answers by
putting comments we will verify your
comments and we will back to you soon so
okay moving forward let's see how we can
style the font in Excel
okay this is something crazy and
interesting
so you will not get bored
and writing from open py Excel
dot styles
import
font
so here I will write for your styling
okay
so what I can do is like WB is equals to
open
p y Excel dot workbook
plus Dot
equals to WB DOT active
I will don't worry I will explain you
rho equals to 1
comma column
equals to 1 equals to
um
and I will set the size of the cell
so WS equals to
sell
rho equals to
1
comma
column
equals to 1
Dot
value
okay here I have to write font
equals to
font
equals to
font and you have to write size
equals to 24.
okay here I have to give
dot value
we can do it for the many others
but we will do it
we will
change
hello
not world again we will write coder
hello coders and
here we will add code with simply learn
and here we will write
hello YouTube
okay
so
what I will do is like it will
2 column two Row 2
2 here
what I will do is font size equals to
24 and and I will make it italic
also
Pro
okay fine and I will make it
old
only question true
and here I will give
three
three
okay and at the end what I can do is
okay bye
Thanks new
one
so let me save it quickly
WB dot save
with a fart
so let's run this and see the results
so let's run this again one more time
so here what I did is just gave the
particular row and column value where we
want to print the value like simply
learn hello coders hello YouTube code
with simply learn
okay and by using font I am giving the
font size and phone like italic bold and
so on like this is italic this is bold
okay
and here you can see at the End by using
name here you can see
at the End by using name
be assigning the styles of the
particular text like Times New Roman and
you can give the name of your choice
so why it is not because
I have gave the it is collapsing
so here I have write 444 okay now it
will it is collapsing now hello YouTubes
and simply learn
so what I will do is now I will run
now you can see
yeah perfect
so this is simple one this is in italic
this is in bold and this hello YouTube
is in Times New Roman Style
okay
so okay let's move forward and do
something with cells
um
like what we can do it let's do color
them okay
let's write code for this
how we can like color a particular cell
we know how to color the particular tab
of sheet
and now let's color it so we can write
is import
open p y Excel
here I will write coloring
particular cooler
cell
okay
so I will first copy this thing here
from here
okay I don't think I want this I have to
write from open
e by Excel
files
port
and fill
I want something like this
okay so WD dot open pyexel workbook
here I can write open pyxl
dot load workbook
okay
workbook
and here I will give the path
it seems good
so what I will do is
make it active and I can write here WS
equals to
WB
square brackets then I can write name of
working sheet
I don't remember
sheet
cheat
so you can write a fill underscore cell
equals to pattern
will
ATT pattern
type should be I will prefer solid
okay
and
comma
comma
FG color
foreground color
equals to
I will write here f
C
2 C
0 3 okay
fine we can make four cells
four cells
four cells are enough I guess
so we can write here cell one
paste copy paste copy paste so we'll
four
three
two
solid solid solid yeah
so let's change color we can give 0 3
FC
F4
here I will give
3 5 F C 0 3.
perfect
here I can give
o f c
a 0 3.
so now let me assign
let me assign the cells to them WS
like
you can assign a
a one
sine A2
dot fill
equals to
fill
underscore cell
KR
ill
underscore cell
one
that seems good
this I can make it for four we we
May 2 a 4 A5
A3
everything seems good
let me save this so I will copy from
here
why error permission denied because it
was open so let me cut this
foreign
so let me see
so here I'm using the pattern field to
fill the color and solid style and
giving the FG color like foreground
color
and at the below you can see I am like
assigning these cells to a particular
pattern fill cells so
let's see the output yeah it's working
here you can see the results we colored
these for the particular cell like A2 A3
A4 A5
so this is something you can style the
color so let's move forward and see how
we can import the image in our workbook
like something here something here
something here let's see
so let me first cut this okay
let me do one more thing yeah
perfect
coloring particular cell
so how we can import
a file or image you can say
so I will write
from
open p y Excel
foreign
workbook
okay load underscore workbook load
underscore workbook
and
I have to write F from
open p by Excel
dot drawing
dot image
import image
okay fine
workbook
write w d equals to
load underscore
workbook
give
let me copy this and this hole
so logo is for
variable used for the image image
okay so I don't have an image for right
now
let me do one thing
uh let me import one image
any kind of image for a while so I can
do like
so this is my image
what can I do is
what can I do is I can give the path
like
uh
path would be same
what would be same
we will change here something don't
worry don't worry
X top
don't remember name of the file
this is picture two
yeah
p is a smaller capital
p is capital so I have to write
pictures
2 dot PNG
so
I will give logo
dot height equals to 150
logo
Dot width
equals to
150 a bit of resizing to not fill the
whole spreadsheet with the logo
so that's why just for that so WS dot at
image
to logo
comma
cell in which cell you want to print I
will M two
find
so here we will
save it
by using this one
okay perfect
cannot image import name image from Opel
file drawing done Dixon
so might be there is problem
open p y Excel Dot
drawing
dot image
okay
import image
I think I is capital here
James
logo image okay ice Capital here
yeah it seems good
yeah here is the picture so what I do is
what can I do is
so here we need one more Library drawing
image a variable logo this one
is for the location of the image and
a giving and height and weight because a
bit of resizing to not fill the whole
spreadsheet with a logo like with the
whole spreadsheet with the logo so just
a bit of resizing
and add image function is used to append
image on it and M2 is for like this M2
M2 is for position m
2 like that it's for in which cell you
want to append it so
image is coming to the particular index
M2
okay
so this is how you can import image and
Excel using python okay let's move
forward and see how we can import data
and time so let's write code for that
quickly
okay
so what I will do is I will write here
import
date time
which I will write
date
time
from
open p y Excel
import
time
so what can I do is
make it WP equals to
workbook
low should be Capital here
and WS equals to WB
DOT active
what can I write is ws.cell
row 1
comma column
goes to one
dot value
equals to
date
and time
something
so I can copy it
paste it
what I can do is
let me first change see
three
oh
equals to three
okay
value
so here I can assign date and time
date
time dot date time
dot now
dot string
format Str f
CRF time
what you have to write it percent why
percent M command
percent D for days
percent h
then percent
I am for minutes and percent
against
so this should be
in between this
okay
yeah right
so let me save this
close it
import date start time not date time
it's date time
okay row equals to 1.
data time node date time
again
date time dot now
daytime Dot date time
so date time objects has no attribute
strf
time okay time it is yeah
okay
yeah
Finance working no errors
so air like okay
so here I have imported some libraries
like date time and so on strf time used
for string formatter and date time now
used for the current date time this date
time now is used for the current date
time
and so let's run this and see
so here you can see the correct time and
date is coming it can be different from
your from which country you are watching
this live session
so
what if you want to print the time after
like two seconds late
so for that we can write here
quickly
so I will copy this hole
we
so like
we have to copy this and just what you
have to do is
let me make for to see the difference
we we okay three are enough
so what I will do is
go four
five
okay fine what I will add here is time
dot sleep
how much second you want like two
for this one and
something for this one like
five seconds for this one
okay
so what we can do is let's save it
and let's see the results
so here I've used the time dot slave
function given parameter as two or four
I guess five for printing the seconds
difference
so here you can see the difference in
seconds okay the time is not coming wait
wait let me check
sorry guys
now I have to run it
open it
and it is not coming why so
shouldn't denied
okay
let me run it again
okay okay because I'm giving seconds
difference not that way that is wise
giving permission denied so let us wait
for two five five seconds
okay I think five seconds are over now
we can open it
yeah you can see 47 seconds and 49s two
second difference and 49 54 5 Second
difference
so here I've used the time slip function
given parameter as 25 for printing the
second different so let's move to the
workbook uh
here you can see the difference in
seconds so I hope you understand till
here if you have any questions or any
query regarding any code or question
just put as in comment our team will
shortly provide you the correct solution
I'm repeating again I hope you
understand till here if you have any
questions or any doubt
or any query regarding any code or
whatever it is so just put as in
comments or our team will shortly
provide you the correct solution
so moving forward
moving forward let's see how we can add
values as an array
okay
so what I will write import
xlsx
writer
WB equals to xlsx
writer Dot
workbook
okay I will give the path
from here
okay WS equals to
WB dot at
worksheet
W should not be Capital here
okay so what I will do is rho equals to
0
and column equals to zero
okay so I will write here uh
data
equals to
and take us the names just names
we
this random names they are
above let's take
one more
so
for
atom
end data
and WS
dot right
I will explain you line by line don't
worry
row comma
column
comma item
perfect so here what I will do is rho
plus equals to 1 or you can write rho
equals to rho plus 1
w b Dot
close
okay
let's save this
let's see our data is appended or not
okay first I will explain this code to
you
so here I have created one new workbook
like name
no I haven't created so let me create
two
okay
example
okay
so I will run this yeah perfect so yeah
I repeat so here I have created one new
workbook name example
so at row 0 column 0 I am printing my
content values
like for data values using for loop I am
moving to the next iteration of the
values like from anju to cos you and
then mayank then so on
so let's see the results
okay
yeah
so here you can see all the values are
printed so using these chords of line
you can do things like this easily
so as we know how to write multiple
values at the same time what if if you
want to retrieve values of a particular
row or column so for that we need
so we need data for that so let me add
the full data
of let me
copy some data from somewhere
so just give me a minute
okay
is the blank worksheet
let me give me one minute guys
okay what I will take is
fun
I can take this
some values we can copy like 50 values
okay
50 is high enough
see
we have data now
we have data in the simply learn okay
now we can write code for that like how
to retrieve multiple column
so the same
I want
see
let's do WB
DOT active
okay
WS
equals to
WB
spreadsheet
column
in worksheet
a
okay
so what I have to do is just print
column values
column dot value
I hope it will work uh
WB DOT active
workbook object has no attribute active
okay let me see
okay sorry
import open p y Excel Excellence writer
is used for
to create new workbook okay open p y
Excel
dot load
underscore workbook
now it will work I guess
still it is not working
it should work I guess
open pyxl
dot load okay okay sorry my bad here Dot
now it will work okay sheet
too much this is sheet one
you never know
when the sheet will create
one
yeah
you can see all the list of column A is
printed here
all the 50 values and you can match from
here to
all the 50s values okay Trail Travis
Scott
so let me match this
every Scott okay
so
I hope you understand till here if you
have any questions or any query
regarding any code so just put as in
comment or our team will shortly provide
you the correct solution after doing
importing exporting the values and all
let's perform some basic operation like
addition average and all okay
so first we will start with the addition
so I will write here so we need data in
workbook for the addition
so let's take some data from
from which set
the same install data set
so I will take this
this
values are enough
okay
copy it
and let me paste here
so what data we have let's code for it
we have to write here import
open p y Excel
we are doing sum of columns
not column column values values
yeah
like from open
v y Excel import
comma load underscore workbook
so WB equals to load
underscore
a note workbook
copy the path
a
so I will make it active
so here what I'm doing is
I will tell you shortly
so it is in C I guess CRP
which column
of P
Oh e
50
one I guess yeah
P51
B should be capital
equals to
see carefully what I am doing like sum
b 1
1
till
B I guess 50 values we have 50.
okay B should be Capital my bad sorry
okay this is what I am doing and w d is
dot save
where is the same one
so here I have added all the values
which are between in the range of B1 to
B 50.
and what I'm doing is the result will be
displayed on the b51 cell okay so let's
run this code
okay
let's go to the b51
yeah result is coming for verifying the
result is correct or not I won't use
calculator
for this so we can use
this code for the same for average
product and count so let's
do it quickly
okay
so let me just copy paste this stuff
c v this is will be for
average
a spelling mistake average
okay what I will do is just write
capital
Bridge here okay and I will print it to
50. 2.
okay code is almost same just one change
like you have to write instead of sum
you have to write average there
and just okay
let's see then
so now you can see this is the 137 point
something something is the average of
these all values
everything look good
let's see the count
quickly let's
uh we
would is almost same just we have to
okay capital
53
visible okay I this count will give you
how many values are there between the
range till 1 to 50. so let's check let's
run it again and check
if it's working or not yeah 49 values
okay
so
what I am planning is let's do one thing
now guys uh you will tell me how I can
find the product of a particular range
I'm writing question as a command uh as
I did before
so
how to
okay
okay
how to find
product
of numbers
like where
yeah
the range is between
eight a two two a 98.
okay
so I'm
giving you one minute for this just
comment the code for this like
multiplication or else you can reply on
chat you can write just syntax only for
the product
I will suggest you to write a comment
because after this session our team can
easily check who gave the correct answer
and who didn't give the correct answer
don't worry I will provide the answers
at the end of this session or our team
will reply at your comment at the same
time
and those who are watching the video
like we watch normal video
or you are not attending the live
session so you can just pause this video
and comment on how we can find the
product in Excel using python
I am repeating again I will suggest you
to write a comment because after this
session our team can easily check who
gave the correct answer and who didn't
give the correct answer don't worry I
will provide you the answers at the end
of this session or our team will reply
at your comment at the same time
those who are watching this video like
we used to watch normal videos uh so
what you can do is just pause this video
and comment on how we can find the
product in Excel using python
so I guess time is up guys I hope most
of you gave the correct answer by
putting comments we will verify your
comments and we will back to you soon
so moving at the last
of this session let's do something with
charts so what I will do is first I will
move it bit up
yeah
so let me write code for that for the
chart
so I will write from open
p y Excel
import
workbook
um open p y Excel
dot chart
import
reference
comma
line chart
okay we will create line chart WB equals
to workbook
WS equals to
workbook DOT active
so after this I will set the chart title
title equals chart
okay
let me create these rows so rows equals
to
uh I will do is
I will create something like
um
X
okay comma
uh
sale
comma
edgy cell
yes
Oma
not here
comma
and I have to give the values weeks are
Monday
okay let me give this same
what cell should be like 100
comma 200
Tuesday
comma 300
Wednesday
300 comma 400
or the Thursday
400
or 20 30
day
20
comma
at today
60 comma 30.
okay it seems good
let me do
like four
s
okay Row in rows
WS dot append
oh
values
equals to
reference
WS comma min
call
s to 2
comma Main
rho equals to 1 comma Max
column equals to 3
comma Max
underscore row equals to seven
okay it's same skirt okay x dot values
equals to
reference
the score string
equals to
chart
A2
A7
so chart equals to line chart we are
making
so let's add the data to line chart
object
chart
dot add underscore data
values
comma titles
t i t
t i t l yes titles
let's go from underscore data
constructo
yeah
it should be capital
so let's set x axis
chart dot set
categories
X underscore values
okay let's set chart
Dot
let's do
tails
chart Dot
access
equals to
weeks
and chart
dot y
axis
dot title
equals to
root
and veggies
sales
chart dot Legend
dot position
is to B
okay
so WS dot add
underscore chart
comma
H1 cell
so WB okay let me copy this code from
here
so here I am importing reference and
charts Library like this reference and
charts Library
and just giving title as charts you can
see here chart
ah so rows are the data from which we
will make chart
and for Loop is to append the data on
the workbook
chart add data chart add data
values titles from data equals to true
this is for adding data to the line
chart object just giving the title X
action and y axis
and all so let's save this and see
whether chart is visible or not
let's save this
okay what is it going cannot import name
reference from open file
reference okay let me see
error in line two error in line two
okay sorry my bad okay
now it will workbook
fourth number okay W should be capital
should be Capital now it will work I
guess
again that's reference
okay
start values
dot my bad
h a r t
c h a r t
dot save okay WB Dot
okay finally no errors
so let's save this and see whether chart
is visible or not
let me go to this simply learn
yeah you can see chart is coming and it
is it's looks so good if you're an
expiring AIML engineer then there is no
better time to train yourself in
exciting field of machine learning if
you are looking for course that covers
everything from fundamentals to Advanced
Techniques like machine learning
algorithm development and unsupervised
learning look no further than our
Caltech in partnership with IBM so why
wait join now see it's a filling fast
find the course link from the
description box below
hello everyone welcome to this session
I'm Mohan from Simply learn and today
we'll talk about interview questions
now this video probably help you when
you're attending interviews or machine
learning positions and the attempt here
is to probably consolidate 30 most
commonly asked questions and to help you
in answering these questions we tried
our best to give you the best possible
answers but of course what is more
important here is rather than the
theoretical knowledge you need to kind
of add to the answers or supplement your
answers with your own experience so the
responses that we put here are a bit
more generic in nature so that if there
are some Concepts that you are not clear
this video will help you in kind of
getting those Concepts cleared up as
well but what is more important is that
you need to supplement these responses
with your own practical experience okay
so with that let's get started so one of
the first questions that you may face is
what are the different types of machine
learning now what is the best way to
respond to this there are three types of
machine learning if you read any
material you will always be told there
are three types of machine
and what is important is you would
probably be better of emphasizing that
there are actually two main types of
emotional learning which is supervised
and unsupervised and then there is a
third type which is reinforcement learn
so supervised learning is where you have
some historical data and then you feed
that data to your model to learn now you
need to be aware of a keyword that they
will be looking for which is labeled
data right so if you just say pass data
or historical data the impact may not be
so much you need to emphasize on labeled
data so what is labeled data basically
let's say if you are trying to do train
your model for classification you need
to be aware of for your existing data
which class each of the observations
belong to right so that is what is
labeling so it is nothing but a fancy
name you must be already aware but just
make it a point to throw in that keyword
labeled so that will have the right
impact okay so that is what is
supervised learning when you have
existing labeled data which you then use
to train your model that is known as
supervised learning and unsupervised
learning is when you don't have this
labeled data so you have data it is not
labeled so the system has to figure out
a way to do some analysis on this okay
so that is unsupervised learning and you
can then add a few things like what are
the ways of performing a supervised
learning and unsupervised learning and
what are some of the techniques so
supervised learning we we perform or we
do
regression and classification and
unsupervised learning video clustering
and clustering can be of different types
similarly regression can be of different
types but you don't have to probably
elaborate so much if they are asking for
just the different types you can just
mention these and just at a very high
level but if they want you to elaborate
give examples then of course then I
think there is a different question
then the third so we have supervised
then we have unsupervised and then
reinforcement you need to provide a
little bit of information around as well
because it is sometimes a little
difficult to come up with a good
definition for reinforcement so you may
have to little bit elaborate on how
reinforcement learning
right so reinforcement learning works in
in such a way that it basically has two
parts to it one is the agent and the
environment and the agent basically is
working inside of this environment and
it is given a Target that it has to
achieve and every time it is moving in
the direction of the target so the agent
basically has to take some action and
every time it takes an action which is
moving uh the agent towards the Target
right towards a goal a Target is nothing
but a goal then it is rewarded and every
time it is going in a direction where it
is away from the goal then it is
punished so that is the way you can
little bit explain and this is used
primarily or very very impactful or
teaching the system to learn games and
so on examples of this are basically
used in alphago you can throw that as an
example where alphaco used reinforcement
learning to actually learn to play the
game of Go and finally it defeated the
co-world champion all right this much of
information that would be good enough
then there could be a question on
overfitting uh so the question could be
what is overfitting and how can you
avoid it so what is overfitting so let's
first try to understand the concept
because sometimes overfitting Maybe
overfitting is a situation where the
model has kind of memorized the data so
this is an equivalent of memorizing the
data so we can draw an analogy so that
it becomes becomes easy to explain this
now let's say you're teaching a child
about recognizing some fruits or
something like that okay and you're
teaching this child about recognizing
let's say three fruits apples oranges
and pineapples okay so this is a small
child and for the first time you're
teaching the child to recognize fruits
then so what will happen so this is very
much like that is your training data set
so what you will do is you will take a
basket of fruits which consists of
apples oranges and pineapples
and you take this basket to this child
and there may be let's say hundreds of
these fruits so you take this basket to
this child and keep showing each of this
root and then first time obviously the
child will not know what it is so you
show an apple and you say hey this is
Apple then you show maybe an orange and
say this is orange and so on and so and
then again you keep repeating that right
so till that basket is over this is
basically how training work in machine
learning also that's how training works
till the basket is completed maybe 100
fruits you keep showing this child and
then the process what has happened the
child has pretty much memorized these so
even before you finish that basket right
by the time you are halfway through the
child has learned about recognizing the
Apple orange and pineapple now what will
happen after halfway through initially
you remember it made mistakes in
recognizing but halfway through now it
has learned so every time you show a
fruit it will exactly 100 accurately it
will identify it will say the child will
say this is an apple this is an orange
and if you show a pineapple it will say
this is a pineapple
so that means it has kind of memorized
this data now let's say you bring
another basket of fruits and it will
have a mix of maybe apples which were
already there in the previous set but it
will also have in addition to Apple it
will probably have a banana or maybe
another fruit like a jackfruit right so
this is an equivalent of your test data
set which the child has not seen before
some parts of it it probably has seen
like the apples it has seen but this
banana and Jackfruit it has not seen so
then what will happen in the first round
which is an equivalent of your training
data set towards the end it has 100 it
was telling you what the fruits are
right Apple was accurately recognized
orange or I was accurately recognized
and pineapples were accurately
recognized right so that is like a
hundred percent accuracy but now when
you get another a fresh set which we're
not a card of the original one what will
happen all the apples maybe it will be
able to recognize correctly but all the
others like the Jackfruit or the banana
will not be recognized by the child
right so this is an analogy this is an
equivalent of overfitting so what has
happened during the training process it
is able to recognize or reach 100
accuracy maybe very high accuracy okay
and we call that as very low loss right
so that is the technical term so the
loss is pretty much zero and accuracy is
pretty much hundred percent whereas when
you use testing there will be a huge
error which means the loss will be
pretty high and therefore the accuracy
will be also low okay this is known as
overfitting this is basically a process
where training is done training
processes it goes very well almost
reaching 100 accuracy but while testing
it really drops down now how can you
avoid it so that is the extension of
this question there are multiple ways of
avoiding overfitting there are
techniques like what you call
regularization that is the most common
technique that is used for avoiding
overfitting and within regularization
there can be a few other subtypes like
drop out in case of neural networks and
a few other examples but I think if you
give example or if you give
regularization as the technique probably
that should be sufficient so so there
will be some questions where the
interviewer will try to test your
fundamentals and your knowledge and
depth of knowledge and so on and so and
so forth and then there will be some
questions which are more like trick
questions that will be more to stop you
okay then the next question is around
the methodology so when we are
performing machine learning training we
split the data into training and test
right so this question is around that so
the question is what is training set and
test set machine learning model and how
is the split
that question can be
can be learning when we are trying to
train the model so we have a three-step
process we train the model and then we
test the model and then once we are
satisfied with the test only then we
deploy the model so what happens in the
train and test is that you remember the
labeled data so let's say you have 1000
records with labeling information now
one way of doing it is you use all the
Thousand records for training and then
maybe right which means that you have
exposed all this thousand records during
the training process and then you take a
small set of the same data and then you
say okay I will test it with this okay
and then you probably what will happen
you may get some good results all right
but there is a flaw there what is the
flaw this is very similar to human
beings it is like you are showing this
model the entire data as a part of
training okay so obviously it has become
familiar with the entire data so when
you're taking a part of that again and
you're saying that I want to test it
obviously you will get good results so
that is not a very accurate way of
testing so that is the reason what we do
is we have the label data of this
thousand records or whatever we set
aside before starting the training
process we set aside a portion of that
data and we call that test set and the
remaining we call as training set and we
use only this for training our model now
the training process remember is not
just about passing one round of this
data set so let's say now your training
set has 800 records it is not just one
time you pass this 800 records what
you'd normally do is you actually as a
part of the training you may ask this
data through the model multiple times so
this thousand records may go through the
model maybe 10 15 20 times still the
training is perfect till the accuracy is
high till the errors are minimized okay
now so
fine which means that your that is what
is known as the model has seen your data
and gets familiar with your data and now
when you bring your test data what will
happen is this is like some new data
because that is where the real test is
now you have trained the model and now
you are testing the model with some data
which is kind of new that is like a
situation like like a realistic
situation because when the model is
deployed that is what will happen it
will receive some new data not the data
that it has already seen right so this
is a realistic test so you put some new
data so this data which you have set
aside is for the model it is new and if
it is able to accurately predict the
values that means your training has
worked okay the model got trained
properly but let's say while you are
testing this with this test data you're
getting lot of errors that means you
need to probably either change your
model or retrain with more data and
things like
coming back to the question of how do
you split this what should be the ratio
there is no fixed number again this is
like individual preferences some people
split it into 50 50 test and 50 training
Some people prefer to have a larger
amount for training and a smaller amount
for test so they can go by either 60 40
or 70 30 or some people even go with
some odd numbers like 65 35 or
63.33 and 33 which is like one third and
two-thirds so there is no fixed rule
that it has to be something
has to be this you can go by your
individual preference all right then you
may have questions around data handling
data manipulation or or what you call
data management or Preparation so these
are all some questions around that area
there is again no one answer one single
good answer to this it really varies
from situation to situation and
depending on what exactly is the problem
what kind of data it is how critical it
is what kind of data is missing and what
is the type of corruption so there are a
whole lot of things this is a very
generic question and therefore you need
to be a little careful about responding
to this as well so you probably have to
illustrate this again if you have
experience in doing this kind of work in
handling data you can illustrate with
examples saying that I was on one
project where I received this kind of
data these were the columns where data
was not filled or these were the this
many rows where the data was missing
that would be in fact a perfect way to
respond to this question but if you
don't have that obviously you have to
provide some good answer I think it
really depends on what exactly the
situation is and there are multiple ways
of handling this data or corrupt data
now let's take a few examples now let's
say you have data where some values in
some of the columns are missing and you
have pretty much half of your data
having these missing values in terms of
number of rows okay that could be one
situation another situation could be
that you have records or data missing
but when you do some initial calculation
how many records are corrupt or how many
rows or observations as we call it has
this missing data let's assume it is
very minimal like 10 percent okay now
between these two cases how do we so
let's assume that this is not a mission
critical situation and in order to fix
this 10 percent of the data the effort
that is required is much higher and
obviously effort means also time and
money right so it is not so Mission
critical and it is okay to let's say get
rid of these records so obviously one of
the easiest ways of handling the data
part or missing data is remove those
records or remove those observations
from your analysis so that is the
easiest way to do but then the downside
is as I said in as in the first case if
let's say 50 percent of your data is
like that because some column or the
other is missing so it is not like every
in every place in every Row the same
column is missing but you have in maybe
10 percent of the records column one is
missing and another 10 percent column
two is missing another 10 percent column
three is missing and so on and so forth
so it adds up to maybe half of your data
set so you cannot completely remove half
of your data set then the whole purpose
is lost okay so then how do you handle
then you
need with
way of fill filling up this data with
some meaningful value right that is one
way of handling so when we say
meaningful value what is that meaningful
value let's say for a particular column
you might want to take a mean value for
that column and fill wherever the data
is missing fill up with that mean value
so that when you're doing the
calculations your analysis is not
completely way off so you have values
which are not missing first of all so
your system will work number two these
values are not so completely out of
whack that your whole analysis goes for
a task right there may be situations
where if the missing values instead of
putting mean may be a good idea to fill
it up with the minimum value or with a
zero so or with a maximum value again as
I said there are so many possibilities
so there is no like one correct answer
for this you need to basically talk
around this and illustrate with your
experience as I said that would be the
best otherwise this is how you need to
handle this
okay so then the next question can be
how can you choose a classifier based on
a training set data size again this is
one of those questions where you
probably do not have like a one size
fits-all answer first of all you you may
not let's say decide your classifier
based on the training set size maybe not
the best way to decide the type of the
classifier and even if you have to there
are probably some thumb rules which we
can use but then again every time so in
my opinion the best way to respond to
this question is you need to try out few
classifiers irrespective of the size of
the data and you need to then decide on
your particular situation which of these
classifiers are the right ones this is a
very generic issue so you will never be
able to just buy if somebody defines a
problem to you and somebody even if they
show the data to you or tell you what is
the data or even the size of the data I
don't think there is a way to really say
that yes this is the classifier that
will work here no that's not the right
way so you need to still you know test
it out get the data try out a couple of
classifiers and then only you will be in
a position to decide which classifier to
use you try out multiple classifiers see
which one gives the best accuracy and
only then you can decide then you can
have a question around confusion Matrix
so the question can be explained
confusion Matrix
Matrix so confusion Matrix I think the
best way to explain it is by taking an
example and drawing like a small diagram
otherwise it can really become tricky so
my suggestion is to take a piece of pen
and paper and explain it by drawing a
small Matrix and confusion Matrix is
about to find out this is used
especially in classification learning
process and when you get the results
when the our model predicts the results
you compare it with the actual value and
try to find out what is the accuracy
okay so in this case let's say this is
an example of a confusion Matrix and it
is a binary Matrix so you have the
actual values which is the labeled data
right and which is so you have how many
yes and how many know so you have that
information and you have the predicted
values how many yes and how many know
right so the total actual values the
total yes is 12 plus 130 and they are
shown here and the actual value those
are 9 plus 3 12 okay so that is what
this information here is so this is
about the actual and this is about the
predicted similarly the predicted values
there are yes are 12 plus 3 15 yeses and
no are 1 plus 9 10 nodes okay so this is
the way to look at this confusion Matrix
okay and uh out of this what is the
meaning converting so there are two or
three things that needs to be explained
out right the first thing is for a model
to be accurate the values across the
diagonal should be high like in this
case right that is one number two the
total sum of these values is equal to
the total observations in the test data
set so in this case for example you have
12 plus 3 15 plus 10 25 so that means we
have 25 observations in our test data
set okay so these are the two things you
need to First explain that the total sum
in this Matrix numbers is equal to the
size of the test data set and the
diagonal values indicate the accuracy so
by just by looking at it you can
probably have a idea about is this an
accurate model is the model being
accurate if they're all spread out
equally in all these four boxes that
means probably the accuracy is not very
good okay now how do you calculate the
accuracy itself right how do you
calculate the accuracy itself so it is a
very simple mathematical calculation you
take some of the diagonals right so in
this case it is 9 plus 12 21 and divide
it by the total so in this case what
will it be let me take a pen so your
your diagonal values is equal to if I
say t is equal to 12 plus 9 so that is
21 right and the total data set is equal
to right we just calculated it is 25 so
what is your accuracy it is 21 by your
accuracy is equal to 21 by 25 and this
turns out to be about 85 percent right
so this is 85 percent so that is our
accuracy okay so this is the way you
need to explain draw diagram Give an
example and maybe it may be a good idea
to be with an example so that it becomes
easy for you don't have to calculate
those numbers on the Fly Right so a
couple of uh hints are that you take
some numbers which are with which add up
to 100 that is always a good idea so you
don't have to really do this complex
calculations so the total value will be
100 and then diagonal values you divide
once you find the diagonal values that
is equal to your percentage okay all
right so the next question can be a
related question about false positive
and false negative so what is false
positive and what is false negative now
once again the best way to explain this
is using a piece of paper and then
otherwise it will be pretty difficult to
so we use the same example of the
confusion Matrix and we can explain that
so A confusion Matrix looks somewhat
like this and when we just take
a remote like this and we continue with
the previous example where this is the
actual value this is the predicted value
and in the actual value we have 12 plus
1 13 yeses and three plus nine twelve
nose and the predicted values there are
12 plus the 15 yeses and 1 plus 9 10
loss okay now in this particular case
which is the false positive what is a
false positive first of all the second
word which is positive okay is referring
to the predicted value so that means the
system has predicted it as a positive
but the real value so this is what the
false comes from but the real value is
not positive okay that is the way you
should understand this term false
positive or even false negative so false
positive so positive is what your system
has predicted so where is that system
predicted this is the one positive is
what yes so you basically consider this
row okay now if you consider this row so
this is this is all positive values this
entire row is positive values okay now
the false positive is the one which
where the value actual value is negative
predicted value is positive but the
actual value is negative so this is a
false positive right and here is a true
positive so the predicted value is
positive and the actual value is also
positive okay I hope this is making
sense now let's take a look at what is
false negative false negative so
negative is the second term that means
that is the predicted value that we need
to look for so which are the predicted
negative values this row corresponds to
predicted negative values all right so
this row corresponds to predicted
negative values and what they are asking
for false so this is the row for
predicted negative values and the actual
value is this one right this is
predicted negative and the actual value
is also negative therefore this is a
true negative so the false negative is
this one predicted is negative but
actual is positive right so this is the
false negative so this is the way to
explain and this is the way to look at
false positive and false negative same
way there can be true positive and true
negative as well so again positive the
second term you will need to use to
identify the predicted row right so if
we say true positive positive we need to
take for the predicted part so predicted
positive is here okay and then the first
term is for the actual so true positive
so true in case of actual is yes right
so true positive is this one okay and
then in case of actual the negative now
we are talking about let's say true
negative true negative negative is this
one and the true comes from here so this
is true negative right 9 is true
negative the actual value is also
negative and the predicted value is also
negative okay so that is the way you
need to explain this the terms false
positive false negative and true
positive true negative then uh you might
have a question like what are the steps
involved in the machine learning process
or what are the three steps in the
process of developing a machine learning
model right so it is around the
methodology that is applied so basically
the way you can probably answer in your
own words but the way the model
development of the machine learning
model
so first of all you try to understand
the problem and try to figure out
whether it is a classification problem
or a regression problem based on that
you select a few algorithms and then you
start the process of training these
models
so you can either do that or you can
after due diligence you can probably
decide that there is one particular
algorithm which is more suitable usually
it happens through trial and error
process but at some point you will
decide that okay this is the model we
are going to use okay so in that case we
have the model algorithm and the model
decided and then you need to do the
process of training the model and
testing the model and this is where if
it is supervised learning you split your
data the label data into training data
set and test data set and you use the
training data set to train your model
and then you use the test data set to
check the accuracy whether it is working
fine or not so you test the model before
you actually put it into production
right so once you test the model you're
satisfied it's working fine then you go
to the next level which is putting it
for production and then in production
obviously new data will come and
inference happens so the model is
readily available and only thing that
happens is new data comes and the model
predicts the values whether it is
regression
you know so this can be an iterative
process so it is not a straightforward
process where you do the training
through the testing and then you move it
to production now so during the training
and test process there may be a
situation where because of either
overfitting or things like that the test
doesn't go through which means that you
need to put that back into the training
process so that can be a an iterative
process not only that even if the
training and test goes through properly
and you deploy the model in production
there can be a situation that the data
that actually comes the real data that
comes with that this model is failing so
in which case you may have to once again
go back to the drawing board or
initially it will be working fine but
over a period of time maybe due to the
change in the nature of the data once
again the accuracy will deteriorate so
that is again a recursive process so
once in a while you need to keep
checking whether the model is working
fine or not and if required you need to
tweak it and modify it and so on and so
forth so let net this is a continuous
process of tweaking the model and
testing it and making sure it is up to
date then you might have question around
deep learning so because deep learning
is now associated with AI artificial
intelligence and so on so can be as
simple as what is deep learning so I
think the best way to respond to this
could be deep learning is a part of
machine learning and then obviously the
question would be then what is the
difference right so deep learning you
need to mention there are two key parts
that interviewer will be looking for
when you are defining deep learning so
first is of course deep learning is a
subset of machine learning so machine
learning is still the bigger let's say
scope and deep learning is one one part
of it so then what exactly is the
difference deep learning is primarily
when we are implementing these our
algorithms or when we are using neural
networks for doing our training and
classification and regression and all
that right so when we use neural network
then it is considered as deep learning
and the term deep comes from the fact
that you can have several layers of
neural networks and these are called
Deep neural networks and therefore the
term deep you know deep learning the
other difference between machine
learning and deep learning which the
interviewer may be wanting to hear is
that in case of machine learning the
feature engineering is done manually
what do we mean by feature engineering
basically when we are trying to train
our model we have our training data
right so we have our training label data
and this data has several let's say if
it is a regular table it has several
columns now each of these columns
actually has information about a feature
right so if we are trying to predict the
height weight and so on and so forth so
these are all features of human beings
let's say we have census data and we
have all this so those are the features
now there may be probably 50 or 100 in
some cases there may be 100 such
features now all of them do not
contribute to our model right so we as a
data scientist we have to decide whether
we should take all of them all the
features or we should throw away some of
them because again if we take all of
them number one of course your accuracy
will probably get affected but also
there is a computational part so if you
have so many teachers and then you have
so much data it becomes very tricky so
in case of machine learning we manually
take care of identifying the features
that do not contribute to the learning
process and thereby we eliminate those
features and so on right so this is
known as feature engineering and in
machine learning we do that manual
whereas in deep learning where we use
neural networks the model will
automatically determine which features
to use and which to not use and
therefore feature engineering is also
done automatically so this is a
explanation these are two key things
probably will add value to your response
all right so the next question is what
is the difference between or what are
the differences between machine learning
and deep learning so here this is a
quick comparison table between machine
learning and deep learning and in
machine learning learning enables
missions to take decisions on their own
based on past data so here we are
talking primarily of supervised learning
and it needs only a small amount of data
for training and then works well on low
end system so you don't need a large
machine and most features need to be
identified in advance and manually coded
so basically the feature engineering
part is done manually and the problem is
divided into parts and solved
individually and then combined so that
is about the machine learning part in
deep learning deep learning basically
enables machines to take decisions with
the help of artificial neural networks
so here in deep learning we use neural
line so that is the key differentiator
between machine learning and deep
learning and usually deep learning
involves a large amount of data and
therefore the training also requires
usually the training process requires
high-end machines
needs a lot of computing power and the
Machine learning features are the or the
feature engineering is done
automatically so the neural networks
takes care of doing the feature
engineering as well and in case of deep
learning therefore it is said that the
problem is handled end-to-end so this is
a quick comparison between machine
learning and deep learning in case you
have that kind of then you might get a
question around the uses of machine
learning or some real life applications
of machine learning in modern business
the question may be worded in different
ways but the meaning is how exactly is
machine learning used or actually
supervised machine learning it could be
a very specific question around
supervised decision learning so this is
like give examples of supervised machine
learning use of supervised machine
learning in modern business so that
could be the next so there are quite a
few examples or quite a few use cases if
you will for supervised machine learning
the very common one is email spam
detection so you want to train your
application or your system to detect
between spam and non-spam so this is a
very common business application of a
supervised machine learning so how does
this work the way it works is that you
obviously have historical data above of
your emails and they are categorized as
spam and not spam so that is what is the
labeled information and then you feed
this information or the all these emails
as an input to your model right and the
model will then get trained to detect
which of the emails are to detect which
is Spam and which is not spam so that is
the training process and this is
supervised machine learning because you
have labeled data you already have
emails which are tagged as spam or not
and then you use that to train your
model right so this is one example now
there are a few industry specific
applications for supervised machine
learning one of the very common ones is
a healthcare diagnostic in healthcare
Diagnostics you have these images and
you want to train models to detect
whether from a particular image whether
it can find out if the person is sick or
not whether a person has cancer or not
right so this is a very good example of
supervised machine learning here the way
it works is that existing images it
could be x-ray images it will be MRI or
any of these images are available and
they are saying that okay this x-ray
image is deflective of the person as an
illness or it could be cancer whichever
illness right so it is stacked as
defective or clear or good image and
effective image something like that so
we come up with a binary or it could be
multi-class as well saying that this is
defective to 10 percent this is 25
percent and so on but let's keep it
simple you can give an example of just a
binary classification that would be good
enough so you can say that in healthcare
Diagnostics using which we need to
detect whether a person is ill or
whether
cancer on there are not here the way it
works is you feed labeled images and you
allow the model to learn from that so
that when New Image is fed it will be
able to predict whether this person is
having that illness or not having cancer
or not right so I think this would be a
very good example for supervised machine
learning in modern business all right
then we can have a question like so
we've been talking about supervised and
unsupervised then so there can be a
question around semi-supervised machine
learning so what is semi-supervised
machine now semi-supervised learning as
the name suggests it falls between
supervised learning and unsupervised
learning but for all practical purposes
it is considered as a art of supervised
learning and the reason this has come
into existence is that in supervised
learning you need labeled data so all
your data for training your model has to
be labeled now this is a big problem in
many Industries or in many under many
situations getting the label data is not
that easy because there's a lot of
effort in labeling this data let's take
an example of a diagnostic images
just
three images now there are actually
millions of x-ray images available all
over the world but the problem is they
are not labeled so their images are
there but whether it is effective or
whether it is good and information is
not available along with it right in a
form that it can be used by a machine
which means that somebody has to take a
look at these images and usually it
should be like a doctor and then say
that okay yes this image is clean and
this image is cancerous and so on and so
forth now that is a huge effort by
itself so this is where semi-supervised
learning comes into play so what happens
is there is a large amount of data maybe
a part of it is labeled then we try some
techniques to label the remaining part
of the data so that we get completely
labeled data and then we train our model
so I know this is a little long winding
explanation but unfortunately there is
no quick and easy definition for
semi-supervised machine learning this is
the only way probably to explain this
concept
we may have another question as what are
unsupervised machine learning techniques
or what are some of the techniques used
for performing unsupervised machine
learning so it can be worded in
different ways so how do we answer this
question so unsupervised learning you
can say that there are two types
clustering and Association and
clustering is a technique where similar
objects are put together and there are
different ways of finding similar
objects so their characteristics can be
measured and if they have in most of the
characteristics if they are similar then
they can be put together
then Association you can I think the
best way to explain Association is with
an example in case of Association you
try to find out how the items are linked
to each other
for example if somebody bought maybe a
laptop or the person has also purchased
a mouse so this is more in an e-commerce
scenario for example so you can give
this as an example so people who are
buying and laptops are also buying a
mouse so that means there is an
association between laptops and
people who are buying bread are also
buying butter so that is a Association
that can be created so this is
unsupervised learning one of the
techniques
all right then we have very fundamental
question what is the difference between
supervised and unsupervised machine
learning so machine learning these are
the two main types of machine learning
supervised and unsuest and in case of
supervised and again here probably the
key word that the person may be wanting
to hear is labeled data now very often
people say we have historical data and
if we run it it is supervised and if we
don't have historical data yes but you
may have historical data but if it is
not labeled then you cannot
so it is it's very key to understand
that we put in that keyword label okay
so when we have labeled data for
training our model then we can use
supervised learning and if we do not
have labeled data then we use
unsupervised learning and there are
different algorithms available to
perform both of these
so there can be another question a
little bit more theoretical and
conceptual in nature this is about
inductive machine learning and
active machine the question can be what
is the difference between inductive
machine learning and deductive machine
learning or somewhat in that manner so
that the exact phrase or exact question
very they can ask for examples and
things like that but that could be the
question so let's first understand what
is inductive and deductive trade
inductive training is induced by
somebody and you can illustrate that
with a small example I think that always
helps so whenever you're doing some
explanation try as much as possible as I
said to give examples from your work
experience or give some analogies and
that will also help a lot in explaining
as well and for the interviewer also to
understand so here we'll take an example
or rather we will use an analogy so
inductive training is when we induce
some knowledge or the learning process
into a person without the person
actually experiencing it what can be an
example so we can probably tell the
person or show a person a video that
fire can burn the thing burn his finger
or fire can cause damage so what is
happening here this person has never
probably seen a fire or never seen
anything getting damaged by fire but
just because he has seen this video he
knows that okay fire is dangerous and if
a fire can cause damage right so this is
inductive learning compared to that what
is deductive learning so here you draw
conclusion or the person draws
conclusion out of experience so we will
stick to the analogy so compared to the
showing a video Let's assume a person is
allowed to play with fire right and then
he figures out that if he puts his
finger it's burning or you throw
something into the fire it burns so he
is learning through experience so this
is known as deductive learning okay so
you can have applications or models that
can be trained using inductive learning
or deductive
all right I think probably that
explanation will
sufficient the next question is are knnn
and K means clustering similar to one
another or are they same right because
the the letter K is kind of common
between them okay so let us take a
little while to understand what these
two are one is KNN another is K means a
KNN stands for K nearest neighbors and K
means of course is the clustering
mechanism now these two are completely
different except for the letter K being
common between them KN is completely
different K means clustering is complete
KNN is a classification process and
therefore it comes under supervised
learning whereas k-means clustering is
actually a unsupervised okay when you
have K and N when you want to implement
k n n which is basically K nearest
neighbors the value of K is a number so
you can say k is equal to 3 you want to
implement k n n with K is equal to 3 so
which means that it performs the
classification in such a way that how
does it perform the classification so it
will take three nearest objects and
that's why it's called nearest neighbor
so basically uh based on the distance it
will try to find out its nearest objects
that are let's say three of the nearest
objects and then it will check whether
the class they belong to which class
right so if all three belong to one
particular class obviously this new
object is also classified as that
particular but it is possible that they
may be from two or three different
classes okay so let's say they are from
two classes and then they are from two
classes now usually you take a odd
number you assign odd number two so if
there are three of them and two of them
belong to one class and then one belongs
to another class so this new object is
assigned to the class to which the two
of them belong now the value of K is
sometimes tricky whether should you use
three should you use five should you use
seven that can be tricky because the
ultimate classification can also vary so
it's possible that if you're taking K
as3 the object is probably in one
particular class but if you take K is
equal to 5 maybe the object will belong
to a different class because when you're
taking three of them probably two of
them belong to a class one and one
belong to class two whereas when you
take five of them it is possible that
only two of them belong to class one and
the three of them belong to Class 2 so
which means that this object will belong
to class 2 right so you see that so it
is the class allocation can vary
depending on the value of K now K means
on the other hand is a clustering
process and it is unsupervised where
what it does is the system will
basically identify how the objects are
how close the objects are with respect
to some of their features okay and but
Sim ilarity of course is the the letter
K and in case of K means also we specify
its value and it could be three or five
or seven there is no technical limit as
such but it can be any number of
clusters that you can create okay so
based on the value that you provide the
system will create that many clusters of
similar objects there is a similarity to
that extent that K is a number in both
the cases but actually these two are
completely different processes
we have what is known as naive based
classifier and people often get confused
thinking that naive base is the name of
the person who found this classifier or
who developed this classifier which is
not 100 True base is the name of the
person bais is the name of the person
but naive is not the name of the person
right so naive is basically an English
word and that has been added here
because of the nature of this particular
classifier an ibase classifier is a
probability based classifier and it
makes some assumptions that presence of
one feature of a class is not related to
the presence of any other feature of
maybe other classes right so which is
not a very strong or not a very what do
you say accurate assumption because
these features can be related and so on
but even if we go with this assumption
this whole algorithm works very well
even with this assumption and uh that is
the good side of it but the term comes
from that
that's the explanation that you can
then there can be question around
reinforcement learning can be
paraphrased in multiple ways one could
be can you explain how a system can play
a game of chess using reinforcement
learning or it can be any game so the
best way to explain this is again to
talk a little bit about what
reinforcement learning is about and then
elaborate on that to explain the process
so first of all reinforcement learning
has an environment and an agent and the
agent is basically performing some
actions in order to achieve a certain
goal and this goals can be anything
either it is related to game then the
goal could be that you have to score
very high score
value High number or it could be that
your number of lives should be as high
as possible don't lose life so this
could be some of them a more advanced
examples could be for driving the
automotive industry self-driving cars
they actually also make use of
reinforcement learning to teach the car
how to navigate through the roads and so
on fourth that is also another example
now how does it work so if a system is
basically there is an agent in the
environment and every time the agent
takes a step or performs a task which is
taking it towards the goal the final
goal let's say to maximize the score or
to minimize the number of lives and so
on or minimize that
well it is rewarded and every time it
takes a step which goes against that
core right contrary or in the reverse
Direction it is penalized okay so it is
like a carrot and last
now how do you use this to create a game
of chess so to create a system to play a
game of chess now the way this works is
and this could probably go back to this
alphago example where alphaco defeated a
human Champion so the way it works is in
reinforcement learning the system is
allowed for example in this case we are
talking about Chess so we allow the
system to first of all watch playing a
game of chess it could be with a human
being or it could be the system itself
there are computer games of Chess right
so either this new learning system has
to watch that game or watch a human
being play the game because this is
reinforcement learning is pretty much
all visual so when you're teaching the
system to play a game the system will
not actually go behind the scenes to
understand the logic of your software of
this game or anything like that it is
just visually watching the screen and
then it learns okay so reinforcement
learning to a large extent works on that
so you need to create a mechanism
whereby your model will be able to watch
somebody playing the game and then you
allow the system also to start playing
the game so it pretty much starts from
scratch okay and as it moves forward it
it's at right at the beginning the
system really knows nothing about the
game of chess okay so initially it is a
clean slate it just starts by observing
how you are playing so it will make some
random moves and keep losing badly but
then what happens is over a period of
time so you need to now allow the system
or you need to play with this system not
just one two three four or five times
but hundreds of times thousands of times
maybe even hundreds of thousands of
times and that's exactly how alphago has
done it played millions of games between
itself and the system right so for the
game of chess also you need to do
something like that you need to allow
the system to play chess and and learn
on its own over a period of repetition
so I think you can probably explain it
this much to this extent and I
now this is another question which is
again somewhat similar but here the size
is not coming into picture so the
question is how will you know which
machine learning algorithm to choose for
your classification problem now this is
not only classification problem it could
be a regression problem I would like to
generalize this question so if somebody
asks you how will you choose how will
you know which algorithm to use the
simple answer is there is no way you can
decide exactly saying that this is the
algorithm I am going to use in a variety
of situations there are some guidelines
like for example you will obviously
depending on the problem you can say
whether it is a classification problem
or a regression problem and then in that
sense you are kind of restricting
yourself to if it is a classification
problem there are you can only apply a
classification algorithm right to that
extent you can probably let's say limit
the number of algorithms but now within
the classification algorithms you have
decision trees you have SPM you have
logistic regression is it possible to
outright say yes so for this particular
problem since you have explained this
now this is the exact algorithm that you
can use that is
okay so we have to try out a bunch of
algorithms see which one gives us the
best performance best accuracy and then
decide to go with that particular
algorithm so in machine learning a lot
of it happens through trial and error
there is no real possibility that
anybody can just by looking at the
problem or understanding the problem
tell you that okay in this particular
situation this is exactly the algorithm
that you should
then the questions may be around
application of machine learning and this
question is specifically around how
Amazon is able to recommend other things
to buy so this is around recommendation
engine how does it work how does a
recommendation engine work so is
basically the question is all about so
the recommendation engine again Works
based on various inputs that are
provided obviously something like uh you
know Amazon a website or e-commerce site
like Amazon collects a lot of data
around the customer Behavior who is
purchasing what and if somebody is
buying a particular thing they are also
buying something else so this kind of
Association right so this is the
unsupervised learning we talked about
they use this to associate and Link or
relate items and that is one part of it
so they kind of build association
between items saying that somebody
buying this is also buying this that is
one part of it then they also profile
the users right based on their age their
gender their geographic location they
will do some profiling and then when
somebody is logging in and when somebody
is shopping kind of a mapping of these
two things are done they try to identify
obviously if you have logged in then
they know who you are and your
information is available like for
example your age maybe your agenda and
where you're located what you purchased
earlier right so all this is taken and
the recommendation engine basically uses
all this information and comes up with
recommendations for a particular user so
that is how the recommendation engine
work all right then the question can be
something very basic like when will you
go for classification versus regression
right when do you do classification
instead of instead of regression or when
we use classification instead of
regression now yes so so this is
basically going back to the
understanding of the basics of
classification and regression so
classification is used when you have to
identify or categorize things into
discrete classes so the best way to
respond to this question is to take up
some examples and use it otherwise it
can become a little tricky the question
may sound very simple but explaining it
can sometimes be very tricky in case of
regression we use of course there will
be some keywords that they will be
looking for so just you need to make
sure you use those keywords one is the
discrete values another is the
continuous value so for regression if we
are trying to find some continuous
values you use regression whereas if you
are trying to find some discrete values
you use classification and then you need
to illustrate what are some of the
examples so classification is like let's
say there are images and you need to put
them into classes like cat dog elephant
tiger something like that so that is the
classification problem or it can be that
is a multi-class classification problem
it could be binary classification
problem like for example whether a
customer will buy or he will not buy
that is a classification binary
classification it can be in the weather
forecast area now weather forecast is
again combination of regression and
classification because on the one hand
you want to predict whether it's going
to rain or not it's a classification
problem that's a binary classification
right whether it's going to rain or not
rain however you also have to predict
what is going to be the temperature
tomorrow right now temperature is a
continuous value you can't answer the
temperature in a yes or no kind of a
response right so what will be the
temperature tomorrow so you need to give
a number which can be like 20 degrees 30
degrees or whatever right so that is
where you use regression one more
example is stock price prediction so
that is where again you will use
regression so these are the various
examples so you need to illustrate with
examples and make sure you include those
keywords like discrete and container so
the next question is more about a little
bit of a design related question to
understand your Concepts and things like
that so it is how will you design a spam
filter so how do you basically design
our developers
so I think the main thing here is he's
looking at probably understanding your
Concepts in terms of uh what is the
algorithm you will use or what is your
understanding about difference between
classification and regression
aggression things like that right and
the process of course the meteorology
and the process so the best way to go
about responding to this is we say that
okay this is a classification problem
we want to find out whether an email is
a spam or not spam so that we can apply
the filter accordingly so first thing is
to identify what type of a problem it is
so we have identified that it is a
classification then the second step may
be to find out what kind of algorithm to
use now since this is a binary
classification problem logistic
regression is a very common very common
algorithm but however right as I said
earlier also we can never say that okay
for this particular problem this is
exactly the algorithm that we can use so
we can also probably try decision trees
or even support Vector machines for
example svm so we will kind of list down
a few of these algorithms and we will
say okay we want to we would like to try
out these algorithms and then we go
about taking your historical data which
is the labeled data which are marked so
you will have a bunch of emails and then
you split that into training and test
data sets you use your training data set
to train your model that or your
algorithm that you have used or rather
the model actually so and you actually
will have three models let's say you are
trying to test out three algorithms so
you will obviously have three models so
you need to try all three models and
test them out as well see which one
gives the best accuracy and then you
decide that you will go with that model
okay so training and test will be done
and then you zero in on one particular
model and then you say okay this is the
model will you use we will use and then
go ahead and Implement that or put that
in production so that is the way you
design a spam the next question is about
random Forest what is random form so
this is a very straightforward question
however the response you need to be
again a little careful while we all know
what is random Forest explaining this
can sometimes be tricky so one thing is
random Forest is kind of in one way it
is an extension of decision trees
because it is basically nothing but you
have multiple decision trees and trees
will basically we will use for doing if
it is classification mostly it is
classification you will use the trees
for classification and then you use
voting for finding the final so that is
the underlyings but how will you explain
this how will you respond to this so
first thing obviously we will say that
random Forest is one of the algorithms
and the more important thing that you
need to probably the interviewer is is
waiting to here is Ensemble learner
right so this is one type of Ensemble
learner what is Ensemble learner
Ensemble learner is like a combination
of algorithms so it is a learner which
consists of more than one algorithm or
more than one or maybe models okay so in
case of random Forest the algorithm is
the same but instead of using one
instance of it we use multiple instances
of it and we sir in a way that is a
random Forest is an ensemble learner
there are other types of Ensemble
Learners where we have like reuse
different algorithms itself so you have
one maybe logistic regression and a
decision tree combined together and so
on and so forth or there are other ways
like for example splitting the data in a
certain way and so on so that's all
about Ensemble we will not go into that
but random Forest itself I think the
interviewer will be happy to
do word Ensemble Learners and so then
you go and explain how the random Forest
works so if the random Forest is used
for classification then we use what is
known as a voting mechanism so basically
how does it work let's say your random
Forest consists of 100 trees okay and
each observation you pass through this
forest and each observation let's say it
is a classification problem binary
classification zero or one and you have
100 trees now if 90 trees say that it is
a zero and ten of the trees say it is a
one you take the majority you may take a
vote and since 90 of them are saying
zero you classify this as zero then you
take the next observation and so on so
that is the way a random Forest works
for classification if it is a regression
problem it's somewhat similar but the
only thing is instead of what what we
will do is sorry in regression remember
what happens you actually calculate a
value right so for example you're using
regression to predict the temperature
and you have 100 trees and each tree
obviously will probably predict a
different value of the temperature they
may be close to each other but they may
not be exactly the same value so these
hundred trees so how do you now find the
actual value the output for the entire
Forest right so you have outputs of
individual trees which are a part of
this Forest but then you need to find
the final output of the forest itself so
how do you do that so in case of
regression you take like an average or
the mean of all the 100 trees right so
this is also a way of reducing the error
so maybe if you have only one tree and
that one tree makes a error it is
basically hundred percent wrong or 100
right right but if you have on the other
hand if you have a bunch of trees you
are basically dedicating that and
reducing that error okay so that is the
way random Forest works so the next
question is considering the long list of
machine learning algorithms how will you
decide on which one to use so once again
here there is no way to outright say
that this is the algorithm that we will
use for a given data set this is a very
good question but then the response has
to be like again there will not be a
one-size-fits all so we need to first of
all you can probably shorten the list in
terms of by saying okay whether it is a
classification problem or it is a
regression problem to that extent you
can probably shorten the list because
you don't have to use all of them if it
is a classification problem you only can
pick from the classification algorithm
right so for example if it's a
classification you cannot use linear
regression algorithm or if it is a
regression problem you cannot use svm or
maybe now you can use svm but maybe a
logistic regression right so to that
extent you can probably shorten the list
but still you will not be able to 100
decide on saying that this is the exact
algorithm that I am going to use so the
way to go about this you choose a few
algorithms based on what the problem is
you try out your data you train some
models of these algorithms check which
one gives you the lowest error or the
highest accuracy and based on that you
choose that particular algorithm okay
all right then there can be questions
around bias and variance so the question
can be what is bias and variance in
machine learning uh so you just need to
give out a definition for each of these
for example a bias in machine learning
it occurs when the predicted values are
far away from the actual value so that
is the bias okay and whereas they are
all all the values are probably they are
far off but they are very near to each
other though the predicted values are
close to each other right while they are
far off from the actual value but they
are close to each other you see the
difference so that is
and then the other part is your variance
now variance is when the predicted
values are all over the place right so
the variance is high that means it may
be close to the Target but it is kind of
very scattered so the point the
predicted values are not close to each
other right in case of bias the
predicted values are close to each other
but they are not close to the Target but
here they may be close to the Target but
they may not be close to each other so
they are a little bit more scattered so
that is what in case of a variance okay
then the next question is about again
related to bias and variance what is the
trade-off between bias and variance yes
I think this is a interesting question
because these two are heading in
different directions so for example if
you try to minimize the bias variance
will keep going high and if you try to
minimize the variance bias will keep
going high and there is no way you can
minimize both of them so you need to
have a trade-off saying that okay this
is the level at which I I will have my
bias and this is the level at which I
will have variance so the trade-off is
that pretty much attack you you decide
what is the level you will all rate for
your bias and what is the level you will
tolerate for variance and a combination
of these two in such a way that your
final results are not way off and having
a trade-off will ensure that the results
are consistent right so that is
basically the output is consistent and
which means that they are close to each
other and they are also accurate right
that means they are as close to the
Target as possible right so if either of
these is high then one of them will go
off the track define precision and
Recall now again here I think it would
be best to draw a diagram and take up in
the confusion Matrix and it is very
simple the definition is like a formula
your Precision is true positive by group
positive plus false positive and your
recall is true positive by group
positive plus false negative okay so
that's you can just show it in a
mathematical way that's pretty much you
know
that's the easiest way to define so the
next question can be about decision tree
what is decision tree pruning and why is
it so basically decision trees are
really simple to implement and
understand but one of the drawbacks of
decision trees is that it can become
highly complicated as it grows right and
the rules and conditions can become very
it and this can also lead to overfitting
which is basically that during training
you will get 100 accuracy but when
you're doing testing you'll get a lot of
Errors so that is the reason pruning
needs to be done so the purpose or the
reason for doing decision tree pruning
is to reduce overfitting or to cut down
on overfitting and what is decision trip
rolling it is basically that you reduce
the number of branches because as you
may be aware a tree consists of the root
node and then there are several internal
nodes and then you have the leaf nodes
now if there are too many of these
internal nodes that is when you face the
problem of overfitting and pruning is
the process of reducing those internal
nodes all right so the next question can
be what is logistic regression uh so
basically logistic regression is one of
the techniques used for a performing
class
especially binary classification now
there is something special about
logistic regression and there are a
couple of things you need to be careful
about first of all the name is a little
confusing it is called logistic
regression but it is used for
classification so this can be sometimes
confusing so you need to probably
clarify that to the interviewer if it's
right and they can also ask this like a
trick question right so that is one part
second thing is the term logistic has
nothing to do with the usual Logistics
that we talk about but it is derived
from the log so that the mathematical
derivation was log and therefore the
name logistic regression so what is
logistic regression and how is it used
so logistic regression is used for
binary classification and the output of
a logistic regression is either a zero
or a one and it varies so it's basically
it calculates a probability between 0
and 1 and we can set a threshold that
can vary typically it is 0.5 so any
value above 0.5 is considered as 1 and
if the probability is below 0.5 it is
considered as zero so that is the way we
calculate the probability of the system
calculates the probability and based on
the threshold it sets a value of 0 or 1
which is like a binary classification
zero or one okay that then we have a
question around K nearest neighbor
algorithm so explain K nearest neighbor
algorithm so first of all what is the K
nearest neighbor algorithm this is a
classification algorithm so that is the
first thing we need to mention and we
also need to mention that the K is a
number it is an integer and this is
variable and we can Define what the
value of K should be it can be 2 3 5 7
and usually it is an odd number so that
is something we need much technically it
can be even number also but then
typically it would be odd number and we
will see why that is okay so based on
that we need to classify objects okay we
need to classify objects so again it
will be very helpful to draw a diagram
you know if you're explaining I think
that will be the best way so draw some
diagram like this and let's say we have
three clusters or three classes existing
and now you want to find for a new item
that has come you want to find out which
class this belongs right so you go about
as the name suggests it you go about
finding the nearest neighbors right the
points which are closest to this and how
many of them you will find that is what
is defined by K now let's say our
initial value of K was Phi okay so you
will find the K the five nearest data
points so in this case as it is
Illustrated these are the five nearest
data points but then all five do not
belong to the same class as our cluster
so there are one belonging to this
cluster one the second one belonging to
this cluster two three of them belonging
to this third cluster okay so how do you
decide that's exactly the reason we
should as much as possible try to assign
an odd number so that it becomes easier
to assign this so in this case you see
that the majority actually if there are
multiple classes then you go with the
majority so since three of these items
belong to this class we assign which is
basically the in in this case the green
or the tennis or the third cluster as I
was talking
so so we assign it to this third class
so in this case it is that's how it is
decided okay so K nearest neighbors the
first thing is to identify the number of
neighbors that are mentioned as K so in
this case it is K is equal to 5 so we
find the five nearest points and then
find out out of these five which class
has the maximum number in that and and
then the new data point is assigned to
that class okay so that's pretty much
how K nearest neighbors work and with
that we have come to end of this ml
expert full course I hope you found it
valuable and entertaining please ask any
question about the topic server in this
video in the comment section below our
experts assist you in addressing your
problem thank you for watching stay safe
and keep learning staying ahead in your
career requires continuous learning and
upskilling whether you're a student
aiming to learn today's top skills or a
working professional looking to advance
your career we've got you covered
explore our impressive catalog of
certification programs in Cutting Edge
domains including data science cloud
computing cyber security AI machine
learning or digital marketing designed
in collaboration with leading
universities and top corporations and
delivered by industry experts choose any
of our programs and set yourself on the
path to Career Success click the link in
the description to know more
hi there if you like this video
subscribe to the simply learned YouTube
channel and click here to watch similar
videos turn it up and get certified
click here
foreign