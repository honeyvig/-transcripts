foreign
boot camp on Amazon web services are you
looking for a career in the tech
industries that pays well and is in high
demand well you have come to the right
place it replaces the leading cloud
service provider with a market share of
over 30 percent in 2022 AWS generated
over a dollar 45 billion in revenues and
has continued to grow year over year as
more companies move their
infrastructures to the cloud they demand
for AWS professionals continues to
Skyrocket according to recent surveys
AWS Cloud architect is one of the
highest paying and most in-demand jobs
in the technology industry with an
average salary of dollar 136 000 per
year in the United States furthermore
LinkedIn ranked AWS Cloud architect as
the number one most in-demand job in the
United States in 2021 with a hundred
percent year-over-year increase in job
openings with AWS growing at the rate of
over 30 percent per year and analytics
predicting that the cloud computing
Market will reach dollar 1 trillion by
2024 with AWS growing at the rate of
over 30 percent year and analytics
predicting that the cloud computing
Market will reach dollar 1 trillion by
2024 there is no better time to become
an AWS professional and if you are also
interested in advancing your career as
an AWS professional look no further than
simply learns AWS Cloud architect master
program in this comprehensive training
program you will learn the core skills
required for Designing and deploying
dynamically stable highly available for
tolerant and reliable applications on
one of the top Cloud platform providers
Amazon web services throughout this
program you will learn how to build
Implement and manage scalable and fault
tolerance systems on AWS you will also
gain expertise in selecting the
appropriate AWS service Based on data
compute database and security
requirements with simply learns AWS
Cloud architect master program you will
be fully prepared to take on the
challenges of the AWS ecosystems and
become a certified AWS professional
coming back to our boot camp our AWS
bootcamp covers a wide range of topics
that will give you a solid foundation in
AWS and help you advance your career we
will start with an overview of what is
AWS and a detailed AWS bootcamp where
you will learn the basics of AWS
terminologies Concepts benefits and
deployment options to meet your business
requirements you will dive deeper into
specific AWS services such as AWS ec2
AWS Lambda AWS S3 and AWS IAM
Additionally you will learn about AWS
Cloud Foundation AWS ECS AWS Route 53
AWS VPC services that allow you to build
Implement and manage scalable and fault
tolerance systems on AWS you will also
gain expertise in AWS CH marker AWS
Cloud phone and AWS Auto scaling
furthermore you will learn about AWS
redshift and data warehouse solution and
compare AWS with other Cloud providers
such as Azure and gcp in AWS versus
assure furthermore you will learn about
AWS redshift a data warehouse solution
and compare AWS with other Cloud
providers such as Azure and gcp in AWS
versus Azure and AWS versus gcp sections
we will also cover kubernetes on AWS and
provide you with the insights on how to
become a solution architect in AWS
finally we will wrap up with the two
sections on AWS interview questions
where you will get a glimpse of what to
expect during an AWS job interview so if
you are ready to explore the world of
AWS Cloud architect and discover the
many opportunities that exit in this
field let's dive into this AWS bootcamp
meet Rob he runs an online shopping
portal the portal started with a modest
number of users but has recently been
seeing a surge in the number of visitors
on Black Friday and other holidays the
portal saw so many visitors that the
servers were unable to handle the
traffic and crashed is there a way to
improve performance without having to
invest in a new server wondered rob a
way to upscale or downscale capacity
depending on the number of users
visiting the website at any given point
well there is Amazon web services one of
the leaders in the cloud computing
Market before we see how AWS can solve
Rob's problem let's have a look at how
AWS reached the position it is at now
AWS was first introduced in 2002 as a
means to provide tools and services to
developers to incorporate features of
amazon.com to their website in 2006 its
first Cloud Services offering was
introduced in 2016 AWS surpassed its 10
billion Revenue Target
and now AWS offers more than 100 cloud
services that span a wide range of
domains thanks to this the AWS cloud
service platform is now used by more
than 45 percent of the global market now
let's talk about what is AWS AWS or
Amazon web service is a secure cloud
computing platform that provides
computing power database networking
content storage and much more the
platform also works with a PSU go
pricing model which means you only pay
for how much of the service is offered
by AWS you use some of the other
advantages of AWS are security AWS
provides a secure and durable platform
that offers end-to-end privacy and
security
experience you can benefit from the
infrastructure management practices born
from Amazon's years of experience
flexible it allows users to select the
OS language database and other services
easy to use users can host applications
quickly and securely
available
depending on user requirements
applications can be scaled up or down
AWS provides a wide range of services
across various domains
what if Rob wanted to create an
application for his online portal AWS
provides compute services that can
support the app development process from
start to finish from developing
deploying running to scaling the
application up or down based on the
requirements
the popular Services include ec2 AWS
Lambda Amazon light cell and elastic
Beanstalk for storing website data Rob
could use AWS storage services that
would enable him to store access govern
and analyze data to ensure that costs
are reduced agility is improved an
innovation accelerated
popular services within this domain
include Amazon S3 EBS S3 Glacier and
elastic file storage
Rob can also store the user data in a
database with aw Services which he can
then optimize and manage popular
services in this domain include Amazon
RDS dynamodb and redshift if Rob's
businesses took off and he wanted to
separate his Cloud infrastructure or
scale up his work requests and much more
he would be able to do so with the
networking Services provided by AWS some
of the popular networking Services
include Amazon VPC Amazon Route 53 and
elastic load balancing
other domains that AWS provide services
in are analytics blockchain containers
machine learning internet of things and
so on and there you go that's AWS for
you in a nutshell now before we're done
let's have a look at a quiz which of
these services are incorrectly matched
one
two
three
four
we'll be pinning the question in the
comment section comment below with your
answer and stand a chance to win an
Amazon voucher several companies around
the world have found great success with
AWS companies like Netflix twitch
LinkedIn Facebook and BBC have taken
advantage of the services offered by AWS
to improve their business efficiency and
thanks to their widespread usage AWS
professionals are in high demand they're
highly paid and earn up to more than one
hundred and twenty seven thousand
dollars per annum once you're AWS
certified you could be one of them too
hello everyone let me introduce myself
as Sam a multi-platform cloud architect
and trainer and I'm so glad and I'm
equally excited to talk and walk you
through the session about what AWS is
and talk to you about some services and
offerings and about how companies get
benefited by migrating their
applications and infra into AWS so
what's AWS let's talk about that now
before that let's talk about how life
was without any cloud provider and in
this case how life was without AWS so
let's walk back and picture how things
were back in 2000 which is not so long
ago but a lot of changes a lot of
changes for better had happened since
that time now back in 2000 a request for
a new server is not an happy thing at
all because a lot of money a lot of
validations a lot of planning are
involved in getting a server online or
up and running and even after we finally
got the server it's not all said and
done a lot of optimization that needs to
be done on that server to make it worth
it and get a good return on investment
from that server and even after we have
optimized for a good return on
investment the work is still not done
there will often be a frequent increase
and decrease in the capacity and you
know even news about our website getting
popular and getting more hits It's still
an Bittersweet experience because now I
need to add more servers to the
environment which means that it's going
to cost me even more but thanks to the
present day Cloud technology if the same
situation were to happen today my new
server it's almost ready and it's ready
instantaneously and with the Swift tools
and technologies that Amazon is
providing uh in provisioning my server
instantaneously and adding any type of
workload on top of it and making my
storage and server secure you know
creating a durable storage where data
that I stored in the cloud never gets
lost with all that features Amazon has
got our back so let's talk about what is
AWS there are a lot of definitions for
it but I'm going to put together a
simple and a precise definition as much
as possible now let me iron that out
Cloud still runs on and Hardware all
right and there are certain features in
that infrastructure in that cloud
infrastructure that makes cloud cloud or
that makes AWS a cloud provider now we
get all the services all the
Technologies all the features and all
the benefits that we get in our local
data center like you know security and
compute capacity and databases and in
fact you know we get even more cool
features like content caching in various
global locations around the planet but
again out of all the features the best
part is that I get or we get everything
on a pay as we go model the less I use
the less I pay and the more I use the
less I pay per unit very attractive
isn't it right and that's not all the
applications that we provision in AWS
are very reliable because they run on a
reliable infrastructure and it's very
scalable because it runs on an on-demand
infrastructure and it's very flexible
because of the designs and because of
the design options available for me in
the cloud let's start talk about how all
this happened AWS was launched in 2002
after the Amazon we know as the online
retail store wanted to sell their
remaining or unused infrastructure as a
service or as an offering for customers
to buy and use it from them you know
sell infrastructure as a service the
idea sort of clicked and AWS launched
their first product first product in
2006 that's like four years after the
idea launch and in 2012 they held a big
sized customer even to gather inputs and
concerns from customers and they were
very dedicated in making those requests
happen and that habit is still being
followed it's still being followed as
reinvent by AWS and at 2015 Amazon
announced its Revenue to be 4.6 billion
and in 2015 through 2016 AWS launched
products and services that help migrate
customer services into AWS well their
web products even before but this is
when a lot of focus was given on
developing migrating services and in the
same year that's in 2016 Amazon's
revenue was 10 billion and not but not
the least as we speak Amazon has more
than 100 products and services available
for customers and get benefited from all
right let's talk about the services that
are available in Amazon let's start with
this product called S3 now S3 is a great
tool for internet backup and it's it's
the cheapest storage option in the
object storage category and not only
that the data that we put in S3 is
retrievable from the internet S3 is
really cool and we have other products
like migration and data collection and
data transfer products and here we can
not only collect data seamlessly but
also in a real-time way monitor the data
or analyze the data that's being
received that they're cool products like
AWS data transfers available that helps
achieve that and then we have products
like ec2 elastic compute Cloud that's an
resizable computer where we can anytime
anytime after the size of the computer
based on the need or based on the
forecast then we have SIMPLE
notification Services systems and tools
available in Amazon to update us with
notifications through email or through
SMS now anything anything can be sent
through email or through SMS if we use
that service it could be alarms or it
could be service notifications if you
want stuff like that and then we have
some security tools like KMS key
management system which uses AES 256-bit
encryption to encrypt our data at rest
then we have Lambda as service for which
we pay only for the time in seconds
seconds it takes to execute our code and
we're not paying for for the
infrastructure here it's just the
seconds that the program is going to
take to execute the code for the short
program will be paying in a milliseconds
if it's a bit bigger program we'll be
probably paying in 60 seconds or 120
seconds but that's a lot cheap lot
simple and lots cost effective as
against paying for service on an hourly
basis which a lot of other services are
well that's cheap but using Lambda is a
lot cheaper than that and then we have
services like Route 53 at DNS service in
the cloud and now I do not have to
maintain and DNS account somewhere else
and my cloud environment with AWS I can
get both in the same place all right let
me talk to you about how AWS makes life
easier or how companies got benefited by
using AWS as their I.T provider for
their applications or for the
infrastructure now Unilever is a company
and they had a problem right and they
had a problem and they picked AWS as a
solution to their problem all right now
this company was sort of spread across
190 countries and they were relying on a
lot of digital marketing for promoting
their products and their existing
environment their legacy local
environment proved not to support their
changing I.T demands and uh they could
not standardize their old environment
now they chose to move part of their
applications to AWS because they were
not getting what they wanted in their
local environment and since then you
know rollouts were easy provisioning
your applications became easy and even
provisioning infrastructure became easy
and they were able to do all that in
push button scaling and needless to talk
about
backups that are safe and backups that
can be securely accessed from the cloud
as needed now that company is growing
along with AWS because of their Swift
speed in rolling out deployments and
being able to access secure backups from
various places and gendered reports and
in fact useful reports out of it that
helps their business now on the same
lines let me also talk to you about
Kellogg's and how they got benefited by
using Amazon now Kellogg's had a
different problem it's one of its kind
now their business model was very
dependent on an infra that will help to
analyze data really fast right because
they were running promotions based on
the analyzed data that they get so
they're being able to respond to the
analyzed data as soon as possible was
critical or vital in their environment
and luckily sap running on Hana
environment is what they needed and you
know they picked that service in the
cloud and that sort of solved the
problem now the company does not have to
deal with maintaining their legacy infra
and maintaining their heavy compute
capacity and maintaining their a
database locally all that is now moved
to the cloud or they are using Cloud as
their I.T service provider and and now
they have have a greater and Powerful it
environment that very much complements
their business let me start this session
with this scenario let's imagine how
life would have been without Spotify for
those who are hearing about Spotify for
the first time a Spotify is an online
music service offering and it offers
instant access to over 16 million
licensed songs Spotify now uses AWS
Cloud to store the data and share it
with their customers but prior to AWS
they had some issues imagine using
spotify before AWS let's talk about that
back then users were often getting
errors because Spotify could not keep up
with the increased demand for storage
every new day and that led to users
getting upset and users canceling the
subscription the problem Spotify was
facing at that time was their users were
present globally and were accessing it
from everywhere and they had different
latency in their applications and
Spotify had a demanding situation where
they need to frequently catalog of the
songs released yesterday today and in
the future and this was changing every
new day and the songs coming in rate was
about 20 000 a day and back then they
could not keep up with this requirement
and needless to say they were badly
looking for way to solve this problem
and that's when they got introduced to
AWS and it was a perfect fit and match
for their problem AWS offered a
dynamically increasing storage and
that's what they needed AWS also offered
tools and techniques like storage life
cycle management and trusted advisor to
properly utilize the resource so we
always get the best out of the resource
used AWS addressed their concerns about
easily being able to scale yes you can
scale the AWS environment very easily
how easily one might ask it's just a few
button clicks and AWS sold spotify's
problem let's talk about how it can help
you with your organization's problem
let's talk about what is AWS first and
then let's into how AWS became so
successful and the different types of
services that AWS provides and what's
the future of cloud and AWS in specific
let's talk about that and finally we'll
talk about a use case where you will see
how easy it is to create a web
application with AWS all right let's
talk about what is AWS AWS or Amazon web
services is a secure cloud service
platform it is also pay as you go type
billing model where there is no upfront
or Capital cost we'll talk about how
soon the service will be available well
the service will be available in a
matter of seconds with AWS you can also
do identity and access management that
is authenticating and authorizing a user
or a program on the Fly and almost all
the services are available on demand and
most of them are available
instantaneously and as we speak Amazon
offers 100 plus services and this list
is growing every new week now that would
make you wonder how AWS became social
successful of course it's their
customers let's talk about the list of
well-known companies that has their ID
environment in AWS Adobe Adobe uses AWS
to provide multi-terabyte operating
environments for its customers by
integrating its system with AWS Cloud
Adobe can focus on deploying and
operating its own software instead of
trying to you know deploy and manage the
infrastructure Airbnb is another company
it's an Community Marketplace that
allows property owners and travelers to
connect each other for the purpose of
renting unique vacation spaces around
the world and the Airbnb Community users
activities are conducted on the website
and through iPhones and Android
applications Airbnb has a huge
infrastructure in AWS and they are
almost using all the services in AWS and
are getting benefited from it another
example would be Autodesk Autodesk
develops software for engineering
designing and entertainment Industries
using services like Amazon ideas or
rational database service and Amazon S3
or Amazon simple storage servers
Autodesk can focus on deploying or
developing its machine learning tools
instead of spending that time on
managing the infrastructure AOL or
American online uses AWS and using AWS
they have been able to close data
centers and decommission about 14 000
in-house and co-located servers and move
Mission critical workload to the cloud
and extend its Global reach and save
millions of dollars on energy resources
bitdefender is an internet security
software firm and their portfolio of
softwares include antivirus and
anti-spyware products bitdefender uses
ec2 and they're currently running few
hundred instances that handle about 5
terabytes of data and they also use
elastic load balancer to load balance
the connection coming in to those
instances across availability zones and
they provide seamless Global delivery of
server service because of that the BMW
group it uses AWS for its new connected
Car application that collects sensor
data from BMW 7 Series cars to give
drivers dynamically updated map
information Canon's office Imaging
products division benefits from faster
deployment times lower cost and Global
reach by using AWS to deliver
cloud-based services such as mobile
print the office Imaging products
division uses AWS such as Amazon S3 and
Amazon Route 53 Amazon cloudfront and
Amazon IM for their testing development
and Production Services Comcast it's the
world's largest cable company and the
leading provider of internet service in
the United States Comcast uses AWS in a
hybrid environment out of all the other
Cloud providers Comcast chores AWS for
its flexibility and scalable hybrid
infrastructure Docker is a company
that's helping redefine the way
developers build ship and run
application this company focuses on
making use of containers for this
purpose and in AWS the service called
the Amazon ec2 container service is
helping them achieve it the esa or
European Space Agency although much of
esa's work is done by satellites some of
the programs data storage and Computing
infrastructure is built on Amazon web
services Esa chose AWS because of its
economical pay as you go system as well
as its quick startup time the Guardian
newspaper uses AWS and it uses a wide
range of AWS services including Amazon
Kinesis Amazon redshift that power and
analytic dashboard which editors used to
see how stories are trending in real
time Financial Times FD is one of the
world's largest leading business news
organization and they used Amazon
redshift to perform their analysis A
Funny Thing Happened Amazon Redshirt
performs so quickly that some analysis
thought it was malfunctioning they were
used to running queries overnight and
they found that the results were indeed
correct just as much faster by using
Amazon redshift FD is supporting the
same business functions with costs that
are 80 percentage lower than what was
before general electric GE is at the
moment as we speak migrating more than
9000 workloads including 300 desperate
Erp systems to AWS while reducing its
data center footprint from 34 to 4 over
the next three years similarly Harvard
Medical School HTC IMDb McDonald's NASA
Kellogg's and lot more are using the
services Amazon provides and are getting
benefited from it and this huge success
and customer portfolio is just the tip
of the iceberg and if we think why so
many adapt AWS and if we let AWS answer
that question this is what AWS would say
people are adapting AWS because of the
security and durability of the data and
end-to-end privacy and encryption of the
data and Storage experience we can also
rely on AWS way of doing things by using
the AWS tools and techniques and
suggested best practices built upon the
years of experience it has gained
flexibility there is a greater
flexibility in AWS that allows us to
select the OS language and database easy
to use swiftness in deploying we can
host our applications quickly in AWS be
it a new application or migrating an
existing application into AWS
scalability the application can be
easily scaled up or scaled down
depending on the user requirement cost
saving we only pay for the compute power
storage and other resources you use and
that too without any long-term
commitments now let's talk about the
different types of services that AWS
provides the services that we talk about
fall in any of the following categories
you see like you know compute storage
database Security customer engagement
desktop and streaming machine learning
developers tools stuff like that and if
you do not see the service that you are
looking for it's probably is because AWS
is creating it as we speak now let's
look at some of them that are very
commonly used within compute Services we
have Amazon ec2 Amazon elastic Beanstalk
Amazon light sale and Amazon Lambda
Amazon ec2 provides compute capacity in
the cloud now this capacity is secure
and it is resizable based on the user's
requirement now look at this the
requirement for the web traffic keeps
changing and behind the scenes in the
cloud ec2 can expand its environment to
three instances and during no load it
can shrink its environment to just one
resource elastic Beanstalk it helps us
to scale and deploy web applications and
it's made with a number of programming
languages elastic Beanstalk is also an
easy to use service for deploying and
scaling web applications and services
deployed a bit in java.net PHP node.js
python Ruby Docker and lot other
familiar services such as Apache
passenger and IIs we can simply upload
our code and elastic Beanstalk
automatically handles the deployment
from capacity provisioning to load
balancing to Auto scaling to application
Health monitoring and Amazon light sale
is a virtual private server which is
easy to launch and easy to manage Amazon
light cell is the easiest way to get
started with AWS for developers who just
need a virtual private server light sail
includes everything you need to launch
your project quickly on a virtual
machine like SSD based storage a virtual
machine tools for data transfer DNS
management and a static IP and that too
for a very low and predictable price AWS
Lambda has taken Cloud Computing
Services to a whole new level it allows
us to pay only for the compute time no
need for provisioning and managing
servers an AWS Lambda is a compute
service that lets us run code without
provisioning or managing service Lambda
executes your code only when needed and
scales automatically from few requests
per day to thousands per second you pay
only for the compute time you consume
there is no charge when your core is not
running let's look at some storage
services that Amazon provides like
Amazon S3 Amazon Glacier Amazon abs and
Amazon elastic file system Amazon S3 is
an object storage that can store and
retrieve data from anywhere websites
mobile apps iot sensors and so on can
easily use Amazon S3 to store and
retrieve data it's an object storage
built to store and to drive any amount
of data from anywhere with its features
like flexibility in managing data and
the durability it provides and the
security that it provides Amazon simple
storage service or S3 is a storage for
the internet and Glacier Glacier is a
cloud storage service that's used for
archiving data and long-term backups and
this Glacier is an secure durable and
extremely low cost cloud storage service
for data archiving and long-term backups
Amazon EVS Amazon elastic blog store
provides Block store volumes for the
instances of ec2 and this elastic Block
store is highly available and a reliable
storage volume that can be attached to
any running instance that is in the same
availability Zone ABS volumes that are
attached to the ec2 instances are
exposed as storage volumes that persist
and independently from the lifetime of
the instance an Amazon elastic file
system or EFS provides an elastic file
storage which can be used with AWS cloud
service and resources that are on
premises and Amazon elastic file system
it's an simple it's scalable it's an
elastic file storage for use with Amazon
cloud services and for on-premises
resources it's easy to use and offers a
simple interface that allows you to
create and configure file systems
quickly and easily Amazon file system is
built to elastically scale on demand
without disturbing the application
growing and shrinking automatically as
you add and remove files your
application have the storage they need
and when they need it now let's talk
about databases the two major database
flavors are Amazon RDS and Amazon
redshift Amazon RDS it really eases the
process involved in setting up operating
and scaling a rational database in the
cloud Amazon RDS provides cost efficient
and resizable capacity while automating
time consuming administrative tasks such
as Hardware provisioning database setup
patching and backups it sort of frees us
from managing the hardware and sort of
helps us to focus on the application
it's also cost effective and resizable
and it's also optimized for memory
performance and input and output
operations not only that it also
automates most of the services like
taking backups you know monitoring stuff
like that it automates most of those
Services Amazon redshift Amazon redshift
is a data warehousing service that
enables users to analyze the data using
SQL and other business intelligent tools
Amazon redshift is an fast and fully
managed data warehouse that makes it
simple and cost effective analyze all
your data using standard SQL and your
existing business intelligent tools it
also allows you to run complex analytic
queries against petabyte or structured
data using sophisticated query
optimizations and most of the results
they generally come back in seconds all
right let's quickly talk about some more
services that AWS offers there are a lot
more services that AWS provides but are
we going to look at some more services
that are widely used AWS application
Discovery Services help Enterprise
customers plan migration projects by
gathering information about their
on-premises data centers in a planning a
data center migration can involve
thousands of workloads they are often
deeply interdependent server utilization
data and dependency mapping are
important early first step in migration
process and this AWS application
Discovery service collects and presents
configuration usage and behavior data
from your servers to help you better
understand your workloads Route 53 it's
a network and content delivery service
it's an highly available and scalable
Cloud domain name system or DNS service
and Amazon Route 53 is fully compliant
with IPv6 as well elastic load balancing
it's also a network and content delivery
service elastic load balancing
automatically distributes incoming
application traffic across multiple
targets such as Amazon ec2 instance
containers and ipu addresses it can
handle the varying load of your
application traffic in a single
available zones and also across
availability zones AWS Auto scaling it
monitors your application and
automatically adjusts the capacity to
maintain steady and predictable
performance at a lowest possible cost
using AWS Auto scaling it's easy to set
up application scaling for multiple
resources across multiple services in
minutes Auto scaling can be applied to
web services and also for DB Services
AWS identity and access management it
enables you to manage access to AWS
services and resources securely using IM
you can create and manage AWS users and
groups and use permissions to allow and
deny their access to AWS resources and
moreover it's a free service now let's
talk about the future of AWS well let me
tell you something cloud is here to stay
here's what in store for AWS in the
future as years pass by we're gonna have
a variety of cloud applications born
like iot artificial intelligence
business intelligence serverless
Computing and so on cloud will also
expand into other markets like
healthcare banking space automated cars
and so on as I was mentioning some time
back lot or greater Focus will be given
to artificial intelligence and
eventually because of the flexibility
and advantage that cloud provides we're
going to see a lot of companies moving
into the cloud all right let's now talk
about how easy it is to deploy an web
application in the cloud so the scenario
here is that our users like a product
and we need to have a mechanism to
receive input from them about their
likes and dislikes and you know give
them the appropriate product as per
their need all right though the setup
and the environment it sort of looks
complicated we don't have to worry
because AWS has tools and Technologies
which can help us to achieve it now
we're going to use services like Route
53 services like Cloud watch ec2 S3 and
lot more and all these put together are
going to give an application that's
fully functionable and an application
that's going to receive the information
like using the services like Route 53
cloudwatch ec2 and S3 we're going to
create an application and that's going
to meet our need so back to our original
requirement all I want is to deploy a
web application for a product that keeps
our users updated about the happenings
and the newcomings in the market and to
fulfill this requirement here is all the
services we would need ec2 here is used
for provisioning the computational power
needed for this application and ec2 has
a vast variety of family and types that
we can pick from for the types of
workloads and also for the intents of
the workloads we're also going to use S3
for storage and S3 provides any
additional storage requirement for the
resources or any additional storage
requirement for the web applications and
we are also going to use cloudwatch for
monitoring the environment and
cloudwatch monitors the application and
the environment and it provides trigger
for scaling in and scaling out the
infrastructure and we're also going to
use Route 53 for DNS and Route 53 helps
us to register the domain name for our
web application and with all the tools
and Technologies together all of them
put together we're going to make an
application a perfect application that
caters our need all right so I'm going
to use elastic Beanstalk for this
project and the name of the application
is going to be as you see GSG sign up
and the environment name is GSG signup
environment one let me also pick a name
let me see if this name is available yes
that's available that's the domain name
so let me pick that and the application
that I have is going to run on node.js
so let me pick that platform and launch
now as you see elastic Beanstalk this is
going launch an instance it's going to
launch the monitoring setup or the
monitoring environment it's going to
create a load balancer as well and it's
going to take care of all the security
features needed for this application
all right look at that I was able to go
to that URL which is what we gave and
it's now having an default page shown up
meaning all the dependencies for the
software is installed and it's just
waiting for me to upload the code or in
specific the page required so let's do
that
let me upload the code I already have
the code saved here
that's my code
and that's going to take some time all
right it has done its thing and now if I
go to the same URL look at that I'm
being thrown an advertisement page all
right so if I sign up with my name email
and stuff like that you know it's going
to receive the information and it's
going to send an email to the owner
saying that somebody had subscribed to
your service that's the default feature
of this app look at that email to the
owner saying that somebody had
subscribed to your app and this is their
email address stuff like that not only
that it's also going to create an entry
in the database and dynamodb is the
service that this application uses to
store data there's my dynamodb and if I
go to tables right and go to items I'm
going to see that a user with name
Samuel and email address so and so has
said ok or has shown interest in the
preview of my site or product so this is
where this is how I collect those
information right and some more things
about the infrastructure itself is it is
running behind and load balancer look at
that it had created a load balancer it
had also created an auto scaling group
now that's the feature of elastic load
balancer that we have chosen it has
created an auto scaling group and now
let's put this URL you see this it's
it's not a fancy URL all right it's an
Amazon given URL a dynamic URL so let's
put this URL behind our DNS let's do
that
so go to Services go to Route 53
go to hosted Zone and there we can find
the DNS name right so that's a DNS name
all right
all right let's create an entry
and map that URL to our load balancer
right
and create now technically if I go to
this URL it should take me to that
application all right look at that I
went to my custom URL and now that's
pointed to my application previously my
application was having a random URL and
now it's having a custom URL choose from
over 300 in-demand skills and get access
to 1 000 Plus hours of video content for
free visit skill up by simply learn
click on the link in the description to
know more
cloud services are available to satisfy
almost any ID requirement
although Cloud Computing Services vary
greatly they all have some basic
qualities and benefits in common
and they can be classified into a few
basic cloud service kinds hello everyone
I'm shamli and I welcome you all to this
new video of Simply learn on Cloud
Computing Services
let me give you an overview of the
concepts you are going to go through in
this video
first we will go through what is cloud
computing
then we will have an introduction to
Cloud Computing Services
followed by the types of Cloud Computing
Services
and at the end we will look into the
features and benefits of Cloud Computing
Services
so let's not waste any more time and get
started with the video
so what is cloud computing
cloud computing is the distribution of
Cloud Computing Services by the internet
including servers storage databases
networking software analytics
intelligence to provide faster
Innovation more flexible resources and
economies of scale
so usually only pay for the cloud
services you use which helps you cut
cost when your infrastructure more
efficiently and scale as your business
grows
now that we know what cloud computing is
let's proceed with understanding Cloud
Computing Services
Cloud Computing Services provide users
with a variety of capabilities including
email storage backup data retrieval app
creation testing data analysis audio
video streaming software on demand
regardless of the time of service
although cloud computing is still a
relatively new technology it is now
being used by a wide range of
organizations including large
Enterprises small businesses non-profit
government agencies and even individual
consumers
so not all clouds are created equal and
not every sort of cloud computing is
appropriate for every situation a
variety of models varieties and services
have evolved to assist you find the best
option for your needs
to begin one must decide on the type of
cloud deployment or cloud computing
architecture that will be used to
implement your cloud services
cloud services can be deployed in three
different ways public Cloud private
cloud or hybrid cloud
now let's dive down deeper into Cloud
Computing Services and explore its type
in more detail
cloud computing unlike a microprocessor
is not a single piece of technology
rather it's a system made up of three
services
infrastructure as a service
platform as a service
and software as a service
so let's have a better understanding of
each
starting with infrastructure as a
service
so what is infrastructure as a service
it is a type of cloud computing that
uses the internet to provide virtualized
Computing resources
the cloud provider controls it
infrastructures such as storage server
and networking resources and also offers
them to subscribe companies via virtual
machines accessible over the internet in
IAS model
for businesses it can provide numerous
advantages including the ability to make
tasks faster easier more flexible and
less expensive
now let's have a look at the working of
iaas
uses connect to resources and services
across a wide area network such as the
internet and then use the cloud provider
services to complete the applications
tag
the user can for example log into the
infrastructure as a service platform to
build virtual machines install operating
systems in each VM
deploy middleware such as databases
create storage buckets for workloads and
backups and install the Enterprise
workload onto that virtual machine
customers can then track cost monitor
performance balance Network traffic
solve application difficulties and
manage Disaster Recovery using the
provider service
so moving ahead let's go through its
advantages and disadvantages
advantages
organizations select infrastructure as a
service because operating a workload
without having to buy manage and support
the underlying infrastructure is
frequently easier faster and more cost
effective a company can rent or lease
infrastructure from another company
using infrastructure as a service
for workloads that are transitory
experimental or change abruptly
it is an effective cloud service
paradigm for example if a company is
creating a new software product hiring
an ieas provider
to host and test the application may be
more cost effective
once the new software has been
thoroughly tested and refined the
company can move it away from the
iaas environment
and into a more traditional in-house
deployment
if the expenses of a long-term
commitment are lower the organization
could commit the piece of software to a
long term
iaas deployment
not disadvantages
billing can be a challenge for some
firms despite its flexible payers you go
model Cloud invoicing is quite detailed
and it is broken down to reflect
specific service usage
when evaluating the invoices for each
resource and service involved in
application deployment uses frequently
experienced sticker shock or discover
expenses that are greater than expected
another issue it face is lack of insight
because its providers own the
infrastructure the configuration and
performance of that infrastructure are
rarely transparent to its con consumers
users may find it more difficult to
operate and monitor systems due to the
lack of transparency
users of infrastructure as a service are
also concerned about service
availability and reliability
the supplier has a big influence on the
workloads availability and performance
the workloads of users will be impacted
if a provider has Network constraint or
any other type of internal or external
outage
furthermore because it is a
multi-theanine design the problem of
noisy neighbors might have an adverse
effect on users workloads
so now these are the top infrastructure
as a service providers
linode is a privately held Cloud hosting
firm based in the United States that
offers virtual private servers
host wins web hosting Cloud hosting and
dedicated server options are all
available from host wins
Microsoft Azure is a cloud computing
service operated by Microsoft for
application
management via Microsoft managed data
centers
digitalocean offers developers cloud
service
that make it easy to deploy and grow
programs that run on several machines at
the same time
Alibaba cloud is a cloud computing firm
that serves online businesses as well as
alibaba's own e-commerce ecosystem
so the second service is platform as a
service
what is platform as a service
platform is a service products which are
geared towards software development
teams include Computing and storage
infrastructure as well as development
platform layer that includes web servers
database Management systems and software
development kits for multiple
programming languages
working of platform as a service
for software development
it does not replace an organization's
complete I.T infrastructure
it's made possible by the hosted
infrastructure of a cloud service
provider
a web browser is the most common way for
users to access the offerings
platform is a service such as
application hosting and Java development
can be supplied by a public private or
hybrid clouds
so now let's look into the advantages
and disadvantages
advantages
the main advantage of platform as a
service for users is its Simplicity and
convenience
much of the infrastructure and other ID
services will be provided by the
platform as a service provider
which users can access from anywhere via
or web browser
the flexibility to pay on a per use
basis allows businesses to forego the
capital costs associated with
on-premises gear and software
many platform as a service solutions are
aimed towards software developers these
platform provider computation and
storage infrastructures as well as text
editing version management compilation
and testing capabilities to assist
developers in swiftly and efficiently
developing new software
coming to the disadvantages platform as
a service on the other hand can cause
issues
with service availability and resilience
customers may suffer as a result of of a
service outage or other infrastructure
Interruption which might result in
costly productivity losses
IT Supplies on the other hand will
typically deliver reasonably high up
times
another widespread problem is when the
lock-in which occurs when users are
unable to Simply transition many of
their services and data from one
platform as a service solution to
another
when choosing a provider users must
consider the business risk of service
outages and vendor lock-in
internal changes to a platform as a
service product could also be a problem
the impact on users might be tough and
receptive
if a platform as a service provider
seizes supporting a programming language
or chooses to use a different set of
development tools
so let's have a look at the top platform
as a service provider
IBM cloud computing refers to a set of
Cloud Computing Services for businesses
provided by IBM a technology corporation
red hat is a collection of tightly
integrated Red Hat Technologies for
building Cloud infrastructure and
developing Cloud native apps on-premises
elastic Beanstalk AWS elastic bean stock
is an Amazon web services application
deployment orchestration solution
Salesforce offers CRM Services as well
as Enterprise applications for customer
service marketing automation analytics
and application development
and software AG cloud is an open and
independent Cloud platform that serves
as your One-Stop shop for all software
AG has to offer in the cloud
now coming to the next service which is
software as a service
so what is software as a service
customer relationship management for CRM
marketing Automation and business
analytics are just a few of the
application Level services offered by
software as a service companies
so how does it work
the provider gives consumers network
based access to a single copy of an
application that the provider design
expressly for software as a service
distribution
in the software on-demand model
the source code for the program is the
same for all clients and new features of
functionalities are rolled out to all
the users at the same time since the
data of each model's customer may be
stored locally in the cloud or both
locally in the cloud depending on the
service level agreement
coming to its advantages
customers subscribe to a software as a
service solution rather than buying and
installing software or additional gear
to support it many firms can now budget
more effectively and predictably by
converting cost to recurrent operating
expenses you can also cancel software as
a service subscriptions at any time to
avoid incurring recurring fees
its systems are frequently customizable
and can be connected with other
corporate applications particularly when
using software for the same vendor
vertical scalability is a feature of
cloud services like software as a
service which allows clients to access
more or fewer services or features on
demand
disadvantages of software as a service
when providers indoor
service delays impose undesirable
modifications to service offerings or
suffer a security breach all of these
things can have a significant impact on
customers
ability to use the software as a service
offering
customers should be aware of their
software as a service providers SLA and
ensure that it is followed
if the provider adopts a new version of
an application it will roll it out to
all of its clients
whether or not they want it
this may need the organization
allocating additional training time and
resources
switching vendors can be tough as it is
with any cloud service provider
customers must migrate massive volumes
of data when switching vendors
furthermore some vendors use
Technologies and data types which can
make transferring client data between
Cloud providers even more difficult
so coming to the top companies that
provide software as a service
Adobe is a collection of adobe
incorporation programs and services that
provide user with access to software for
graphic design video editing web
development and photography as well as a
set of mobile apps and certain optional
cloud services
sap is a platform as a service designed
by sapse for developing new application
and enhancing
existing ones in a secure cloud
computing environment managed by sap
Google Cloud Google Cloud platform is a
set of Cloud Computing Services provided
by Google that run on the same
infrastructure as Google's internal
products such as Google search Gmail
drive and YouTube
freshworks Cloud platform allows users
to manage their identities and access
across all of their freshworks products
and adulation Advanced features such as
a 99.9 uptime SLE unlimited storage and
premium support provide themes the
confidence to scale reliably
now that we have all the in-depth
information about different types of
Cloud Computing Services let's move
ahead and explore its benefits and
features which makes it so popular and
convenient
first the supplier host and maintains
the site in their own facility the cloud
hosting provider acquires host and
maintains the necessary hardware and
software
users avoid the construction
expenditures and maintenance problem
that would be incurred if the service
was established on premise
second set service using a web-based
interface through a web interface
serviced users can initiate certain
service functions as well as increase or
reduce their service consumption level
with little or no intervention from the
service provider
third is you must pay to utilize the
facility
users of the service only pay for the
services they use when compared to the
typical method of building on-site it
capacities
targeted for the highest usage
situations and then having that capacity
to go unused for the most of the time
this can result in significant cost
savings
and the fourth one is scalability that
comes close to being infinite cloud
computing service providers usually have
the instruction infrastructure in place
to deliver their services at a large
scale that means that cloud service
consumers can readily accommodate
business expansion or periodic surges in
service usage
well this is all about Cloud Computing
Services hello everybody Welcome to
Simply learns AWS S3 tutorial for
beginners my name is Kent and today I'm
going to be covering these following
points I'm going to be covering what is
cloud computing I'm going to be
showcasing what AWS is in terms of cloud
computing and I'm also going to be
covering the core fundamental service of
the simple storage service which is a
object storage service we're going to be
covering the benefits of the simple
storage service better known as S3 what
objects and buckets are and we're going
to be seeing how things are working in
the background and implementing that in
terms of lab assisted demonstrations in
order for you to see what those features
of S3 are and then we'll perform a wrap
up of the material we've done in a
conclusion
so let's get started what is cloud
computing
so here is a definition of cloud
computing but I'm going to paraphrase it
cloud computing is nothing more than you
gaining access to infrastructure
through web services with a
pay-as-you-go model now this is a very
very different model than where
traditionally
involved in on a on-prem data center so
when I mean on-prem I'm saying on
premise okay so you'll hear that a lot
on-prem now when we are on on-premise
data center we have to provision lots of
infrastructure even before we get
started with
deploying any type of application
whether it be a mobile application a web
application etc etc
so every application needs some sort of
underlying infrastructure whether that
be bare bone servers databases they're
obviously going to be needing some types
of storage
etc etc so all this
is something that we have to kind of
fight against in order just to get
started with deploying our application
so this text usually traditionally
depending on the size of the company
about three months to set up
so what if we could Leverage
all this infrastructure
as a service and just through some API
call that's what the web services is all
about we could provision a server or we
could provision a database within
minutes and then deploy any type of
application that we want to on top of
that stack and also take advantage of
any storage that we may want we may want
object level storage which is what we're
going to cover we may want an elastic
file system right so there are different
types of storages that were eventually
going to look at as you go down your
learning path with simply learn so cloud
computing is really about provisioning
of all these kinds of services but not
only at the infrastructure level we're
going to see how we could move up the
stack and get entire platforms as a
service or even softwares as a service
so what is AWS
while AWS is really a cloud computing
platform that will offer us many many
services through these API calls some
are again just bare infrastructure like
a service other can be a service as a
software for example email so here we
have an example
of the public data storage this is S3
the simple storage service icon that
you're going to see over and over again
this is a representation of an actual
bucket in S3 so this will become more
clear as we continue the slide deck and
actually reinforce this with some
Hands-On lab but as you can imagine you
may want to provision a private
software-defined Network in the cloud
and so we can do that in AWS we can
provide load balancing we could provide
scalability in terms of Auto scaling
groups that will respond to increased
demand as your application let's say
becomes more popular you want that
infrastructure to grow elastically so
doing this on premise is traditionally
not only extremely expensive and thus
prohibitive but very difficult to
implement physically speaking as well
so by going with a cloud provider like
AWS we can severely reduce the expense
and the complexity of setting all of
this up because
we're trading Capital expense for
variable expense so Capital expense is
about you procurring all the hardware
that you need beforehand and the budget
being approved before even getting that
Hardware in through your door right and
then you have to configure it so there's
that complexity Factor
whereas if you're doing this through AWS
you can within minutes
just provision all of that very quickly
so you're also not paying for everything
all up front you're paying this with a
pay as you go model so very different
we're now removing what's called
undifferentiated lifting which is
everybody has to do the same thing but
it's not really bringing any value to
their end product we all have to buy
servers and configure them and scale
them etc etc so if we remove that
element
and have AWS handle that we can
concentrate on our business which is our
application and improving that
application
and responding more to our clients
request can be more agile more flexible
so all of this leads to these points
that we see here on this slide much
easier to use much easier to get our
application to Market we're actually
going to be leveraging the physical
security that AWS implements for us they
have data centers all over the world
that we're going to plug into and that
physical security is already implemented
for us there are varying levels of
security that we're going to get into
through our AWS Journey but right off
the bat you know that you have physical
security of course we're going to be
talking about storage in this
demonstration or in this slide deck over
here but the nice thing is is we only
pay for what we use so that's that
variable expense that I was talking
about that we've traded for Capital
expense we can easily set up databases
we can easily automate our backups and
have those backups stored
in the cloud through that S3 storage
that we're going to get into okay so
lots of benefits of going through AWS
here
and it's also comprehensive and simple
to use
comprehensive meaning that there are
hundreds and hundreds of services
it's going to be actually quite time
consuming for you to just know what each
service does in general but you'll get
there through your journey it's all
about sticking with it and learning
something new every day which is what
simple learn is all about
um over here we have infrastructure as a
service platform as a service and
software as a service so these are three
different types of distinctions we have
in the cloud industry to basically
cover what's going on here on the right
hand side so on premise you're
traditionally involved in everything
from networking which is you for example
buying the switches the routers the
cabling everything all the way up the
stock to deploying your application and
you see everything in between there okay
so this is a very time consuming job not
only that but it costs a lot of money
because you have to buy all these
machines up front and you have to have
the individuals with the right knowledge
to maintain all of this stack all right
so different individuals and different
teams maintaining this whole on premise
data center you may also have different
data centers deployed globally so you
can imagine the cost overhead of this
so everything in green is basically
you're managing it now we can offset
that and ask AWS to handle part of this
stack over here so
on this second Point column over here we
see infrastructure as a service which
basically says AWS I want you to handle
all the networking my software-defined
network I want you to handle my storage
elasticity I want you to handle my
compute power my virtual machines I want
you to handle even
uh administrating those machines at a
physical layer as well and I want you to
physically secure them so this will
allow you more time to focus on all the
green up here of course we can move up
the stack and say I even want AWS to
take on more responsibility
and that would be more of a platform as
a service where AWS would also install
and maintain the operating system for
you so you can imagine things like new
versions of operating systems security
patches maintenance patches all of the
like and any middleware or run time that
you have on top of that think of
something like the Java a virtual
machine perhaps that needs to be updated
now you wouldn't be
responsible for maintaining all of
what's in Orange over here you would
only be responsible for deploying your
application and making sure that your
data is saved and secured right so again
platform as a service is a more
hands-off approach so think of it as you
wanted to set up a CI CD deployment
environment
for your development team and you want
it to be very
involved in just handling your
application code and making sure that it
was properly compiled built tested and
deployed right well this could be
implemented as a platform as a service
or you could take your code as a zip
file and give it to aws's
a service called Beanstalk which would
automatically deploy your code and make
sure that it's highly available scalable
and fault tolerant across all of the
orange stack over here right so more of
a hands-off approach of course we can go
all the way up to stock and tell AWS to
take care of everything for us and just
ask for a service much like a utility
like electricity or water we might want
to say we just want an email software
service and we're not really concerned
about how the underlying operating
system or servers or network are
configured for that we just want to use
the software as a service we don't want
to administrate anything else we just
want to be sort of like an end user to
it so AWS will manage everything
underneath the hood so there are varying
types of services you pick and choose
whichever one makes more sense for you
and your team
so what is S3 S3 stands for simple
storage service and it is an object
storage service in Amazon what this
means is that it doesn't matter what
kind of file you upload to S3 it will be
treated as an abstraction meaning an
object so you can upload a PDF file you
can upload a JPEG file you can upload a
database backup it doesn't really matter
all that is abstracted Away by the use
of storing it within an object we're
going to talk more about what composes
an object but by doing so what happens
is S3 allows us to have
industry leading scalability so it
doesn't matter if you're going to make a
request for that database backup let's
say that object
five times or ten thousand times that
request that demand will scale the
underlying infrastructure that is
implemented behind the scenes is handled
for you so in a sense it's kind of like
a serverless service a storage service
where you're not implicated in handling
anything underneath the covers you're
just uploading your objects and
accessing them later on in terms of data
availability very powerful as well
because it takes those objects and will
replicate them across at least three
availability zones of course these
availability zones have one or more data
centers attached to them so you have
lots and lots of copies of your objects
distributed globally
or at least at a regional in a regional
area you can do this as well globally if
you enable global replication
and you will see that your level of your
availability will Skyrocket and you're
just not going to worry about losing an
S3 object
data security well we can encrypt our
data at rest our objects at rest we can
encrypt it in transit we can also come
up with security policies at the Bucket
Level which we're going to talk about
when a bucket is very very very soon
these are going to be implemented
through what's called I am policies and
you're going to be able to control who
or what has access to your objects
and of course because everything is
handled underneath the covers all the
servers all the storage nodes are Cloud
optimized for the best possible
performance
so let's continue on our journey on what
is S3 we're going to take a look at what
is a bucket and what is an object as you
can see here you have inside this bucket
which is a logical container for an
unlimited amount of objects you could
have objects of different shapes and
sizes like I said you could have
pictures database backups Etc so this is
what's being represented here by
different shapes
and you can think of a bucket really as
almost like a folder where objects or
files are placed within them
so there are many ways for us to place
objects within a bucket we can do this
through the AWS console which I'll be
showing you very shortly we can do this
via the command line interface or we can
do this to a software development kit at
the end of the day all those three
options go through an API right so it's
all about picking the right method that
is best suited for whatever kind of end
user or application that needs access to
these buckets and to these objects now
once you've got the data or objects
within this bucket
you can control pretty much how the data
is accessed how it's stored whether you
want it to be encrypted and how it's
even managed so you can have
organizations that have specific
compliance needs for example any objects
that are placed in or perhaps in PDFs
are not to be modified or not to be
touched for 90 days let's say we can
imply object locks if we want to we can
imply security guards we can also for
example record all API actions
that try to access to list to delete any
kind of API operations on those objects
at the Bucket Level or on the object
level we can record if that is some sort
of organizational compliance need for
auditing or for whatever internal
auditing reasoning that you may have
so continuing on here you can see that
there are many organizations that use S3
one in point is Amazon itself S3 is so
popular and so durable that Amazon
internally uses it to store its own data
you're guaranteed you know almost that
you're not going to lose any object in
there because it has what we call 11 9's
durability now 11 9's durability is
really an extreme amount of durability
where it's mathematically
almost impossible for you to lose an
object once you've placed it in S3
you're in fact more liable to get hit by
meteorite than you are to lose an object
in S3 statistically speaking which is a
pretty incredible statistic but it's but
it's true
so if we continue here with the benefits
of S3 some of these I've already
described but let's talk about the full
picture here we see the performance
scalability availability and durability
of S3 are really uh you know first class
in the industry again durability we were
talking about that 11 9 silver that
really means is
99.99999 in total if you count all the
nines that's 11 of those nine so
um it's most durable in the industry by
far and again because everything is
handled underneath the covers it's a
serverless service they ensure the
scalability and availability and the
performance of the inner workings of
that object level storage now cost is of
course always important and S3 really
shows up here with first class support
again for cost it's got very very low
cost we can have object level storage
for let's say a terabyte of object level
storage for as little as a dollar
a month so again it will depend on the
kind of storage class we're going to
have a discussion on different types of
storage classes that we might want to
transition or move over our objects for
one storage class to the next depending
on how frequently accessed that data is
as you know data over time seems to get
less and less accessed as you know your
needs shift from
one place to another we're going to talk
about that coming up very soon so we're
going to want to really focus on that to
reduce our end of the month costs for
storage with S3 of course security is
always at the Forefront and we want to
make sure that we either simply secure
by encrypting everything right off the
bat either at rest in transit or we want
to put an object lock or just maintain
security at the Bucket Level maintaining
let's say who or what has access to that
data because by default no one has
access to the data unless we open up the
door and also we want to make sure that
we don't give public access to our
bucket for obvious reasons of
I'm giving away important information by
mistake so AWS makes it extremely
difficult for you to accidentally do
this and I'm going to show this in a
demonstration lab coming up very soon
and we can also query in place which is
very interesting so you can imagine you
putting let's say
a file that's in CSV format or a Json
format or parquet format some sort of
structured format or semi-structured
format in S3 and you could actually use
SQL queries on the spot to filter that
data in place at the Bucket Level you
could also have other services that may
want to extract some business
intelligence from that data in your S3
bucket directly as well so some other
more advanced querying operations can
take place at the S3 level so this you
will learn during your journey with
simply learn as you're covering all the
AWS technology stacks
and here the most widely used storage
cloud service in the industry because of
all the points that we just covered it
really is the number one storage or
object level storage solution in the
industry
let's now take a look at objects and
buckets in S3 so the objects are really
the fundamental entities that are stored
in S3 or the lowest common denominator
which means that we're not really
interested in what kind of data is
within the object at the layer of S3
because S3 doesn't have direct access to
the data within an object like I said
before it's an abstraction of the data
so we don't know if it's a cat's picture
if it's a backup of your database it's
just treated as a fundamental entity aka
the object now every object is
associated with metadata so data about
itself things like what's the name of
the object what's the size the time and
date that the object was uploaded things
of that nature are categorized
as metadata so S3 does have direct
access to that now the data within that
object of course is accessible by other
services so it's not that once you've
uploaded it you've totally lost the data
and it's only can be treated as an
object it's just that at this layer
it's simply just assigned metadata and a
version ID a unique version ID and if
you re-upload the exact same object a
second or third time to S3 it will have
its own version ID number so a new
unique version ID number will be
generated
so really what buckets are are there
logical containers to store those
objects so of course at an extremely
high level a bucket would be like your
root file system if you want to think
about it like that but that doesn't mean
you can't go into this bucket and create
separate folders now when you create
separate folders in a bucket because you
might want to logically organize all
your objects
you might be fooled by the fact that you
think this is a hierarchical storage
system when in fact it is not and I'll
talk about that in a second so you
cannot store an object without first
storing it into a bucket so of course
the first step would be for us to create
a bucket and then upload objects within
that bucket so an object connect can
cannot exist without its container or
its bucket
so there's no windows explore view like
we're used to in an operating system
because this is not a hierarchical view
no matter if you create
folders within folders within folders in
fact S3's internal architecture is a
flat hierarchy
what we do instead is we assign prefixes
which are treated as folders in order to
logically organize our objects within
our buckets and I'm going to Showcase a
prefix in one of the demonstrations
coming up very soon
so when you create or when you're
working with S3 first of all you're
working at a regional level you're gonna
have to pick a region for example you
might pick us East one region or USC's
two region and the bucket that you're
going to be creating will be replicated
across several availability zones but
within that region
also that data those objects can be
accessed globally because S3 uses the
HTTP protocol HTTP protocol is very
permissive everybody knows how to
administrate it and work with it so we
just need to give access to that object
in terms of a permission policy
and give it the proper
URL or give whoever needs access to the
proper URL and they can access that via
HTTP globally so first when you're
creating a bucket you have to select the
region and the object will live within
that region but that doesn't mean that
it still can't be accessed globally
so let's now take a minute go over to
the AWS console and actually let me
showcase how to create
objects and buckets
so for our first lab we're going to be
going and creating an S3 bucket and
uploading an object to it so we first
have to log into the AWS console which
we have up here in the address bar
let's click on either create free
account if it's your very first time or
as in is in my case already have an
account we're going to sign in here
you're gonna have to pass in your IM
username password and your account ID of
course
once you've logged in you can search for
the simple storage service S3 either by
coming up here in the search box and
typing S3 and you'll find it there
that's probably the easiest way or if
you want you can take a look at all the
services and it'll be under storage over
here the first one okay so pick whatever
method you see best for yourself
once we're here we want to create
our first bucket
now I've already have a couple of
buckets here so we're going to go on and
create a new bucket the very first thing
is you have to specify a bucket name now
this bucket name has to be globally
unique if you take a look here at the
region that we're in it doesn't actually
select a specific region it selects the
global option which means that this
bucket will become globally accessible
so that is why it needs to have a unique
name much like a DNS name has to be
unique for your website right so I'm
going to come up here
and pick a name that I think is going to
be unique so I'm going to say simply
learn
S3 demo
all right let's take a look at if that's
going to work out of course we have to
pick a region but it is globally
accessible so you either pick a region
that's closest to your end users or in
our case since we're just doing a
demonstration we can do whichever is
closer to us right now okay and we're
going to skip these options for now
we're going to come back to them later
on we're just going to create
the bucket
now hopefully that was a unique name and
allowed me to create that and it looks
like it did so if I scroll down you can
see that it clearly got created over
here
now we're going to click on that and
we're going to start
uploading objects to this bucket
so let me click on the upload button and
you can either select one or more files
or you can select an entire folder so
I'm going to just go and select a
specific cat picture that I have here
okay and again we'll go through some of
those other options later on and we'll
just click upload
now this should take no time at all
because of the fact that's a very small
object or file that's being uploaded so
it is has succeeded we can close this up
and we see now clearly that the object
is in our bucket if we go to properties
we can see the metadata associated with
this bucket we can see the region that
it's in we can see the Arn which is the
Amazon resource name which uniquely
identifies this resource this bucket
across the globe so if ever you needed
to reference this let's say in an IM
policy or with other service needed to
communicate with S3 you would need this
Arn it's a very important piece of
information and of course we have the
creation date some some high level
metadata so objects as we have covered
already consist of not only the data
itself but the metadata so there's lots
of metadata and there's a lot of other
features that we can go here and enable
very easily and this will be the basis
of the future demonstrations that I'm
going to do all right so just to recap
what we just did we created a unique
bucket
give it a name of
simply learn S3 demo and uploaded our
first object to it
so let's now take a look at the inner
workings of the Amazon S3
when we upload an object into a bucket
we have to select which one of these
storage classes the object will reside
in so you see have six storage classes
here at the bottom and each have their
own characteristics that we're going to
get
into by default
if you don't specify anything it'll get
placed in What's called the S3 standard
storage class which is the most
expensive out of all these storage
classes
once your object gets colder and what I
mean by colder is your axis patterns
diminish meaning that you're accessing
that file less and less over the course
of time so it gets colder
you will transition that object from one
tier to the next all the way to for
example S3 deep archive so again deep
archive signifies extremely cold so
maybe you're only referencing this data
once a year once every couple of years
and so you want to have the cheapest
possible storage available so right now
you can get about one terabyte of
storage per month for about a dollar a
month with S3 Glacier deep archive so
you are going to be very interested in
knowing how to transition from one or
more of these storage classes over time
in order to save on your storage costs
that's really why we're doing this so
let's go through some of these storage
classes by default like I said whenever
you upload an object you automatically
get placed into the standard storage
class so any files that you're working
on frequently daily this is the best fit
for it got the highest level
of accessibility and durability as well
not that the others don't have the same
level of durability however we'll see
how when you transition from one
storage tier to the next some
characteristics do change in order for
you to save on some cost we're going to
go through some of those now this would
be considered hot data right data that's
used all the time maybe just by you
maybe by everybody right so that's the
perfect place to place it in the
standard storage class now over time
like I said you may find yourself
working
less and less perhaps on a document that
was due by the end of the month that
document was submitted and then
afterwards you don't work on that
document anymore perhaps you're only
working on revisions based on feedback
from your colleagues that are asking you
to make some corrections or some
amendments and so only those Corrections
or amendments come in perhaps once a
month and so in that case you might find
yourself
finding a justification for moving that
document from the standard tier to the
standard IA or infrequently accessed
here maybe any objects not modified for
more than 30 days are a good fit for
that and that's really the criteria for
IA in active access is that S3
or AWS itself recommends to only put
objects in there if they haven't been
asked to access for at least 30 days so
you get a a price reduction a rebate for
putting objects that are not accessed
frequently in here of course if you
remove objects let's say before the
30-day limit then you are charged a
surcharge for retrieving an object that
you said was infrequently accessed but
it really was not so bear that in mind
if you're going to place objects in in
frequent access
be somewhat reasonably assured that
you're not going to be going there and
accessing them you can still access
those files no problem just as quickly
they have the same level of
accessibility and durability however
like I said anything less than 30 days
you'll get a price ding on that
if you want to have long-term storage
we're talking about Amazon Glacier so
this is more anything over 90 days that
hasn't been modified or 180 days there's
two subcategories of Amazon Glacier that
we're going to get into and this is the
cheapest storage by far and Amazon
Glacier doesn't really operate through
the AWS console as the same as the
standard and the infrequent access you
can't really upload objects to Glacier
via the console in the browser
you can only do so let's say through the
command line interface or through an SDK
the only thing you can do on the web
console with Glacier is actually create
what's called a vault which is a logical
container for your archives but then
after that you have to go through the
CLI or the SDK to do the rest of the
work there
if we continue on there's some gray
areas between uh the S3 standard in a
glacier one is the one zone in
frequently accessed storage class so if
we go back to the regular standard and
IA storage class all of these objects
are stored across a minimum of three
availability zones
if you want a further price reduction
you can store your objects in a one zone
IA storage class which means that
instead of taking that object and
replicating it across three or more
availability zones it will only start in
a single availability Zone therefore
reducing
at the level of availability that you
have to that object so in this case here
if that single availability Zone would
go down for example you would not have
access to that object once it would come
back up of course you would the other
thing is is if there was a an immense
catastrophe where the actual
availability Zone was destroyed well of
course then your object is also gone so
if that's something that doesn't worry
you because you have already many copies
of this object maybe lying around on
premise then this is a good option for
you because it's data that you're
willing to lose or lose access to for
short periods of time if ever that
single availability Zone goes down so
it's about an extra 20 off the price
from already the normal IA standard
price
there is another one called the standard
reduced redundancy storage
this one is kind of getting phased out
as we speak because the same price for
this storage class is about the same
amount you're going to pay for the
normal IA standard class what this does
is again is a good fit for
um your objects that you're not really
worried about losing if there is some
sort of catastrophe that happens in an
availability Zone there's less copies of
it that are stored and so if that data
center and that availability Zone goes
down then you lose your object so of
course it offered at the time the
highest price reduction possible but now
the difference between this one and the
normal IA standard storage class is so
small in terms of price that you're
probably not going to migrate to or
navigate to this storage class but it is
still there in the documentation and it
may very well come up still in the
certification exam so at least be aware
of that
let's now take a look at some individual
features of S3 starting off with life
cycle management so life cycle
management is very interesting because
it allows us to come up with a
predefined rule that will help us
automate the transitioning of objects
from one storage class to another
without us having to manually copy
things over of course you could imagine
how time consuming that would be if we
had to do this manually
so we're going to see this very soon in
a lab however let me discuss how how
this works
so once we it's basically a graphical
user interface it's very very simple to
use once you come up with these uh
lifecycle management rules but you're
going to Define two things you're going
to define the transition action and the
expiration action so the transition
action is going to be something like
well I want to transition an object
from maybe it's all objects or maybe
it's just a specific type of object in a
folder example that has a specific
prefix from one storage class let's say
standard two standard inactive or
infrequent access maybe only after 45
days after at least a minimum of 30 days
like we spoke of before and then maybe
after
90 days you want to transition the
objects in IA to right away Glacier deep
archive or 180 days you come up with
whatever combination you see fit okay it
doesn't have to be sequential for ms3 to
IA to one zone et cetera et cetera
because like we discussed before it
depends what kind of objects that you're
interested in putting in one zone
objects that you don't really mind
losing if that one availability Zone
goes down so you're going to be deciding
those rules
it ends up that this even is not a
simple task because you have to monitor
your usage patterns to see which data is
hot which data is cold and what's the
best kind of life cycle management to
implement to reap the benefits of the
lowest cost so you have to put somebody
on this job and make the best informed
decisions based on your access patterns
and that is something that you need to
consistently monitor
so what we can do is we can instead opt
for something called S3 intelligent
tiering
which basically analyzes your workload
using machine learning algorithms
and after about a good 30 days of
analyzing your access patterns
will automatically be able to transition
your objects from S3 standard to S3
standard infrequent access okay it
doesn't go past the Ia one it doesn't go
after the glacier and whatnot okay so it
can then offer you
um that at a reduced price overhead so
there is a monitoring fee that is
introduced in order to implement this
feature it's a very nominal very very
low monitoring fee
and the nice thing is is if ever you
take out an object out of the infrequent
axis before the 30-day limit as we spoke
of before you will not be charged
um an overhead charge because of that
why because you're using the
intelligence tiering you're already
paying an overhead for the monitoring
fee so at least in that sense the
intelligent tearing will take the object
out of IA and put it back into the S3
standard class if you need access to it
before the 30 days and in that case you
will be charged that overhead so that is
something that is very
um that is very um good to to to do in
order not to have to put somebody on
that job so yes you're paying a little
bit of overhead for that monitoring fee
but at the other side of the spectrum
you're not investing in somebody uh
working many hours to Monitor and put
into place a system to monitor your data
access patterns so let's take a look at
how to do this right now let's Implement
our own
life cycle management rules
so let's now create a lifecycle rule
inside our bucket first off we're going
to need to go to the management tab
and the bucket that we just created and
right on the top you see right away life
cycle rule we're going to create
lifecycle Rule and we're going to name
it so I'm just going to say something
very simple like simply
learn
lifecycle rule
and we have the option of creating this
rule for every single object in the
bucket or we can limit the scope to a
certain type of file perhaps with a
prefix like
I could see one right now something like
logs so anything that we categorize as a
log file will transition from one
storage tier to the next as per our
instructions we're doing this because we
really want to save on costs right it's
not so much of organizing what's your
older data versus your newer data it's
more about
reducing that storage cost as your
objects get less and less used so in
this case the logs are a good fit
because perhaps you're using your logs
for the first 30 days you're sifting
through them you're trying to get
insights on them but then you kind of
move them out of the way because they
become old data and you don't need them
anymore so we're going to see how we can
transition them to another pricing to
another storage tier
uh we could also do this with object
tags which is a very powerful feature
and in the livestocker rules action you
have to at least pick one of these
options now since we haven't enabled
versioning yet what I'm going to do is
just select transition to current
version of the object between the
storage classes
so as a reminder of what we already
covered in the slides are storage
classes are right over here so the one
that's missing is obviously
the default standard storage class which
all objects are placed in by default so
what we're going to say is this we want
our objects that are in the default
standard storage class to go to the
standard inactive access storage class
after 30 days and that'll give us a nice
discount on those objects being stored
then we want to add another transition
and let's say we want to transition them
to Glacier after 90 days
and then as a big finale we want to go
to Glacier deep archive you can see the
rest are grayed out would it make sense
to go back and maybe after 180 days we
want to go there okay now there's a
little bit of um a warning or a call to
attention here they're saying if you're
going to store very small files
into Glacier not a great idea there's an
overhead in terms of metadata that's
added and also there's an additional
cost associated with storing small files
in Glacier so we're just going to
acknowledge that of course for the
demonstration that's fine in real life
you'd want to store a very big tar files
or zip files that had you know one or
more log files in there okay that would
bypass that that surcharge that you
would get
and over here you have the timeline
summary of everything we selected up
above so we have here after 30 days the
standard inactive access after 90 days
Glacier and after 180 days Glacier deep
archive so let's go and create that rule
all right so we see that the rule is
already enabled and at any time you
could go back and disable this if ever
you had a reason to do so we can easily
delete it as well or view the details
and edit it as well so if we go back to
our bucket
now what I've done
is created that prefix with the slash
logs since we're not doing this from the
command line we're going to create a
logs folder over here that will fit that
prefix so I'm going to create logs
create folder
and now we're going to upload our let's
say our Apache log files in here so
we're going to upload one demonstration
Apache log file that I've created with
just one line in there of course just
for demonstration purposes we're going
to upload that
and now we have we're going to close
that and now we have our our Apache log
file in there so what's going to happen
because we have that life cycle rule in
place after 30 days anything any file
that has the logs prefix where basically
is placed inside this folder will be
transitioned as per that life cycle Road
policy that we just created so
congratulations you just created your
first S3 lifecycle rule policy
let's now move over to bucket policies
so bucket policies are going to allow or
deny access to not only the bucket
itself but the objects within those
buckets to either specific users or
other services that are inside the AWS
Network now these policies fall under
the category of I am policy so IM stands
for identity and access management and
this is a whole other topic that deals
with security at large so there are no
services in AWS which are allowed to
access other services or data for
example within S3 without you explicitly
allowing it through these IM policies so
one of the ways we do that is by
attaching one of these policies which
are written in a Json on format so it's
a text file that we write at the end of
the day that's the artifact and that's a
good thing because we can use that
artifact
and we can configuration control it in
our source control and version it and
put it alongside our source code
so when we deploy everything it is part
of our deployment package so in this
case here we have several ways of doing
this we can use What's called the policy
generator which is a graphical user
interface that allows us to Simply click
and point and populate certain text
boxes which will then generate that Json
document that will allow us to attach
that to our S3 bucket and that will
determine like I said which users or
Services have access to
whatever API actions are available for
that resource so we might say we want
certain users to be able just to list
the contents of this bucket not
necessarily be able to delete or upload
new objects into that bucket so you can
get very fine-grained permissions based
on the kind of actions you want to allow
on this resource so in order to really
we bring this home let's go and perform
our very own lab on this
let's now see how to create an S3 bucket
policy going back to our bucket we're
now going to go into permissions
so the whole point of coming up with a
bucket policy is that we want to control
who or what the what being other
services have access to our bucket and
our objects within our bucket so there
are several ways we can go about doing
this let's edit a bucket policy
one we can go and look at a whole bunch
of pre-canned examples which is a good
thing to do
two we could actually go in here and
code the Json document ourselves which
is much more difficult of course so what
we're going to do is we're going to look
at a policy generator which is really a
form-based graphical user interface that
allows us to generate through the
answers that we're going to give here
the Json document for us
first question is we got to select the
policy type of course we're dealing with
S3 so it makes sense for us to create an
S3 bucket policy
the two options available to us are
allowing or denying access to
RS3 bucket now in this case here we
could get really
um fine-grained and specify certain
kinds of services or certain kinds of
users but for the demonstration we're
just going to select star which means
anything or anybody can access this S3
bucket all right now depending on also
the actions that we're going to allow so
in this case here we can get very fine
grained and we have all these check
boxes that we can check off to give
access to certain kind of API action so
we can say we want to give access to you
know just deleting the bucket which
obviously is something very powerful but
you can get more fine-grained as you can
see you have more of the Getters over
here and you have more of the
the the listing and the putting new
objects in there as well so you can get
very fine grain now for demonstration
purposes we're going to say all action
so this is a very
Broad and wide ranging permission
something that you really should think
twice about before doing where it's
basically saying we want to allow
everybody and anything any service all
API actions on this S3 bucket so that's
no a small thing
we need to specify the Amazon resource
name the Arn of that bucket specifically
so what we're going to do is go back to
our bucket
and you can see here uh the bucket Arn
okay so we're just going to copy this
paste it in this policy generator and
just say add statement you can see here
kind of a resume of what we just did and
we're going to say generate policy and
this is where it creates for us and make
this a little bit bigger for us it
creates that Json document so we're
going to take this we're going to copy
it and we're going to paste it into
the generator okay now of course we
could flip this and change this to a
deny
right which would basically say we don't
want anybody to have access or any thing
any other service to have access to this
S3 bucket we could even say
slash star to also encapsulate all the
objects within that bucket so if I save
this right now
you have a very Ironclad
S3 bucket policy which basically denies
All Access
to this bucket and the objects within of
course this is on the other side of the
spectrum very very secure so we might
want to for example host a static
website
through our S3 bucket so in this case
here allowing access would make more
sense right so if I save changes you see
that we get an error here saying that we
don't have permissions to do this
and the reason for that is because it
realizes that this is extremely
permissive so in order to give access to
every single object within this bucket
as in the case that I was stating of a
static website being hosted on your S3
bucket it would be much better to also
it first enable that option so I'm just
going to duplicate the tab here
and once you go back to the permissions
tab one of the first things that shows
up is this block Public Access setting
right right now it's completely blocked
and that's what's stopping us from
saving our policy we would have to go in
here
unblock it and save it right and it's
also kind of like a double clutch
feature you have to confirm that just so
you don't do that by accident
right so now what you've effectively
done is you've really opened up the
floodgates to have public access to this
bucket it's something that can't be
accidentally done it's kind of like
having to perform these two actions
before the public access can be granted
now historically this was something that
AWS was
um
was guilty of was making it too easy to
have Public Access so now we have this
double clutch now that this is enabled
or turned off we can now save
our changes here successfully
and you can see here that now it's
publicly accessible which is a big red
flag that perhaps this is not something
that you're interested in doing now if
you're hosting a public website and you
want everybody just to have read access
to every single object in your bucket
yes this is fine however please make
sure that you
um
pay very close attention to this type of
access flagged over here on the console
so congratulations you just got
introduced to your first bucket policy a
permissive one but at least now you know
how to go through that graphical user
interface through the policy generator
and create them and paste them inside
your S3 bucket policy pane
so let's continue on with data
encryption so any data that you place in
S3 bucket can be encrypted at rest very
easily
using an AES 256 encryption key so we
can have server-side encryption we could
have AWS handle all the encryption for
us and the decryption will also be
handled by AWS when we request our
objects later on but we could also have
client-side encryption where we the
client that are uploading the object
have to be responsible for also passing
over our own generated key that will
eventually be used by AWS to then
encrypt that object on the bucket side
of course once that happens then the key
is discarded the client key is discarded
and you have to be very mindful that
since you've decided to handle your own
encryption client-side encryption that
if ever you lose those keys well that
data is not going to be recoverable in
that bucket on the AWS Network so be
very careful on that point
we can also have a very useful feature
called versioning which will allow you
to have a history of all the changes of
an object over time so versioning sounds
exactly how it's named every time you
make a modification to a file and upload
that new version to S3 it will have a
brand new version ID associated with
that so over time you get a sort of
stack of a history of all the file
changes over time so you can see here at
the bottom you have an ID with all these
ones and then an ID with one two one two
one two So eventually if ever you wanted
to revert back to a previous version you
could do so by accessing one of those
previous versions of course versioning
is not an option that's enabled by
default you have to go ahead and enable
that yourself it is extremely simple
thing to do
and so there may be a situation where
you already have objects within your
buckets and you only then enable
versioning well versioning would only
apply to the new objects that would get
uploaded from the point that you enabled
versioning the objects that were there
before that point will not get a
specific version number attached in fact
they will have a sort of null marker the
version number that will get attached to
them it's only after that you modify
those objects later on and upload a new
version that they will get their own
version numbers
so right now what we're going to be
doing is a lab on actual versioning so
let's go ahead and do that right now
in this lab we're going to see how to
enable versioning in our buckets enabled
versioning is very easy we're simply
going to click on our bucket go into
properties and there is going to be a
bucket versioning section
I'm going to click on edit and enable it
once that's done
any new objects that are uploaded to
that S3 bucket will now benefit from
being tracked by a version number so if
you upload
objects with the same file name after
that they'll each have a different
version number so you'll have version
tracking a history of the changes for
that object let's actually go there and
upload a new file
I'll upload one called index.html so
we're going to simulate a situation
where we've decided we're going to use
an S3 bucket as the source of our static
website to deploy one and in this
index.html file if you take a look right
now let's take a look at what's in there
you can see that we have welcome to my
website and we're at version two okay so
if I click on this file right now
and I go to versions
I can clearly see that there's some
version activity that's happening here
okay we have here
at 1456 which is the latest one the
latest one is on the top the current
version we have a specific version ID
and then we have a sort of history of
what's going on here now I purposely
enabled versioning before and then try
to
delete version or disable versioning but
here's the thing with versioning you
cannot disable it fully once it's
enabled you can only suspend it
right now suspending means that whatever
version numbers those objects had before
you decided to suspend it will remain so
you can see I have an older version here
that has an older version number and at
this point here I decided to suspend
versioning and so what it does instead
of disabling the entire history it puts
what's called a delete marker okay you
could always roll back to that version
if you want now in the demonstration
when we started it together I enabled it
again so you can see this is actually
the brand new version number as we did
it together but you don't lose the
history of previous versioning if ever
you had suspended it before so that's
something to keep in mind right and
it'll come up in the exam where they'll
ask you can you actually disable
versioning once it's enabled and the
answer is no you can only suspend it and
your history is still maintained now we
have that version there and let's say I
come to this file and I want to upgrade
this I don't know I say version three
right and now it's going to happen
is if I click on this version this is
the current one with version two and I
open this we should see version two
which is fine that's that's that's
expected if we go back to our bucket and
upload that new version file that has
version three in there
I want to just modify it
we should now see in that index.html
file a brand new version that was
created
under the versions Tab and there you go
1458 just two minutes after you can see
here we have a brand new version ID
right and if I open this one
you can see version three so now you
have a way to enable versioning very
easily in your buckets and you also have
seen what happens when you want to
suspend versioning what happens to the
history of those versions files before
just to actually go back here to the
properties
uh where we enabled versioning in the
first place if I want to go back in here
and disable it like I said you can't
disable you can only suspend and that's
where that delete marker gets placed but
all your previous versions retained
their version ID so don't forget that
because that will definitely be a
question on your exam if you're
interested in taking the certification
exam so congratulations you just learned
how to enable
versioning
let's move on
to cross region replication or crr as it
is known there will be many times when
you find yourself with objects in a
bucket and you want to share those
objects with another bucket now that
other bucket could be within the same
account could be within another account
within the same region or could be
within a separate account
in a different region so there's varying
levels of degree there the good thing is
is all of those
um combinations are available so crr if
we're talking about cross region
replication is really about replicating
objects across regions something that is
not enabled by default because that will
incur a replication charge because it's
syncing objects across regions of course
you are spanning a very wide area
network in that case so there is a
surcharge for that
now doing so is quite simple to do but
one of the things that we have to be
mindful of is to give permissions for
the source bucket which has the
originals
to allow for this copying of objects to
the destination bucket so if we're doing
this across regions of course we would
have to come up with IM policies and we
would also have to exchange credentials
in terms of
IM user credentials in terms of account
IDs and the such we're going to be doing
a demonstration in the same account in
the same region but largely this would
be the same steps if we were going to go
cross region so this is something you
might find yourself doing if you want to
share data with other
um
entities in your company maybe you're a
multinational and you want to have all
your lock files copied over to another
bucket in another region for another
team to analyze to extract business
insights from or it might just be that
you want to aggregate data in a separate
data Lake in an S3 bucket in another
region or in a like I said it could be
even in the same region or in the same
account so it's all about organizing
moving data around across objects across
these boundaries and let's actually go
through a demonstration and see how we
can do crr
let's now see how we can perform cross
region replication
we're going to take all the new objects
that are going to be uploaded in the
simply learn S3 demo bucket and we're
going to replicate them into a
destination bucket so what we're first
going to do is create a new bucket okay
and we'll just tack on the number two
here and this will be our destination
bucket where all those objects will be
replicated to we're going to demonstrate
this within the same account but it's
the exact same steps when doing this
across regions
one of the requirements when performing
cross-region replication is to enable
versioning so if you don't do this you
can do it at a later time but it is
necessary to enable it at some point in
time before coming up with a cross
region replication rule
all right so let me create that bucket
and now after the bucket is created I
want to go to the source bucket and I
want to configure under the managements
tab here
a replication rule
so I'm going to create a replication
rule call it
simply learn
rep rule
and I'm going to enable this right off
the bat
The Source bucket of course is the
simply learn S3 demo we could apply this
to all objects in the bucket or perform
a filter once again let's keep it simple
this time and apply to all objects in
the bucket of course caveat here this
will only now apply to any new objects
that are uploaded into this Source
bucket and not the ones that are already
pre-existing there okay now in terms of
the a destination bucket we want to
select the one we just created so we can
choose a bucket in this account or if we
really want to go across region or
another account in another region we
could specify this and put in the
account ID in the bucket name in that
other account
so we're going to stay in the same
account we're going to browse and select
the newly created bucket
and we're also going to need permissions
for the source bucket to dump those
objects into the destination bucket so
we can either create the role ahead of
time or we can ask this user interface
to create a new role for us so we'll opt
for that
and we'll skip these additional
features over here that we're not going
to talk about in this demonstration
we're just going to save this so that
will create our replication rule that is
automatically enabled for us right now
so let's take a look at the overview
here you can see it's been enabled just
to double check the destination bucket
is the demo two we're talking about the
same region
and again here we could opt for
additional
um parameters like different storage
classes in the destination bucket that
that object is going to be deposited in
etc etc for now we just created a simple
rule now if we go back to
the original Source bucket which we're
in right now and we upload a new file
which will be transactions file in a CSV
format
once this is uploaded
that cross region replication rule
will kick in and will eventually right
it's not immediate but will eventually
copy
the file inside the demo 2 bucket now I
know it's not there already so what I'm
going to do is pause the video and come
back in two minutes and when I click on
this the file should be in there
okay so let's now double check and make
sure that object has been replicated
and there it is been replicated as per
our rule so congratulations you just
learned how to perform your first same
account S3 bucket region replication
rule
I'll take a look at transfer
acceleration
so transfer acceleration is all about
giving your end users the best possible
experience when they're accessing
information in your bucket so you want
to give them the lowest latency possible
you can imagine if you were serving a
website
uh and you wanted people to have the
lowest latency possible of course that's
something that's very desirable so in
terms of traversing long distances if
you have your bucket that is in for
example the U.S east wine region in the
United States in the Virginia region and
you had users let's say in London that
want to access those objects of course
they would have to Traverse a longer
distance than users that were based in
the United States and so if you wanted
to bring those objects closer to them in
terms of latency then we could take
advantage of What's called the Amazon
cloudfront delivery Network the CDN
Network which extends the AWS backbone
by providing what's called
Edge location so Edge locations are
really data centers that are placed in
major city centers where our end users
mostly are located more densely
populated areas and your objects will be
cached in those locations so if we go
back to the example of your end users
being in London well they would be
accessing a cached copy of those objects
that were stored in the original bucket
in the for example Us East one region of
course you will get most likely a
dramatic performance increase by
enabling transfer accelerations very
simple to enable this just bear in mind
that when you do so that you will incur
a charge for using this feature the best
thing to do is to show you how to go
ahead and do this so let's do that right
now
let's now take a look at how to enable
transfer acceleration on our simply
learn S3 demo bucket
by simply going to the properties tab we
can scroll down and look for a heading
called transfer acceleration
over here and very simply just enable it
so what does this do
this allows us to take advantage of
What's called the contents delivery
Network the CDN which extends the AWS
Network backbone
the CDN network is strategically placed
into more densely populated areas for
example major city centers and so if
your end users
are situated in these more densely
populated areas they will reap the
benefits of having transfer acceleration
enabled because the latency that they
will experience will be severely
decreased so their performance is going
to be enhanced if we take a look at the
speed comparison page for transfer
acceleration we can see that once the
page is finished loading it's going to
do a comparison it's going to perform
first of all what's called a multi-part
upload and it's going to see how fast
that upload was done with or without
transfer acceleration enabled now this
is relative to where I am running this
test so right now I'm actually running
it from Europe so you can see that I'm
getting very very good results if I
would enable transfer acceleration and
my users were based in Virginia so of
course now I have varying
uh differences in percentage as I go
closer or further away from my region
where my bucket or my browser is being
um is being referenced so you can see
here United States I'm getting pretty
good uh percentages as I go closer to
Europe it gets lower of course but still
very very good Frankfurt again this is
about us probably the worst I'm going to
be getting here since I'm situated in
Europe
and of course as I go look more towards
you know the Asian regions you can see
once again it kind of scales up in terms
of better performance so of course this
is an optional feature once you enable
it as I just showed over here this is a
feature that you pay additionally for
so bear that in mind make sure that you
take a look at the pricing page in order
to figure out how much this is going to
cost you so that is it congratulations
you just learned how to
simply enable transfer acceleration to
lower the latency from the end user's
point of view
foreign
we're now ready to wrap things up in our
conclusion and go over at a very high
level what we just spoke about so we
talked about what S3 is which is a core
service one of the original Services
published by AWS in order for us to have
unlimited object storage in a secure
scalable and durable fashion we took a
look at other aspects of S3 in terms of
the benefits we mainly focused on the
cost savings that we can attain
in S3 by looking at different storage
classes now of course S3 is industry
recognized as one is the cheapest object
storage
Services out there that has the most
features available we saw what goes into
the object storage in terms of creating
first our buckets which are our
containers high level containers in
order for us to store our objects in
again objects are really an abstraction
of the type of data that are in there as
well as the metadata associated with
those objects
we took a look at the different storage
tiers the default being this standard
all the way till the cheapest one which
is the glacier which are meant for a
long term archived objects for example
log files that you may hold on to for a
couple of years may not need to access
routinely and we'll have the cheapest
pricing option by far so we have many
pricing tiers and if you want to
transition from one tier to the next you
would Implement a life cycle policy or
use the intelligent tiering option that
can do much of this for you we took a
look at some very interesting features
starting from the life cycle management
policies that we just talked about all
the way till versioning cross region
replication and transfer acceleration so
with this conclusion you are now ready
to at least start working with S3 hello
everybody my name is Kent and today
we're going to be covering AWS
identity and access management also
known as IM tutorial by simply learn
so these are the topics which we'll be
covering today we'll be defining what
AWS security is the different types of
security in AWS what exactly is identity
and access management
the benefits of identity and access
management
how I am works
its components its features and at the
end we're going to do a great demo on IM
with multi-factor authentication and
users and groups
so without any further Ado let's get
started
so let's get started with what is AWS
security
your organization May span many regions
or many accounts and you may Encompass
hundreds if not thousands of different
types of resources that need to be
secured and therefore you're going to
need a way to secure all that sensitive
data in a consistent way across all
those accounts in your organization
while meeting any compliancy and
confidentiality standards that have to
be met so for example if you're dealing
with Healthcare data or credit card
information or personal identification
information like addresses this is
something that needs to be thought out
across your organization so of course we
have Individual Services in AWS which we
will explore
in this video however in order to govern
the whole process at an organizational
level we have AWS security Hub now AWS
security Hub is known as a cspm which
stands for a cloud security posture
management tool so what does that really
mean well it's going to Encompass like I
said all these tools underneath the hood
in order to bring a streamlined way for
you to organize and adhere to these
standards across organization it'll
identify any misconfiguration issues and
compliant risks by continuously
monitoring your Cloud infrastructure for
the gaps in your security policy
enforcements now why is that important
well these misconfigurations
can lead to unwanted data breaches and
also data leakages so in order to govern
this at a very high level like I said we
need to incorporate AWS security Hub
that will automate and manage all these
underlying services for us in order to
perhaps take automated action we call
that remediary actions and you can
approve these actions
manually or you can automate those
actions as you see fit across your
organization so let's delve a little bit
deeper into what is AWS security and
then we'll start looking individually at
some of these services
and so our organization has special
needs across different projects and
environments so of course the production
development test environments all are
going to have their own unique needs
however it doesn't matter which project
or Which business unit we're talking
about and what are their storage backups
needs they should all be implemented in
a concise standardized way across your
organization to meet that compliancy
we're just talking about so we want to
automate all these tedious manual tasks
that we've been doing like ensuring that
perhaps our buckets in S3 are all
encrypted or all our EBS volumes are
encrypted and we want to have an
automated process in place in order to
give us time on other aspects of our
business perhaps our application
development to bring better business
value and that allows to grow and
innovate the company and in a way that's
best suited for it so let's take a look
at the different types of AWS security
now there are many different types of
Security Services out there however
we're going to concentrate primarily on
IM in this video tutorial so I like to
think of I am as the glue between all
AWS Services because by default we don't
have permission for any one service to
communicate with another so let's just
take for example an ec2 instance that
wants to retrieve an object from S3
while those two Services could not
interact unless we interacted with I am
so something that's extremely important
to learn and by the end of this video
tutorial you'll be able to understand
what im is we have Amazon guard Duty
here which is all about logs basically
Aggregates all logs from example
cloudtrail which we still haven't talked
about at the end of this list over here
but instead of looking at cloudtrail
individually guard Duty will actually
take a look at Trail cloud trail we'll
take a look at your VPC flow logs and
we'll take a look at your DNS logs and
monitor that account and your network
and your data access via machine
learning algorithms and it'll ID any
threat
where you can automatically remediate
the workflow that you've approved so for
example if you know of a malicious IP
address that's making API calls well
those API calls are registered in
cloudtrail right so this machine
learning
algorithm will be able to detect that
and take action
so this is really governed at a higher
level than you having to you know
individually inspect all your DNS locks
flow logs and Cloud Trails individually
and take action through some scripting
fashion that that you've come up with so
guard Duty manages all that we have
Amazon Macy which once again uses
machine learning but also uses pattern
matching pattern matching can be used
with
matching kind of libraries that are
already in place or you can come up with
your own pattern matching and that's
used to discover and protect your
sensitive data so for example if you had
Healthcare data also known as HIPAA
compliancy or credit card data that's
lying around well Macy will Discover it
and will protect it
so as your data grows across your
organization that might be harder and
harder to do in your own way so Macy
really facilitates discovering and
protecting that so you can concentrate
on other things
AWS config is something that kind of
works in tandem with all the other
services it's able to continuously
monitor your resources configurations
and ensure that they match your desired
configuration so for example
um maybe you state that your S3 bucket
should be encrypted by default all of
them or you want to make sure that your
IM user access keys are rotated and if
they're not then take remediary action
so a lot of these automated remediary
actions right are basically executed by
AWS config so you'll see the AWS config
actually used by other services
underneath the hood and then you have
cloudtrail cloudtrail is all about
logging every single type of API call
that's made so it'll um it'll always
record the API call who made it from
what source IP the parameters that were
sent with the API
the response all that data which is
really a gold mine for you to
investigate if there's any security
threat and again this Trail over here
and there can be many and there's lots
and lots of data generated by these
Trails can be analyzed by these other
services
specifically guard Duty here and that
automates that process so you don't have
to
so let's get to the next one which is
what is identity and access management
so what is I am like I said IM is the
glue between all AWS Services it will
allow those services to communicate with
each other once you put IM into place
the other thing is is it allows to
manage our AWS users which are known as
IM users and group them together into
groups to facilitate assigning and
retrieving or removing permissions from
them instead of doing it on an
individual scale so of course if you had
200 IM users and you wanted in one shot
to add a permission to a group that
contained 200 IM users where you can do
that in one operation instead of 200.
now we do have a distinction between IM
users and end users end users use
applications online whereas IM users are
your employees in your organization that
are interacting directly with AWS
resources so for example an ec2 instance
would be a resource that let's say a
developer would be interacting with and
traditionally those are resources that
are used
full time from nine to five let's say uh
five days a week 365 days a year okay
now sometimes those users have to have
elevated permissions for a temporary
amount of time and that is more suited
for a role and that'll eliminate the
need to create separate accounts just
for you know type of actions that are
needed for let's say an hour a month or
two hours a a week or something like
that something like backing up an ec2
instance removing some files doing some
cleanups traditionally you don't have
those permissions as a developer but you
can be given temporary permissions to
assume this role so it's kind of like
putting a hat on your head and
pretending you're somebody that you're
usually not for a temporary amount of
time once you take off the hat you go
back to being your role boring I am user
self which doesn't have access to
performing backups or cleanups and stuff
like that so the rules interact with a
service called the secure token service
which gives us specific Keys three
um specifically an access key a secret
key much like a username and password
and then we have a token key which only
gives you access to this role this
elevated permission for 1 to 12 hours so
we'll talk more about rules as we go on
through this video tutorial but it's
important that you at least remember at
this point even if you don't know all
the details that roles have to do with
temporary permissions elevated
permissions
so what are the benefits of I am
well across our organization
we're going to have many accounts and
like I said hundreds if not thousands of
resources so scalability is going to be
an issue if we don't have a very high
level uh visibility and control over the
entire security process and once we have
a tool like for example security Hub
which we saw in the first slide we can
continuously Monitor and maintain a
standard across our organization so very
very important to have that visibility
and we do when we integrate with AWS
security
now we also need to eliminate the human
factor right so
we don't want to manually put out fires
every single time of course there will
be times when something new occurs that
we've never seen before that that you
know will be needed however Once An
Occurrence happens and reoccurs we can
obviously come up with a plan to
remediate the action so once we automate
we can definitely reduce the time to fix
that reoccurring error or it could be a
new air though we don't have to interact
with because we're using machine
learning algorithms services like I was
talking about like our Duty or Macy and
we can really reduce the risk of
security intrusions and data leakage and
such by using IM to facilitate this
of course you may have many compliance
needs you may be dealing with
applications that use Healthcare data or
credit card information for payments or
you might be dealing with a government
agency let's say in the US that has
certain compliancy requirements that
needs assurances that if their data is
stored on the cloud that you're still
following the same compliance controls
that you were let's say on premise so if
we have a very very long list of
compliance requirements that each AWS
service adheres to and you can go on the
um AWS documentation online and you
could figure out if for example dynamodb
is compliant with HIPAA compliancy and
it is and so you you can be sure that
you can use that so AWS itself has to
constantly maintain these compliancy
controls in place and they themselves
have to pass all these certifications
and that gets passed on to us so much
less work for us to implement these
compliance controls we can just inherit
them by using AWS a security model by IM
and last but not least we can build the
highest standards for privacy and data
security now having all this on our own
on-premise data center uh poses security
challenges especially physical security
challenges you have to physically secure
the data center you might have a
security Personnel that you need to hire
uh 24 7 camera surveillance etc etc so
just by migrating to the cloud we can
take advantage of aws's global
infrastructure they're very very good at
building data centers that's part of
their business and securing them so of
course we have a shared security model
in place however you can rest assured
that at least as the lowest common
denominator physical security has been
put into place for us again as other
services are used more managed services
or more higher end Services used in AWS
more and more of that security model
will be aws's responsibility and less
yours we will always have a certain type
of responsibility for our applications
but we can again use these high-level
services to ensure that our ability to
encrypt the data at rest and in transit
are always maintained by coming up with
specific rules that need to be adhered
to and if those rules are not adhered to
then we have remediary actions that take
place in order to maintain again that
cross account or organizational wide
security control okay so lots of
benefits of using IM and let's now take
a look at exactly how IM works and delve
deeper into
authentication and authorization
so there are certain terms here that we
need to know about one of them is
principle now principle is nothing more
than an entity that is trying to
interact with an AWS service for example
an ec2 instance or an S3 bucket now that
could be a user an IM user it could be
even a service or it could be a role so
we're going to take a look how
principles need to be specified
inside I am policies
authentication so authentication is
exactly how it sounds who are you could
be something as simple as username and
password or it could involve an email
address example if you're the root user
of the account when you're creating the
account for the very first time
other normal users will not need to log
in with their user email it'll only be
their root user that needs that
you could also have developer type
access which needs access let's say via
command line interface or a software
development kit also known as an SDK now
for those kind of access
points we're going to need access keys
and secret keys or public private Keys
let's say if you want to even
authenticate and log in through SSH to
an ec2 instance so there are many
different types of authentication that
we could set up when creating IM users
based on what kind of access they need
do they just need access to the console
do they need access to just Services uh
in terms of as a programmatic needs so
those are different types of
Authentication
and then we have the actual request now
we could make a request in AWS through
various ways we're going to be exploring
that via the console the AWS console
however every button you click every
drop down list you select that invokes
an API call behind the scenes so
everything goes behind a centralized
well-documented API every service has an
API so if you're dealing with
Lambda let's say well Lambda has an API
if you're dealing with ec2 ec2 has an
API now you can interact with that API
like I said via the console
indirectly you can use a command line
interface you can use an SDK for your
favorite programming language like
python so all of them get funneled
through all your requests get funneled
through an API
but that doesn't mean you're allowed to
do everything just because we know who
you are and you have access to some keys
let's say to make some API calls you
have to be authorized in order to
perform certain actions so
maybe you're only authorized to
communicate with dynamodb and also maybe
you're only authorized to perform reads
and not write so we can get some very
fine-grained
authorization through some IM policies
in order to control not only who has
access to our aw races but what they're
allowed to do so very fine-grained
actions read actions write actions both
maybe just describe or list some buckets
so depending on the AWS user that's
logged in we can control exactly what
actions because every API has
fine-grained actions that we can control
through I am and of course many many
different types of resource
or coming up with these policies all
these actions can be grouped into
resources so perhaps we can say well
this user only has access or read-only
access to S3 specifically these buckets
and can write to dynamodb table but
cannot
terminate or shut down an ec2 instance
so all these kind of actions can be
grouped into resources on a pre-resource
basis based on the kind of role you have
in the projects or in the company
so now let's take a look at the
components of IM
we've already seen that we can create IM
users and that how they are different
than your normal end users now those
entities represent a person that's
working for organization that can
interact with AWS services
and some of those permissions that we
assign through what's called identity
based policies to such a user will have
permissions for example that are always
necessary every time that they're logged
in so perhaps this user Tom always needs
access to the following Services over
here for example an RDS instance or an
S3 bucket etc etc and sometimes they
will need temporary access to other
services for example dynamodb
so in those cases there would be a
combination of normal IM policies
assigned to the user and certain assume
roll calls done at runtime in order to
acquire this temporary credential
elevated permission
there is also the root user which is
different than your typical super user
administrator access there are some
things that a root user can do that
administrator access cannot
for example a root user first of all
logs in with their email address and can
also have the option to change that
email address something that is very
tightly coupled to the AWS account so
you cannot change the email address with
an administrator account
the other thing your user can do is they
can change the support contract they can
also view billing information or taxes
information that information for example
so the best thing to do the best
practice is that once you've created
your AWS account you are obviously the
root user the best thing to do is to
enable multi-factor authentication store
away those keys in a secure location and
your first administrative task should be
to create an administrator super user
from which that point on you will log in
as only an administrative user and you
will create other type of administrative
users and those administrative users
will create other I am users so never
log into your AWS account as a root user
unless you need to access that specific
functionality that I said that is over
and above a super user administrator
account
so here we have a set of IM users and we
could assign to them directly IM
policies which give them access to
certain Services however the best
practice is to create groups which is
just a collection of IM users and
suppose we have a developer group and a
group of security administrators of
course we could assign different types
of policies to that group and every user
that is assigned to the developer group
will inherit those I am permissions
makes it much easier to manage your IM
users this way
for um and for the fact that also you
can have users that can be assigned to
groups as you can see here but what's
not shown here that is possible as well
is you could have a user that is part of
two groups at the same time the thing
that you cannot have though is the
notion of subgroup so I can't extend
this group and create a smaller group of
developers out of this big group here so
we don't have hierarchical groups but we
do allow users to partake in different
groups at the same time so they will
inherit those permissions those I am
policy permissions from for example if
user a was part of developers and user a
was part of security they would inherit
these two policy statements over here
now there is another type of I am policy
and that is called a resource-based
policy so resource-based policies are
not attached directly to for example
users but they're attached more to
resources like an S3 bucket for example
and so when you have such a case a user
group can't be designated as a principal
to a group that's just one of the
restrictions however there are other
ways to get around that
as I said the when you think of an I am
role I want you to think that these are
temporary credentials that are acquired
at runtime
and are only good for a specific amount
of time
now an IM role is really made up of two
components we have what's called a trust
policy
so who do you trust to assume this role
who do you attract to even make an API
call to assume the role after that we
have an IM policy once you've been
trusted to assume the role and you get
those temporary tokens what are you
allowed to do maybe you're allowed to
have read write access to the dynamodb
so
here we have no long-term credentials
that are connected to role you do not
assign a role to a user the user or
service will assume the role via API
call via the secure token service STS
and with those temporary credentials
we'll then be able to access for example
an ec2 instance if the IM policy
attached to the imro says that you can
do so
so your users
and your applications perhaps running on
an ec2 instance and the awf AWS Services
themselves don't normally have access to
the AWS resources but through assuming a
role they can dynamically
attain those permissions at runtime
so it's a very flexible model that'll
allow you to bypass the need to embed
security credentials all over the place
and then have to maintain those
credentials and make sure they're
rotated and they're not acquired by
anybody trying to find a security hole
in your system so from a maintenance
point of view they're really an
excellent tool to use in your toolbox
your security toolbox
so let's take a look at the components
of I am we have the I am policy over
here which is really just a Json
document and that Json document will
describe what this user can do if it's
attached directly to it or if the I am
policy is attached to the group whatever
user is attached to that group will also
have the same policy inherited so these
are more long-term credentials in terms
of uh or permissions rather that are
attached to your login for the duration
of your login whereas roles are as we've
already stated temporary and again once
you've established a trust relationship
then we can assign a policy for example
a policy that gives you only read only
access to a cloud trail to a role now
it's up to the user to assume that role
either through the console or through
some programmatic means whichever way is
performed the user will have to make a
director indirect call to STS for the
assume roll call
now these permissions are examined by
AWS
when a user submits a request so this
all happens at runtime dynamically so
when you change a policy it will take
effect pretty much immediately as soon
as the user makes the next request that
new security policy whether you've just
added or remove the permission will take
immediate effect so what we're going to
do now is just really quickly go to the
AWS console and actually show you how an
IM policy looks like
so I've just logged into my AWS
Management console and if I search for
identity and access management it should
be the first one to show up I'll just
click on that and I want to show you
actually how these IM policies look like
if you go to the left hand side here
there is an entry for policies and there
are different types of policies that we
can filter out so let's filter them out
by type we have customer managed we have
AWS managed and More in line with job
functions that AWS managed also so we're
going to take a look at AWS manage all
that means is that AWS has created and
will actually update maintain these
pre-established and already vetted
identity policies IM identity policy so
I could add further down maybe
look at all the S3 policies that are
available and we can see there are some
that allows only read only access there
are some that will give us full access
and there are of course other types here
that we're not all going to explore but
let's just easily kind of take a look
here at the full access you can see it's
just a a Json document
and of course you cannot modify it since
this is a managed policy however you can
copy it and modify it and in that case
it will become a a your own version of
it and so it'll be customer defined and
so when you go here and you do a filter
you can filter on your own customer
managed policies so over here we are
simply allowing certain actions right we
talked about actions before that were
based on API so here we have an API that
starts with S3 so this is for the S3 API
and instead of filtering out exactly
specifically what API calls are there
we're basically saying all API calls so
star representing all API actions and
again there is another category of S3
which deals with object Lambda and we
are allowing all operations on that
specific API as well we could if this
was our own customer managed
all right then uh policy actually scale
down on which resource which bucket
let's say or which folder in a bucket uh
this would apply to but in this case
since this is a full access we're
basically
being given permission to execute every
single API action under S3 for all
buckets we're going to see in the lab or
the demonstration at the end of this
tutorial that I'm going to show you how
to scale that down okay so that's how I
am policy looks like and if you want to
assign an IM policy to a user well stay
tuned for the demonstration at the end
of the video tutorial what's not at the
end of video tutorial is really how to
create a role so I'll show you what a
role is here again there's a whole bunch
of roles already set up that we can go
and and take a look at here but we can
go and create our own role as well
and I might create a role
for example that gives an ec2 let's
choose choose ec2 access to S3 so if I
click ec2 and click next permissions
I can actually assign the S3 permission
right over here just an a policy just
like we took a look at
obviously assign it a tag and review it
and I'm just going to call this
my ec2 role
2s3
create the role
and because I selected ec2 as a service
it automatically will include that in
the trust policy so now when I go and I
create an ec2 instance I can attach this
role to that ec2 instance and that ec2
instance will automatically be able to
if we're using the Linux 2mi let's say
have access to or full admin access to
any S3 bucket
uh in the AWS account so let's go take a
look at where we would actually attach
that role on the ec2
so when you're creating an ec2 let's
just say we go here we say launch an ec2
instance
and we'll pick this Ami over here
because it'll already include the
packages necessary to perform the assume
role call to STS for us so we won't have
to code that
and we don't have to embed any
credential keys in order to get this
running which is really good which is
the whole point of what I'm doing
because by assuming a role I don't have
to manage that and I'm just going to
pick the free T2 micro and it is here
once you you know pick whatever you need
whatever VPC and whatnot that you come
to the IM role and you select the role
to be assumed dynamically now I have a
whole bunch of them here but this is the
one that
I had created here my ec2 role to S3 and
if I go and I launch this ec2 instance
and then I had an application let's say
that I needed to contact S3 or even if I
went on the command line of the ec2
instance I would be able now to
communicate with the S3 service because
they would use vaping AWS would actually
use this role to perform an assume roll
call to get those temporary STS tokens
for me which will then allow me to
access my I am policy which gives me
full access to S3 and off I go so this
is a great way like I said to avoid
having to administrate any embedded keys
in this ec2 instance and I could take
this role away at any time as well so
there you have it a little demonstration
on how to create a role how to attach it
to an ec2 instance and also how manage
and customer
managed policies uh look like in I am
of course there is also the fact that we
have multiple accounts and so we could
assign a policy that will also allow a
user in another account access to our
account so across accounts access is
something that's very very well needed
and also is a feature of I am so when
you need to share access to resources
from one account to another in your
organization we are in need of an IM
policy to facilitate that so also roles
can come into play here for that
by also assigning granular permissions
we can make sure that we implement the
concept or the best practice of the
least privileged access
and also we have secure access to AWS
resources because by default again we
have the principle of least privilege
and no service can communicate with
another until we enable it via IM so
very very secure out of the box
of course if you want to add a
additional layer of authentication of
security we have multi multi-factor
authentication rather and that'll allow
you to ensure through a hardware device
perhaps let's say a hardware token
device or your cell phone that has some
software that generates a code that will
allow to identify you are actually the
person that is typing the username and
password and not somebody who's actually
stolen your username and password so at
the end of this video tutorial
we will show you or I will show you how
to enable that
and of course identity Federation is a
big thing where your users may have been
defined outside of AWS you may have
thousands of users that you've already
defined let's say in an active directory
or in an ldap environment on-prem and
you don't want to recreate all those
users again in AWS as I am users so we
can tie in we can link the users your
user database that's on Prem
or even users that have been your user
account that's been defined let's say on
a social site like
um Facebook or Twitter or Google and tie
that into your AWS account and though
there is an exchange of information
there is a setup to do but at the end of
the day it is either a role in
combination with an I am a policy that
will allow to map what you're allowed to
do once you've been authenticated by an
external system
so the this has to do with perhaps like
I said ldap or active directory and you
can tie that in with AWS SSO there are
many different
Services here that can be leveraged to
facilitate what we call identity
Federation
and of course I am itself is free so
it's not a service per API request AWS
is very concerned to put Security First
and so I am is free
it's just the services that you use for
example if you're securing an ec2 of
course that ec2 instance falls under the
current pricing plan
so of course compliance like I mentioned
is extremely important making sure that
it's business as usual in the cloud and
so for example if you were using the
payment card industry
uh data security standard to make sure
that you're storing and processing and
transmitted credit card information
appropriately in order to do your
business well you can be assured that
the services you use in AWS are PCI DSS
compliance and there's many many other
types of compliance that AWS adheres to
as well
so password policies of course are
important by default we do have a
password policy in place but when you go
to the IM console you're free to update
that password policy make it more
restrictive based on whatever policy you
have at your company
so many features and so I think we're
ready now to go into a full
demonstration and I will show you how to
incorporate I am users groups and also
multi-factor authentication so get ready
let's do it
I'm going to demonstrate now how to
attach an S3 bucket policy via IM I've
already created a bucket called simply
learn s3im demo and what we're going to
do is we're going to attach a bucket
policy right over here we're going to
have to edit it of course and the basis
of the demonstration will be to allow a
user that has MFA so multi-factor
authentication set up to have access to
the contents of a folder within this
bucket so of course now I have to create
a folder just simply call it folder one
create that folder and now I'm going to
go into that folder and upload a file so
that we can actually see this in action
so I'm going to select a demo file that
I prepared in advance called demo file
user 2 MFA
I'm going to upload that
all right and now what's going to happen
is eventually when I create R2 IM users
one will have access to view the
contents of this file which should look
like something very simple if I open
this up within the console you'll be
able to see well it's pretty small right
now I'll make this a little bit bigger
of course
it says this is a demo file which user 2
MFA should only be able to have access
to so what's left to do now is to create
two users one call user one and one
called user2 Dash MFA and that's what
we're gonna do right now
let's head over to IM identity and
access management
and on the left hand side here we're
going to click on users
you can see here I don't have much going
on I just got administrator user so I
want to create two new users we'll start
with one at a time so one called user
one now by default we have you know the
principle of least privilege which means
this user is not allowed to do anything
unless we give them access to either the
console right over here which I will do
right now and also if they were a kind
of programmatic user that needed access
to specific keys in order to
interact with the apis via the command
line interface or the software
development kit for example if you were
a Java developer so in this case that is
not what I want so I'm just going to
assign console access and I'm also going
to create a default password
for this user
but I will not allow them to reset it on
the first login just for demonstration
purposes
now over here we have the chance to add
the user straight to a group but I'll
only do that later on at the end of
demonstration for now I want to show you
how to attach to this user and I am
policy which will give them access to
specific services so if I type in S3
full access here this is an
administrative access to an S3 bucket if
you take a look at the Json
document attached or representing that
IM policy you can see that it is
allowing all API actions across S3 star
and also object Lambda across all
buckets and I say all buckets because
the star represents all buckets
so it is a very broad permission so be
very careful when you're actually
assigning this to one of your users make
sure that they deserve to have such a
permission
so I'm not going to assign any tags here
but I'm just going to review what I've
done and consider I've just attached an
S3 full access permission to this one
user and I'm going to create that user
now
of course now is the time for me to
download the CSV file which will contain
those credentials if I don't do it now I
will not have a second chance to do this
and also to send an email to this user
in order for them to have a link to the
console and also some instructions I'm
not going to bother with that I'm just
going to say close
and now I have my user one you can see
here that it says None for multi-factor
authentication so I have not set
anything up right now for them and
that's what I want to do for this second
user that I want to create called user 2
Vash
MFA or MFA router all right so now I
want to do the same exact thing as
before
I'm going to assign a custom password
and I'm going to go through the same
steps as we saw before I'm an attach an
existing
managed policy that AWS has already
vetted for us
review create
now right now they're exactly the same
so I have to go back to user 2 and set
up multi-factor Authentication
and to do that you have to go into
security credentials tab
and you'll see here assigned MFA device
we have not assigned any yet so we're
going to manage that right now and we
have the option to order and get an
actual physical device that's uniquely
made for performing multi-factor
authentication however we're going to
opt for the virtual MFA device which is
a software that we're going to install
on our cell phones let's say so I can
actually open this link just to show you
the um very many
options that we have for that right over
here I'm going to go with Google
Authenticator so go to the App Store on
your Android device and you can download
that of course if you have an Android
device if not you have other options for
iPhone and other softwares for each one
of those so I'll let you do that once
you've installed that software on your
cell phone you could come here and show
the QR code now what that is going to do
is you're going to need to go to your
phone now and I'm going to my phone
right now as we speak and I'm going to
open up
that MFA software in this case the
Google Authenticator and I'm going to
say there's a little plus sign at the
bottom I'm going to say scan QR code I'm
going to point my phone to this Square
code here this QR code
and it's going to pick it up I'm going
to say add account there's a little
button there at account and it's going
to give me a code so I'm going to enter
that code right now
so once I've entered the code I have to
wait for a second code and that's
because this code on the screen of my
cell phone is going to expire it's only
there for about 20 to 30 seconds once
that token expires it refreshes the
display with another token so I have
another token now
that's only good for a couple of seconds
again and now I'm going to say assign
MFA what this does is it completes the
link between my personal cell phone and
this user right now so now that this has
been set up we can see we actually have
an Arn
to this device which we are going to
possibly need depending on how we write
our policies in our case here I'm going
to show you a way to kind of bypass
having to put that
so now we've got those two users one
with an MFA device and another one
without okay so now the lab is going to
be or the demonstration rather is going
to be how do we allow user to MFA
access to
this file in folder one and not allow
user one
the distinguishing Factor will be that
user 2 has an MFA setup in user one
doesn't So based off of just that
condition
that'll be the determining Factor if
they can view the file or not so let's
get back to S3 and actually create this
I am bucket policy
and see how it's done
so back we are in our bucket and now we
have to go to permissions in order to
set up this bucket policy we're going to
click on edit over here
and we're going to click on policy
generator to help us generate that Json
file through a user interface of course
we're dealing with an S3 bucket policy
now what we're going to do is we're
going to deny
all users and services so star
S3 is great out here because we selected
S3 up here we want to deny all S3 API
actions we're not going to go and
selectively go down the list
the specific
bucket and the folder so in this case
here we're missing the Amazon resource
name so we have to go back here
and we have to copy the Arn of our
bucket right so we're going to copy that
come back
Ctrl V and we're going to of course have
to specify the folder as well so I'm
going to just do slash folder one and
then I'm also going to do slash star for
all the objects in that folder okay so
here's the MFA condition that I want to
add though because we cannot assign the
MFA condition up here we have to say add
conditions very powerful feature here
that's optional but very powerful we
want to look for a condition that says
Boolean
if exists or Bool if exists all right so
there's got to be a key value pair that
exists and there are many but we're
going to look for one that is
specifically known as multi-factor
authentication present and we want to
make sure that value is
equal to false so I'm going to add that
condition and then I'm going to add that
statement and I want to generate the
policy just to show you now how that
user interface has generated this Json
document for us that we're going to
attach to our S3 bucket what I want to
do is I want to take this in its
entirety copy it and go back to our S3
bucket that's just waiting for us to
paste our policy in here okay so let's
see what's actually going on here
we've created a policy again this policy
could have any ID it's just the an ID
that was given by the user interface you
could name that whatever you want of
course something that makes total sense
and then we have one or more statements
and then we only have actually one
statement and again you can select any
ID here that was just Auto generated by
the IDE
we're saying that all actions on the
simple storage service I say all again
because of the star are denied
specifically on this resource which is
our bucket
and the folder named folder one and all
the objects within that folder
under the certain conditions
so the condition is if you have
a Boolean value of false associated with
a key called multi-factor authentication
present and it has to exist okay
then if that value is equal to false you
don't have access to the objects in this
folder which means that
if you do have MFA setup you will be
allowed access to the contents of this
folder so if we assign this and apply
this to all principles which would
Encompass user one and user 2 Dash MFA
then let me just apply this that would
mean
that user 1 would not be able to see the
contents of
our object inside folder one and user 2
MFA would
so let's go take a look and log into
let's log out first and then when we're
going to log back in we're going to log
in as the individual user one and
user2mfa and prove this is actually the
case
so I am now logging back in as user one
that we created moments ago
and of course if I for example go see a
service like ec2 we don't have access to
ec2 we've just given ourselves access to
S3 so this makes total sense
we're just going to go see what we have
access to which is S3
should be able to list and describe all
the buckets there because we've been
giving pretty much administrative access
now if we go here to our
created bucket and go inside folder one
and want to actually open up this
document we can see now that that's not
happening okay and that's because of
that condition that is checking if we
actually have a value of of true for
that MFA key that we specify okay so
this is expected and this is exactly
what we wanted now what we're going to
do
is we're going to log out once again
and we're going to do the same thing but
with user 2 Dash MFA
let me log back in
and then specify the new password we
created
of course now I have an MFA code right
because I've set that up for user two so
back on my cell phone I go
I am opening up the Google Authenticator
application and I have in front of me a
new Amazon web services code that is
only good for like I said maybe 20
seconds so it's a little nerve-wracking
if you're if you're not good Under
Pressure you're going to choke
so I just entered the code and I have to
wait for uh you know that to actually
work now it looks like I actually didn't
check off
um not to recreate the the password so
unfortunately I must have missed that
check mark so I wanted to kind of avoid
this here so I'm going to start over and
create a brand new password
a little painful for
you know demonstrations but it's good
for you guys to see how that actually
looks like so because I've set up my MFA
device properly I am able to log in
after I enter that MSA code and once
again if I go to ec2
now I haven't been given the permissions
for easy to have only been given the
permissions for S3 so nothing changes
here I'm going to go to S3
and I'm going to go into folder one
of our bucket in question and I'm going
to see if I have access now to opening
up and seeing the contents and here I
have the contents this is a demo file
which user2mfa should only be able to
have access to so that worked so now you
guys know how to create
a bucket permission policy
that is also known as a resource policy
because it's assigned to the resources
assigned to the bucket and let me
actually show it to you once one last
time here it is and that'll only allow
us to have access to the contents of
folder one if we have multi-factor
authentication set up in our account so
that's really good the last thing I want
to show you now is how to maybe create
IM users in a better way in terms of not
having to assign over and over again
the same permissions like you saw me do
for user 1 and user two uh of course now
I cannot do anything I am why because
I'm user I'm user two I don't have
access to Im so once again I'm logging
out and we're gonna actually see the
best practice in terms of how do I
assign I am policies to a group and then
assign users to that group in order to
inherit those permission policies so
let's get to it let's sign up first
so let us go back to I am and we're
going to be creating our first group
we're going to Simply create the button
or click on the button create group and
we'll come up with a
group called testers
we have the
ability to add existing users to this
group so we will add for example user
one only to the testers group and we can
also add permissions
to this group so we're going to go here
and add for example the ec2
full permissions
create group
and what's going to happen now if we go
take a look at testers we can see we
have that one user that we added and we
also have
the permissions associated with that
group so this means now that any new
users we create will automatically
inherit whatever permissions the group
has which is really good for
maintainability so
for example let's say we were to create
many new testers
so let's say user 3 over here now of
course I'm going to have to go in this
case through the same steps as I did
before
I'm just going to remember this time not
to forget to check uncheck that and
instead of attaching policies directly
I'm going to say well I want to inherit
the policy permissions that were
assigned to the group
so I'm going to say this user is going
to be part of the testers group and of
course I'm just going to skip through
the remaining repetitive tasks that I've
already described
and this user 3 now
is already going to have the permissions
to access ec2
that they've gained through the group
testers
whereas if we go back to user 1
this user also has
the permissions from the group but has a
unique permission added to itself so in
this case
you do have the option of doing that you
have the option of adding a permission
over and above that is assigned to the
group to yourself directly
so this is really good adding
permissions to groups when you have of
course lots of users in your company in
your organization
and instead of going to them and adding
permissions after permission you can
centrally access this in one or manage
this in one place so for example if I go
back to my roles
uh groups rather in my testers
and let's say I forgot to add a
permission to all the users in this
group so let's say there was 20 users
attached to this group okay I'm not
going to bore you with creating 20 users
and we have we have two now that's good
enough to show the demonstration and
let's say now I need to add another
permission right so in this case here
I'm going to add a permission uh for
example let's say based on dynamodb
right so I'm going to get full access to
dynamodb I'm going to add that
permission
and what's going to happen now is if I
go back to my users for example user 3
I'm going to see that user 3
automatically has inherited that
permission and so has user one
all right show two more and there they
are so in this case it really allows for
you to easily manage a lot of users in
that group because you can simply go to
one place which is the group and add and
remove policies as you see fit and the
whole team will inherit that so it saves
you lots of work and that is the best
practice so there you have it guys you
have learned in this demonstration
how to create users how to create groups
how to add users to groups and manage
their policies in terms of best practice
and you've also learned how to attach to
an S3 bucket a policy that allows you to
permit access based off of if a user has
multi-factor authentication setup and we
showed you how to actually set that up
when creating an IM user so again
permissions tab is where this all
happens hope you enjoyed that put that
to good use and I'll see you in the next
demonstration
so I hope you enjoyed that demonstration
we're just going to wrap things up in a
summary and kind of see what was looked
at throughout the tutorial we first
started with what is AWS security we
took a look at how important it was
across our organization to maintain best
practice and also standardize that
practice across accounts organization we
took a look at the topmost services in
which we concentrated then after on I am
and we took a look at what exactly what
I am was in terms of IM users groups
long-term credentials that are applied
as IM policies and then the concept of a
role we took a look at the benefits of
IM and the actual terminology used with
working with I am of course principal
being an entity that is a either a user
or a service itself that can gain access
to an AWS
resources and we actually saw the
different types of authentication and
what we can do to implement
authorization via IM policies and also
through the use of roles so we took a
look how to organize our users into
groups and what actually goes into
acquiring a role through a demonstration
we took a look at how to actually create
a role and attach that role to an ec2
instance and then we took a look at the
high level features
of I am which allowed us to either Grant
access to another IM user from another
group Implement multi-factor
authentication like we just did in the
demonstration
or ensure ourselves that we are
following uh all the compliancy
standards that have been adhered to by
whatever project we're working on and
that AWS is there to support us so here
we're going to talk about Amazon ECS a
service that's used to manage Docker
containers so without any further Ado
let's get started in this session we
would like to talk about some Basics
about AWS and then we're going to
immediately dive into why Amazon ECS and
what is Amazon ECS in general and then
it uses a service called Docker so we're
going to understand what Docker is and
there are competitive services available
for ECS I mean you could ECS is not the
on and only service to manage Docker
containers but why ECS advantage of ECS
we will talk about that and the
architecture of ECS so how it functions
what are the components present in it
and what are the functions that it does
I mean each and every component what are
all the functions that it does all those
things will be discussed in the
architecture of Amazon ECS and how it
works how it all connects together
that's something we will discuss and
what are the companies that are using
ECS what were the challenge and how ECS
helped to fix the challenge that's
something we will discuss and finally we
have a wonderful lab that talks about
how to deploy Docker containers on an
Amazon ECS so let's talk about what is
AWS Amazon web service in short call as
AWS is an web service in the cloud that
provides a variety of services such as
compute power database storage content
delivery and a lot of other resources so
you can scale your business and grow not
focus more on your I.T needs and the
rest of the ID demands rather you can
focus on your business and let Amazon
scale your it or let Amazon take care of
your it so what is that you can do with
AWS with AWS we can create the deploy
any application in the cloud so it's not
just deploying you can also create your
application in the cloud it has all the
tools and services required the tools
and services that you would have
installed in your laptop or you would
have installed in your on-premises
desktop machine for your development
environment you know the same thing can
be installed and used from the cloud so
you can use cloud for creating and not
only that you can use the same Cloud for
deploying and making your application
available for your end user the end user
could be internal internal users the end
user could be the could be in the
internet the end user could be kind of
spread all around the world it doesn't
matter so it can be used to create and
deploy your applications in the cloud
and like you might have guessed now it
provides service over the Internet
that's how your users worldwide would be
able to use the service that you create
and deploy right so it provides Service
uh over the Internet so that's for the
End customer and how will you access
those Services that's again through the
internet it's like the extension of your
data center in the internet so it
provides all the services in the
internet it provides compute service
through the internet so in other words
you access them through the internet it
provides database service through the
internet over the internet in other
words you can securely access your
database through the internet and lot
more and the best part is this is a pay
as you go or pay only for what you use
there is no long term or you know
beforehand commitment here most of the
services does not have any commitment so
there is no long term and beforehand
commitment you only pay exactly for what
you use there's no overage there's no
overpaying right there's no buying in
advance right you only pay for what you
use let's talk about what ECS is so 4
ECS before containers right ECS is a
service that manages Docker containers
right it's not a product or it's not a
feature all by itself it's a service
that's dependent on Docker container so
before Docker containers all the
applications were running on VM or on an
host or on a physical machine right and
that's memory bound that's latency bound
the server might have issues on and on
right so let's say this is Alice and
she's trying to access her application
which is running somewhere in her on
premises and the application isn't
working what could be the reason some of
the reasons could be Memory full the
server is currently down at the moment
we don't have another physical server to
launch the application a lot of other
reasons so a lot of reasons why the
application wouldn't be working in
on-premises some of them are Memory full
issue and server down issue very less
High AI availability or in fact single
point of failure and no high
availability if I if I need to tell it
correctly with ECS the services can kind
of breathe free right the services can
run seamlessly now how how is that
possible now those thing we will discuss
in the upcoming sessions so because of
containers and easiest managing
containers the applications can run in a
high available mode they can run in a
high available mode meaning if something
goes wrong right there's another
container that gets spunmed up and your
application runs in that particular
container very less chances of your
application going down that's what I
mean this is not possible with a
physical Host this is very less possible
within a VM or at least it's going to
take some time for another VM to get
spun up so why ECS or what is ECS Amazon
ECS maintains the availability of the
the application and allows every user to
scale containers when necessary so it
not only meets the availability of the
application meaning one container
running your application or one
container hosting your application
should be running all the time so it
means that high availability
availability is making sure your service
is running 24 7. so Container makes sure
that your services run 24 bar 7 not only
that not only that suddenly if there is
an increase in demand how do you meet
that Demand right let's say you have
like thousand users suddenly the next
week there are like 2000 users all right
so how do you meet that demand Container
makes it very easy for you to meet that
demand in case of VM or in case of
physical host you literally will have to
go buy another physical host or you know
add more RAM add more memory add more
CPU power to it all right or kind of
Club two three hosts together clustering
you would be doing a lot of other things
to meet that the high availability and
also to meet that demand but in case of
ECS it automatically scales the number
of containers it automatically scales
the number of containers needed and it
meets your demand for that particular R
so what is Amazon ECS the full form of
ECS is elastic container service right
so it's basically a container Management
Service which can quickly launch and
exit and manage Docker containers on a
cluster so it's the function of ECS it
helps us to quickly launch and quickly
exit and manage Docker container so it's
kind of a Management Service for the
docker containers you will be running in
Amazon or running in the AWS environment
so in addition to that it helps to
schedule the placement of container
across your cluster so it's like this
you have two physical host you know
joined together as a cluster and ECS
helps us to place your containers now
where should your container be placed
should it be placed in host one should
be placed in host too so that logic is
defined in ECS we can Define it you can
also let ECS take control and Define
that logic most cases you will be
defining it so schedule the placement of
containers across your cluster let's say
two containers want to interact heavily
you really don't want to place them in
two different hosts all right you would
want to place them in one single host so
they can interact with each other so
that logic is defined by us and these
container services you can launch
containers using AWS Management console
and also you can launch containers using
SDK kids available from Amazon you can
launch through a Java program you can
launch container using an.net program
you can launch container using an
node.js program as reason when the
situation demands so there are multiple
ways you can launch containers through
Management console and also
programmatically and ECS also helps to
migrate application to the cloud without
changing the code so anytime you think
of migration the first thing that comes
to your mind is that how will that
environment be based on that I'll have
to alter my code what's what's the IP
what is the storage that's being used
what what are the different parameters
I'll have to include the environment
parameters of the new environment with
containers now that worry is already
taken away because we can create an
pretty exact environment the the one
that you had an on-premises the same
environment gets created in the cloud so
no worries about changing the
application parameter no worries about
changing the code in the application
right you can be like if it ran in my
laptop a container that I was running in
my laptop it's definitely going to run
in the cloud out as well because I'm
going to use the same container in the
laptop and also in the cloud in fact
you're going to ship it you're going to
move the container from your laptop to
Amazon ECS and make it run there so it's
like the same the very same image the
very same container that was running in
your laptop will be running in the cloud
or production environment so what is
Docker we know that it ECS helps to
quickly launch exit and manage Docker
containers what is Docker let's let's
answer that question what is a Docker
now Docker is a tool that helps to
automate the development of an
application as a lightweight container
so that the application can work
efficiently in different environments
this is pretty much what we discussed
right before the slide I can build an
application in my laptop or in on
premises in a container environment that
Docker container environment and anytime
I want to migrate right I don't have to
kind of rewrite the code and then rerun
the code in that new environment I can
simply create an image a Docker image
and move that image to that production
or the new Cloud environment and simply
launch it there right so no compiling
again no relaunching the application
simply pack all your code in a Docker
container image and ship it to the new
environment then launch the container
there that's all so Docker container is
a light weight package of software that
contains all the dependencies so because
you know when packing you'll be packing
all the dependencies you'll be packing
the code you'll be packing the framework
you'll be packing the libraries that are
required to run the application so in
the new environment you can be pretty
sure you can be guaranteed that it's
going to run because it's the very same
code it's the very same framework it's
the very same libraries that you have
shipped right there's nothing new in
that new environment it's the very same
thing that's going to run in that
container so you can be rest assured
that they are going to run in that new
environment and these Docker containers
are highly scalable and they are very
efficient suddenly you wanted like 20
more Docker containers to run the
application think of adding 20 more
hosts 20 more VMS right how much time
would it take and compared to that time
the amount of time that Docker
containers would require to kind of
scale to that amount like 20 more
containers it's very less or it's
minimal or negligible so it's a highly
scalable and it's a very efficient
service you can suddenly scale number of
Docker containers to meet any additional
demand very short boot up time because
it takes uh it's not going to load the
whole operating system and this Docker
containers you know they use the Linux
kernel and features of the kernel like C
groups and namespaces to kind of
segregate the processes so they can run
independently any environment and it
takes very less time to boot up and the
data that are stored in the containers
are kind of reusable so you can have an
external data volume and I can map it to
the container and whatever the space
that's occupied by the container and the
data that the container puts in that
volume they are kind of reusable you can
simply remap it to another application
you can kind of remap it to the next
successive container you can kind of
remap it to the next version of the
container next version of the
application you'll be launching and you
don't have to go through building the
data again from the scratch whatever
data the container was using previously
or the previous container was using that
data is available for the next container
as well so the volumes that the
containers users are very reusable
volumes and like I said it's it's
isolated application so it kind of
isolates by its nature it kind of by the
way it's designed by the way it is
created it isolates one container from
another container meaning anytime you
run applications on different containers
you can be rest assured that they are
very much isolated though they are
running on the same host though they are
running on the same laptop let's say
though they are running on the same
physical machine let's say running 10
containers 10 different applications you
can be sure that they are well
disconnected or well isolated
applications now let's talk about the
advantages of ECS the advantage of ECS
is improved security it's security is
inbuilt in ECS with ECS we have
something called as container registry
you know that's where all your images
are stored and those images are accessed
only through https not only that those
images are actually encrypted and access
to those images are allowed and denied
through identity and access management
policies IAM and in other words let's
say two container running on the same
instance the one container can have
access to S3 and the others or the rest
of the others are denied access to S3 so
that kind of granular security can be
achieved through containers when we mix
and match the other security products
available in Amazon like IAM encryption
accessing it using https these
containers are very cost efficient like
I've already said these are lightweight
processors right we can schedule
multiple containers on the same node and
this actually allows us to achieve high
density on an ec2 instance imagine an
ec2 instance that that's very less
utilized that's not possible with a
container because you can actually dance
or crowd an ec2 instance with more
container in it so to best use those
resources in ec2 straightforward you can
just launch one application but with
when we use containers you can launch
like 10 different applications on the
same ec2 server that means 10 different
applications can actually feed on those
resources available and can benefit the
application and ECS not only deploys the
container it also maintains the state of
the containers and it makes sure that
the minimum a set of containers are
always running based on the requirement
that's another cost efficient way of
using it right and anytime an
application fails and that has a direct
impact on the revenue of the company and
is just make sure that you're not losing
any Revenue because your application has
failed and ECS isn't pretty extensible
Services it's like this in many
organization there are majority of
unplanned work because of environment
variation a lot of firefighting happens
when we kind of deploy by the chord from
one or kind of move the code or redeploy
the code in a new environment a lot of
firefighting happens there right this
Docker containers are pretty extensible
like we discussed already environment is
not a concern for containers because
it's going to kind of shut itself inside
a Docker container and anywhere the
docker container can run the application
will run exactly the way it performed in
the past so environment is not a concern
for the docker containers in addition to
that ECS is easily scalable we have
discussed this already and it improves
it has improved compatibility we have
discussed this already let's talk about
the architecture of ECS like you know
now the architecture of ECS is the ECS
cluster itself that's group of servers
running the ECS service and it
integrates with Docker right so we have
a docker registry Docker registry is a
repository where we store all the docker
images or the container images so it's
like three components ECS is of three
components one is the ECS cluster itself
right when I say easy as itself I'm
referring to easiest cluster cluster of
servers that will run the containers and
then the repository where the images
will be stored right the repository
where the images will be stored and the
image itself so container image is the
template of instructions which is used
to create a container right so it's like
what's the OS what is the version of
node that should be running and any
additional software do we need so those
question gets answered here so it's the
template template of instructions which
is used to create the containers and
then the registry is the service where
the docker images are stored and shared
so many people can store or there and
many people can access or if there's
another group that wants to access they
can access the image from there or one
person can store the image and rest of
the team can access and the rest of the
team can store image and this one person
can pick the image from there and kind
of ship it to the customer or ship it to
the production environment all that's
possible in this container registry and
Amazon's version of the container
registry is ECR and there's a third
party Docker itself has a container
registry that's Docker Hub ECS itself
which is the the group of servers that
runs those containers so these two the
container image and the container
registry they kind of handle Docker in
an image format just an image format and
an ECS is where the container gets live
and then it becomes an compute resource
and starts to handle requests now starts
to serve the page and starts to do the
batch job you know whatever your plan is
with that container so the cluster of
servers ECS integrates well with the
familiar services like VPC VPC is known
for securing VPC is known for isolating
the whole environment from rest of the
customers or isolating the whole
environment or the whole infrastructure
from the rest of the clients in your
account or from the rest of the
applications in your account on and on
so VPC is a service that provides or
gives you the network isolation ECS
integrates well with VPC and this VPC
enables us to launch AWS resources such
as Amazon ec2 instance in a virtual
private Network that we specified this
is basically what we just discussed now
let's take a closer look at the ECS how
does ECS work let's find answer for this
question how does ECS work ECS has got a
couple of components within itself so
these easier servers can run run across
availability Zone as you can see there
are two availability zones here they can
actually run across availability zones
and ECS has got two modes of fargate
more and the ec2 mode right here we're
seeing forget more and then here we're
seeing nothing that means it's an ec2
mode and then it has got different
network interfaces attached to it
because they need to be running in an
isolated fashion right so anytime you
want Network isolation you need separate
IP and if you want separate IP you need
separate network interface card and
that's what you have elastic network
interface card separate elastic network
interface card for all those tasks and
services and this runs within an VPC
let's talk about the far gate service
tasks are launched using the far gate
service so we will discuss about task
what is forget now forget is a compute
engine in ECS that allows users to
launch containers without having to
monitor the cluster ECS is a service
that manages the containers for you
right otherwise managing containers will
be an full-time job so ECS manages it
for you and if you and you get to manage
ECS that's the basic service but if you
want Amazon to manage ECS and the
containers for you we can go for forget
so fargate is a compute engine in ECS
that allows users to launch containers
without having to monitor the ECS
cluster and the tasks the tasks that we
discussed the tasks has two components
you see task right here so they have two
components we have ECS container
instance and then the container agent so
like you might have guessed right now
easiest container instance is actually
an easy to instance right capable of
running containers not all ec2 instances
can run containers so these are like
specific easy two instances that can run
containers they are ECS container
instances and then we have container
agent which is the agent that actually
binds those clusters together and it
does a lot of other housekeeping work
right kind of connects clusters makes
sure that the version needed is present
so it's all part of that agent or it's
all job of that agent container
instances container instances is part of
Amazon ec2 instance which run Amazon ECS
container agent pretty straightforward
definition and then a container agent is
responsible for communication between
ECS and the instance and it also
provides the status of the running
containers kind of monitors the
container monitors the state of the
container make sure that the content is
up and running and if there's anything
wrong it kind of reports it to the
appropriate service to fix the container
on and on it's a container agent when we
don't manage container agent it runs by
itself and you really don't have to do
anything to make the container agent
better it's already better you really
won't be configuring anything in the
agent and then elastic network interface
car is in Virtual interface Network that
can be connected to an instance in VPC
so in other words elastic network
interface is how the container interacts
with another container and that's how
the container interacts with the ec2
host and that's how the container
interacts with the internet external
world and a cluster a cluster is a set
of ECS container instances it's not
something that's very difficult to
understand it's simply a group of ec2
instances that runs that ECS agent and
this cluster it cluster handles the
process of scheduling monitoring and
scaling the request we know that ECS can
scale the containers can scale how does
it scale that's all Monitor and managed
by this ECS cluster let's talk about the
companies that are using Amazon ECS
there are a variety of companies that
use ACS clusters to name a few okta
users easiest cluster and OCTA is a
product that use identity information to
Grant people access to applications on
multiple devices at any given point of
time they make sure that they have a
very strong security protection so OCTA
uses Amazon ECS to run their OCTA
application and serve their customers
and abima abhima is an TV channel and
they chose to use microservices and a
Docker containers they already had
microservices and Docker containers and
when they thought about a service that
they can use in AWS ECS was the only
service that they can immediately adapt
too and because in abima TV the
engineers have already been using Docker
and Docker containers it was kind of
easy for them to adapt themselves to ECS
and start using it along with the
benefits that ECS provides previously
they had to do a lot of work but now ECS
does it for them all right similarly
remind and Ubisoft GoPro or some of the
famous companies that use Amazon ECS and
get benefited from its scalability get
benefited from its cost gets benefited
from its Amazon managed Services get
benefited from the portability that ECS
and the migration option that ECS
provides let's talk about how to deploy
a Docker container on Amazon ACS the way
to deploy Docker container on ECS is
first we need to have an AWS account and
then set up and run our first ECS
cluster so in our lab we're going to use
um the launch wizard to run an ECS
cluster and run containers in them and
then task definition task definition
tells the size of the container the
number of the container and when we talk
about size it tells how much of CPU do
you need how much of memory do you need
and talking about numbers you know it
requires how many numbers of container
you're going to launch you know is it 5
is it 10 or is it just one running all
the time now those kind of information
goes in the task definition file and
then we can do some Advanced
configuration on ECS like load balancers
and you know what port number you want
to allow when you don't want to allow
you know who gets access who shouldn't
get access and what's the IP that you
want to allow and deny requests from on
and on and this is where we would also
mention the name of the container so the
differentiate one container from the
other and the name of the service now is
it an backup job is it a web application
is it an a data container is it going to
take care of your data data backend and
the desired number of tasks that you
want to be running all the time those
details go in when we try to configure
the ECS service right and then you
configure cluster you put in all the
security in the configure your cluster
step or configure cluster stage and
finally we will have an instance and
bunch of containers running in that
instance all right let's do a demo so
here I have logged in to my Amazon
portal and let me switch to the
appropriate region I'm going to pick not
Virginia North Virginia look for ECS and
it tells ECS is a service that helps to
run and manage Docker containers well
and good click on it I'm a not Virginia
I just want to make sure that I'm in the
right region and go to clusters and here
we can create cluster this is our forget
8 and this is our ec2 type launching for
Linux and windows environment but I'm
going to launch through this walkthrough
portal right this gives a lot of
information here so the different steps
involved here is creating a container
definition which is what we're going to
do right now and then a task definition
and then service and finally the cluster
it's a four-step process so in container
definition we Define the image the base
image we are going to use now here I'm
going to launch an httpd or a simple
HTTP web page right so a simple httpd
2.4 image is fair enough for me and it's
not an heavy application so 0.5 gigabit
of memory is enough and again it's not a
heavy application so 0.25 virtual CPU is
enough in our case right you can edit it
based on the requirement you can always
edit it and because I'm using httpd the
port mapping is already Port 80 that's
how the container is going to receive
the request and there's no health check
as of now when we want to design
critical and complicated environments we
can include health check right and this
is the CPU that we have chose we can
edit it and I'm going to use some bash
commands to create an HTML page right
this page says that you know Amazon ECS
sample app right and then it says Amazon
ECS sample app your application is
running on a container in Amazon ECS so
that's the page the HTML page that I'm
going to create
index.html so I'm going to create and
put it in an appropriate location so
those pages can be served from the
container right if you replace this with
any of your own content then it's going
to be your own content ECS comes with
some basic logs and these are the places
where they get stored that's not the
focus as of now all right so I was just
saying that you can edit it and
customize it to your needs we're not
going to do any customization now we're
just getting familiar with ECS now and
the task definition name of the task
definition is first run task definition
and then we are running it in a VPC and
then this is an fargate mode meaning the
servers are completely handled by Amazon
and the task memory is 0.5 gigabit and
the task CPU is 0.25 virtual CPU name of
the service is it a batch job is it an
you know a front end is it an back end
or is it a simple copy job what's the
service name of the service goes here
again this you can edit it and here's a
security group as of now I'm allowing
for 80 to the whole world if I want to
restrict to a certain IP I can do that
the default option for load balancing is
no load balancer but I can also choose
to have a load balancer and use port 80
to map that Port 80 to The Container
Port 80 right I can do that the default
is no load balancer all right let's do
one thing let's use load balancer let's
use load balancer and Port 80 that
receives information on Port 80 HTTP
what's going to be the cluster name when
the last step what is the cluster name
cluster name can be simply learn ECS
demo next we're done and we can create
so it's launching a cluster as you can
see and it's picking the task definition
file that we've created and it's using
that to launch and service and then
these are the log groups that we
discussed and it's creating your VPC
remember ECS clubs well with the VPC
it's creating a VPC and it's creating
two subnets here for high availability
it's creating that Security Group Port
80 allowed to the whole world and then
it's putting it be behind and load
balancer it generally would take like
five to ten minutes so which is need to
be patient and let it complete its
creation and once this is complete we
can simply access these servers using
the load balancer URL and when this is
running let me actually take you to the
other products or the other services
that are integrated with ECS it's
getting created our service is getting
created as of now ECR repository this is
where all our images are stored now as
of now I'm not pulling my image from ECR
I'm pulling it directly from the
internet Docker Docker Hub but all
custom images all custom images they are
stored in this repository so you can
create a repository call it app one
create a repository so here's my
repository so any image that I create
locally or any Docker image that I
create locally I can actually push them
push those images using these commands
right here and they get stored here and
I can make my ECS connect with ECR and
pull images from here so they would be
my custom images and as of now because
I'm using a default image it's directly
pulling it from the internet let's go to
ec2 and look for a load balancer because
we wanted to access the application from
behind a load balancer right so here is
a load balancer created for us and
anytime I put the URL so cluster is now
created you know you see there's one
service running all right let's click on
that cluster here is the name of our
application and here is the tasks the
different containers that we are running
and if you click on it we have an IP
right IP of that container and it says
it's running it was created at such and
such time and started at such and such
time and this is the task definition
file that it this container uses meaning
the template the details to all the
version details they all come from here
and it belongs to the cluster called
simply learn ECS demo right and you can
also get some logs container logs from
here so let's go back and there are no
ECS instances here because remember this
is forget you're not managing any ECS
instance all right so that's why you're
not seeing any ECS instance here so
let's go back to tasks and go back to
the same page where we found the IP pick
that IP put it in the browser and you
have this sample HTML page running from
an container so let me go back to load
balancer ec2 and then under ec2 I'll be
able to find a load balancer find that
load balancer pick that DNS name put it
in the browser and now it's accessible
to the load balancer URL right now this
you URL can be mapped to other services
like DNS this URL can be emboded in any
of your application if you want to make
that application connect with this
container Now using IP is not all that
advisable because these containers can
die and then a new container gets
created and when a new container gets
created it gets a new IP right so a
hardcoding IP is not hardcoding Dynamic
IPS are not advisable so you would be
using load balancer and putting that URL
in that application that you want to
make it interact with this container
instance it was a wonderful experience
in Walking you through this ECS topic
and in here we learn about what AWS is
and why we're using ECS and what is
easiest in general what is Docker it's
specific and we also learn about the
advantages of ECS the architecture the
different components of ECS and how ECS
works when they're all connected
together and we also looked at the
companies that use ECS and their use
cases and finally a lab how we can
launch ECS fargate through the portal
I'm very glad to walk you through this
lesson about route 53. so in this
section we are going to talk about
basics of AWS and then we're going to
immediately dive into why why we need
Amazon Route 53 and then we're going to
expand and talk about the details of
Amazon Route 53 the benefits it provides
over its competitors and the different
types of routing policy it has and some
of Amazon route 53's key features and
we're going to talk about how to access
Route 53 I mean the different ways the
different methods you can access Route
53 and finally we're going to end with
an a wonderful demo in route 53. so
let's talk about what is AWS Amazon on
web services or AWS in short is a cloud
provider that offers a variety of
services such as a variety of ID
services or infrastructure services such
as a compute power database a Content
delivery and other resources that helps
us to scale and grow our business and
AWS is hard AWS is picking up AWS is
being adapted by a lot of customers
that's because AWS is easy to use even
for a beginner and talking about safety
the the AWS infrastructure is designed
to keep the data safe irrespective of
the size of the data with small data be
it very minimal data the all the data
that you have in terabytes and in
petabytes Amazon can keep it safe in
their environment and the wonderful
thing and the most important reason why
why a lot of customers move into the
cloud is that the pay as you go pricing
there is no long-term commitment and
it's very cost effective what this means
is that you're not paying for resource
that you're not using in on-premises you
do pay for resources you're not using a
lot meaning you go and buy a server you
do the estimate for the next five years
and only after like three or four years
you'll be hitting the peak capacity but
still you would be buying that capacity
before four years right and then you
will gradually be you know utilizing it
from you know 40 percent date 60
percentage 70 80 and then 100. so what
you have done is that even though you're
not using the full capacity you still
have bought it and are and are paying
for it from day one but in the cloud
it's not like that you only pay for the
resources that you use anytime you want
more you scale up the results and you
you pay for the scaled up resource and
anytime you want less you scale down the
resource and you pay less for that scale
down resource let's talk about why
Amazon Route 53 let's take this scenario
where Rachel is trying to open her web
browser and the URL that she hit isn't
working a lot of reasons behind why the
URL isn't working it could be the server
utilization that went High it could be
it could be the memory usage that went
High a lot of reasons and she starts to
think is there an efficient way to scale
resources according to the user
requirements or is there an efficient
way to kind of mask all those failures
and kind of divert the traffic to the
appropriate active you know active
resource or active service that's
running our application you always want
to hide the failures right in I.T kind
of mask the failure and direct the
customer to another healthy service
that's running right none of your
customers would want to see a server not
available or you know none of the
customers your customers would want to
see yourself is not working not
impressive to them and this is Tom Tom
is an I.T guy and he comes up with an
idea and he's answering Rachel yes we
can scale resources efficiently using
Amazon a Route 53 in a sense he's saying
that yes we can mask the failure and we
can keep the services up and running
meaning we can provide more High
availability to our customers with the
use of Route 53 and then he goes on and
explains Amazon Route 53 is a DNS
service that gives developers an
efficient way to connect users to
internet applications without any
downtime now downtime is the key Amazon
Route 53 helps us to avoid any downtime
that customers will experience you still
will have downtime in your server in
your application but your customers will
not be made aware of it and then Rachel
is kind of interested and she's like
yeah that sounds interesting I want to
learn more about it it and Tom goes on
and explains the important concepts of
Amazon Rock 53 that's everything that
I'm going to explain it to you as well
alright so what is Amazon Route 53
Amazon raw 53 is a highly scalable DNS
or domain name system web service this
service this Amazon raw 53 it functions
three main things or it has three main
functions so the first thing is if a
website needs a name Route 53 registers
the name for the website domain let's
say you want to buy google.com you want
to buy the domain name let's say you
want to buy that domain name you buy
that through route 53. secondly a Route
53 is the service that actually connects
your server which is running your
application or which is holding which is
serving your web page so that's the
service that actually Route 53 is the
service that connects the user to your
server when they hit google.com in the
browser or whatever domain name that you
have purchased so you bought a domain
name and the user types in
yourdomainame.com and then Roth 53 is a
service that helps the user to connect
their browser to the application that's
running in an ec2 instance or any other
server that you are using to serve that
content now not only that Route 53
checks Health off the resource by
sending automated requests over the
internet to a resource so that's how it
identifies if there is any resource that
has failed can I say resource I'm
referring to any infrastructure failure
any application Level failure so it kind
of keeps checking so it understands it
first before the customer notices it and
then it does the magic kind of shifts
the connection from one server to the
other server we call it routing we will
talk about that as we progress so the
benefits of using Route 53 it's highly
scalable meaning suddenly let's say the
number of requests the number of people
trying to access your website through
that domain name that you have bought
let's say it has increased Route 53 is
highly scalable right it can handle even
millions and millions of requests
because it's highly scalable and it's
managed by Amazon well the same thing
it's reliable it's a highly scalable it
can handle large queries without the
users Without You interacting without
the user who bought it interact with it
you don't have to scale up you know when
you're expecting more requests it
automatically scales and it is very
reliable in a sense that it's very
consistent it has the ability to Route
the users to the appropriate application
through the logic that it has it's very
easy to use when we do the lab you're
going to see that it's very easy easy to
use you buy the domain name and then you
simply map it to the application you
simply map it to the server by putting
in the IP or if you you can simply map
it to another load balancer by putting
in the load balancer URL you can simply
map it to another S3 bucket but simply
putting the S3 bucket name or the S3
bucket URL it's pretty straightforward
easy to set up and it's very cost
effective in a way that we only pay for
the service that we have used so no
wastage of money here so the billing is
set up in such a way that you are paying
only for the amount of requests that you
have received right the amount of
traffic the amount of requests that you
have received and couple of other things
the the number of uh hosted zones that
you have created right and couple of
other things it's very cost effective in
such a way that you only pay for the
service that you are using and it's
secure in a way that access to Route 53
is integrated with the identity and
access management IAM so you only have
authorized users gain access to Route
53. the trainee who just joined s today
won't get access and the contractor or
the consultant the third party
consultant content you have given access
or who is using your environment you can
block access to that particular person
because he's not the admin or he's not a
privileged user in your account so only
privileged users and admin gain access
to Route 53 through IAM now let's talk
about the routing policies so when you
create a record in in round 53 recorder
is nothing but an entry so when you do
that you choose a routing policy all
right routing policy is nothing but it
determines how Route 53 responds to your
queries how the DNS queries are being
responded right that's that's a record
or that's a routing policy so the first
one is a simple routing policy so we use
Simple routing policy for a single
resource in other words simple routing
allows to configure DNS with no special
Route 53 routing it's kind of one to one
you use an single resource that performs
a given function to your domain for
example if you want to Simply map an URL
to a web server that's pretty
straightforward simple routing so it
routes traffic to a single resource
example web server to a website and with
simple routing multiple records with the
same name cannot be created but multiple
values can be created in the same record
the second type of routing policy is
failover routing so we would be using
failover routing when we want to
configure active passive failover if
something failed right you want to fail
over to the next resource which was
previously the backup resource now the
active resource or which was previously
the backup server now it's an active
server so you would be failing over to
that particular resource or that
particular IP if you want to do that we
use failover routing so failure routing
routes traffic to a resource when the
resource is healthy or to a different
different resource when the previous
resource is unhealthy in other words any
time a resource goes unhealthy I mean it
does all that's needed to shift the
traffic from the primary resource to the
secondary resource in other words from
the unhealthy resource to the healthy
resource and this records can Route
traffic to anything from an Amazon S3
bucket or you can also configure a
complex tree of Records now when we
configure the records it will be more
clear to you so as of now just
understand that Route 53 can route or
this routing policy the failover routing
policy can Route traffic to Amazon S3
bucket or to a website that has complex
tree of Records geolocation routing
policy now geolocation routing just like
the name says it takes that routing
decision based on the geographic
location of the user in other words you
know when you one with all traffic based
on the location of the user so that's
your primary criteria for sending that
request to the appropriate server we
will be using jio location routing so it
localizes the content and presents a
part or the entire website in the
language of the user for example a user
from us you would want to direct them to
an English website and a user from
German if you want to send them to the
German website and a user from France
you know you want to send those requests
or you want to show content specific to
a customer who lives in France a French
website so this is if that's your
condition this is the routing policy we
would be using and the geographic
locations are specified by either
continent or by country or by state in
the United States so only in the United
States you can actually split it to
state level and for the rest of the
countries you can do it on a country
level on a and high level you can also
do it on a continent level the next type
of routing policy would be your
proximity routing geoproximity routing
policy when we want to Route traffic
based on the location of our resource
and optimally shift traffic from
resources in one location to resource in
another location we would be using
geoproximity routing so geoproximity
routing routes traffic to the resources
based on the geographic location of the
user and the resources they want to
access and it also has an option to
Route more traffic or less to a given
resource by specifying a value known as
a biased kind of weight but we also have
weighted routing that's different so
we've chosen different name bias you can
send more traffic to a particular
resource by having a bias on that
particular routing condition and a bias
expands or shrinks the size of the
geographic region from which traffic is
routed to a resource and then we have
latency based routing just like the name
says we use latency based routing if we
have resources and multiple AWS regions
and if you want to Route traffic to the
region that provides the best latency at
any given point of time so let's say if
one single website needs to be installed
and hosted on multiple AWS regions then
latency routing policy is what is being
used it improves the performance of the
users by serving the request from the
AWS region that provides the lowest
latency so at any given point if
performance is your criteria and at any
given point of time irrespective of what
happens in Amazon infrastructure
irrespective of what happens in the
internet if you want to route your users
to the best performing website best
performing region then we would be using
latency based interrupting and for using
latency based routing we should create
latency records for the resources in
multiple AWS regions and then the other
type of routing policy is multi-value
routing policy where we can make Route
53 to respond to DNS queries with up to
eight healthy records selected at random
so you're not kind of loading one
particular server we can Define eight
records and on a random basis Route 53
will respond to queries from these eight
records so it's not one server that gets
all the requests but eight servers gets
the request in a random fashion so it's
multi-value routing policy and what we
get by this is that we are Distributing
the traffic to many servers instead of
just one server so multi-value routing
configures Route 53 to return multiple
values in response to a single or
multiple DNS queries it also checks the
health of order services and Returns the
multiple values only for the healthy
resources let's say out of the eight
servers we have defined one server is
not doing healthy it will not respond to
the query with the details of the
unhealthy server right so now it's going
to treat it as only seven servers in the
list because one server is unhealthy and
it has the ability to return multiple
Health checkable IP addresses to improve
availability and load balancing the
other type of routing policy is weighted
routing policy and in here we use to
Route traffic or this is used to Route
traffic to multiple resources in a
proportion that we specify so this is an
weighted routing and weighted routing
routes multiple resources to a single
domain name or a subdomain and control
the traffic that's routed to each
resources so this is very useful when
you are doing load balancing and testing
new versions of the software so when you
have a new version of this software you
really don't want to send 100 of the
traffic to it so you want to get
customers feedback about the new
software that you've launched new
version or new application that you've
launched so you would kind of send only
20 of the traffic to that application
get customer feedback and if all is good
then we would move the rest of the
traffic to that new application so any
software launches application launches
will be using weighted routing now let's
talk about the key benefits or key
features of Route 53 some of the key
features of Route 53 are traffic flow it
routes end users to the endpoint that
should provide the best user experience
that's what we discussed in the routing
policies right it uses a routing policy
a latency based routing policy and jio
based routing policy and then failover
routing policy so it kind of improves
the user experience and the key feature
the other key feature of Route 53 is we
can buy domain names using a Roth 53
using Route 53 console we can buy it
from here and use it in route 53.
previously it was not the case but now
we can buy it directly from Amazon
through Route 53 and we can assign it to
any resources that we want so anybody
browsing that URL the connection will be
directed to the server in AWS that runs
our website a health checks it monitors
health and performance of the
application so it comes with an health
check attached to it health check are
useful to make sure that the unhealthy
resources are retired right the
unhealthy resources are taken away or
your customers are not kind of hitting
the unhealthy resources and they see an
service down page or something like that
we can have way weighted round robin
load balancing that's helpful and
spreading traffic between several
services or service we are round robin
algorithm so no one server is fully hit
or no one server kind of fully absorbs
all the traffic you know you can shift
you can split and shift the traffic to
different servers based on the weight
that you would be configuring and also
weighted routing also helps with a soft
launch soft launch of your new
application or the new version of your
website there are different ways we can
access Amazon Route 53 so you can access
Amazon Route 53 through AWS console you
can also access Amazon Route 53 using
AWS sdks and we can access it using we
can configure it using the apis and we
can also do it through the command line
interface that's Linux type Linux flavor
aw command line interface we can also do
that using Windows command line Windows
Powershell flavored command line
interface as well now let's look at some
of the companies that are using raw 53.
so some of the famous companies that use
Route 53 are medium medium is an online
publishing platform and it's more like a
social journalism it's kind of having
hybrid collection of professionals
people and Publications or exclusive
blogs or Publishers on medium it's kind
of an blog website and that uses Route
53 for the DNS service a Reddit is an
social news aggregation or web content
rating and discussion website that uses
route 53s so these are some websites
that that are accessed throughout the
world and they are using Roth 53 and
it's highly scalable suddenly there is a
new news right their website will be
accessed a lot and they need to keep
their service up and running all the
time more availability otherwise
customers will end up in a broken page
and the number of customers who will be
using the website will come down so it's
very critical on these sites these
companies are very critical you know
they're being highly available their
page their site being highly available
and the internet is very critical and
crucial for them and they rely and use
Route 53 to meet that particular demand
and Airbnb is another company
instacart kozra is another company
stripe is another company that uses
Route 53 to as their DNS provider for
the DNS service they use Route 53 so
their customers get best performance
they use Route 53 so their website is
highly available they use Route 53 to
kind of shift to the traffic between the
resources so their resources are
properly used with all the weighted
routing the resources are properly used
now let's quickly look at a demo I'm in
my AWS console and I'm in Route 53 so
let me click on Route 53. so in this lab
we're actually going to simulate buying
a domain name and then we're going to
create an S3 static website and we're
going to map that website to this DNS
name right so the procedure is the same
for mapping load balancer the procedure
is the same for mapping cloudfront the
procedure is the same for mapping ec2
instances as well we're picking S3 for
Simplicity right but our focus is
actually on Route 53. so let's go in
here and I will see if we can but we
will buy a domain name here so let's
first check the availability of a domain
name called simply learn hyphen demo
hyphen Route 53 let's check its
availability it is available for 12 so
let me add it to cart and then come back
here and then once you continue It'll
ask for personal information once you
give personal information you finally
check out and then it gets added to your
shopping list once you pay for it Amazon
takes like 24 to 40 dollars to make that
a DNS name available so the next stage
would be contact details and then the
third stage would be a verify and
purchase so once we've bought the domain
name it will become available in our DNS
portal and I do have a domain name which
I bought some time back and it's now
available for me to use so I can go to
hosted Zone and simply start creating I
can go to hosted Zone and then here it's
going to list all the domain names for
me all right click on the domain name
and then click on the record set and
here I can actually map elastic load
balancer S3 website VPC endpoint API
Gateway and cloudfront elastic bean
stock domain names right all that gets
mapped through this portal quite simple
like four or five step button clicks and
then it it will be done so I have an
domain name bot and then I'm going to go
to S3 and now I'll show you what I've
done in S3 so I've created a bucket name
called as a DNS name let me clear the
content in them so I've created a bucket
and then permissions I've turned off
Public Access blocking and then I've
created an a bucket policy so this
bucket is now publicly accessible and
then I went on to properties and created
the static website hosting right and
I've pointed that this is the file
that's my index file that I'm going to
put or name of the file that's going to
be my index file that I'm going to put
in this S3 bucket so put the index
file.html saved it and we're going to
create a file now we're going to create
an index file so this is a sample code
it says amazon.53 getting started
routing internet traffic to S3 bucket
for your website and then couple of
other information so save it as an
index.html file in my desktop so let me
upload that from my desktop into this
bucket so that's index.html and it's in
capital I so let me go to properties and
go to static website hosting and make
sure that I spell it properly right it's
case sensitive and then save it so now
this means that my website should be
running through this URL and it does
it's running to the static website URL
we're halfway through so now let me go
back to Route 53 go back to Route 53 go
back to hosted zones go into the domain
name and then create a record set and
it's going to be an alias record and I I
see my S3 static website endpoint there
all right so click on it and create it
has now created an record that's
pointing my domain name to the S3
endpoint that I have created and my
static website is running from it so let
me test it right so let me go to the
browser put the domain name in there and
sure enough the domain name when we
browse the queried for the domain name
around 53 returned a response saying
this domain name is actually mapped to
the S3 bucket historic website hosting
enable S3 bucket and this is the URL for
that static website hosting and then my
browser was able to connect to that S3
bucket and download the details and show
it in my browser all right so it's that
simple and pretty straightforward
today's session is on AWS elastic
Beanstalk so what's in it for you today
we'll be discussing about what is AWS
why we require AWS elastic bean stock
what is AWS elastic bean stock the
advantages disadvantages the components
of mean stock along with that the
architecture and the companies that are
primarily using the AWS bin stock so
let's get started and first understand
what is AWS AWS stands for Amazon web
services it's a cloud provider and that
offers a variety of services such as
compute power database storage content
delivery and many other resources so we
know that aw this is the largest cloud
provider in the market and so many
services are available in the AWS where
you can apply the business Logics and
create the solutions using the cloud
platforms now why AWS elastic mean stock
now what happened earlier and that
whenever the developer used to create
the software or the modules related to
the software has to be joined together
to create a big application now one
developer creates a module that has to
be shared with another developer and if
the developers are geographically
separated then it has to be shared over
a medium probably an internet so that is
going to take some time it would be a
difficult process and in return it makes
the application or a software
development early in their process the
building of the software development
linear process so there were challenges
which the developers were facing earlier
and to overcome that we have the mean
stock as a service available in the AWS
so why AWS elastic bean stock is
required now AWS elastic beam stock has
made the life of the developers quite
easy uh in terms of that they can share
the applications across different
devices at a shorter time duration now
let's understand what is AWS elastic
Beanstalk a AWS elastic bean stock is a
service which is used to deploy and
scale web applications by developers not
only web application any application
that is being developed by the
developers this is a simple
representation of the AWS plastic bean
stock NOW along with that the AWS
elastic beam stock supports the
programming language the runtime
environments that are java.net PHP
node.js python Ruby go and Docker and in
case if you're looking for any other
programming language or a runtime
environment then you can make a request
with AWS to arrange that for you now
what are the advantages associated with
the elastic bean stock First Advantage
is that it's a highly scalable service
now when we talk about a scalability it
means that whenever we require the
resources in demand we can scale up the
resources or we can scale down the
resources so that is kind of a
flexibility we get in terms of changing
the type of resources whenever we need
it and in that case the elastic bean
stock is a highly scalable service now
that is something which is very
difficult to achieve in case of an
on-prem environments because you have to
plan for the infrastructure and in case
if you're short of the resources within
that infrastructure then you have to
procure it again the second Advantage
associated with the Beanstalk is that
it's a fast and simple to begin now when
we say it's fast and simple that means
that you just have to focus on the
development of an application building
an application and then you can just
deploy the application directly using
the Beanstalk what the bean stock is
going to do that every networking aspect
is being taken care by the bean stock it
deploys your application in the back end
on the servers and then you can directly
access your application using the URL or
through the IP address the third
advantage is that it offers the quick
deployment that is what we discussed in
the fast and simple to begin as well so
why it offers a quick deployment you
don't have to bother about the
networking Concepts you just have to
focus on the application development and
then you can just upload your
application deploy that and then you are
good to go the other Advantage is that
it supports multi-tenant architecture
when we talk about tenants or
multi-tenants that means we can have a
virtual environments for separate
organizations or the divisions within
the organizations that will be virtually
isolated so likewise you can have a
virtually isolated environments created
on the Beanstalk and they can be
separated used as a separate entities or
a separate divisions within the
organization and we know that it's a
flexible service since it's a scalable
then which is a flexible also now coming
to the simplifies operations as an
advantage now once the application is
deployed using the Beanstalk then it
becomes very easy to maintain and
support that application using the will
be in stock Services itself and the last
advantage that we can have from the
Beanstalk is that it's a cost efficient
service the cost efficient as we know
that many of the AWS services are cost
effective the cost optimization can be
better managed using the AWS mean stock
as compared to if you are developing or
if you are deploying any kind of an
application or a solution on the on-prem
servers now there are some components
that are associated with the AWS bean
stock and it has to be created in the
form of a sequence manner so AWS elastic
means law consists of few important
components which are required while
developing an application now what are
these components these are four
components one is application the second
is application version the third is
environment and the fourth one is the
environment tier and we have to progress
while deploying our applications or the
softwares using the same sequence now
let's understand what are the different
components of the Beanstalk are the
application it refers to a unique label
which is used as a Deployable code for a
web application so generally you deploy
your web application or you create your
application and that is something which
is basically uh used as a unique label
then the second component is application
versions so it resembles a folder which
stores a collection of components such
as environments versions and environment
configurations so all these components
are being stored using the application
version the third most important
component is the environment in the
environment only the current versions of
the applications runs now remember that
elastic mean stock supports multiple
versions as well and using the
environment you can only run the current
version of the application file if you
wanted to have another version of an
application to be running then you have
to create another environment for that
then comes the environment tier and in
the environment here it is basically it
designates the type of application that
the environment runs on now generally
there are two types of environment here
one is the web and the other one is the
worker node and that's something which
we'll be discussing later as well now
let's understand how does elastic bean
stock in AWS works so first we have to
create an application and this is a task
that would be done by the developers and
for that you can actually select any
runtime environment or a programming
language like Java Docker Ruby gopal or
python as well and once you select that
environment you can develop your
application using that runtime
environments now after that once the
application is created then you have to
upload the version of an application on
the AWS and after that once the version
is uploaded and then you have to launch
your environment so just have to click
on the buttons that's it nothing more
you have to do once the environment is
launched then you can actually view that
environment using a web URL or using the
IP address now what happens in that case
is when you launch an environment in the
back end the elastic Beanstalk runs
automatically runs any ec2 instance and
using a metadata the mean stock deploys
our application within that ec2 instance
that is something which you can look
into the ec2 dashboard as well so you
don't have to take care of the security
groups you don't have to take care of
the IP addressing and even you don't
have to login into the instance and
deploy your application it would be done
automatically by the Beanstalk it's just
that you just have to monitor the
environment and the statistics will be
available there itself in the bin stock
dashboard otherwise you can view those
statistics in the cloudwatch logs as
well now in case if you wanted to update
any kind of a version then you just
upload a new version and then just apply
that and then monitor your environment
so these are the essentials to create a
local applications for any platform
whether it's a node.js python Etc these
are the things that you have to actually
take care and this is the sequence you
have to follow while creating an
environment so you can say that it's a
four steps creation of or deployment of
your application that's it now after
users upload their versions the
configuration is automatically deployed
with a load balancer yes and with the
load balancer uh that means you can
access the applications using the load
balancer DNS also and apart from load
balancer if you wanted to put any other
feature that includes the auto scaling
for example if you wanted to create your
ec2 instances where the application will
be deployed within the virtual private
cloud or in a particular subnet within
the VPC all those features that are
available and you can select them using
the mean stock itself you don't have to
move out to the VPC you don't have to
actually go to the ec2 dashboard and
select all those separately everything
would be available within the beanstock
dashboard so that's what it says in the
presentation that after creating an
application the deploy service can be
specifically accessed using the URL so
once the environment is created there
will be a URL defined now you can put a
URL name also that is something which
you wanted to put for your application
you can Define that you can check for
the availability of that URL and then
you have to use that URL to access your
application or the browser now once it
is done then in in the monitor
environment it says the environment is
monitored provided capacity provisioning
load balancing Auto scaling and Hand
Motor features all those features are
available there itself in the mean stock
now let's understand the architecture of
AWS elastic Beanstalk now there are two
types of environments that you have to
select you can select one is the web
server environment and the other one is
the worker environment So based on the
client requirement Beanstalk gives you
two different types of environment that
you have to select generally the web
server environment is the front-end
facing and that means the client should
be as accessing this environment
directly using a URL so mostly a web
applications are deployed using that
environment the worker environment is
the backend applications or on the micro
apps which are basically required to
support the running of the web
applications now it depends on the
client requirement what kind of an
environment you wanted to select now in
the web server environment it only
handles the HTTP request from the
clients so that's why we use the web
server environment mostly for the web
applications or any application which
works on the HTTP https requests so it's
not only the HTTP you can use the httpss
as well the worker environment it
process background tasks and minimizes
the consumption of resources so again it
is just like a kind of a micro service
or an application services that are
running in the back end to support the
web server environment now coming to the
understanding of the AWS main stock so
this is how the architecture of the AWS
bean stock is designed and you can refer
to that image also now in the web server
environment let's say if we select a web
server environment and it says that if
the application receives client request
the Amazon Route 53 sends his request to
the elastic load balancer now obviously
we discussed here that the web server
environment is primarily an environment
which receives the HTTP requests it's a
kind of a client-facing environment now
if the application receives a client
request Amazon from the Amazon Route 53
this Route 53 is a service which is
primarily used for DNS mapping it's a
global Service and it may route you can
route the traffic from the Route 53
matching your domains towards the load
balancer and from the load balancer you
can point the traffic to the web server
environment obviously the web server
environment is nothing it's just the ec2
instances that would be running in the
back end now here in the diagram you can
see that there are two web server
environments and they are created in the
auto scaling group that means there is
some kind of scaling options that are
defined as well and these instances are
created in an availability zone or they
can be created in a different
availability Zone also for the
redundancy as well and these web
application servers are further
connected to your databases which
primarily will be in a different
security groups probably it can be an
RDS database also so all these
functionalities all these features are
basically available on the elastic mean
stock dashboard itself now what happens
in that case is if the application
receives client requests Amazon Route 53
send these requests to the load balancer
later the load balancer shares those
requests among the ec2 instances how
does that happen it happens using a
predefined algorithm the equal
distribution of a load is distributed to
both the ec2 instances or n number of
ec2 instances running in the
availability zone now in the
availability zones every ec2 instance
would have its own Security Group they
can have a common Security Group also
they can have their own Security Group
as well now after the security group the
load balancer is then connected to the
Amazon ec2 instance which are part of
the auto scaling group so that's
something which we have discussed
already now this Auto scaling group is
would be defined from the Beanstalk
itself and there will be some scaling
options that will be created it could be
a possibility that it might be the
minimum number of instances that would
be running as of now and based on the
threshold defined it may increase the
number of ec2 instance and the load
balancer will keep on Distributing the
load to as many instances that will be
created inside the available resource
obviously there will be an internal hell
check that the load balancer will be
first doing before Distributing the
real-time traffic to this instances
created by the mean stock now what does
Auto scaling group does it automatically
starts the additional ec2 instance to
account increasing load on your
application that's something which we
know that and also it monitors and
scales instances based on the workload
as well so it depends on what kind of a
scaling threshold you have defined in
the auto scaling groups and when the
load of an application decreases the ec2
instance will also be decreased so
whenever we talk about the auto scaling
generally it comes in our mind is that
we scale up the resources that means we
it increases the ec2 instances in the
auto scaling you might have the scale
down option also scale down policy also
created in which if the load minimizes
it can dominate the additional ec2
instances as well so that is something
which will be automatically managed all
these features can be achievable using
the elastic mean stock and with this
feature accommodated it gives you the
better cost optimization in terms of
managing your resources now it says that
elastic bean stock has a default
Security Group and the security group
acts as a Firefall for the instances now
here in this diagram it says about the
security group Auto scaling also you
might create it in a default VPC also
you might create it in your custom EPC
also where you can have the additional
level of security is also created you
can have the nacls knuckles also defined
here before the security groups so that
would give you the additional filtering
option or the firewall option now it
says that with these groups with these
security groups it allows establishing
security groups to the database server
as well so every database would also
have its own Security Group and the
connection can be created between the
web servers environment that is created
by the Beanstalk to the database
security groups as well now let's
discuss about the worker environment now
understanding the worker environment
what happens is that the client the web
server environment is the client facing
the client sends a request for an access
to the web server and in this diagram
the web server further sends it to the
sqs which is a simple queue service and
the queue service send it to the worker
environment and then whatever the worker
environment is created for doing some
kind of a processing or some kind of an
application that is running in the back
end that environment initiates and then
send back the results to this sqs and my
servers so let's understand the
architecture of a AWS plastic bean stock
with the worker environment so when a
worker environment here is launched AWS
elastic bean stock install the server on
every ec2 instance so that is in the
case of a web server environment also
and later the server passes the request
to the simple queue service now this
service is an asynchronous service
instead of a simple queue service you
can have other services also it is not
necessary that you need to have the sqs
also this is an example that we are
discussing about and the sqs shares
those message via a post request to the
HTTP path over the worker environment
and there are many case studies also
with respect to this kind of an
environment that is being created that
is being done on many customers and you
can search for these kind of a case
studies available on the internet now
the worker environment executes the task
given by the sqs with the HTTP response
after the operation is completed now
here what happens is a quick recap the
client request for an access of an
application to a web server using an
HTTP request the web server passes that
request to the Q service the queue
service shares the message with the
worker probably a worker might be the
manual worker and generally it's an
automated worker so it would be shared
via the worker environment only and the
worker sends back the response with the
HTTP response back to the queue that
response can be viewed directly from the
queue service by the client using the
web server so this is one of the example
likewise as I said that there can be
many other examples also where you can
have the worker environments defined now
what are the companies that are using
the elastic bean stock these are few of
the companies that are primarily using
on a Zillow jelly button games then you
have League of Women Voters eBay these
are some of the few listed companies and
obviously you search on the AWS site and
you'll find many more organizations that
are using the elastic mean stock
primarily for deploying their
applications now the next thing is to go
with the practicals that how actually we
use the elastic beam stock so let's look
into the demo using the AWS elastic
Beanstalk now first you have to login
into the AWS console and I'm sure that
you might be having the accounts created
or you can use the IM credentials as
well and then you have to select the
region also now I am in the North
Virginia region likewise you can select
any of the regions that are listed here
now click on the services and you have
to search for the elastic bean stock you
can find the elastic bean stock under
the compute section so here itself
you'll find the elastic bean stock as a
service now open this service and there
it will give you an option to create an
environment you have to specifically
select an environment properly a worker
environment or a web service environment
so let's wait for the service to open so
we have the dashboard now available
available with us this is how the
elastic bean stock looks and this is the
symbol representation of a beanstalk now
what you have to do is we have to click
on get started and that will load and
you have to create a web app so instead
of creating a web app what we'll do
we'll create a new application so just
click on create a new application put an
application name let's say we put
something like x y z you can put any
description to your application let's
say it's a demo app and click on create
now it says you have to select an
environment now the environment the
application name XYZ is created you just
have to select an environment so click
on create one now and it is going to ask
you that what kind of an environment
here you wanted to select so as we
discussed that there are two types of
environments one is the web server and
the other one is the worker and one
let's look into it what is defined by
the AWS AWS says that it has two types
of environment tiers to support
different types of web applications web
servers are standard applications that
lets lesson 4 and then process HTTP
request typically over port number 80.
workers are specialized applications
that have a background processing task
that listens for message on an Amazon
sqs queue workers application posts
those messages to your application by
using the HTTP response so that's what
we saw in the case of the Beanstalk
slides also now the usability of a
worker environment can be anything now
we'll do a demo for creating a web
server environment so just click on
select and you we have the environment
name created now we can Define our own
domain it ends with the region dot
elasticbeanstalk.com let's say I look
for a domain which is XYZ only that's
the environment name now I'll check for
the availability whether that domain
name is available with us or not and it
says we don't have that domain name so
probably I'll try to make it with some
other name and let's look for the
availability XYZ ABC and it says yes it
is available now one side deploy my
application I would be able to access
the application using this complete DNS
so you can put a description it's a demo
app that we are creating and then you
have to define a platform as well now
these are the platforms that are
supported by the AWS let's say I wanted
to run a node.js environment so I'll
just click on the node.js platform the
application codes is something which is
basically developed by the developers
and you can upload the App application
right now or you can do that later as
well once the environment is ready now
either you can select to create an
environment if you wanted to go with all
the default settings otherwise if you
wanted to customize it more you can
click on configure more options so let's
click on configure more options and here
you would be able to define various
different features like the type of an
instance for example what kind of an ec2
instance or a server that should be
running so that the Beanstalk can deploy
our applications over it if you wanted
to modify just click on a modify button
and here you can modify your instances
with respect to the storage as well now
apart from that if you wanted to do some
modification in the case of monitoring
in the case of databases in the case of
security or in the case of a capacity
let's look into the capacity so here you
can actually do the modification so in
the capacity you can select the instance
type also by default it is t2.micro but
in case if your application requires a
larger type of an instance then you can
actually go for the instance type as
well similarly you can Define your Emi
IDs also because obviously for the
application to run you would require the
operating system also so you can select
that particular Ami ID for your
operating system as well let's cancel
that likewise you have many other
features that you can actually Define
here from the dashboard and you don't
have to go to the ec2. to do the
modifications now let's go and create an
environment let's assume that we are
going with the default configuration so
this is going to create our environment
the environment is being created and you
can get the environment and the logs
defined in the dashboard itself so
you'll see that the Beanstalk
environment is being initiated the
environment is being started and in case
if there would be any errors or if it is
deployed correctly you'll get all the
logs here itself now the environments
are basically color coded so there are
different color codings that are defined
if you get the environment in a green
color that means everything is good to
go so here you can see that it has
created an elastic IP it has checked the
health of the environment now it has
created the security groups and that
would be an auto security groups created
by the mean stock and the environment
creation has been started you can see
that uh elastic bean stock as Amazon S3
storage bucket for your environment data
as well this is the URL through which
you will be accessing the environment
but right now we cannot do that since
the environment is being created let's
click on the application name and here
you can see that it is in a gray color
that means right now the build is being
done it is being created once it will be
successfully created it should change to
the green color and then we will be able
to access our environment using the URL
now if I move to the ec2 instances and
see in the ec2 dashboard if I see
whether the instance is being created by
the Beanstalk or not so let's see and
let's see what are the differences in
terms of creating an instance manually
and getting it created from the
Beanstalk so click on the ec2 let's go
to the old ec2 experience that's what we
are familiar with and let's see what's
there in the dashboard so here you can
see one running instance let's open that
and the XYZ environment which was
created from the Beanstalk is being
initiated the instance is being
initiated and that is something which is
being done by the mean stock itself we
have not gone to the dashboard and
created it manually now in the security
groups if you see that here the AWS mean
stock security groups are defined it has
the elastic IPS also defined so
everything is being created by the
Beanstalk itself right now let's go back
to the Beanstalk and let's look into the
status of our environment whether the
color coding has been changed from Gray
to green or not and here you can see the
environment is successfully created and
we have that environment colored in
green we'll access the environment and
it says it's a web server environment
its platform is node.js running on
64-bit Amazon Linux Ami and it's a
sample app sample application health
status is okay now the other thing is
that if you do not want to use the web
console the Management console to access
the main stock then the Beanstalk offers
you the elastic Beanstalk CLI as well so
you can install the command line
interface and then you have the command
references CLI command references that
you can actually play with and get your
applications deployed using the
Beanstalk itself so this is one of the
sample CLI commands that you can
actually look into now let's look into
the environment let's click on the
environment and we'll be represented
with the URL it says health is okay
these are the logs that you have to
follow in case if there are any issues
the platform is node.js that is what we
selected now the next thing is you just
have to upload and deploy your
application so just click on upload and
deploy select the version label or the
name select file and wherever your
application is hosted at just select
that upload it and deploy your
application you'll see that the like
your environment is created similarly
your application will be deployed
automatically on the instance and from
this URL you will be able to view the
output it is as simple as just like you
have to follow these four steps now
let's see whether the node.js
environment is running on our instance
before deploying an application so we'll
just click on this URL since the
Beanstalk has already opened up the
security groups or HTTP Port 80 for all
we can actually view that output
directly from the URL so we have the
node.js running that's visible here and
after that you just have to upload and
deploy your application and then from
that URL you can get the output now this
URL you can map it with the root 53
service so using the root 53 DNS
Services the domain name can be pointed
to the elastic bean stock URL and from
there it can be pointed to the
applications that are running on the ec2
instance whether you wanted to point it
to the URL directly using the mean stock
you can do that otherwise as we saw in
the slides you can use the root 53
pointer to the load balancer and then
point it to the instances directly also
once it is created by the mean stock so
that was the demo guys with respect to
the bean stock and how we can actually
run the environments apart from that the
operational task like system operations
you can manage all these things from the
environment dashboard itself so you have
the configurations you have the logs you
can actually check the health status of
your environment you can do the
monitoring and you can actually get the
alarms and the events here so let's say
if I wanted to if I wanted to see the
logs I can request for the logs here
itself and I'll be represented with the
full log report and I can now download
that log file and I can view logs so
it's in D so we have this bundle locks
in the zip file all right so if you want
to see some kind of logs with respect to
elastic bean stock activity it's in the
form of a notepad and here you can see
what all configurations the Beanstalk
has done on your environment on your
instance similarly you can go for the
health monitoring alarms events and all
those things if getting your learning
started is half the battle what if you
could do that for free visit scale up by
simply learn click on the link in the
description to know more
I
the AWS Solutions architect course
migrating to the cloud doesn't mean that
resources become completely separated
from the local infrastructure
in fact running applications in the
cloud will be completely transparent to
your end users
AWS offers a number of services to fully
and seamlessly integrate your local
Resources with the cloud
one such service is the Amazon virtual
private cloud
this lesson talks about creating virtual
networks that closely resemble the ones
that operate in your own data centers
but with the added benefit of being able
to take full advantage of AWS
so let's get started
[Music]
in this lesson you'll learn all about
virtual private clouds and understand
their concept
you'll know the difference between
public private and elastic IP addresses
you'll learn about what a public and
private Southerner is
and you'll understand what an internet
gateway is and how it's used
you'll learn what root tables are and
when they are used
you'll understand what and that Gateway
is
we'll take a look at security groups and
their importance
and we'll take a look at Network ACLS
and how they're used in Amazon VPC
we'll also review the Amazon VPC best
practices
and also the costs associated with
running a VPC in the Amazon Cloud
welcome to the Amazon virtual private
cloud and subnet section
in this section we're going to have an
overview of what Amazon VPC is and how
you use it and we're also going to have
a demonstration of how to create your
own custom virtual private cloud
we're going to look at IP addresses and
the use of elastic IP addresses in AWS
and finally we'll take a look at subnets
and there'll be a demonstration about
how to create your own subnets in an
Amazon VPC
and here are some other terms that are
used in vpcs
the subnets root tables elastic IP
addresses internet gateways Nat gateways
Network ACLS and security groups and in
the next sections we're going to take a
look at each of these and build our own
custom VPC that we'll use throughout
this course
Amazon defines a VPC as a virtual
private Cloud that enables you to launch
AWS resources into a virtual Network
that you've defined this virtual Network
closely resembles a traditional Network
that you'd operate in your own data
center but with the benefits of using
the scalable infrastructure of AWS
a VPC is your own virtual Network in the
Amazon Cloud which is used as the
network layer for your ec2 resources and
this is a diagram of the default VPC now
there's a lot going on here so don't
worry about that what we're going to do
is break down each of the individual
items in this default VPC over the
coming lesson
but what you need to know is that a VPC
is a critical part of the exam and you
need to know all the concepts and how it
differs from your own Networks
throughout this lesson we're going to
create our own VPC from scratch which
you'll need to replicate at the end of
this so you can do well in the exam
each VPC that you create is logically
isolated from other virtual networks in
the AWS Cloud it's fully customizable
you can select the IP address range
create subnet configure root tables set
up Network gateways Define security
settings using security groups and
network access control lists
so each Amazon account comes with a
default VPC that's pre-configured for
you to start using straight away so you
can launch your ec2 instances without
having to think about anything we
mentioned in the opening section A VPC
can span multiple availability zones in
a region
and here's a very basic diagram of a VPC
it isn't this simple in reality
and as we saw in the first section
here's the default Amazon VPC which
looks kind of complicated
but what we need to know at this stage
is that cidr block for the default VPC
is always a 16 subnet mask so in this
example it's
172.31.0.0 16. what that means is this
VPC will provide up to 65
536 private IP addresses
so in the coming sections we'll take a
look at all of these different items
that you can see on this default VPC but
why wouldn't you just use the default
VPC
well the default VPC is great for
launching new instances when you're
testing AWS
but creating a custom VPC allows you to
make things more secure and you can
customize your virtual Network as you
can Define your own IP address range you
can create your own subnets that are
both private and public and you can
tighten down your security settings
by default instances that you launch
into a VPC can't communicate with your
own network
so you can connect your vpcs to your
existing data center using something
called Hardware VPN access so that you
can effectively extend your data center
into the cloud and create a hybrid
environment
now to do this you need a virtual
private Gateway and this is the VPN
concentrator on the Amazon side of the
VPN connection then on your side in your
data center you need a customer Gateway
which is either a physical device or a
software application that sits on your
side of the VPN connection
so when you create a VPN connection a
VPN tunnel comes up when traffic is
generated from your side of the
connection
VPC pairing is an important concept to
understand
appearing connection could be made
between your own bpcs or with a VPC in
another AWS account as long as it's in
the same region
so what that means is if you have
instances in vpca they wouldn't be able
to communicate with instances in vpcb or
C unless you set up appearing connection
pairing is a one-to-one relationship a
VPC can have multiple peering
connections to other vpcs but and this
is important transitive peering is not
supported in other words
vpca can connect to B and C in this
diagram but C wouldn't be able to
communicate with B unless they were
directly paired
also vpcs with overlapping cidrs cannot
be paired so in this diagram you can see
they all have different IP ranges which
is fine but if they have the same IP of
ranges they wouldn't be able to be
peered
and finally for this section if you
delete the default VPC you have to
contact AWS support to get it back again
so be careful with it and only delete it
if you have good reason to do so and
know what you're doing
this is a demonstration of how to create
a custom VPC
so here we are back at the Amazon web
services Management console and this
time we're going to go down to the
bottom left where the networking section
is I'm going to click on VPC
and the VPC dashboard will load up now
there's a couple of ways you can create
a custom VPC there's something called
the VPC wizard
which will build vpcs on your behalf
from a selection of different
configurations for example a VPC with a
single public subnet or a VPC with
public and private subnets now this is
great because you click a button type in
a few details and it does the work for
you however you're not going to learn
much or pass the exam if this is how you
do it so we'll cancel that and we'll go
to your vpcs and we'll click on create a
VPC
and we're presented with the Creator VPC
window
so let's give our VPC a name
I'm going to call that simply learn
underscore VPC
and this is the kind of naming
convention I'll be using throughout this
course
next we need to give it the cidr block
or the classes into domain routing block
so we're going to give it a very simple
one
10.0.0.0 and then we need to give it the
subnet mask so you're not allowed to go
larger than 15. so if I try to put 15 in
it says no not going to happen
for a reference subnet mask of 15 would
give you around 131 000 IP addresses
and
subnet 16 will give you 65
536 which is probably more than enough
for what we're going to do
next you get to choose the tenancy and
there's two options default and
dedicated if you select dedicated then
your ec2 instances will reside on
Hardware that's dedicated to you so your
performance is going to be great but
your cost is going to be significantly
higher so I'm going to stick with
default
and we just click on yes create
it'll take a couple of seconds
and then in our VPC dashboard we can see
our simply learned VPC has been created
now if we go down to the bottom here to
see the information about our new VPC we
can see it has a root table associated
with it
which is our default root table
so there it is and we can see that it's
only allowing local traffic at the
moment
we go back to the VPC again we can see
it's been given a default Network ACL
and we'll click on that and have a look
and you can see this is very similar to
what we looked at in the lesson
so it's allowing all traffic from all
sources inbound and outbound
now if we go to the subnet section
widen the VPC area here
you can see there's no subnets
associated with the VPC we just created
so that means we won't be able to launch
any instances into our VPC
and to prove it I'll just show you we'll
go to the ec2 section
so this is a glimpse into your future
this is what we'll be looking at in the
next lesson
and we'll just quickly try and launch an
instance we'll select any instance it
doesn't matter
any size not important so here the
network section if I try and select
simply learn VPC it's saying no subnets
found this is not going to work
so we basically need to create some
subnets in our VPC
and that is what we're going to look at
in the next lesson
now private IP addresses are IP
addresses that are not reachable over
the internet and they're used for
communication between instances in the
same network
when you launch a new instance it's
given a private IP address and an
internal DNS hostname that resolves to
the private IP address of the instance
but if you want to connect to this from
the Internet it's not going to work
so then you'd need a public IP address
which is reachable from the internet you
can use public IP addresses for
communication between your instances and
the internet
each instance that receives a public IP
address is also given an external DNS
hostname
public IP addresses are associated with
your instances from the Amazon Pool of
public IP addresses
when you stop or terminate your instance
the public IP address is released and a
new one is associated when the instance
starts
so if you want your instance to retain
this public IP address
you need to use something called an
elastic IP address
an elastic IP address is a static or
consistent public IP address that's
allocated to your account and can be
Associated to and from your instances as
required an elastic IP address remains
in your account until you choose to
release it there is a charge associated
with an elastic IP address if it's in
your account but not actually allocated
to an instance
this is a demonstration of how to create
an elastic IP address
so we're back at the Amazon web services
Management console we're going to head
back down to the networking VPC section
and we'll get to the VPC dashboard on
the left hand side we'll click on
elastic IPS
now you'll see a list of any elastic IPS
that you have associated in your account
and remember any of the elastic IP
address that you're using that isn't
allocated to something you'll be charged
for so I have one available and that is
allocated to an instance currently so we
want to allocate a new address
and it reminds you that there's a charge
if you're not using it I'm saying yes
allocate
and it takes a couple of seconds
and there's our new elastic IP address
now we'll be using this IP address to
associate with the NAT Gateway when we
build that
AWS defines a subnet as a range of IP
addresses in your VPC
you can launch AWS resources into a
subnet that you select you can use a
public subnet for resources that must be
connected to the internet and a private
subnet for resources that won't be
connected to the internet
the netmask for the default Subnet in
your VPC is always 20 which provides up
to 4096 addresses per subnet and a few
of them are reserved for AWS use
a VPC can span multiple availability
zones but the subnet is always mapped to
a single availability Zone this is
important to know so here's our basic
diagram which we're now going to start
adding to so we can see the virtual
private cloud and you can see the
availability zones and now inside each
availability Zone we've rated a subnet
now you won't be able to launch any
instances unless there are subnets in
your VPC
so it's good to spread them across
availability zones for redundancy and
failover purposes
two different types of subnet public and
private
you use a public subnet for resources
that must be connected to the internet
for example web servers
a public subnet is made public because
the main root table sends to subnets
traffic that is destined for the
internet to the internet gateway and
we'll touch on internet gateways next
private subnets are for resources that
don't need an internet connection or
that you want to protect from the
internet for example database instances
so in this demonstration we're going to
create some subnets a public and a
private subnet and we're going to put
them in our custom VPC in different
availability zones
so we'll head to networking and VPC
wait for the VPC dashboard to load up
we'll click on subnets
we'll go to create subnet
and I'm going to give the subnet a name
so it's good to give them meaningful
names
so I'm going to call this first one for
the public subnet
10.0.1.0 and I'm going to put this one
in
the US East
one
B availability Zone
and I'm going to call that simply learn
public
so it's quite a long name I understand
but at least it makes it clear for
what's going on in this example so we
need to choose a VPC so we obviously
want to put it in our simply learn VPC
and I said I wanted to put it in Us East
1B I'm using the North Virginia region
by the way
so we click on that then we need to give
it the cidr block
now as I mentioned earlier when I typed
in the name
that's the range I want to use and then
we need to give it the subnet mask and
we're going to go with
24 which should give us 251 addresses in
this range which obviously is going to
be more than enough if I try and put a
different value in that's unacceptable
to Amazon it's going to say
again it's going to give me an error and
tell me not to do that
let's go back to 24. and click I'm gonna
cut and paste this by the way just
because I need to type something very
similar for the next one
create
takes a few seconds
okay so there's our new subnet and I
just widen this
you can see so that's the IP range
that's the availability Zone it's for
simple learn and it's public
so now we want to create
the private
so we put the name in I'm going to give
the private the IP address block
of that
I'm going to put this one in Us East 1C
and it's going to be the private
stop now
obviously I want it to be in the same
VPC
Reddit is the zone of Us East 1C
and we're going to give it
10.0.2.0 Dash
24.
and we'll click yes create
and again it takes a few seconds
okay let me sort by name
so there we go we can see now we've got
our private subnet and our public Subnet
in fact let me just type in simply then
there we are so now you can see them
both there
and you can see they're both in the same
VPC
simply learn VPC
now if we go down to the bottom you can
see the root table associated with these
vpcs
and you can see that they can
communicate with each other internally
but there's no internet access
so that's what we need to do next in the
next lesson you're going to learn about
internet gateways and how we can make
these subnets have internet access
welcome to the networking section
in this section we're going to take a
look at internet gateways route tables
and NAC devices and we'll have a
demonstration on how to create each of
these AWS VPC items
so to allow your VPC the ability to
connect to the internet you need to
attach an internet gateway
and you can only attach one internet
gateway per VPC
so attaching the internet gateway is the
first stage in permitting internet
access to instances in your VPC now
here's our diagram again and now we've
added the internet gateway which is
providing the connection to the internet
to your VPC
but before you can configure internet
correctly there's a couple more steps
for an ec2 instance to be internet
connected you have to adhere to the
following rules
firstly you have to attach an internet
gateway to your VPC which we just
discussed
then you need to ensure that your
instances have public IP addresses or
elastic IP addresses so they're able to
connect to the internet
then you need to ensure that your
subnet's root table points to the
internet gateway and you need to ensure
that your network access control and
Security Group rules allow relevant
traffic to flow to and from your
instance so you need to allow the rules
to let in the traffic you want for
example HTTP traffic after the
demonstration for this section we're
going to look at how root tables Access
Control lists and security groups are
used
in this demonstration we're going to
create an internet gateway and attach it
to our custom VPC
so let's go to networking bpc
bring up the VPC dashboard
and on the left hand side we click on
internet gateways
so here's a couple of Internet gateways
I have already
but I need to create a new one so create
internet gateway
I'll give it a name which is going to be
simply learn
internet gateway igw and I'm going to
click create so this is an internet
gateway which will connect a VPC to the
internet because at the moment our
custom VPC has no internet access
so there it is created
you simply then igw
but this state is detached because it's
not attached to anything so let me try
and attach it to a VPC
and it gives me an option of all the
vpcs that have no
internet gateway attached to them
currently
so I only have one which is simpler than
VPC
yes attach
now you can see our VPC has internet
attached and you can see that down here
so let's click on that and it will take
us to our VPC
but before any instances in our VPC can
access the internet we need to ensure
that our subnet root table points to the
internet gateway
and we don't want to change the main
route table we want to create a custom
root table and that's what you're going
to learn about next
a root table determines where Network
traffic is directed
it does this by defining a set of rules
every subnet has to be associated with a
root table and a subnet can only be
associated with one root table
however multiple subnets can be
associated with the same root table
every VPC has a default route table and
it's good practice to leave this in its
original state and create a new root
table to customize the network traffic
routes associated with your VPC
so here's our example and we've added
two root tables the main root table and
the custom root table
the new routable or the custom root
table will tell the internet gateway to
direct internet traffic to the public
subnet
but the private subnet is still
Associated to the default route table
the main root table which does not allow
internet traffic to it
all traffic inside the private subnet is
just remaining local
in this demonstration we're going to
create a custom root table associated
with our internet gateway and Associate
our public subnet with it
so let's go to networking and VPC
the dashboard will load and we're going
to go to Route tables
now our VPC only has its main route
table at the moment the default one it
was given at the end time it was created
so we want to create a new root table
and we want to give it a name so we're
going to call it simply learn
I'm going to call it root table rtb for
sure
and then we get to pick which VPC we
want to put it in so obviously we want
to use simply learn VPC
so we click create
which will take a couple of seconds and
here you are here's our new root table
so what we need to do now is change its
route so that it points to the internet
gateway so if we go down here to root
at a minute you can see it's just like
our main root table it just has local
access
so we want to click on edit and we want
to add another root
so the destination is
the internet
which is all the zeros
and our Target and we click on this it
gives us the option of our internet
gateway which we want to do
so now we have internet access to this
subnet sorry to this root table
and we click on Save
save for successful so now we can see
that as well as local access we have
internet access
now at the moment if we click on subnet
associations
you do not have any subnet associations
so basically both are subnets the public
and private subnets are associated with
the main root table which doesn't have
internet access so we want to change
this we'll click on edit
and we want our public subnet to be
associated with this root table
click on Save
so it's just saving that
so now we can see that our public subnet
is associated with this route table
and this route table is associated with
the internet gateway so now anything we
launch into the public subnet will have
internet access
but what if we wanted our instances in
the private subnet to have internet
access
well there's a way of doing that with a
Nat device and that's what we're going
to look at in the next lecture
you can use a Nat device to enable
instances in a private subnet to connect
to the Internet or other AWS services
but prevent the internet from initiating
connections with the instances in the
private subnet
so we talked earlier about public and
private subnets to protect your assets
from be directly connected to the
internet
for example your web server would sit in
the public Subnet in your database in
the private subnet which has no internet
connectivity
however your private subnet database
instance might still need internet
access or the ability to connect to
other AWS resources
if so you can use a network address
translation device or a Nat device to do
this
and that device forwards traffic from
your private subnet to the internet or
other AWS services and then sends the
response back to the instances
when traffic goes to the internet The
Source IP address of your instance is
replaced with the NAT device address and
when the internet traffic comes back
again then that device translates the
address to your instance's private IP
address
so here's our diagram which is getting
ever more complicated and if you look in
the public subnet you can see we've now
added a Nat device and you have to put
Nat devices in the public subnet so that
they get internet connectivity
AWS provides two kinds of nat devices
and that Gateway and in that instance
AWS recommends in that Gateway as it's a
managed service that provides better
availability and bandwidth than that
instances
each Nat Gateway is created in a
specific availability Zone and is
implemented with redundancy in that zone
and that instance is launched from a Nat
Ami an Amazon machine image and runs as
an instance in your VPC so it's
something else you have to look after
whereas in that Gateway being a fully
managed service means once it's
installed you can pretty much forget
about it
and that Gateway must be launched into a
public subnet because it needs internet
connectivity it also needs an elastic IP
address which you can select at the time
of launch
once created you need to update the root
table associated with your private
subnet the point internet bound traffic
to the NAT Gateway this way the
instances in your private subnets can
communicate with the internet
so if you remember back to the diagram
when we had the custom root table which
was pointed to the internet gateway
now we're pointing our main root table
to the NAT Gateway so that the private
subnet also gets internet access but in
a more secure manner
welcome to the create and that Gateway
demonstration where we're going to
create a Nat Gateway so that the
instances in our private subnet can get
internet access
so we'll start by going to networking
and VPC
and the first thing we're going to do is
take a look at our subnets and you'll
see why shortly so here are our simply
learned subnets so this is the private
subnet that we want to give internet
access but if you remember from the
section
and that gateways need to be placed in
public subnets so I'm just going to copy
the name of this subnet ID
for the public subnet and you'll see why
in a moment
so then we go to Nat gateways on the
left hand side
and we want to create a new Nat Gateway
so we have to put a subnet in there so
we want to choose our public subnet as
you can see
it truncates a lot of the subnet names
on this option so it's a bit confusing
so we know that we want to put it in our
simply learned VPC
in the public subnet but you can see
it's truncated so it's actually this one
at the bottom but what I'm going to do
is just paste in
the subnet ID which I copied earlier
so there's no confusion
then we need to give it an elastic IP
address now if you remember from the
earlier demonstration we created one so
let's select that but if you hadn't
allocated one you could click on the
create new EIP button
so we'll do that
okay so it's telling me my Nat Gateway
has been created and in order to use you
on that Gateway ensure that you edit
your root table to include a route with
a target of and then on that Gateway ID
so it's giving us the option to click on
our edit root tables so we'll go
straight there now here's our here's our
root tables
now
here's the custom root table that we
created earlier and this is the default
the main route over which was created
when we launched our when we created our
VPC so we should probably give this a
name so that we know what it is so let
me just call this simply learn rtb
Main
so now we know that's our main root
table
so if you take a look at the main root
table
and the subnet associations
you can see that our private subnet is
associated with this table
so what we need to do is put a root in
here that points to the NAT Gateway so
if we click on roots and edit
and we want to add another route
and we want to say that all traffic
can
either go to the simply the internet
gateway which we don't want to do we
want to point it to our nap instance
which is this Nat ID here
and we click save
so now any instances launched in our
private subnet we'll be able to get
internet access via around that Gateway
foreign
groups and network ACL section
in this section we're going to take a
look at security groups and network ACLS
and we're going to have a demonstration
on how we create both of these items in
the Amazon web services console
a security group acts as a virtual
firewall that controls the traffic for
one or more instances
you add rules to each security group
that allow traffic to or from its
Associated instances
basically a security group controls the
inbound and outbound traffic for one or
more ec2 instances
security groups can be found on both the
ec2 and VPC dashboards in the AWS web
Management console we're going to cover
them here in this section and you'll see
them crop up again in the ec2 lesson
and here is our diagram and you can see
we've now added security groups to it
and you can see that ec2 instances are
sitting inside the security groups and
the security groups will control what
traffic Flows In and Out
so let's take a look at some examples
and we'll start with a security group
for a web server now obviously a web
server needs HTTP and https traffic as a
minimum to be able to access it
so here is an example of the security
group table and you can see we're
allowing HTTP and https the ports that
are associated with those two and the
sources and we're allowing it from the
internet we're basically allowing all
traffic to those ports
and that means any other traffic that
comes in on different ports would be
unable to reach the security group and
the instances inside it
let's take a look at an example for a
database server Security Group
now imagine you have a SQL Server
database
then you would need to open up the SQL
Server Port so that people can access it
and which is Port 1433 by default so
we've added that to the table and we've
allowed the source to come from the
internet now because it's a Windows
machine you might want RDP access so you
can log on and do some Administration
so we've also added RDP access to the
security group now you could leave it
open to the internet but that would mean
anyone could try and hack their way into
your box so in this example we've added
a source IP address of
10.00.0.0 so only IP arranges from that
address can RDP to the instance
now there's a few rules associated with
security groups by default security
groups allow all outbound traffic so if
you want to tighten that down you can do
so in a similar way to you can Define
the inbound traffic
Security Group rules are always
permissive you can't create rules that
deny access so you're allowing access
rather than denying it
security groups are stateful so if you
send a request from your instance the
response traffic for that request is
allowed to flow in regardless of the
inbound Security Group rules
you can modify the rules of a security
group at any time and the rules are
applied immediately
welcome to the create Security Group
demonstration where we're going to
create two security groups one the host
DB servers and one the host web servers
now if you remember from the best
practices section it said it was always
a good idea to tier your applications
into security groups and that's exactly
what we're going to do
so if we go to networking and VPC to
bring up the VPC dashboard
on the left hand side under security we
click on security groups now you can
also get to security groups from the ec2
dashboard as well
so here's a list of my existing security
groups but we want to create a new
security group and we're going to call
it
simply learn
web server
SG Security Group and we'll give the
group name as the same and our
description is going to be simply learn
web servers
security groups
okay and then we need to select our VPC
now it defaults to the default VPC but
obviously we want to put it in our
simply learned VPC so we click yes
create
takes a couple of seconds
and there it is there's our new Security
Group
now if we go down to the rules the
inbound rules
you can see there are none so by default
a new security group has no inbound
rules
but what about outbound rules
if you remember from the lesson
a new Security Group by default allows
all traffic to be outbound
and there you are all traffic
has destination of everywhere so all
traffic is allowed but we want to add
some rules so let's click on inbound
rules click on edit
now this is going to be a web server so
if we click on the drop down
we need to give it http
so you can either choose custom TCP Rule
and type in your own port ranges or you
can just use the ones they have for you
so http
this pre-populates the port range and
then here you can add the source
now if I click on it it's giving me the
option of saying allow
access from different security groups
so you could create a security group and
say I only accept traffic from a
different Security Group which is a nice
way of securing things down you could
also put in here just your IP address so
that only you could do HTTP requests to
the instance but because it's a web
server
we want people to be able to see our
website otherwise it's not going to be
much use so we're going to say all
traffic so all source
traffic can access for instance on Port
http 80.
I want to add another rule because we
also want to do https
cheers hiding from me
there we are and again we want to do
the same
and also because this is going to be a
Linux instance we want to be able to
connect to the Linux instance to do some
work and configuration so we need to
give it SSH access
and again it would be good practice to
tie it down to your specific IP or an IP
range but we're just going to do all for
now and then we click on Save
and there we go there we have our ranges
so now we want to create our security
group for our DB servers so let's click
create Security Group
and then we'll go through it and give it
a similar name
simply learn
DB servers s key
and the description it's going to be
simpler than DB servers Security Group
and our VPC is obviously going to be
simpler than VPC
just click yes create wait a few seconds
and here's our new security group as you
can see it has no inbound Rules by
default and outbound rules allow all
traffic
so this is going to be a SQL Server
database server and so we need to allow
SQL Server traffic into the instance
so we need to give
it Microsoft SQL Port access Now the
default port for Microsoft SQL Server is
1433
now in reality I'd probably change the
port the SQL server was running on to
make it more secure
but we'll go over this for now and then
the source so we could choose the IPA
ranges again but what we want to do is
place the DB server in the private
subnet and allow the traffic to come
from the web server
so the web server will accept traffic
and the web server will then go to the
database to get the information it needs
to display on its web on the website or
if people are entering information into
the website we want the information to
be stored in our DB server so basically
we want to say that this the DB servers
can only accept SQL Server traffic from
the web server Security Group
so we can select the simply then web
server Security Group as the source
traffic
but Microsoft SQL Server data
so we'll select that now our SQL Server
is obviously going to be a Windows
instance so from time to time we might
not we might need to log in and
configure it so we want to give RDP
access
now again you would probably put a
specific IP range in there we're just
going to do all traffic for now
then we click save
and there we are so now we have two
security groups
DB servers and web servers
a network ACL is a network access
control list
and it's an optional layer of security
for your VPC that acts as a firewall for
controlling traffic in and out of one or
more of your subnets
you might set up Network ACLS with rules
similar to your security groups in order
to add an additional layer of security
to your VPC
here is our Network diagram and we've
added Network ACLS to the mix now you
can see they sit somewhere between the
root tables and the subnets
this diagram makes it a little bit
clearer and you can see that a network
ACL sits in between a root table and a
subnet
and also you can see an example of the
default Network ACL
which is configured to allow all traffic
to flow in and out of the subnets to
which It's associated
each Network ACL includes a whose rule
number is an Asterix this rule ensures
that if a packet doesn't match any of
the other numbered rules it's denied you
can't modify or remove this rule
so if you take a look at this table you
can see on the inbound some traffic
would come in and it would look for the
first rule which is 100 and that's
saying I'm allowing all traffic from all
sources so that's fine the traffic comes
in if that rule 100 wasn't there it
would go to the Asterix Rule and the
Asterix rule is saying traffic from all
sources is denied
let's take a look at the network ACL
rules
each Subnet in your VPC must be
associated with an ACL if you don't
assign it to a custom ACL it will
automatically be Associated to your
default ACL
a subnet can only be associated with one
ACL however an ACL can be associated
with multiple subnets
an ACL contains a list of numbered rules
which are evaluated in order starting
with the lowest as soon as a rule
matches traffic it's applied regardless
of any higher numbered rules that may
contradict it
AWS recommends incrementing your rules
by a factor of 100 so there's plenty of
room to implement new rules at a later
date
unlike security groups ACLS are
stateless responses to allowed inbound
traffic are subject to the rules for
outbound traffic
welcome to the network ACL demonstration
where we're just going to have an
overview of ACLS where they are in the
dashboard
now you don't need to know a huge amount
about them for the exam you just need to
know how they work and where they are so
let's go to networking and VPC
and on when the dashboard loads on the
left hand side under security there's
Network ACLS so let's click on that now
you can see some ACLS that are in my my
AWS account so we want the one that's
associated with our simply learned VPC
so if we extend this VPC
column
that's our Network ACLS simply learn VPC
now let's give it a name because it's
not very clear to see otherwise also I'm
kind of an obsessive tagger so let's
call it simply learn ACL and click on
the tick so there we go so now it's much
easier to see
so we click on inbound rules so this is
exactly what we showed you in the lesson
the rule is 100 so that's the first rule
that's going to get evaluated and it's
saying allow all traffic from all
sources
and the outbound rules are the same
so if you wanted to tighten down the new
rule you could click edit we would give
it a new rule number say which would be
200 so you should always increment them
in 100 so that means if you had 99 more
rules you needed to put in place you'd
have space to put them in in between
these two
and then you could do whatever you
wanted you could say you know we are
allowing HTTP access from
all traffic
and we're allowing or you could say
actually you know what we're going to
deny it so this is the way of
blacklisting traffic into your VPC
now I'm not going to save that because
we don't need it
but this is where Network ACL sit and
this is where you would make any changes
it's also worth having a look at the
subnet associations with your ACL
so we have two subnets in our simply
learn VPC so we would expect to see both
of them associated with this network ACL
because it's the default
and there they are it's both our public
and our private subnets are associated
and you can also see up here on the on
the dashboard it says default so this is
telling us this is our default ACL
if you did want to create a new network
ACL you would click create network ACL
you'd give it a name it's just a new ACL
and then you would associate it with
your VPC so we would say simply learn
DPC
takes a few seconds
and there we are there we have our new
one now you can see this one says
default no because it obviously isn't
the default ACL for our simply learn VPC
and it has no subnets associated with it
so let's just delete that because we
don't need it but there you are there's
a very brief overview of network ACLs
welcome to the Amazon VPC best practices
and costs where we're going to take a
look at the best practices and the costs
associated with the Amazon virtual
private cloud
always use public and private subnets
you should use private subnets to secure
resources that don't need to be
available to the internet such as
database services
to provide secure internet access to the
instances that reside in your private
subnets you should provide a Nat device
when using Nat devices you should use a
Nat Gateway over Nat instances because
there are managed service and require
less Administration effort
you should choose your cidr blocks
carefully
Amazon VPC can contain from 16 to 65
536 IP addresses so you should choose
your cidr block according to how many
instances you think you'll need
you should also create separate Amazon
vpcs for development staging test and
production or create one Amazon VPC with
separate subnets with a subnet each for
production development staging and test
you should understand the Amazon VPC
limits there are various limitations on
the VPC components for example you're
allowed five vpcs per region
200 subnets per VPC
200 route tables per VPC
500 security groups per VPC
50 in and outbound rules per VPC
however some of these rules can be
increased by raising a ticket with AWS
support
you should use security groups and
network ACLS to secure the traffic
coming in and out of your VPC Amazon
advises to use security groups for white
listing traffic and network ACLS for
blacklisting traffic
citizen recommends tiering your security
groups you should create different
security groups for different tiers of
your infrastructure architecture inside
VPC
if you have web tears and dbters you
should create different security groups
for each of them creating toy security
groups will increase the infrastructure
security inside the Amazon VPC
so if you launch all your web servers in
the web server security group that means
it'll automatically all have HTTP and
https open conversely the database
Security Group will have SQL Server
ports already open
you should also standardize your
Security Group naming conventions
following a security group naming
convention allows Amazon VPC operation
and management for large-scale
deployments to become much easier
always span your Amazon VPC across
multiple subnets in multiple
availability zones inside a region this
helps in architecting high availability
inside your VPC
if you choose to create a hardware VPN
connection to your VPC using virtual
private Gateway you are charged for each
VPN connection hour that your VPN
connection is provisioned and available
each partial VPN connection hour
consumed is billed as a full hour
you'll also incur standard AWS data
transfer charges for all data
transferred via the VPN connection
if you choose to create a Nat Gateway in
your VPC you are charged for each Nat
Gateway hour that your net Gateway is
provisioned and available data
processing charges apply for each
gigabyte processed through the net
Gateway each partial Nat Gateway hour
consumed is billed as a full hour
this is the practice assignment for
designing a custom VPC where your create
a custom VPC using the concepts learned
in this lesson
using the concepts learned in this
lesson recreate the custom VPC as shown
in the demonstrations
the VPC name should be simply learned
VPC the cidr block should be
10.0.0.016 there should be two subnets
one public with a range of
10.0.1.0 and one private with a range of
10.0.2.0 and they should be placed in
separate availability zones
there should be one internet gateway and
one that Gateway and also one custom
root table for the public subnet
also create two security groups simply
learn web server Security Group and
simply learn DB server Security Group
so let's review the key takeaways from
this lesson
Amazon virtual private cloud or VPC
enables you to launch AWS resources into
a virtual Network that you've defined
this virtual Network closely resembles a
traditional Network that you'd operate
in your own data center but with the
benefits of using scalable
infrastructure of AWS
there are three types of IP address in
AWS
private IP address this is an IP address
that's not reachable over the internet
and it's used for communication between
instances in the same network
a public IP address is reachable from
the internet which you can use for
communication between your instances and
the internet
and there's an elastic IP address this
is a static public persistent IP address
that persists after an instance restarts
whereas a public IP address is
associated after each restart
Amazon defines a sudden it as a range of
IP addresses in your VPC you can launch
AWS resources into a subnet that you
select and a subnet is always mapped to
a single availability Zone use a public
subnet for resources that must be
connected to the internet and a private
subnet for resources that won't be
connected to the internet
to allow your VPC the ability to connect
to the internet you need to attach an
internet gateway to it and you can only
attach one internet gateway per VPC
a root table determines where Network
traffic is directed it does this by
defining a set of rules
every subnet has to be associated with a
root table and a sudden that can only
have an association with one root table
however multiple subnets can be
Associated to the same root table
and you can use a Nat device to enable
instances in a private subnet to connect
to the Internet or other AWS services
but and that device will prevent the
internet from initiating connections
with instances inside your private
subnet
a security group acts as a virtual
firewall that controls the traffic for
one or more instances you add rules to
each security group that allow traffic
to or from its Associated instances
a network access control list or network
ACL is an optional layer of security for
your VPC that acts as a firewall for
controlling traffic in and out of one or
more of your subnets
today's session is on AWS stage maker
let's look into what we have in our
today's session
so what's in it for you we would be
covering what is AWS
why do we need AWS sagemaker what is AWS
sagemaker services
what are the benefits of using the AWS
sagemaker
machine learning with AWS sagemaker how
to train a model with AWS stagemaker how
to validate a model with AWS
and the companies that are using AWS
stagemaker along with that we will be
covering up one live demo on the AWS
platform
now let's understand what is AWS so what
is AWS it's an Amazon web services it's
a largest or most widely used public
Cloud platform
offered by Amazon it provides services
over the internet
AWS Services can be used to build
Monitor and deploy any type of
application in the cloud
AWS also uses the subscription pricing
model that means you only pay for
whatever the services you use for
now why do we need AWS stage maker let's
look into it so let's consider an
example of one of the company that is
proquest now before AWS stage maker the
proquest is a Global Information content
and technology company
that provides valuable content such as
ebooks newspapers Etc to the users
before AWS sagemaker the proquest
requirement was to have a better user
experience maximum relevant search
results now after AWS stage maker they
were able to achieve those results so
they achieved more appealing video user
experience they achieved more relevant
search results for the users now what do
we mean by AWS sagemaker why this
service is primarily used so Amazon
sagemaker is a cloud machine learning
platform that helps users in building
training tuning and deploying machine
learning models in a production ready
hosted environment
so it's kind of a machine learning
service which is already hosted on the
AWS platform now what are the benefits
of using AWS sagemaker uh the key
benefits of using aw sagemaker are it
reduces machine learning data cost so
you can do the cost optimization
while running this particular service on
the AWS all ml components are stored in
a particular place in a dashboard so
they can be managed together
highly scalable so it can be scalable on
you can scale this particular service on
the Fly
it trains the models quite faster
maintains the uptime so you can be
assured that your workloads will be
running all the time it will be
available all the time
High data security so security becomes a
major concern on the cloud platforms and
it ensures that you have the high data
security along with that you can do a
data transfer to different AWS services
like S3 bucket and all with this simple
data transfer techniques
now machine learning with AWS stage
maker let's look into it
so machine learning with AWS sagemaker
is a three-step function
so one is to build second is to test and
tune the model and third is to deploy
the model now with the build it provides
more than 15 widely used ml algorithms
for training purpose now to build a
model you can collect and prepare
training data or you can select from the
Amazon S3 bucket also choose and
optimize the required algorithm so some
of the algorithms that you can select
are k-means linear regressions logistic
regression sagemaker helps developers to
customize ml instances with the Jupiter
notebook interface in the test and tune
you have to set up and manage the
environment for training so you would
need some sample data to train the model
so train and tune a model
with the Amazon sagemaker sagemaker
implements hyper parameter tuning by
adding a suitable combination of
algorithm parameters also it divides the
training data and stores that in the
Amazon S3 S3 is a simple storage service
which is primarily used for storing the
objects and the data hence it is used
for storing and recovering data over the
internet and Below you can see that AWS
stage maker uses Amazon S3 to store data
as it's safe and secure also it divides
the training data and stores in Amazon
S3 where the training algorithm code is
stored in the ECR ECR stands for elastic
container registry which is primarily
used for containers and Dockers ECR
helps users to save Monitor and deploy
Docker and the containers later
sagemaker sets up a cluster for the
input data train set and stores it in
the Amazon S3 itself so this is done by
the sagemaker itself after that you need
to deploy it so suppose you want to
predict limited data at a time you use
Amazon sagemaker hosting services for
that okay but if you want to get
prediction for an entire data set prefer
using Amazon sagemaker batch transform
now the last step that is to deploy the
model
so once tuning is done models can be
deployed to sagemaker endpoints
and in the endpoint real-time prediction
is performed so you would have some data
which you would reserve and
validate your model whether it is
working correctly or not now evaluate
your model and determine whether you
have achieved your business goals now
the other aspect is how we can train a
model with AWS stagemaker
so this is basically a flow diagram
which shows you how to train a model
with the AWS stage maker and here we
have used couple of Services of an AWS
to get that done
so model training in AWS admaker is done
on machine learning compute instances
and here we can see there are two
machine learning compute instances used
as helper code and the training code
along with that we are using two S3
buckets and the ECR for the container
registry
now let's look into what are the ways to
train the model as per the slides
So Below are the following requirements
to train a model so here in the diagram
you can see these are the following
requirements to train a model
the URL of an Amazon S3 bucket where the
training data is stored that is
mentioned
the computer sources on machine learning
compute instances so these are all your
machine learning compute instances
then the URL of an Amazon S3 bucket
where the output will be stored
and the path of AWS elastic container
registry where the code data is safe the
inference code image lies in the elastic
container registry now what are these
calls these are called as the training
jobs
now when a user trains a model in Amazon
sagemaker he she creates a training job
so we need to First create a training
job and then the input data is fetched
from the specified Amazon S3 bucket once
the training job is built Amazon
sagemaker launches the ml compute
instances so these compute instances
will be launched once the training job
is built then it trains the model with
the training code and the data set and
it shows the output and model articrafts
in the AWS S3 bucket so this is done
automatically now here the helper code
performs a task when the training code
fails the interference code which is in
the elastic container registry consists
of multiple linear sequence containers
that process the request for inference
on data the ec2 container registry is a
container registry that helps users to
say Monitor and deploy container images
whereas container images are the ready
applications
once the data is trained the output is
stored in the specified Amazon S3 bucket
so here you can see the output will be
stored here to prevent your algorithm
being deleted save the data in Amazon's
sagemaker critical system
which can process you on your ml compute
instances now how to validate a model
let's look into it so you can evaluate
your model
using offline or using the historical
data so first thing is that you can do
the offline testing to validate a model
you can do an online testing with the
live data so if you have a live data
coming or real-time streams coming you
can validate a model from there itself
you can validate using a holdout set and
also you can validate using the k-fold
validation now use historical data to
send requests to the model through the
jupyter notebook in Amazon sagemaker for
the evaluation online testing with live
data deploys multiple models into the
endpoints of Amazon sagemaker and
directs live traffic to the model for
validation validating using a holdout
set is part of the data is set aside
where which is called hold outset so the
part of the data is left
which is basically called as the holdout
set
this data is not used for the model
training so later when the model is
trained
with the remaining input data and
generalize the data based on what is
learned initially so whatever the data
which is left out will be used for
validating a model because we have not
used the data while training a model the
k-fold validation is the input data is
split into two parts one part is called
K
which is the validation data for testing
the model and the other part is K minus
1. which is used as a training data
now based on the input data the machine
learning model evaluates the final
output now
the companies that are using AWS stage
maker one is the ADP so you must be
knowing about ADB zalando Dow Jones
which is the stock market proquest
and the Intuit now let's look into the
demo that how we can actually run the
AWS stage maker so we'll use the r
algorithm and then package the algorithm
as a container for building training and
deploying a model
we are going to use the Jupiter notebook
for that for model building for model
training for model deployment and the
code for the demo is in the below link
so you can see here that from this link
you can get the code for the demo let's
try to
do a demo on the AWS
now I would be using a link which is
provided by Amazon to build train and
deploy the machine learning model on the
sagemaker as you can see on my screen
and in this tutorial you would have some
steps where you can put those steps and
the code python codes into your AWS
stagemaker Jupiter lab
so in this tutorial you will learn how
to use Amazon sagemaker to build train
and deploy a machine learning model and
for that we will use the popular XT
boost ml algorithm for this exercise
so first of all what you need to do is
you have to go to the AWS console and
there you have to create a notebook
instance so in this tutorial you will be
creating a notebook instance you will
prepare the data train the model to
learn from the data deploy the model
evaluate your ml models performance and
once all those activities are done then
we'll see how we can actually remove all
the resources in order to prevent the
extra costing
now the first step is we have to enter
to the Amazon sagemaker console so here
you can see I'm already logged in into
the sagemaker console you can click on
the services
search for the sagemaker here
and here you get the Amazon sagemaker
service
now the next step is
that we have to create a notebook
instance so we will select the notebook
instance from the sagemaker service and
then after the notebook instance is
selected we will put a name to our
instance and we'll create a new IM role
for that
so let's wait for the sagemaker studio
to open
so here you can see the studio is open
and you just have to click on the
notebook instances
and here you have to create a notebook
instance so here you can see couple of
notebook instances have already been
created one of them is in service so
this is The Notebook instance that we
are going to use for creating the demo
model
I'll show you how you can create a
notebook instance you just have to click
on create notebook instance button and
put your notebook instance name so you
can put something like demo Dash
sagemaker 987 or we can put it as model
we'll go with notebook instance type as
default which is mlt2.media
and in the permission and encryptions
under the IM role we'll click on create
a new IM role
now why we are creating a new IM row so
that we can allow the sagemaker to
access any S3 bucket that has been
created on our account
just click on create a role and here you
would see that the new IM role will be
created with the set of permissions
then rest of the things will keep it as
default and then you just have to click
on create a notebook instance
The Notebook instance creation takes
some time so you just have to wait for a
couple of minutes to get that in service
we already have one of the notebook
instance that has been created so we
will be using that to create a demo
now going back to the steps so these are
the steps that we have already performed
now once the notebook instance is
created then we have to prepare the data
so in this step we will be using the
sagemaker notebook instance to
pre-process the data that would require
to train the machine learning model
and for that we would be opening up the
Jupiter notebook and then we have to
select an environment a kernel
environment in the Jupiter notebook that
would be conda underscore python 3. so
let's follow these steps go back to the
sage maker click on the notebook
instances
select the running notebook instance and
here you would select the open Jupiter
lab
now here you would see that the
sagemaker would try to open up the
Jupiter Notepad
and we would be performing all our
inputs into that jupyter notebook
and executing the results there itself
so just wait for the notebook to open
now here you can see the Jupiter lab
notebook has been open so I would be
selecting one of the notebook that has
been created
so this one so likewise you can create
your own notebook also how you can do
that
first of all let me select the kernel
environment so I would be selecting
Corner underscore Python 3 and just
click on select so how you can create
your own notebook just have to click on
file click on new and here you can
select the notebook just name your
notebook select the environment conda
underscore Python 3 to run this demo
so I have my notebook open so in the
tabs I would be putting up the python
codes and I would be executing those
codes to get the output directly
so the next step is
to prepare the data train the ml model
and deploy it we will need to import
some libraries
and Define a few environment variables
in the jupyter notebook environment so I
would be copying this code
which you can see that would try to
import numpy pandas these are all
required to run the python
syntax so just
copy this code
and paste it into your
notebook
right so once you do that execute your
code
and here you can see that you get the
output which is
that it has imported all the necessary
libraries that have been defined in the
code
now the next step is we would create an
S3 bucket
into the S3 service and for that you
have to copy this python code just that
you have to edit it so you have to
specify this bucket name that you want
to get created
so here I would
provide the bucket name which should be
unique
should not overlap
so something like Sage maker
Dash
demo
is the name that I have selected for the
bucket and now you have to execute that
code
it says that the S3 bucket has been
created successfully with the name
sagemaker Dash demo 9876 so this is
something which you can verify so you
can go to the S3 service and there you
can verify whether the bucket has been
created or not now the next task is that
we need to download the data
to the AWS sagemaker instance and load
it into the data frame and for that we
have to follow this URL so from this URL
which is buildtrain deploy machine
learning model would have a data in the
form of bank underscore clean dot CSV
and this will be deployed onto our
sagemaker instance we'll copy this code
and paste it here
and execute the code
so change that it has successfully
downloaded Bank underscore clean.csv
which is which has the data inside it
and that has to be loaded into the
sagemaker data frame successfully
now we have a data to build an train our
machine learning model
so what we are going to do we are going
to shuffle the data and we are going to
split it one into the training data set
and the other one into test data set
so for the training data set we are
going to use 70 of the customers that
are listed in the CSV file and 30 of the
customers in the CSV file data will be
using it as a test data to train the
model
so we'll copy the following code into a
new code cell and then we are going to
run that code cell
so I'll just copy it
for training the data so that we can
segregate the data model 70 for building
the model and 30 for testing the data so
click on
run the execution and here you can see
that we got the output successfully
foreign
we have to train the model from the data
so how we are going to train that model
and for that we will use sagemaker
rebuild XG boost model which is an
algorithm
so you will need to reformat the header
and First Column of the training data
and load the data from the S3 bucket so
what I'll do is I'll copy
this syntax
and paste it
in the node cell
so it has the train data it would train
the model
click on run execution
now it is changing the S3 input class
which will be renamed to training input
because now we are training the model
with the training data
so we just have to wait for some time
till it gets executed completely
now the next thing is that we need to
set up the amazon sagemaker session to
create an instance of the XG boost model
so here we are going to create this
sagemaker session say we are going to
create an instance of the XT boost model
which is an estimator
so just copy that copy that code
and paste it here
executed
and here you can see that it will start
it has basically changed the parameter
image name to the image underscore URI
in this sagemaker python SDK V2
now we'll follow the next step that is
with the data loaded in the XT boost
estimator we'll set up train the model
using gradient optimization
and we'll copy the following code and
that would actually start the training
of the model
so copy this code
and this would actually
start training the model using our input
data that we have reserved 70 of the
data that we have reserved for training
the model so just copy that
again initiate the execution and it will
start the training job
now we'll deploy the model and for that
I would copy the deploy code put that in
the cell
and execute it
so
it says parameter image will be renamed
to image URI and using already existing
model so XD boost was deployed already
uh if you have not done that if you're
doing it the first time so it will
initiate another XT boost instant so
where you can find your XT boosts
endpoints created you just have to
scroll down and here under the inference
click on the endpoints and you should
find the XT boost endpoints defined here
so here you can see that today I have
created one XD boost
endpoint and that is now in process of
creating so just refresh it
so it is still created it is going to
take some time to get that in service
now our endpoint is in service state so
now we can use it so going forward with
the next steps we will try to predict
whether the customer in the test data
enroll for the bank product or not for
that we are going to copy this code put
that in the Jupiter cell function
and execute it
so here it gives you the output that it
has actually evaluated and the same
output we got in the screenshot of the
demo as well
now we are going to evaluate the model
performance so what we are going to do
we are going to get the prediction done
so based on the prediction we can
conclude that you predicted a customer
that will enroll for a certificate of
deposit accurately for 90 of the
customers in the test data with the
Precision of 65 percent for enrolled and
90 which are which haven't enrolled for
it so for that we are going to copy this
code
and execute it here in the cell
so if it is predicted correctly that
means our model is working absolutely
fine so here you can say the overall
classification rate is 89.5 percent and
there is the accurate
prediction that has been made by the
model and that's what the output we can
see here
in the screenshot of a model so that
means our model is absolutely working
fine it has been built deployed and
trained correctly now the next thing is
that once you are done with that you
terminate your resources and for that
you just have to copy
this code and put that in the cell
function so that the additional
resources and the endpoints and the
buckets that have been created by the
Jupiter notepad should be uh terminated
so that you would not be incurred with
the extra costing so just execute it and
here you would see that it is tried to
it would try to terminate all the
additional resources that we have
created from the Jupiter notebook
today's tutorial is on AWS cloudfront
let's look into what we have today in
the cloud front
so what's in it for you we would be
covering up the concept of what is AWS
what was earlier before AWS Cloud front
after AWS cloudfront what for the
services that were introduced how it
manifested what do we mean by AWS
cloudfront benefits of the using AWS
cloudfront and how AWS cloudfront
actually is known as a Content delivery
service the name of the companies that
are using AWS cloudfront and we would be
covering up on live demo
now AWS is the Amazon web services it's
a cloud service provider that basically
offers a multiple Services variety of
services such as compute power database
storage networking and other resources
so that you can create your Solutions on
the cloud and help the business grow now
with AWS you only pay for whatever the
services you use so for example if you
are using a service for a couple of
hours then you pay for only that many
hours that you have use that service
before AWS cloudfront so there is an
example that we are going to talk and
that is you must be aware of an
application called Spotify so when you
used to access Spotify and when you
click on it it kept on loading and at
the end you used to get the error and
the error was that the connection failed
and why you received that error because
of a latency issue probably a network
error right so how you can solve these
kind of a latency issues and that is
also going to these kind of an issues
are also going to impact the performance
of an application
so with the introduction of AWS cloud
from this problem of loading the
application got result so after AWS
cloudfront with the help of AWS
cloudfront Spotify gives the facility of
updating new features access to million
songs that you can access instantly
so with the use of AWS cloudfront or the
latency issues were solved and
successfully you can basically access
your application
what we mean by AWS cloudfront so AWS
cloudfront is a globally distributed
Network that is offered by AWS Amazon
web services which securely delivers
content to the end users across any
geography with a higher transfer speed
and an improve or a low latency now what
are the benefits of AWS cloudfront there
are multiple benefits one is the cost
effective so it helps you to do the cost
optimization when you use the cloud
front it is time saving so it is
implemented easily and also lot of
issues with respect to accessing the
application with respect to latency and
all can be resolved
content privacy so the content is placed
to the end users and also to the
cloudfront servers in a secured manner
in a secured way it is highly
programmable and you can make the
changes amend the changes on the Fly
and you can Target any location any
geography across the globe along with
that it helps you to get the content
delivered quickly now how AWS cloudfront
delivers the content let's look into the
architecture
so this is a flow and the flow is with
respect to how the user is going to get
a content from the cloud front now the
client first access a website by typing
a URL on the browser and in the step one
itax tries to access the application
then the client requests when the
website is open the client request for
an object to download such as for
example a particular file now at that
time the DNS routes user request to
download that file to AWS cloudfront
the AWS Cloud front connects to the
nearest Edge locations Edge locations
are basically the servers
where it caches
the files documents and the web codes
AWS cloudfront connects to its nearest
Edge location in order to serve the user
the request
at Edge location
AWS cloudfront looks for its requested
cache file once the file is found let's
say if the file is available in the
cache of an edge location AWS cloudfront
then sends the file to the user
otherwise if the file is not found in
the cache memory of an edge location
AWS cloudfront compares the requirement
with the specification and share it with
a respected server that means a web
server or a server where the file is
actually available the server the web
server responds to the edge location by
sending the file back to the cloud front
edge location
and then as soon as the AWS cloudfront
receives the file it shares with the
client also adds the file to the cache
of an edge location for a future
reference this is how the flow of a
cloud front is now
the name of the companies that are using
the AWS Cloud front
so one of them is jio 7 app it is which
is a very popular app so it uses Amazon
cloudfront to deliver 15 petabytes of
audio and video to its subscribers
globally which is a huge data Sky News
it uses the service in order to unify
the content for faster distribution to
scrub subscribers
of the discovery Communications uh also
uses the cloud front it uses the service
for delivering API static asset and also
the dynamic content
and then the TV1 EU streaming Europe is
basically also uses uh the cloudfront
service that helps in improving latency
and performance
that results in fastest delivery of
content
now let's look into the demo how to use
cloudfront to serve private S3 bucket as
a website
now I'm going to
run a cloudfront distribution demo on
the AWS console and we'll basically try
to deliver the content from a private S3
bucket and then map that with the domain
name using the Route 53 service
so what we need for this demo we would
need a domain URL we would need a Route
53 service
we would need
cloudfront we have to create a
cloudfront distribution and that will be
linked with our S3 bucket the private S3
bucket right and in the S3 bucket we
would have one HTML file the index.html
file
so let's uh move into AWS console
so right now I have opened up the
cloudfront distribution
and here you can see uh that couple of
distributions have already been created
so what I'm going to do I'm going to
create a distribution
now there are two types of delivery
method for your content one is the web
distribution and the other one is rtmp
rtmp stands for Real Time audio or video
distribution it's basically used for
distribution of a media content or a
media file which are available in the S3
bucket
here we are going to select a web
distribution because primarily we will
be using files which uses protocols HTTP
or the https
so you have to click on get started
and in the origin domain name you have
to specify the bucket where your code is
available
so I have a bucket already created here
you can see and what you need to do is
you have to create a bucket with a URL
name or the domain name which you would
be mapping with the Route 53 service
so this is a bucket that has already
been created let me show you in a new
tab
so here you have to go to the storage
under the storage you have to select the
S3 bucket
let's open the link in the new tab and
let's look into
how we can create the S3 buckets
now here are a couple of buckets already
created I have created one bucket with a
domain name that I'm going to use and
map it with the root 53 service
so that is basically mapped to a region
which is in Ohio
and if you open up this bucket
here you will find an HTML web page the
index.html has been already added
right so similarly you have to create a
bucket with a domain and an index.html
page needs to be uploaded there
now again we'll go to the cloud front we
will try to create a distribution
just have to click on create
distribution
select a web delivery method
select An Origin domain which is
sunshinelearning.in
and origin paths you don't have to
specify origin ID is this one
so basically when you define An Origin
domain name automatically the origin ID
appears
you can customize this origin ID as per
your requirement also
so rest of the things primarily we keep
it as a default settings only until and
unless if we require some customized
settings to be done
so let's say if you have to change the
cache Behavior settings you can do that
otherwise we'll keep it as default
now in the distribution setting you can
either select use all Edge locations for
the best performance so what does AWS
basically do here it uses all the edge
locations which are associated with AWS
across the globe
otherwise you can specify based on the
regions also right
apart from that if you want to enable
firewalls or the access control list you
can specify here
and then
what you need to do is in the default
root object here you have to specify
your index.html
page which is in the S3 bucket
the distribution state has to be enabled
and if you want to use IPv6 as well you
need to enable it
click on create distribution
now you can see here a distribution has
been created it's in progress and it is
enabled
and it takes around 15 to 20 minutes to
get that distribution completed the
reason is that the web codes the web
pages will be
distributed across all the edge
locations across the globe so hence it
takes time to get that done right anyway
let's move on to Route 53 service
and let's create the hosted zones
so we'll type root 53 here scalable DNS
and domain name registration
and what we are going to do here is
we are going to map our URL the domains
pointed to
the name servers that will be provided
by the root 53 so we have to create a
hosted Zone
let's wait for it
so now the root 53 dashboard is open and
you can see one hosted zone is already
created so just click on the hosted
zones
and in order to point the traffic from
the external domains towards the AWS you
have to first point the domain's traffic
to the
hosted Zone in the route 53.
so
I'll click on create hosted Zone but
before that I will first delete the
existing one
and then I'll create another record
Another hosted Zone
right
put your domain name which I put as
sunshinelearning.in
and it is acting as a public hosted Zone
rest of the things will be default click
on create hosted Zone
now it gives you phone name servers and
these fourname servers has to be updated
in the domain so you have to update
these name servers in a platform from
where you have purchased the domain
right so this is half of the work done
then what you need to do is you have to
go and create records
now in the records you can select a
routing policy so right now what we are
targeting we are targeting basically
that the traffic from the domain should
be pointed directly towards the cloud
front distribution hence we are going
with a simple routing right now
click on next
here you have to specify the record sets
so we are going to create the records
uh just click on the file simple record
put here as World Wide Web
and you have to select an endpoint so
endpoint we are selecting for the cloud
front distribution so we have to specify
Alias for the cloudfront distribution
now here we are going to put the
cloudfront distribution URL and then we
are going to define the simple report
set
so what we need is we need a cloudfront
distribution URL which you can find it
uh from the cloudfront service itself
and you can find the domain name here
itself
you just have to copy that
and then
paste it here in the distribution
and then just click on Define simple
records again click on create records
and here you can see
the record set has been updated now this
domain is basically pointed towards
these name servers which are further
pointed towards the cloudfront
distribution
right now the only thing which is left
is that within the domain from wherever
you have purchased the domain you should
update these phonem servers and then you
can see the live traffic coming on this
domain will have an output from the
cloudfront distribution
today's topic is on AWS Auto scaling so
this is Akil I would be taking up a
tutorial on the auto scaling let's begin
with our tutorial and let's look into
what we have in today's session
so I would be covering up why we require
AWS Auto scaling what is AWS Auto
scaling
what are the benefits of using the
scaling service
how this Auto scaling works the
different scaling plans we have
uh what is the difference between the
snapshot and the Ami
what is the load balancer and how many
types of load balancers we have and
along with that I would be covering up a
real life demo on the AWS
let's begin with why we require AWS
order scaling
now before AWS Cloud scaling there was a
question in the mind of Enterprises that
they were spending a lot of money on the
purchase of the infrastructure if they
have to set up some kind of a solution
so they have to purchase an
infrastructure and a one-time cost was
required so that was a burden for them
in terms of procuring a server Hardware
software and then having a team of
experts to manage all those
infrastructure
so they used to think that no longer
they require these resources if there
was a cost efficient solution for their
project that was the project manager
used to think
now after the AWS Cloud scaling that was
introduced automatically the auto
scaling maintains the application
Performance Based on the user
requirements at the lowest possible
price
so what does the auto scaling does is
that whenever there is a scalability
require it manages it automatically
and hence the cost optimization became
possible
now what is AWS Auto scaling let's look
into deep so AWS Auto scaling is a
service that helps users to monitor
their applications and the servers and
automatically adjust the capacity of
their infrastructure to maintain the
steadiness so they can increase the
capacity they can even decrease the
capacity also for the cost optimization
and also predictable performance at the
lowest possible cost
now what are the benefits of Auto
scaling it gave the better fault
tolerance applications you can get the
servers created and you can have a clone
copy of the servers so that you don't
have to deploy the applications again
and again better cost management because
the scalability is decided by the AWS
automatically based on some threshold
parameters
it was a reliable service and whenever
the scaling is created or initiated you
can get the notifications onto your mail
IDs or to your cell phones
scalability as I mentioned is always
there in the auto scaling it can scale
up it can scale down as well
and it has the flexibility the
flexibility in terms of whenever you
want to schedule it if you want to stop
it if you want to keep the size of the
servers at a fixed number you can always
make the changes on the Fly
and the better availability
now
with the use of the auto scaling we come
around with the terminology called
snapshot and Ami let's look into the
difference between the snapshots and the
Ami
uh snapshots versus Ami so in a company
there was one of the employee that was
facing an issue with launching the
virtual machines
so he asked his colleague a question is
it possible to launch multiple virtual
machines with a minimum amount of time
because it takes a lot of time in terms
of creating the virtual machines
the other colleague said that yes it is
possible to launch multiple ec2 instance
and that can be done at a lesser time
and with the same configuration and this
can be done either you use a snapshot or
the Ami on the AWS
then the colleague said that what are
the differences between the snapshot and
Amir
uh let's look into the difference now
the snapshots basically kind of a backup
of a single EDS volume which is just
like a virtual hard drive
that is attached to the ec2 instance
whereas the Ami it is basically used as
a backup of an ec2 instance only
the snapshots opts for this when the
instance contain multiple static EVS
when you opt for the snapshot whenever
the instance contains multiple Statics
EVS volumes
Ami this is widely used to replace the
failed ec2 instance
in the snapshots here you pay only for
the storage of the modified data
whereas with the Ami you pay only for
the storage that you use
the snapshots are non-bootable images on
ABS volume
whereas Ami are bootable images on the
ec2 instance
however creating an Emi image will also
create the EVS snapshots
now how does AWS Auto scaling work let's
look into it
so for the AWS Auto scaling to work you
have to configure single unified scaling
policy for application resource
and this scaling policy with that you
can explore the applications also
and then select the service you want to
scale also for the optimization select
do you want to optimize the cost or do
you want to optimize the performance
and then keep track of scaling by
monitoring or getting the notifications
now what are the different scaling plans
we have so in the auto scaling a scaling
plan basically helps a user to configure
a set of instructions for scaling based
on the particular software requirement
the scaling strategy basically guides
the service of AWS Auto screening on how
to optimize resources in a particular
application so it's basically a kind of
uh the parameters that you set it up so
that how the resource optimization can
be achieved in the auto scaling
with the scaling strategies users can
create their own strategy based on their
required metrics and thresholds and this
can be changed on the fly as well
what are the two types of scaling
policies we have so there are basically
Dynamic scaling and the predictive
scaling
uh now what is dynamic scaling it
basically guides the service of AWS Auto
scaling on how to optimize the resources
and it is helpful in optimizing
resources for availability and
particular price
now with scaling strategies users can
create their plan based on the required
metrics and thresholds so a metric can
be like let's say a networking Network
out or it can be a CPU utilization
memory utilization likewise
now in the predictive scaling its
objective is to predict future workload
based on daily and weekly Trends and
regular forecast future network traffic
so it is kind of a forecast that happens
based on the previous past experiences
it uses a machine learning technique for
analyzing that Network graphic and this
scaling is like how weather forecast
works right
it provides schedule scaling actions to
ensure the resource capacity is
available for application requirement
now with the auto scaling you would need
the load balancers also because if there
are multiple instances that are created
then you would need a load balancer to
distribute the low to those instances so
let's understand what do we mean by a
load balancer
a load balancer basically acts as a
reverse proxy and it is responsible for
Distributing the network or the
application traffic across multiple
servers
with the help of a load balancer you can
achieve a reliability you can achieve a
fault tolerance of an application that
is basically it increases the fault
tolerance and the reliability
so for example when there is a high
Network traffic that is coming to your
application and if that much traffic
comes to your application to the
instances your instances May crash
so how you can avoid that situation so
you need to manage the network traffic
that is coming to your instances and
that can be done with the load balancer
so thanks to the AWS load balancers
which helps in distributing Network
traffic across backend servers in a way
that it increases performance of an
application
here in the image you can see the
traffic coming from a different
resources landing on to the ec2 instance
and the load balancer is actually
Distributing that traffic to all the
three instances hence managing the
network traffic quite properly
now what are the types of load balances
we have there are three types of load
balancers on the AWS one is the classic
load balancer second is the application
load balancer and the third one is the
network load balancer
let's look into what we have in the
classical manager
so the classic load balancer is the most
basic form of load balancing and we call
it as a primitive load balancer also and
it is widely used for the ec2 instances
it is based on the IP address and the
TCP port and it routes Network traffic
between end users as well as in between
the backend servers
and it does not support host based
routing and it results in low efficiency
of resources
let's look into what we have in the
application load balancer
this is one of the advanced forms of
load balancing it performs a task on the
application Level in the OSI model
it is used when there are HTTP and https
traffic routing is required
and also it supports the whole space and
Pathways routing and performs well with
the micro services or the backend
applications
the network load balancer performs the
task at layer 4 of the connection level
in The OSI model
the prime role of the network load
balancer is to Route the TCP traffic
and it can manage a massive amount of
traffic and is also suitable to manage
the low latencies
let's look into the demo and see how
practically we can
create the order scale
hi guys let's look into the demo for how
we can create an auto Skilling on the
AWS console
so right now I'm logged into the AWS
console and I am in the Mumbai region
what you need to do is you have to go to
the compute section and under that click
on the easy to service
let's wait for the ec2 servers to come
now just scroll down
and under the load balancing there is an
option called Auto scaling so there
first you have to create a launch
configuration and then after that you
have to create the auto scaling groups
so click on launch configuration
and then you have to click on create
launch configurations
so click on create launch configuration
now this launch configuration is
basically this set of parameters that
you define while launching and auto
scaling so that this uniformity is
maintained with all the instances
so that includes let's say if you select
a Windows Os or a Linux OS that
particular type of an operating system
will be implemented in all the instances
that will be part of an auto scale
so there are certain set of parameters
that we have to specify during the
launch configuration so that we can have
a uniformity in terms of launching the
servers so here I would select an Amazon
Linux Ami
and then I would select the type of a
server which will be t2.micro
click on configure details put the name
to the launch configuration let's say we
put it as a demo
and the rest of the things we'll keep it
default
click on add storage since it's a Linux
Ami we can go with the 8GB storage that
should be fine click on configure
Security Group
let's create a new Security Group which
has the SSH Port open and that is open
for anywhere which is basically source
ipv4 and IPv6 IPS any IP would be able
to access that
click on review just review your launch
configuration if you want to make
changes you can do that otherwise click
on create a launch configuration
uh you would need the key pair and this
key pair will be a unique key pair which
will be used with all the instances that
are part of the auto scaling group so we
can select an existing key pair if you
have that otherwise you can create a new
keeper so I have an existing key pair
I'll go with that acknowledge it and
click on create launch condition
now we have successfully launched the
configuration of an auto scaling the
next thing is to create an auto scaling
group so click on create an auto scaling
group using this launch configuration
put a group name let's say we put
something like test
and the group size to start with it says
one instance so that means at least a
single instance will always be running
and it will be initiated and running 24
cross 7 till the auto scaling is
available you can increase the size of
the minimum base instances Also let's
say you can change it to 2 also so you
would get at least two servers running
all the time
so we'll go with the one instance uh the
network would be the VPC default and uh
in the VPC that particular region we can
select the availability zones so let's
say if I select availability Zone 1A and
then availability is on 1B so how the
instances will be launched so one
instance will be launched in money in
the other one in b1b the third one in
the one a fourth one and the 1B likewise
it will be equally spreaded among the
availability zones
next part is to configure the sailing
policies so click on it
if you want to keep this group at its
initial size let's say if you want to go
with only a single instance or two
instances and you don't want the scaling
to progress you can put it keep this
group at its initial size so this is
basically a way to Halt the scaling
but we'll use the scaling policies to
adjust the capacity of this group so
click on it and we would scale between
let's say minimum one instance that we
have and we scale it between one two
four instances
and
what condition on what basis these
instances will be scaled up or scaled
down would be defined in the scale group
size
so the scaling policies you can
Implement based on a scale group size or
using the simple scaling policies using
the steps
so in the scale group size you have a
certain metrics you can use average CPU
utilization you can define a metric
related to average networking average
Network out or the load balancer request
counts per Target and if you create a
simple scaling policies using steps then
you need to create the alarms and there
you can get some more metrics that you
can add up as a parameter for the auto
scaling
let's go over the scaling group size
let's go with the metric type as average
CPU utilization and the target value
here you have to specify what would be
the threshold that when the instance CP
utilization is crossed then a new
instance should be initiated
so you can put a reasonable threshold
for that let's say we put something like
85 percent and whenever the instant CP
utilization is crossed 85 threshold you
will see that there will be a new
instance created
let's go to the next configure
notifications
and here you can add notifications so
let's say if there is a new instance
that is initiated and you want to
basically be notified so you can get
notifications over your email IDs or you
can get it on the cell phones so for
that for adding the notification you
would need the SNS service and that is
called as a simple notification service
and you have to create a topic there you
have to subscribe for the topic using
your email ID and then you should get
the notifications
uh click on configure tags the tags are
not mandatory you can basically
put the tag let's say if you want to
identify the instance for the purpose it
was created otherwise you can leave it
blank also click on review
and review your scaling policies
notification tags as well as the scaling
group details
click on create auto scaling group and
here you go your scaling has been
launched click on close
and you should get at least a single
instance initiated automatically by the
auto scaling
so let's wait for the details to appear
so here you can see
on launch configuration name demo
Auto scaling group name test
minimum instance we want one the maximum
instances we want four
we have selected two availability zones
AP South one AP South 1B
and uh
the instance one has been initiated and
if you want to verify where exactly this
instance has been initiated just click
on the instances here and here you will
see that our single instance has been
initiated that is in service and that
has been initiated in AP South 1B now
once the threshold of this instance
crosses 85 percent that is what we have
defined in the scaling policies
then you should see that another
instance will be initiated
so likewise this is basically I have
created steps to
initiate a scaling policy that means to
increase the number of servers whenever
the threshold crosses likewise here
itself you can add another policy to
scale down
the resources in case if the CPU
utilization goes to a normal value today
we are going to discuss about Amazon
redshift which is one of the data
warehouse service on the AWS so let's
begin with Amazon redshift and let's see
what we have for today's session so
what's in it for you today we'll see
what is AWS why we require Amazon
redshift what do we mean by Amazon
redshift the advantages of Amazon
redshift the architecture of Amazon
redshift some of the additional Concepts
associated with the redshift and the
companies that are using the Amazon
redshift and finally we'll cover up our
one demo which will show you the
Practical example that how you can
actually use the redshift service now
what is AWS as we know that AWS stands
for Amazon web service it's one of the
largest cloud providers in the market
and it's basically a secure cloud
service platform provided from the
Amazon also on the AWS you can create
and deploy the applications using the
AWS service along with that you can
access the services provided by the AWS
over the public network that is over the
Internet they are accessible plus you
pay only whatever the service you use
for now let's understand why we require
Amazon redshift so earlier before Amazon
redshift what used to happen that the
people used to or the developers used to
fetch the data from the data warehouse
so data warehouse is basically a
terminology which is basically
represents the collection of the data so
a repository where the data is stored is
generally called as a data warehouse now
fetching data from the data warehouse
was a complicated task because might be
a possibility that the developer is
located at a different geography and the
data data warehouses at a different
location and probably there is not that
much network connectivity or some
networking challenges internet
connectivity challenges security
challenges might be and a lot of
Maintenance was required to manage the
data warehouses so what are the con of
the traditional data warehouse Services
it was time consuming to download or get
the data from the data warehouse
maintenance cost was high and there was
the possibility of loss of information
in between the downloading of the data
and the data rigidity was an issue now
how these problems could overcome and
this was basically solved with the
introduction of Amazon redshift over the
cloud platform now we say that Amazon
redshift has solved traditional data
warehouse problems that the developers
were facing but how what is Amazon
redshift actually is so what is Amazon
redshift it is one of the services over
the AWS Amazon web services which is
called as a data warehouse service so
Amazon redshift is a cloud-based service
or a data warehouse service that is
primarily used for collecting and
storing the large chunk of data so it
also helps you to get or extract the
data analyze the data using some of the
bi tools so business intelligence tools
you can can use and get the data from
the redshift and process that and hence
it simplifies the process of handling
the large-scale data sets so this is the
symbol for the Amazon redshift over the
AWS now let's discuss about one of the
use case so DNA is basically a
telecommunication company and they were
facing an issue with managing their
website and also the Amazon S3 data
which led down to slow process of their
applications now how could they overcome
this problem let's see that so they
overcome this issue by using the Amazon
redshift and all the company noticed
that there was a 52 increase in the
application performance now did you know
that Amazon redshift is basically cost
less to operate than any other cloud
data warehouse service available on the
cloud computing platforms and also the
performance of an Amazon redshift is the
fastest data warehouse we can say that
that is available as of now so in both
cases one is that it saves the cost as
compared to the traditional data
warehouses and also the performance of
this redshift service or a data
warehouse service the fastest available
on the cloud platforms and more than 15
000 customers primarily presently they
are using the Amazon redshift service
now let's understand some of the
advantages of Amazon redshift first of
all as we saw that it is one of the
fastest available data warehouse service
so it has the high performance second is
it is a low cost service so you can have
a large scale of data warehouse or a
databases combined in a data warehouse
at a very low cost so whatever you use
you pay for that only scalability now in
case if you wanted to increase the nodes
of your databases in your redshift you
can actually increase that based on your
requirement and that is on the fly so
you don't have to wait for the
procurement of any kind of a hardware or
the infrastructure it is whenever you
require you can scale up or scale down
the resources so this scalability is
again one of the advantage of the Amazon
red chair availability since it's
available across multiple availability
zones so it makes this service as a
highly available service security so
whenever you create whenever you access
redshift you create a clusters in the
redshift and the Clusters are created in
the you can define a specific virtual
private Cloud for your cluster and you
can create your own security groups and
attach it to your cluster so you can
design the security parameters based on
your requirement and you can get your
data warehouse or the data items in a
secured Place flexibility and you can
remove the Clusters you can create under
clusters if you are deleting a cluster
you can take a snapshot of it and you
can move those snapshots to different
regions so that much flexibility is
available on the AWS for the service and
the other Advantage is that it is
basically a very simple way to do a
database migration so if you're planning
that you wanted to migrate your
databases from the traditional data
center over the cloud on the redshift it
is basically a very simple to do a
database migration you can have some of
the inbuilt tools available on the AWS
access you can connect them with your
traditional Data Center and get that
data migrated directly to the redshift
now let's understand the architecture of
the Amazon redshift so architecture of
an Amazon redshift is basically it
combines of a cluster and that we call
it as a data warehouse cluster in this
picture you can see that this is a data
warehouse cluster and this is a
representation of a Amazon redshift so
it has some of the compute nodes which
does the data processing and a leader
node which gives the instructions to
these compute nodes and also the leader
node basically manages the client
applications that require the data from
the redshift so let's understand about
the components of the redshift the
client application of Amazon redshift
basically interact with the leader node
using jdbc or the odbc now what is jdbc
it's a Java database connectivity and
the odbc stands for open database
connectivity the Amazon redshift service
using a jdbc connector can monitor the
connections from the other client
applications so the leader node can
actually have a check on the client
applications using the jdbc connections
whereas the odbc allows a leader node to
have a direct interaction or to have a
live interaction with the Amazon
redshift so odbc allows a user to
interact with live data of Amazon
redshift so it has a direct connectivity
direct access of the applications as
well as the leader node can get the
information from the compute nodes now
what are these compute nodes these are
basically kind of a databases which does
the processing so Amazon redshift has a
set of computing resources which we call
it as a nodes and the nodes when they
are combined together they are called it
as a clusters now a cluster a set of
computing resources which are called as
nodes and this gathers into a group
which we call it as a data warehouse
cluster so you can have a compute node
starting from 1 to n number of nodes and
that's why we call that the redshift is
a scalable service because we can scale
up the compute nodes whenever we require
now the data warehouse cluster or the
each cluster has one or more databases
in the form of a nodes now what is the
leader node this node basically manages
the interaction between the client
application and the compute node so it
acts as a bridge between the client
application and the compute nodes also
it analyzes and develops designs in
order to carry out any kind of a
database operations so leader node
basically sends out the instructions to
the compute nodes basically perform or
execute that instruction and give that
output to the leader node so that is
what we are going to see in the next
slide that the leader node runs the
program and assign the code to
individual compute nodes and the compute
nodes execute the program and share the
result back to the leader node for the
final aggregation and then it is
delivered to the client application for
analytics or whatever the client
application is created for so compute
nodes are basically categorized into
slices and each node slice is allotted
with specific memory space or you can
say a storage space where the data is
processed these node slices Works in
parallel in order to finish their work
and hence when we talk about a redshift
as a fast a faster processing capability
as compared to other data warehouses or
traditional data warehouses this is
because that these node slices work in a
parallel operation that makes it more
faster now the additional concept
associated with Amazon redshift is there
are two additional Concepts associated
with the Redshirt one is called as the
column storage and the other one is
called as the compression let's see what
is the column storage as the name
suggests column storage is basically
kind of a data storage in the form of a
column so that whenever we run a query
it becomes easier to pull out the data
from The Columns so column storage is an
essential factor in optimizing query
performance and resulting in quicker
output so one of the examples are
mentioned here so below example show how
database tables store record into disk
block by row so here you can see that if
we wanted to pull out some kind of an
information based on the city address
age we can basically create a filter and
from there we can put out the details
that we require and that is going to
fetch out the details based on the
column storage so that makes data more
structured more streamlined and it
becomes very easier to run a query and
get that output now the compression is
basically to save the column storage we
can use a compression as an attribute so
compression is a column level operation
which decreases the storage requirement
and hence it improves the query
performance and this is one of the
syntax for the column compression now
the companies that are using Amazon
redshift one is Lya the other one is
Equinox the third one is the Pfizer
which is one of the famous
Pharmaceuticals company McDonald's one
of the burger chains across the globe
and Philips it's an electronic company
so these are one of the biggest
companies that are basically relying on
the they are putting their data on the
redshift data warehouse service now in
another video we'll see the demo for
using the Amazon redshift let's look
into the Amazon redshift demo so these
are the steps that we need to follow for
creating the Amazon redshift cluster and
in this demo what we'll be doing is that
we'll be creating an IM role for the
redshift so that the redshift can call
the services and specifically we'll be
using the S3 service so the role that we
will be creating will be giving the
permission to redshift to have an access
of an S3 in the read-only format so in
the step one what we require we'll check
the prerequisites and what you need to
have is the AWS credentials if you don't
have that you need to create your own
credentials and you can use your credit
and the debit card and then in the Step
2 we'll proceed with the IM role for the
Amazon redshift once the rule is created
we'll launch a sample Amazon redshift
cluster mentioned in the step 3 and then
we'll assign a VPC security groups to
our cluster now you can create it in the
default PC also you can create a default
security groups also otherwise you can
customize the security groups based on
your requirement now to connect to the
sample cluster you need to run the
queries and you can connect to your
cluster and run queries on the AWS
Management console query editor which
you will find it in the redshift only or
if you use the query editor you don't
have to download and set up a SQL client
application separately and in the step 6
what you can do is you can copy the data
from the S3 and upload that in the
redshift because the redshift would have
an access a read-only access for the S3
as that will be created in the IM role
so let's see how we can actually use the
redshift on the AWS so I am already
logged in into my account I am in North
Virginia region I'll search for redshift
service and here I find Amazon redshift
so just click on it
let's wait for the redshift to come now
this is a redshift dashboard and from
here itself you have to run the cluster
so to launch a cluster you just have to
click on this launch cluster and once
the cluster is created and if you wanted
to run queries you can open query editor
or you can basically create queries and
access the data from the redshift so
that's what it was mentioned in the
steps also that you don't require a
separate SQL client application to get
the queries run on the data warehouse
now before creating a cluster we need to
create the role so what we'll do is
we'll click on the services and we'll
move to IM role section so IM rule I can
find here under the security identity
and compliance so just click on the
identity access management and then
click on create roles so let's wait for
IM page to open so here in the IM
dashboard you just have to click on the
rules I already have the role created so
what I'll do is I'll delete this role
and I'll create it separately so just
click on a create role and under the AWS
Services you have to select for the
redshift because now the the redshift
will be calling the other services and
that's why we are creating the role now
which other services that the redshift
will be having an access of S3 why
because we'll be putting up the data on
the S3 and that is something which needs
to be uploaded on the redshift so we'll
just search for the redshift service and
we can find it here so just click on it
and then click on redshift customizable
in the use case now click on next
permissions and here in the permissions
give the access to this role assign the
permissions to this role in the form of
an S3 read-only access so you can search
here for the S3 Also let's wait for the
policies to come in here it is let's
type S3 and here we can find Amazon S3
read-only access so just click on it and
assign the permissions to this role tags
you can leave them blank click on next
review put a name to your role let's put
my redshift role and click on a create
role now you can see that your role has
been created now the next step is that
we move to Redshirt service and we'll
create one cluster so click on the
services click on Amazon redshift you
can find that in the History Section
since we browsed it just now and from
here we are going to create a sample
cluster now to launch a cluster you just
have to click on launch this cluster
whatever the uncompressed data size you
want in the form of a gigabyte terabyte
or petabyte you can select that and
let's say if you select in the form of
GB how much GB memory you want you can
Define it here itself this also gives
you the information about the costing On
Demand is basically pay as you use so
they are going to charge you 0.5 dollars
per hour for using the two node slices
so let's click on launch this cluster
and this will be a dc2 DOT large kind of
an instance that will be given to you it
would be in the form of a solid state
drive ssds which is one of the fastest
way of storing the information and the
nodes two are mentioned by default that
means there will be two node slices and
that will be created in a cluster you
can in increase them also let's say if I
put three note slices so it is going to
give us 3 into 0.16 DB per node storage
now here you have to define the master
username password for your redshift
cluster and you have to follow the
password instructions so I would put a
password to this cluster and if it
accepts that means it does not give you
any kind of a warning otherwise it is
going to tell you about you have to use
the ASCII characters and all and here
you have to assign this cluster the role
that we created recently so in the
available IM rules you just have to
click on my redshift role and then you
have to launch the cluster if you wanted
to change something in with respect to
the default settings let's say if you
wanted to change the VPC from default
VPC to your custom VPC and you wanted to
change the default security groups to
your own security groups so you can
switch to advanced settings and do that
modification now let's launch the
cluster and here you can see the
redshift cluster is being created now if
you wanted to run queries on this thread
shift cluster so you don't require a
separate SQL client you just have to
follow the simple steps to run a query
editor and the query editor you will
find it on the dashboard so let's click
on the cluster and here you would see
that the redshift cluster would be
created with the three nodes in the US
East 1B availability zone so we have
created the redshift cluster in the Ohio
region and now what we'll do is we'll
see how we can create the tables inside
the redshift and we'll see how we can
use the copy command so that we can
directly move the data uploaded on the
S3 bucket to the redshift database
tables and then we'll query the results
of a table as well so how we can do that
first of all after creating the redshift
cluster we have to install SQL workbench
slash J this is not a MySQL which is
managed by Oracle and you can find this
on the Google you can download it from
there and then you have to connect this
client with the direct shift database
how you can do click on file click on
connect window and after connecting a
window you have to paste the URL which
is a jdbc driver this driver link you
can find it onto the AWS console so if
you open up a redshift cluster there you
would find the jdbc driver link let's
wait for it so this is our cluster
created let's open it and here you can
find this jdbc URL and also make sure
that in the security groups of a
redshift you have the port 5439 open for
the traffic incoming traffic you also
need to have the Amazon redshift driver
and this is the link where you can
download the driver and specify the path
once you are done with that you provide
the username and the password that you
created while creating the redshift
cluster click on OK so this connects
with the database and now the database
connection is almost completed now what
we will be doing in the SQL workbench
will be first creating the sales table
and then in the sales table we'll be
adding up the entries copied from the S3
bucket and then move it to the redshift
database and after that we'll query the
results in the sales table now whatever
the values you are creating in the table
the same values needs to be in the data
file and I have taken up this sample
data file from this link which is
docs.aws.amazon.com redshift sample
database creation and here you can find
a download file ticketdb.zip file this
folder has basically multiple data files
sample data files which you can actually
use it to practice uploading the data on
the redshift cluster so I have extracted
one of the files from this folder and
then I have uploaded that file in the S3
bucket now we'll move into the S3 bucket
let's look for the file that has been
uploaded on the S3 bucket so this is the
bucket sample and sales underscore tab
dot text is the file that I have
uploaded this has the entries data
entries that will be uploaded using a
copy command onto the redshift cluster
now after executing after putting up the
command for creating up the table then
we'll use a copy command and copy
command we have to define the table name
the table name is sales and we have to
define the path from where the data
would be copied over to the sales table
in the redshift now path is the S3
bucket and this is the redshift bucket
sample and we it has to look for the
data inside the sales underscore tab dot
text file also we have to define the
role Arn that was created previously and
once it is done then the third step is
to query the results inside the sales
table to check whether our data has been
uploaded correctly on the table or not
now what we'll do is we'll execute all
these three syntax it gives us the error
because we have to connect it again to
the database let's wait for it execute
it it's again gives us the error let's
look into the name of the bucket it's
redshift bucket sample so we have two
T's mentioned here right let's connect
with the database again and now execute
it so table sales created and we got the
error the specified bucket does not
exist a redshift bucket sample let's
view the bucket name redshift bucket
sample let's copy that put it here
connect to the window connect back to
the database right and now execute it so
table sales created uh the data in the
table has been copied from the S3 bucket
to sales underscore tab dot text to the
redshift and then the query of the
results now the results from the table
has been it hi guys I'm Raul from Simply
learn and today I'd like to welcome you
all to the greatest debate of the
century today I am joined by two giants
of the cloud computing industry they'll
be going head to head with each other to
decide who amongst them is better it's
going to be one hell of fight now let's
meet our candidates on my left we have
AWS who's voiced by a picture hi guys
and on my right we have Microsoft Azure
who's voiced by Anjali hey there so
today we'll be deciding who's better on
the basis of their origin and the
features they provide their performance
in the present day and comparing them on
the basis of pricing market share and
options free tier and instance
configuration now let's listen to their
opening statements let's start with AWS
launched in 2006 AWS is one of the most
commonly used cloud computing platforms
across the world companies like Adobe
Netflix Airbnb HTC Pinterest and Spotify
have put their faith in AWS for their
proper functioning it also dominates the
cloud computing domain almost 40 percent
of the entire market share so far
nobody's even gotten close to beating
that number AWS also provides a wide
range of services that covers a great
number of domains domains like compute
networking storage migration and so much
more now let's see what Azure has to say
about that Azure was launched in 2010
and is trusted by almost 80 percent of
all Fortune 500 companies the best of
the best companies in the world choose
to work only with Azure Azure also
provides its services to more regions
than any other cloud service provider in
the world Azure covers 42 regions
already and 12 more are being planned to
be made Azure also provides more than
100 Services spanning a variety of
domains now that the opening statements
are done let's have a look at the
current market status of each of our
competitors this is the performance
route here we have the stats for the
market share of AWS Azure and other
cloud service providers this is for the
early 2018 period Amazon web services
takes up a whopping 40 percent of the
market share closely followed by sgr at
30 and other cloud services adding 30
this 40 indicates most organizations
clear interest in using AWS VR number
one because of our years of experience
and Trust we've created among our users
sure you're the market leader but we are
not very far behind let me remind you
more than 80 percent of the Fortune 500
companies trust Azure with their cloud
computing needs so it's only a match of
time before Azure takes the lead the
rest of the 30 percent that is an AWS or
Azure accounts to the other cloud
service providers like Google Cloud
platform backspace IBM software and so
on now for our next round the comparison
round first we'll be comparing pricing
we'll be looking at the cost of a very
basic instance which is a virtual
machine of two virtual CPUs and 8gbs of
RAM for AWS this will cost you
approximately
0.0928 US dollars per hour and for the
same instance in Azure it will cost you
approximately 0.096 US dollars per hour
next up let's compare market share and
options as I mentioned before AWS is the
Undisputed market leader when it comes
to the cloud computing domain taking up
40 of the market share by 2020 AWS is
also expected to produce twice its
current Revenue which comes close to 44
billion dollars not to mention AWS is
constantly expanding its already strong
roaster of more than 100 services to
fulfill the shifting business
requirements of organizations all that
is great really good for you but the
research company Gartner has released a
magic quadrant that you have to see you
see the competition is now neck to neck
between Azure and AWS it's only a match
of time before Azure can increase from
its 30 market share and surpass AWS this
becomes more likely considering how all
companies are migrating from AWS to
Azure to help satisfy their business
needs Azure is not far behind AWS when
this comes to Services as well azure's
service offerings are constantly updated
and improved on to help users satisfy
their cloud computing requirement now
let's compare AWS and azure's free
offerings AWS provides a significant
number of services for free helping
users get hands-on experience with the
platform products and services the free
tier Services fall under two categories
services that will remain free forever
and the others that are valid only for
one year the always free category offers
more than 20 services for example Amazon
SNS sqs cloudwatch Etc and the valid for
your category offers approximately 20
services for example Amazon S3 ec2
elastic cache Etc both types of services
have limits on the usage for example
storage number of requests compute time
Etc but users are only charged for using
services that fall under the valid for a
year category after a year of their
usage a shop provides a free tier as
well it also provides services that
belong to the categories of free for a
year and always free there are about 25
plus always free services provided by
Azure these include app service
functions container service active
directory and lots more and as of the
valid for a year there are eight
services offered there's Linux or
Windows Virtual machines blob storage
SQL database and few more Azure also
provides the users with credits of 200
US dollars to access all their services
for 30 days now this is a unique feature
that Azure provides which users can use
their credits to utilize any service of
a choice for the entire month now let's
compare instance configuration the
largest instance that AWS offers is that
of a whopping 256 GBS of RAM and 16
virtual CPUs the largest that Azure
offers isn't very far behind either 224
GBS of RAM and 16 virtual CPUs and now
for the final round each of our
contestants will be shown facts and they
have to give explanations for these
facts we call it the rapid fire round
first we have features in which AWS is
good and Azure is better AWS does not
cut down on the features it offers as
its users however it requires slightly
more Management on the user's part Azure
goes slightly deeper with the services
that fall under certain categories like
platform as a service and infrastructure
as a service
next we have hybrid Cloud where AWS is
good and Azure is better OK although AWS
did not emphasize on hybrid Cloud
earlier they are focusing more on
technology now Azure has always
emphasized on hybrid cloud and has
features supporting it since the days of
its inception for developers AWS is
better and Azure is good of course it's
better because AWS supports integration
with third-party applications well Azure
provides access to data centers that
provide a scalable architecture for
pricing both AWS and Azure are at the
same level it's good for AWS because it
provides a competitive and constantly
decreasing pricing model and in the case
of azure it provides offers that are
constantly experimented upon to provide
its users with the best experience and
that's it our contestants have finished
giving their statements now let's see
who won surprisingly nobody each cloud
computing platform has its own pros and
cons choosing the right one is based
entirely on your organization's
requirements
hi guys today we got something very
special in store for you we're going to
talk about the best cloud computing
platform available Amazon web services
uh Rahul I think you said something
wrong here the best cloud computing
platform is obviously Google Cloud
platform no it isn't AWS is more than
100 services that span a variety of
domains all right that Google Cloud
platform has cheaper instances what do
you have to say about that well I guess
there's only one place we can actually
discuss this a boxing ring so guys I'm
apeksha and I will be Google Cloud
platform and I'm Rahul I'll be AWS so
welcome to fight night this is AWS
versus gcp the winner will be chosen on
the basis of their origin and the
features they provide the performance in
the present day and comparing them on
the basis of pricing market share and
options the things they give you for
free and instance configuration now
first let's talk about AWS AWS was
launched in 2004 and is a cloud service
platform that helps businesses grow and
scale by providing them Services then
number of different domains these
domains include compute database storage
migration networking and so on a very
important aspect about AWS is its years
of experience now AWS has been in the
market a lot longer than any other cloud
service platform which means they know
how businesses work and how they can
contribute to the business growing also
AWS is over 5.1 billion dollars of
Revenue in the last quarter this is a
clear indication of how much faith and
trust people have in AWS they occupy
more than 40 of the market which is a
significant chunk of the cloud computing
Market they have at least 100 services
that are available at the moment which
means just about every issue that you
have can be solved within AWS service
now that was great but now can we talk
about gcp I hope you know that gcp was
launched very recently in 2011 and it is
already helping businesses significantly
with a suite of intelligent secure and
flexible Cloud Computing Services it
lets you build deploy and scale
applications websites services on the
same infrastructure as we will the
intuitive user experience that gcp
provides with dashboards Wizards is way
better in all the aspects gcp has just
stepped in the market and it is already
offering a modest number of services and
the number is rapidly increasing and the
cost for a CPU instance or Regional
storage that gcp provides is a whole lot
cheaper and you also get a
multi-regional cloud storage now what do
you have to say on that I'm so glad you
asked let's look at present day in fact
let's look at the cloud market share of
the fourth quarter of 2017. this will
tell you once and for all that AWS is
the leader when it comes to cloud
computing Amazon web services
contributes 47 of the market share
others like Rackspace or Verizon cloud
contribute 36 Microsoft Azure
contributes 10 the Google Cloud platform
contributes four percent and IBM
software contributes three percent 47
percent of the market share is
contributed by AWS you need me to
convince you any more wait wait wait all
that is fine but we only started a few
years back and have already own so much
in such a less span of time having to
heard the latest news our revenue is
already a billion dollars per quarter
wait for a few more years and the world
shall see and AWS makes 5.3 billion
dollars per quarter it's going to take a
good long time before you can even get
close to us yes yes we'll see now let's
compare a few things for starters let's
compare price for AWS a compute instance
of two CPUs and 8 GB ram costs
approximately 68 US Dollars now a
computer instance the virtual machine in
which you can specify what operating
system Ram or storage you want to have
for cloud storage it costs 2.3 cents per
GB per month with AWS you really want to
do that because gcp wins this hands down
let's take the same compute instance of
two CPUs with 8GB Ram it will cost
approximately 50 dollars per month with
gcp and as per my calculations that's a
25 annual cost reduction when compared
to AWS talking about cloud storage costs
it is only 2 cents per GB per month with
gcp what else do you want me to say
let's talk about market share and
options now AWS is the current market
leader when it comes to cloud computing
now as you remember we contribute at
least 47 of the entire market share AWS
also has at least 100 services available
at the moment which is a clear
indication of how well AWS understands
businesses and helps them grow yeah
that's true but you should also know
that gcp is steadily growing we have
over 60 services that are up and running
as you can see here and a lot more to
come it's only a matter of time when we
will have as many services as you do
many companies have already started
adopting gcp as a cloud service provider
now let's talk about things you get for
free with AWS you get access to almost
all the services for an entire year with
usage limits now these limits include an
hourly or by the minute basis for
example with Amazon ec2 you get 750
hours per month you also have limits on
the number of requests to services for
example with AWS Lambda you have 1
million requests per month now after
these limits are crossed you get charged
standard rates with gcp you get access
to all Cloud platform products like
Firebase the Google Maps API and so on
you also get 300 in credit to spend over
a 12 month period on all the cloud
platform products and interestingly
after the free trial ends you won't be
charged unless you manually upgrade to a
paid account now there is also the
always free version for which you will
need an upgraded billing account here
you get to use a small instance for free
and 5 GB of cloud storage any usage
above this always free usage limits will
be automatically built at standard rates
now let's talk about how you can
configure instances with AWS the largest
instances offered is of 128 CPUs and 4
degrees of ram now other than the
on-demand method like I mentioned before
you can also use Spartan sensors now
these are for situations where your
application is more fault or rent and
can handle an interruption now you pay
for the spot price which is effective at
a particular God so these spot prices do
fluctuate but are adjusted over a period
of time the largest instance offered
with Google cloud is 160 CPUs and 3.75
TBS Ram light spot instances of AWS
Google Cloud offers short-lived compute
instances suitable for bad jobs and for
tolerant workloads they are called as
preemptable instances so these instances
are available at 80 percent off on
on-demand price hence they reduce your
compute engine costs significantly and
unlike AWS these come at a fixed price
Google Cloud platform is a lot more
flexible when it comes to instance
configuration you simply choose your CPU
and RAM combination of course you can
even create your own instance types this
way before we wrap it up let's compare
on some other things as well Telemetry
it's a process of automatically
collecting periodic measurements from
remote devices for example GPS gcp is
obviously better because they have
Superior Telemetry tools which help in
analyzing so services and providing more
opportunities for improvement when it
comes to application support AWS is
obviously better since they have years
of experience under their bed AWS
provides the best support that can be
given to the customers containers are
better with gcp a container is a virtual
process running in user space as
kubernetes was originally developed by
Google TCP has full native support for
the tools other cloud services are just
fine-tuning a way to provide kubernetes
as a service also the containers help
with abstracting applications from their
environment they originally run it the
applications can be deployed easily
regardless of their environment when it
comes to geographies AWS is better since
they have a head start of a few years
AWS in this span of time has been able
to cover a larger market share and
geographical locations now it's time for
the big decision so who's it going to be
yeah who is it going to be gcp or AWS I
think I'm going to go for in cloud
computing the competition for leadership
is a quite a tough three-way race
Amazon web services Microsoft Azure and
Google Cloud platform clearly are the
top Cloud companies hold a commanding
lead in the infrastructure as a service
and platform as a service markets
hey everyone I'm shamli and I welcome
you all to this new video of Simply
learn on AWS versus Azure versus gcp
in this video we will compare and
contrast AWS Azure and gcp based on a
few related Concepts around these cloud
computing platforms
it will help us understand the
functioning of these top Cloud platforms
and will also let us figure out the
individuality of each one of them
but before starting with the comparison
let's have a quest introduction of AWS
versus Azure versus gcp so let's get
started
Amazon web services for AWS is a cloud
computing platform that manages and
maintains hardware and infrastructure
reducing the expense and complexity of
purchasing and running resources on site
for businesses and individuals
these resources are available for free
or for a fee per usage
Microsoft Azure is a cloud computing
service that it offers a collection of
Cloud Computing Services for building
testing deploying and managing
application in the cloud including
remotely hosted and managed versions of
Microsoft Technology Google Cloud
platform offers a variety of Cloud
Computing Services for building
deploying scaling monitoring and
operating a cloud
the services are identical to those that
power Google products such as Google
search Gmail YouTube and Google Drive
now let's move on to the comparison
between AWS Azure and gcp
we will be comparing them based on few
major parameters like
origin
service integration
availability Zone
Cloud tools like compute
storage
networking
market share
pricing
and at last who uses them
now let's move ahead and start with the
first comparison origin
in the year 2006 Amazon web services AWS
was introduced to the market and in the
year 2010 Azure launched its services
whereas on the other hand gcp was
established in the year 2008.
from the start AWS has been supportive
of the open source concept but the open
source Community has a tense
relationship with azure
on the other hand gcp similar to AWS
provides Google cloud with manage open
source services that are tightly linked
AWS offers services on a large and
complex scale that can be manipulated
but Azure support is comparatively low
quality
whereas gcp's monthly support price is
almost 150 dollars for the silver class
which is the most basic of services and
is quite expensive
now let's move on to the service
integration of these Cloud platforms
service integration is a set of tools
and technology that connects different
applications systems repositories and
data and process interchange in real
time
AWS makes it simple for users to combine
services such as Amazon ec2 Amazon S3 be
installed and others
on the other hand Azure allows customers
to effortlessly combine Azure VMS Azure
app service SQL databases and other
services
whereas users can utilize gcp to combine
services such as compute engine cloud
storage and Cloud SQL now that we know
briefly about all these Cloud platforms
let's have a look at the availability
zones of these platforms
biggest AWS was the first in the cloud
domain they have had more time to build
and extend their Network
but Azure and gcp both have various
locations around the world
but the distinction is in the amount of
availability zones they have
AWS now offers 66 availability zones
with an additional 12 on the pipeline
close to it Azure is available in 140
countries and is available in 54 regions
throughout the world
but Google Cloud platform is now
available in 20 Global areas with three
more on the way
now let's move on to the next important
factor which is tools now let's move
ahead and have have a look at the first
feature
which is compute
elastic compute cloud or ec2 is AWS
compute service which offers a wide
range of features including a large
number of instances support for both
windows and Linux high performance
Computing and more
as shown on the other hand virtual
machines is Microsoft azure's core
cloud-based compute solution it includes
Linux Windows server and other operating
systems as well as better security and
Microsoft program integration
in comparison to its competitors
Google's Computing Services catalog is
somewhat smaller compute engine that
companies principal service offers
custom and predefined machine types
post-second invoicing Linux and Windows
support and carbon neutral
infrastructure that uses half the energy
of traditional data centers
within the compute category Amazon's
different containers services are
gaining prominence it has Docker
kubernetes and it's also forget service
which automates server and cluster
management when using containers as well
as other alternatives
Azure unlike AWS uses virtual machine
scale sets of two container services
Azure container service is based on
kubernetes and container service
uses Docker Hub and Azure container
registry for management
for Enterprises interested in deploying
containers Google offers the kubernetes
engine it's also worth noting that
Google has significantly involved in the
kubernetes project providing it
extensive knowledge in this field
now let's move on to the next parameter
of comparison which is storage
simple storage service
for object storage elastic block storage
for persistent block storage and elastic
file system for file storage are among
AWS storage offerings
blob storage for rest based object
storage of unstructured data queue
storage for a large volume workloads
file storage and this storage are among
Microsoft azure's core storage services
gcp offers an increasing number of
storage options its unified object
storage service cloud storage also has a
persistent disk option relational
database service or RDS dynamodb no SQL
database elastic cache in memory data
store redshift data warehouse Neptune
graph database and database migration
service are all SQL compatible databases
offered by Amazon
the database choices in Azure are
specially wide SQL database MySQL
database and postgre SQL database are
the three SQL based choices when it
comes to databases gcp offers the SQL
based Cloud SQL and Cloud spanner a
relational database build permission
critical workloads now the next
parameter is networking
AWS uses Amazon virtual private cloud or
VPC
on the other hand Azure uses Azure
virtual Network we net
and gcp uses Cloud virtual Network
now let's move on to the another factor
which is market share and pricing
all these cloud services are based on
comparative pricing strategy which means
you need to pay on the basis of its
usage according to analyst the worldwide
Cloud Market Rose 35
241.8 dollar billion in the first
quarter of 2021. AWS accounts for 32 of
the market with Azure accounting for 19
and Google accounting for 7 percent
on one hand Amazon charges on a yearly
basis and on the other hand Microsoft
Azure and Google services charge on a
minute basis and also of them provide
you a standard price for you to access
these services
AWS charges roughly 69 per month for a
very basic instance with two virtual
CPUs and 8 GB of RAM and AWS largest
instance with 3.84 TB of RAM and 128
vcpus will set you back roughly 3.97 per
hour
but in Azure the same type of instance
one with two V CPUs and 8 GB of RAM cost
roughly 70 dollar U.S per month and
Asia's largest instance has 3.89 TB of
RAM
and 128 virtual CPUs
it costs about 6.79 per hour
compared to AWS gcp will supply you with
the most basic instance which includes
two virtual CPUs and 8 gigabytes of RAM
for 25 less as a result it will set you
back roughly to 52 dollar every month
and the largest instance that include is
3.75 DB of RAM and 160 vcpus it will
cost you around 5.32 dollar per hour
Amazon other than this also provides for
instances reserved instances and
dedicated host where you can look for
multiple offers and discounts
but for Azure it provides special prices
to developers based on situations or
even Azure hybrid benefit which benefits
your organization up to 40 if it loses
Microsoft software
in their data centers whereas Google
offers quite a sorted pricing to its
customer compared to other two
it gives you sustained use discounts
which activate if you use the same
instance for a month
the Primitive instance which is very
similar to Amazon spot instances
but one thing is common in all three
cloud services which is the all-offer
long-term discount
now let's have a look at the last
comparison which is companies using them
because AWS is the oldest player in the
cloud business it has the largest user
base and Community Support as a result
AWS has the largest number of high
profile and well-known clients including
Netflix
Airbnb Unilever BMW Samsung Mi Zenga and
others
with time Azure is getting a large
number of high profile customers as you
are currently boast around 80 percent of
Fortune 500 firms as customers Johnson
Controls polycom Fujifilm HP Honeywell
apple and others are among its key
clients
Google Cloud on the other hand uses the
same infrastructure as Google search and
YouTube and as a result many high-end
Enterprises trust Google Cloud HSBC pay
per 20th Century Fox Bloomberg Domino's
and other among Google Cloud's many
clients hey guys this is chidanan from
Simply learn and I welcome you to this
short tutorial and demo on kubernetes
kubernetes specific to AWS Cloud
platform so what's a part of this
tutorial and demo what's in store for
you at our offset I would like to cover
the basics of orchestration tools as
most of you would know kubernetes is one
of the most popular orchestration tools
in recent times specifically for
applications which are Cloud native and
deployed on some or the other types of
containers but what are these
orchestration tools why would one need
to use orchestration tools what are the
facilities of features provided by this
orchestration tools that's the first
thing that I'm going to cover after that
I will pick two orchestration tools
specific to container management Docker
swarm versus kubuntis I am going to
compare them with regard to the features
that they provide the use cases that
they cover what facilities that they
provide and when to use what after that
I will get into little bit details of
the kubernetes architecture what exactly
is required for setting up our
kubernetes cluster what runs in a
control plane or a master node what runs
as a part of the worker node after that
I will end the tutorial by running youth
by setting up a three node kubernetes
cluster on AWS platform I will use
something called as cops which is one of
the admin tools for setting up
production grade kubernetes cluster so I
will use this to set up a three node
cluster on AWS all right now that we set
the context right let me get started so
what are orchestration tools the
application development and the life
cycle management from the time the
application is developed connecting the
source code to your continuous
integration to testing them all along
your development process and eventually
moving it down to production managing
your production servers the dependencies
that your software has the requirement
that it has in terms of the hardware the
features of fault tolerance self-healing
capabilities Auto scaling all this as
complicated over the last few years as a
part of devops one thing that everyone
is interested in is managing all these
application dependency or lifecycle
management using some or the other kind
of a tool so that's where these
orchestration tools have become very
very popular so what kind of features
that they provide is you'll have to just
let them know what are the kinds of tool
sets that is required what are the fault
tolerance mechanisms that it has to be
adopted to what kind of a self-healing
capabilities that application will have
to need and if at all there is any auto
scaling features that is required if at
all you can bake in all these specific
parameters into your tool your
orchestration tool becomes a One-Stop
shop for all your deployment and
configuration management needs that's
where this orchestration tools have
become a very very popular and relevant
specifically these days when people are
more and more interested in adopting
devops practices all right so having
said that about orchestration tools
let's concentrate on two of these
orchestration tools specifically for
container management Docker swarm and
kubuntus many of the applications that
are written for these kind of containers
are called or kind of fall into a
category of cloud native where the
application need not know much about the
underlying infrastructure or the
platform where these applications are
running so these two along with Apache
me Source there are many other container
management systems but I'm going to fake
Docker swarm and kubernetes for
comparing the features that is provided
by these two Frameworks for the sake of
comparison I picked up Docker swarm and
kubuntase just because of the reason
that both of them operate in the space
of container management they do
something very very similar to
Containers that's the similarity that
exists between these two orchestration
tools however there's a big difference
that exists when it comes to the types
of containers that both of them cater to
and also the capacity of workloads they
can be used for Docker swarm is a
cluster management solution that is
provided by Docker container Docker is
one of the very very popular containers
of recent times and in case you have
your applications that is totally
powered by only Docker containers if you
have a need where you want to run a
cluster of servers which are running
only Docker containers Docker swarm
should be your choice for your
orchestration tool kubernetes this on
the other hand can cater to any other
types of containers other than Docker
and including Docker as well so in case
you have your applications which have
got Docker in them which I've got rkt in
them which they have got Alex in them or
any other type of container kubernetes
should be your choice of orchestration
too a Docker swarm can manage up to 40
50 or 60 Max nodes so in case your
application is totally written in Docker
containers and the load or the expected
cluster size is about 50 60 nodes Docker
swarm should be your choice of
orchestration tool on the other hand
kubernetes is something that was open
sourced by Google and the kind of the
scale of operations that kubernetes can
cater to is Google scale so if in case
your applications are more than a
thousand nodes that is where kubernetes
comes into play that's the big
difference that exists between Docker
swarm and kubernetes putting those
differences aside let me compare docus
warm and kubernetes based upon other
features which are similar in nature the
first and important feature that
kubernetes provides is something called
as Auto scaling if at all the load on
your cluster is too high and an
application is experiencing more load so
kubernetes can add new nodes to your
cluster of course you'll have to
configure kubernetes in order to have
those capabilities where it can spin up
new VMS or new nodes if at all you do
that configuration correctly kubernetes
has got the capacity or it has got the
feature where it can bring up a new node
and add it to the cluster on the Fly
based upon the load that exists at that
moment on similar lines if at all the
load is not too high it can identify few
of those nodes which have got less
number of replicas or less number of
application containers running on it it
can move them to some other node and
also scale down by deleting few nodes
from your cluster so that is the of less
of Auto scaling which is provided by
kubernetes and this unfortunately does
not exist in Docker swarm the other
feature of load balancing specifically
application load balancing so Docker
swarm gives you an application Level
Auto load balancing however kubernetes
gives you the flexibility of manually
configuring any other type of load
balancer of your choice for application
load balancing installation as I
mentioned earlier Docker swarm is a
loose cluster of containers which are
running on nodes it is very easy to spin
up a new node and then connect them to a
swarm and this can be done in a very
very loose coupled way where you can
create swarms of your choice notes are
allowed to connect to swarms and quit or
leave the Swarms on the fly so in and
all the installation is pretty easy and
fast kubernetes on the other hand the
configuration of kubernetes is the way
to spin up a big cluster specifying the
size of the node how many Master known
so many config planes that you want how
many nodes that you want it's a pretty
tough thing to bring up a kubernetes
scalability kubernetes strength is very
very strong they are very tightly
coupled and even they have the
capability where on the cluster size
things or the nodes can be increased on
the Fly based upon the requirement on a
Docker swarm the cluster strength is
weak as compared to kubernetes worker
nodes can be added to a swarm worker
nodes can be asked to leave us form or
can be taken out of a swarm based upon
the requirement so this is kind of a
Loosely coupled architecture for Docker
swarm while kubernetes the cluster is
very tightly coupled since Docker swarm
runs only Docker containers containers
find it very easy to share data and a
lot of other things with each other
because they all have the same signature
they are all from the same family so
it's very easy for them to share not
just volumes but a lot of other things
however for kubernetes since it manages
containers of different types if
application as to share some data across
different containers there's a little
bit of a complexity in how you would
want your containers to share data also
when it comes to kubernetes kubernetes
groups containers in something called as
pods while a pod can have either one
container that's a preferred choice or
multiple containers and the idea of pod
is like each pod can run in any other
node it's not guaranteed that you would
want to have two or three parts to be
running on the same node that makes data
sharing a little bit of a different
thing compared to Dockers form uh GUI so
there's not a good enough UI tool in
Dockers form at least the CE edition of
it which will allow you to get to know
what is running on your containers what
is the size of the containers what is
the volume of the containers and all
that stuff there are some free and open
source tools like portaner which gives
you a good visibility into your running
Docker containers however there's
nothing at a swarm level that is
provided by docker abilities gives you
an out of the box dashboard which is
easy to configure and set up you can
also Club it with some metrics services
and you can also get information about
the size of the cluster that is running
what is the load on the nodes and stuff
like that all this across your cluster
in a very beautiful dashboard
perspective so that way this is little
bit of a good UI for kubernetes file for
Dockers form there isn't anything that
is provided out of the box let me spend
some time in explaining a little bit
about the kubernetes architecture what
comprises of the kubernetes cluster what
resides in a master or a control plane
and what resides in a worker node this
is a very high level depiction of a
kubernetes cluster so this is the master
node which has got something called as a
cluster store a controller a scheduler
component and an EPA server and these
are the bunch of nodes that are
connected or administered by the master
node the master node can be 1 3 Phi
typically an odd number this is as per a
typical cluster management thing where
you would need the master nodes to be in
odd numbers so that whenever there's any
contention in terms of what needs to be
deployed where or where to give the job
to whom and stuff like that all the
Masters would cast their vote and they
would decide on the outcome so that's
the master node and there are a bunch of
nodes which can be connected and
administered by the master node cubic
CTL or the command from using which
anybody can assign some workloads to the
cluster can also be run either on the
same node or on a separate node so Cube
CTL is the command line tool that we
will be installing and using in order to
fire up our commands to our cluster so
if at all I have to put up a job on our
cluster if I have to give out any
specific commands to my cluster all this
is done using cubic CTL there's a bunch
of rest apis which is used by Q CTL and
Cube CTL will talk to the API server and
fire up the commands specific to my
cluster what runs in the master node
Master node without saying it's the most
important component of any of the
cluster if at all you're running
um Thruster with only one must node if
your master node goes down there's no
other way that you know you can or any
user can talk to the different nodes in
the cluster so Master node is the most
vital component responsible for the
complete kubernetes cluster there's
always one node that should be running
as a master node that's the bare minimum
requirement for your cluster so what are
the other components in the master node
the most important one is called etcd so
this is nothing but a data store a value
of key value pair which is stored which
contains all information about your
cluster so all the configuration details
which node is up which worker node is
down all this information is stored in
the cluster store so all the managers
would access this cluster store be 4
they go ahead and decide any other work
item or anything else that has to be
done specific to your cluster what's in
the controller as the name says
controller is something it's like a
Daemon server that is running in a
continuous loop all the time it is
responsible for controlling the set of
replicas the kind of workload that is
running based upon the configuration
that the user has set in so if at all if
you are aware of something called as
replication controllers endpoint
controllers namespace controllers all
these controllers are managed by the
controller component if any user asked
for some three replicas of a particular
pod or a service to be running and if at
all any of the nodes goes down or the
Pod goes down for whatever reason the
controller is the one who wakes up and
assigns this particular job or the Pod
to some other available node by looking
up for the details using in the cluster
store that's the importance of the
control manager or the controller the
scheduler the scheduler assigns the
tasks based upon whoever is asked for
any job to be scheduled based upon a
time frame or based upon some criteria
it also tracks the working load as to
what what exactly is the load who is
running what in the cluster and places
the workload on whoever is the available
resource at that time all right APA
server this is one other important
component in in our kubernetes cluster
where how would the end user deploy or
give out any sort of a workload onto
your cluster all the requests come to
the API server so this is an entry point
for all your requests that come into
your cluster anybody wants to deploy
something anybody want to scale up a
controller anybody who wants to bring
down few Services anybody wants to put
in a service all this will have to come
in as a part of a rest API endpoint and
API server is the entry point in case
you don't want want you know somebody to
access your cluster in case you want
only specific people or specific users
to be running some specific workloads
you can set all those role-based access
control for this API server so this is
the entry point for anyone who wants to
submit any job to your cluster so that's
a quick overview about the master node
what what are the important components
of a master node now let's go over to
what runs in a worker or a slave node as
I mentioned earlier slave nodes are
something where the job that is
submitted to your cluster is eventually
run as a pod as a service as a container
so in kubernetes world there's something
called as a pod a pod is nothing but a
combination of a container it is a
wrapper around your running containers
so slave nodes are the worker nodes
typically run these parts so that is
where the whole workload typically gets
run but there are a bunch of other
components in the in the node which also
manages what runs in the Pod who has to
have access to the Pod what is the state
of the Pod is it in a running state is
it going down for some reason and all
this stuff so let's quickly go over
those components that is there in the
slave node the most important component
in my opinion that should be on the on
the Node should be the container runtime
as I mentioned earlier kubernetes can
run any different types of containers
not just Docker so in case you want to
have a node which wants to have Docker
running on it rkt running or at LKC
running on it or any other container
environment that is running on it you
have to ensure that the specific
container runtime environment is
available on that specific node so that
whenever a job is submitted description
of what needs to be run as a part of the
Pod what should be the image with it
should be powered and what kind of a
container to spin up so that's what you
know when the job is finally assigned to
this particular node it would definitely
need a container runtime to be up and
running so that exists only if at all
the container runtime is installed and
running on your kubernetes worker node
okay now that we know what our
kubernetes node can run how would
somebody assign a job to our node that
is using something called as a cube
alert as the name says cubelet or cube
alert is a small subset which talks to
the cube API server so any node which
has to run any kind of a pod all these
instructions are passed around by the
qbps server to the tube light and
cubelet is capable of processing
whatever job that is assigned to it and
ensuring that so many parts and so many
services are spun up based upon the
requirement the last component that
exists in the worker node is the cube
proxy or kubernetes proxy this plays a
very very important role acting as a
load balancer and a network proxy so
what typical happens is whenever the
pods are running in nodes the parts can
be typically running in any node there
is no um of affinity towards any node on
which these parts are running because
pods or containers are something called
as ephemeral they can run anywhere so
how would somebody reach out to these
applications that is running in some pod
which is running in one container now
and running in probably another
container another node all together
tomorrow so that is where your
kubernetes proxy contains a picture and
this component will ensure that any
container that is spun up or any pod
that is spun up it keeps track of all
these spots and kind of connects the end
points or acts like a DNS server so that
it knows when somebody is trying to
reach out to this particular service
which is the pod on which the service is
typically running so that plays a very
very important role by acting as a load
balancer and a network proxy now that at
a very high level we know what are all
the components that make up of our
kubernetes cluster let's go ahead and
create a small cluster we'll spin up
this cluster on AWS platform finally we
get to the demo section of my tutorial
now that you guys are aware as to what
is kubernetes what is the use of
kubernetes you also know at a very very
high level what are the components of a
cluster what is a master node or a
control plane what are the components of
a master node and what should be
existing in a worker node probably
you're thinking that is maybe pretty
hard to kind of set this up so let me
demystify that by running you with a
quick demo of how to set up a simple
free node kubernetes cluster using a
tool called cops and I will be doing
this whole setup on my AWS Cloud okay so
what is cops cops is nothing but a
simple tool this is an admin tool that
allows you to bring up production grade
kubernetes environments as I said
setting up a kubernetes cluster on a
bare metal is little challenging so that
is why I would use something called as a
cops and I will use this on my cluster
which I'll spin up on my AWS instance so
I have an AWS account and what I'll do
first is I I will first pin up an ec2
server I will power this with one of the
Linux Amis and I will install cops on it
now this will be my starting point from
where I'm going to trigger running up a
big cluster in our case I'll try to keep
it little small I don't want too many
nodes in my cluster I'm going to have
one master node and two worker nodes
this whole operation would be done
programmatically by installing this cops
admin tool so as you may know AWS is
little hard for me to programmatically
run other components or rather bring up
servers it's not pretty simple so what I
would do is I will create an IAM role
which will attach to my S2 server so
that my ec2 server gets powered with all
the required role so that it can go
ahead and spin up cluster based upon my
configuration that I'm going to specify
all right so once I have my cluster set
up I will also install cubic CTL which
you probably know by now is nothing but
account Mainline utility using which I
can kind of connect to my cluster view
out some jobs put some parts and stuff
like that so I will use Cuba CTL I will
also have a pair of SSH keys so that
from this machine I would be able to
successfully get onto the master node
and submit jobs on my behalf so let me
Begin by first logging into my AWS
account all the steps that I will be
following to bring up my AWS cluster
kubernetes cluster will be documented
and I'll be sharing you this document in
a GitHub repository so that in case
anybody wants to try you'll be more than
happy to find all the required
instructions in one place okay now the
first thing that I would need is to
launch an AWS ec2 instance now this is
an instance where I am going to install
cops and I will use it as a base server
from where I'm going to fight some
commands which will bring up my
kubernetes cluster so let me log into my
AWS account and let me stick to one one
particular region and let me bring up
some a one ec2 instance all right so let
me launch my instance let me choose
um it doesn't matter in the ways but in
case you want to try it on your free
tire I would recommend choose free tire
only and choose an Ami of your choice
let me choose this instance I will make
a T2 micro that is good for me
configuration instance details all right
I don't want to change anything here
that looks good maybe I will change this
30 gig that's good for me let me add a
tag to my instance I'll name this as my
cops server alright I would need a bunch
of ports to be opened up so what I'll do
is I'll create a new security group
called open to the word I would not
recommend that you do this since I don't
want to it will take a long time for me
to specifically uh pick and choose the
ports that has to be opened for running
my cluster I don't want to do that I
will open up all the HTTP and https
ports so that it's quicker so I will say
all TCP I would say from anywhere just
to be on the safer side also open HTTP
and https and I will make this anywhere
and anywhere this is good for me I will
say review on launch all right so I
chose a T2 micro instance just for the
sake of easiness I've opened up all the
ports and the instant details are you've
seen what is that I've chosen all right
now important part is like I would need
a keypad I need to create a new keypad I
will call this as simply learn keys I
will download this key pair and then I
will launch the instance it will take
some time for my ec2 instance to come up
in the meanwhile what I'll do is I will
convert I have this pem file which is
the pair of keys the SSH keys with which
I need to log into my ec2 instance so
I'll need to convert this because I'm
trying to connect from my Windows box I
would need to convert this into a PPK
file so I have putition I am going to
load my set of keys here and convert
that key into a PPK key all right open
all right I would say save hang on Save
privately yes I would say you simply
learn or simply
non-private private key I will save this
here done that that's all so this is my
public key which is the pem file and
this is my PPK or my private key Now
using the private key I will log into my
ec2 instance okay now my ec2 instance is
up and running it is running in this
Ohio region and this is the IP address
of my public IP address of my machine so
let me connect to it so I use mobile
exterm which is nothing but an SSH
emulator you can use putty or any of
these sessions which allow you to do an
SSH this is my ipe address of my machine
and since I've spun up an Amazon Ami the
user this is the default user I need to
specify the private keys for my session
and this is the private key for my
session say okay I am in my Amazon ec2
instance so let me look at what are the
other things that I need to do all right
so as I said I just brought up my ec2
instance but I would need my ec2
instance to run few things on my behalf
so that it can spin up easy to other ec2
instances it can also talk to an S3
bucket where I'm going to store the
instant state of my kubernetes cluster
also some sort of an auto scaling group
because I want to spin up more and more
instances I also want to have a private
hosted Zone using my Route 53 so I would
need my ec2 instance to have all these
sort of permissions for my server so
what I would do is I'll go and create a
permission a rule in the ec2 or rather
in the AWS IM role and I will attach
this I am role to my ec2 instance all
right so let me go to my IAM which is
nothing but the identity and access
management I will create a role called
as possible Corps role I will create a
role this role will be used by my ec2 so
I'm going to click on ec2 and say next
permissions they would need a lot of
permissions uh specific permissions to
be very honest and the permissions are
all listed out here S3 ec2 and all this
stuff just to keep it pretty simple what
I would do is I'll create a role with
the administrative axis all right so I
don't want any tags I will review I'll
create a role name I will say cops fold
for ec2 all right so I'm going to create
a rule which has got administrative
access role so I'm going to create a
role so this would ensure that my ec2
instance from which I am going to run a
my cluster would have all the
prerequisite permission so that it can
go and spin up instances talk to S3
buckets and all that stuff all right now
let us have our running instance get
powered by this role so I will connect
this role to my running instance alright
so this is my easy to instant that is
running I will say action attack
attach attach attach replace IM roles so
there is no rule as of now I would want
my cops role for e-0 that I created so I
want this new rule to be assigned to my
ec2 all right great now my ec2 instance
which is my cop server has got all the
required permissions so that he can
create a cluster on my behalf great now
before this let me do a sudo yum update
hyphen y so that any of the because this
is a newly provisioned VM I just want to
ensure that all my library system
libraries are all updated so that when I
do install cop and Cube CTL and all
those things none of them will fail
because of some dependency or package
issues so I'm just running a sudo update
iPhone y so that all the libraries are
updated okay looks like it is done so
let me go ahead and install cops cops is
a pretty simple installation it is
available in a particular location I
have to just copy this and ensure that I
do call and I install this particular
stuff all right so it is fetching the
cops tool it's installing for me once it
is copied down let me change the mode so
that I can execute it and then also let
me move it to my user local bin so that
it is available for me in my path okay
that's pretty much the cops installation
let me also do one other thing let me
install something called as cubic CTL so
this is what would be you know a tool
using which I'm going to be firing my
commands to my kubernetes cluster once
it comes up right this is a pretty
smaller executable so keep sitting here
all right so I have Cube CTL as well as
cops installed on my ec2 server now okay
now what I'll do next is I'm going to
create an S3 bucket in AWS so that the
equivalent is a cluster state is
persisted in this bucket so let me
create a simple bucket with this name S3
bucket with this name so let me go to S3
and let me create a simple bucket with
this name name I will just say create a
bucket okay so this simply learn dot
kubernetes is the bucket that I created
so this bucket will store all
information about my cluster let me now
go and create a DNS entry or a DNS zone
or AWS Route 53 hosted zone for my
cluster so this is required so that you
know I can give a cluster name for my
kubernetes cluster that I'm coming up so
I will create a private hosted Zone and
possibility possibly I will call this as
simply learn.in so this will be my name
of my private hosted Zone in case you
already have any other public hosted
zones in your name you can always put a
Route 53 hosted Zone specific for your
domain name since I don't have any one
I'm just going to create a simple
private hosted zone so let me head down
to Route 53 click on any of these and
you'll get these hosted zones out here
so I'm going to create a hosted Zone
here I'm going to create a hosted Zone
click on create hosted Zone um this
would be simply learn.in I want to
create a public no I don't want public I
want a private hosted Zone and I'm going
to associate my VPC ID for this hosted
Zone since I'm going to try out all my
exercises in the Ohio region I will
associate the VPC of the Ohio region for
this all right so this is the one that
is specific to the Ohio region so I will
say this one I'll say create a hosted so
great now let me come back to my ec2 box
my ec2 is all powered with whatever it
needs in order to go out with the
installation of my cluster only few
things that I need to take care of is
that you know now that I put a name for
my cluster I also have an S3 bucket that
I've configured for my cluster I will
have to ensure that I need to put in
these two configurations as a part of my
cluster building activity so I will open
my bash RC file and I'm going to export
these two variables if at all you
remember well these two variables are
nothing but the cluster name which is
nothing but the public or sorry the
private hosted Zone that it created and
the S3 bucket where I'm going to create
or rather I'm going to store my cluster
state so let me copy these two and open
up my bash RC file all right so I will
just add these two and copy these and
Export them out as my variable I'm going
to save this here now let me ensure that
this gets picked up all right I've got
these two that have configured and I've
also ensure that this environment
variables are set now let me create a
pair of SSH keys this is to ensure that
I can log into the box that I'm going to
be provisioning as a part of my cluster
SSH iPhone Keygen I don't want to give
any parameters let it go to the home
directory with the default name and
without any passphrase all right so I
created a bunch of my keypad now I'm all
set to go ahead and run my cops now cops
will ensure that this will take some
time for the cluster to come up but this
is exactly how I would Define my cops
command I already got a copy here so
cops create cluster the state of the
cluster will be stored in this
particular variable which is what I have
exported out the number of node count is
two node count is the worker node count
if at all I don't specif there's a
configuration for specifying the master
or the control pane as well if I don't
specify that the default one is one so
I'm going to create uh one primary or
the master node and two worker nodes uh
size of my master Note size of the
worker nodes uh what are the zones where
I want this to be created and where do I
store the cluster information and okay
I've already added this must account
this is actually not required but this
is the command that I'm going to fire
off so that I bring up my cluster all
right that went off uh very fast but um
this actually did not create a cluster
it is just the definition of a cluster
and that's why this came out very fast
now that everything looks good with
whatever configuration that is specified
now let me go ahead and create the class
intervacing cops update cluster hyphen
iPhone 8 yes now this will take some
time a good 5-10 minutes for it too
because this will actually start
provisioning the servers and as I
mentioned earlier based upon my
configuration I'm going to come up with
one master server and uh two nodes or
the worker nodes all right so this is
the command for me to validate the
cluster and I'm going to try it out
first I'm pretty sure that will fail
because all my servers are not up yet it
is taking some time the validation
everything failed but let me try to look
at my ec2 instances and see how many
servers do I have running as of now if
you see I had only one server that had
started which was my cop server and
automatically the other three instances
one is called notes dot simply not in
and this these two are the nodes and
this is the master node so these three
got automatically provisioned by the
cops command that I ran all right so
this may take a while for it to get
validated a good five to ten minutes so
what I'll do is I'll pause the video now
and I'll come back once on the server
the cluster is up and running it's been
a good uh eight to nine minutes uh since
I started my cluster so let me validate
now okay seems good so there's a master
node uh minimum One Max one some nodes
which are nothing but the slave nodes or
the worker nodes there are two of them
due to micro uh subnet so my clusters
seem to be up and running and my cluster
name is billon.in so what I would want
to do is now let me just log into the
master and see if I can run some parts
so let me get back to my installation
steps here the variation cluster is done
so let me log into my since my cluster
name is simply learn.n this is the way
to log into my box so let me get into my
logs so if you see here this is the host
name of the machine that I'm currently
in if you see this this is nothing but
our this cop server this is a server now
from here I am trying to get into the
master box all right so if you see the
IP address has changed I was in a
different box I'm in a different box now
if I see host name you'll find our
different host name so this is 153 which
is nothing but the master node yep
that's 153 this is the particular host
so I'm getting into this machine now so
I started the cluster from my cop server
here it ran and brought up three nodes
so I'm actually getting into my master
node and see if I can run some bots on
it alright so let me try Cube CTL get
cluster info right Cube CTL get nodes
all right so there are three nodes here
Master node and one master node and two
worker nodes so let me see if I have
some pods here cubicle get pods so
there's no pod as of now so what I'll
try to do is let me just pin up a very
very simple Pawn just to check if my
connections is everything is correct or
not correct I have a simple node.js
application that I have built and I've
got a container for that this is already
pushed to the docker up registered is
called simply on Docker Hub and the
image name that I have here is called my
node app and I will use this image to
power one of my parts that I will run so
this is the way in which I am going to
launch my pod let me show you these
commands Cube CTL run the name for my
deployment that I'm going to run hyphen
iPhone image and this is the image that
is going to be powering my container so
this is simply learn Docker forward
slash my node app hyphen F1 replica is
equal to I want two replicas to be
running and the port that I want to
expose this particular part is on 8080.
so let me run this as soon as this is
run it creates something called as a
deployment all right now let me just say
I'm not really interested in the
deployment I am interested in checking
effect all the parts are up and running
so I will say Cube CTL get what all
right so that was pretty fast so I have
two parts that are running here these
are two replicas because I asked for two
replicas these parts are running so I
can run Cube CTL describe pod pickup all
right I can pick up any of the Pod name
and see what is the Pod what is the
information does it contain from what is
the image that is pulling what is the
image image Name ID this is actually
running my thought which is actually
spinning up a container great so first
of good so in kubernetes the pods can
are ephemeral so they can be there at
any time cannot be runtime if I need to
expose my Pawn outside I will have to
create a service for my pod so what I'll
do is I'll create a very very simple
service by exposing this particular
deployment before that you check Cube
CDL get deployment so there's a
deployment that is created the
deployment name is simply in an app so I
will expose my deployment simply learn
app as um yeah I'll just expose this let
me see all right I'm not specifying what
type and all I don't want to get into
the complications of what are the
different types of exposing this service
and all that stuff so if I don't specify
anything it gives me something cluster
ID so this is where my pod is actually
exposed so let me just check if at all
this part is happening running I'll just
try a simple curl command or curl HTTP
colon this is my cluster IP and the port
is 880. if at all I hit this it's
actually hitting my application and
giving me uh whatever I put a simple uh
sys dot out kind of a thing where I'm
just printing
um the container ID of whatever pod is
serving uh mix out of so I'm actually
eating my container and I'm getting this
output from the content so everything
looks good my content is up and running
so let me just go and clean it up I will
say Cube CTL delete deployment simply
learn app this should get rid of all the
parts that I created Cube CTL get pod
all right these parts are interminating
things let me just check Cube CTL get
Services there's one service I don't
want this service let me delete that
keep CTL delete service I want this all
right that's good so let me come back to
my host my cop server from where I'm
running so I managed to successfully
verify this part see if everything is up
and running so what I'll do is I'll just
go ahead and complete this demo by going
and getting rid of my cluster so cops
delete cluster hyphen iPhone yes all
right so this will ensure that it will
clean up my complete cluster that I
created so I had three or four running
instances if you see that all the three
are shutting down because you know the
cop server which had cops installed on
it uh has now got a mandate to go ahead
and shut down and clean up the whole
instances and all those things that I've
created as a part of my deployment cloud
computing is becoming more prominent
among tech companies and organizations
making this
in demand in the sector
cloud-based Solutions are proven to be a
boon for company management around the
world because they provide convenience
at an inexpensive price
this means that if you want to work in
technology a job dealing with
cloud-based systems could be a great fit
hey everyone I'm shamli and I welcome
you all to this new video of Simply
learn on how to become a cloud engineer
according to Market and markets the
global cloud computing industry was
valued at
371.4 billion USD in 2020
it has been predicted that cloud
engineer will be among the top 10
in-demand ID jobs in 2021
therefore in this video we will come
across some of the very important
aspects needed to become a cloud
engineer like first of all we will
understand what is cloud computing
then we will understand who is a cloud
engineer
after that we will look into the
technical skills needed to become a
cloud engineer
after that we will understand what are
the roles and responsibilities of a
cloud engineer
then we will have a look into the road
map of becoming a cloud engineer
and what is the average salary of a
cloud engineer in both the USA and in
India
and at last we will look at the
company's hiring Cloud engineers
if getting your learning started is half
the battle what if you could do that for
free visit skillup by simply learn click
on the link in the description to know
more
thing we will come across is what is
cloud computing
cloud computing is the distribution of
Computing Services via the Internet
or the cloud including servers storage
databases networking software analytics
and intelligence to provide faster
Innovation more flexible resources and
economies of scale
you usually only pay for the cloud
services you use which helps you cut
costs run your infrastructure more
efficiently and scale as a business
grows
that brings us to who is a cloud
engineer
Cloud Engineers are the I.T experts in
charge of accessing an organization's
technology infrastructure and
transferring operations and procedures
to a cloud-based architecture
they assist with the migration of
critical corporate applications and
processes to private public and hybrid
Cloud environments
a cloud engineer's job is to access a
company's ID infrastructure and migrate
certain processes and operations to the
Cloud public private and hybrid cloud
computing systems are among these
functions and operations
now that we know who a cloud engineer is
let's have a look at the skills required
to become one
Cloud Engineers must have a diverse set
of technical talents that they will use
in a variety of positions
several key skill sets that cloud
Engineers need to be successful include
the ability to program and code
cloud-based applications and processes
the ability to troubleshoot technical
challenges through problem solving
the ability to plan and design software
and Cloud applications using critical
and creative thinking
should also include data analysis skills
and should be able to strategically plan
and to fulfill projects using Technical
Resources
now let's have a look at the roles and
responsibilities of cloud engineer
there are three types of cloud engineers
in particular in a small business one
individual may be in charge of all three
roles
Cloud architect
Cloud software engineer and Cloud
security engineer
Cloud architect and organizations cloud
computing strategy include application
design management and monitoring is
overseen by a cloud architect
Cloud Architects connect various aspects
of cloud computing including data and
networks with tools and services
Cloud software engineer a cloud software
engineer create software for cloud
computing systems and isn't in charge of
managing their development and
maintenance
Cloud security engineer control based
Technologies and procedures that enable
Regulatory Compliance regulations are
included in security
when dealing with the cloud the major
purpose is to protect information data
data applications and the infrastructure
now let's focus on the roadmap to become
a cloud engineer
Cloud Engineers generally begin their
careers with a four-year degree in
computer technology
and the steps below Define the
Fundamental part to becoming a cloud
engineer
obtain a bachelor's degree
in software and systems infrastructure
Computing or other technical areas
your bachelor's degree program should
emphasize software and system
infrastructure Computing and other
technical courses
a bachelor's degree in a computer or
technology related field will also
qualify you for advanced degrees if you
choose to pursue them
learn programming languages python C
plus plus Java and Ruby for example are
great options to start with
aside from programming languages you'll
want to brush up on your Knowledge and
Skills with some of the most popular
cloud services including AWS Hadoop and
azure
it's important that you gain experience
in cloud computing via internships and
open source projects
this will help you build a portfolio of
work that you can use to demonstrate
your competence in future job interviews
it will also teach you vital skills that
you'll need to need on job
consider a master's degree
to enhance your cloud computing
abilities while also advancing your
career because of their higher level of
Education Cloud Engineers with a
master's degree in a tech subject may
have additional career prospects a
master's degree in software engineering
or system engineering for example will
help you develop more of these technical
abilities and expertise advancing your
career
get cloud computing certification
to demonstrate your understanding and
skill level in Cloud engineering you can
also check out our official website for
cloud computing certification it will
help you enhance your knowledge in this
particular field
now that we know everything about cloud
computing and how to become a cloud
engineer let's have a quick look at the
average salary of a cloud engineer in
India and in the USA
in India the average salary of a cloud
engineer is rupees 12 lakh 41 000 per
annum
and in the USA the average salary of a
cloud engineer is
128
837 dollars per annum
now Cloud Engineers may earn a
substantial income with a national
average salary but it can vary though
depending on your location and how many
years you have been in the field for
example a cloud engineer with a master's
degree and several years of experience
may make more than a new graduate with a
four-year degree who is just starting
out in their career
now let's dive down and explore the
company's hiring Cloud engineers
cloud computing is gradually
establishing itself as the industry
standard for data storage and
administration by 2022 according to IBC
over a million cloud computing jobs
would be generated in India Amazon IBM
Wipro Infosys Oracle Cisco these are the
few top companies who are some of the
top Recruiters in this field every
Journey begins with a solitary
foundational level certification and for
AWS that starting point is the AWS
certified Cloud practitioner
hello everyone I'm Shanley and I welcome
you all to this new video of Simply
learn on how to become an AWS certified
Cloud practitioner
the AWS certified Cloud practitioner
certification gives you a high level
overview of AWS it doesn't go into a
great detail about in one service
instead focuses on the overall structure
of AWS and a basic understanding of what
the various service accomplish
so in this video I will take you through
the path of becoming
a successful AWS certified Cloud
practitioner starting with what is an
AWS Cloud practitioner
followed by the eligibility criteria for
the exam
then we will discuss the exam objectives
then we will look and do the exam
content
followed by how to prepare for the exam
then we will have a look at some of the
sample questions for your reference
and then we'll discuss some tips you
should remember while taking the exam
so without any further delay let's get
started with the video
AWS Cloud practitioner
the AWS certified Cloud practitioner
certification is a foundational level
test designed for people who can
successfully show an overall
understanding of the AWS Cloud according
to Amazon
the skills necessary for specific
employment types such as Developers
administrators and solution Architects
are not assessed by this certification
AWS recommends that you have
at least six months of AWS Cloud
experience
now let's look into the eligibility
criteria for the exam to proceed further
in one with a basic understanding of the
AWS platform should take the exam
we recommend that you have the following
factors before taking this exam
first
six months of AWS Cloud experience
second understanding ID services and how
they are used in the AWS Cloud platform
is a must
and third is basic understanding of AWS
services and use cases invoicing and
pricing mechanisms security Concepts and
how the cloud affects your organization
moving on let's have an overview of the
exam
the exam consists of 65 questions and
has a 90 minute time restriction
pass the exam you must get at least 700
out of 1000 points that is 70 percent
the exam has multiple choice one right
response from four possibilities and
multiple response questions two correct
responses from five options
the cost of applying for the exam is
hundred dollars and the certification
needs to be renewed after every two
years
now that we know of the eligibility
criteria for the exam let's move ahead
and have a look at its objectives
it will enable you to articulate the
value proposition of AWS cloud
next you will be able to define the AWS
cloud and AWS Global infrastructure with
it
also it will help you understand and
explain the fundamental AWS Cloud
architectural ideas
it will enable you to describe the AWS
platform's essential security at
compliance feature as well as the shared
security model
you'll be able to Define invoicing
account management and price models with
it
and it will also enable you to describe
the fundamental core properties of AWS
Cloud deployment and operation
now let's proceed further and have a
look at the content for the exam for
better and organized preparation for the
exam
the required knowledge is divided into
four domains for the exam these are
various objectives within each test
domain that broadly reflect the
knowledge and expertise required to pass
the exam
Cloud concept this domain makes up 28 of
the exam security this domain makes up
24 of the exam
technology makes 36 percent of the exam
and billing and pricing makes up 12 of
the exam
the following three goals are included
in Cloud Concepts they find the AWS
cloud and the value proposition it
provides recognize characteristics of
the AWS Cloud's economics
and make a list of the various Cloud
architecture design principles you
should be able to explain the advantages
of public cloud services and Define the
services that are offered on AWS like
iaas eaas and SAS
you must comprehend the financial
benefits of cloud computing and the
distinction between
capex and Opex this relates to item 1
you should be familiar with Cloud
architect textural design principles
such as loose coupling scaling
bootstrapping and automation to mention
a few
now comes security
you should be familiar with the AWS
shared responsibility model which
specifies who is in charge of which
component of the technology stack from
the data center to servers firewall
rules and data encryption
AWS offers tools and services for
deploying security evaluating your
security posture and creating alert and
compliance reports you must be have a
thorough understanding of these services
and tools in order to describe their use
and benefits now comes technology you
must know what the basic AWS services
are and how they are used
you usually don't need a thorough
understanding of a Services specifics
but you do need to comprehend its goal
benefits and use cases easy to ECS
Lambda light sale EBS EFS S3 RDS
dynamodb redshift elastic load balancing
Auto scaling Cloud front root S3 Cloud
watch Cloud fail and SNS are all core
services
you should be familiar with the AWS
Cloud's worldwide infrastructure regions
availability zones and Edge locations
are all part of this
then comes billing and pricing the
majority of AWS services are available
on a pay-per-use basis
however there are also alternators to
save money by signing one or three year
contract with a variety of payment
options
you must comprehend these models and the
services to which they apply
make sure you know what AWS will charge
you and what will be free inbound data
transport for example is free whereas
outbound data transfer is usually
expensive
VPC cloud formation and IAM are all free
services for the resources you generate
with them may not be you must be aware
of the potential for cost to arise now
let's have a look at how to prepare for
the exam
step one
start with AWS training classes AWS
Cloud practitioner Essentials AWS Cloud
practitioner technical Essentials and
Cloud Business Essentials are a few
courses that will help you build your
knowledge in AWS Step 2 go to exam
guides for AWS certified Cloud
practitioner certification because they
give you an idea of important Concepts
to focus on for the exam as well as give
you an overview of all the exam
objectives and the pattern of questions
step 3 get familiar with subject areas
before the exam analyze the important
subjects for the exam and keep polishing
your knowledge in that area by constant
revisions
step 4 sell study to build your strength
self-study is ample to clear this exam
just go through the AWS training go
through the study material available
online and keep revising
step 5 examine sample questions and take
a free practice test practice sample
papers from Wheels website present
online to see where you stand and how
much you know and how much time you take
to keep it track of your performance and
improve yourself
and six
scheduled exam and get certified once
you feel you are ready prepared and
confident enroll for the exam choose a
center near you for the AWS Cloud
practitioner exam website and just
register yourself
so let me give you a glimpse of some
sample questions based on each domain
first question
which feature of AWS allows you to
deploy a new application for which the
requirements may change over time
Option 1 elasticity option 2 fault
tolerance option 3 disposable resources
and option for high availability
the answer is option one you can deploy
your app without worrying about whether
it will require more or less resources
in the future thanks to elasticity
infrastructure can scale up and down on
demand thanks to elasticity
question two which AWS service is used
to enable multi-factor Authentication
Option 1 Amazon SDS
option 2 AWS iam
option 3 Amazon ec2
and option for AWS KMS
well the answer is option 2 IAM is used
to manage multi-factor authentication
and to securely regulate individual and
group access to AWS resources
question 3 a company would like to
maximize their potential volume and RI
discount across multiple accounts and
also apply service control policies on
member accounts what can they use to
gain these benefits
Option 1 AWS budgets option 2 AWS cost
Explorer option 3 AWS IAM option 4 AWS
organizations
well the answer is
option 4. AWS organizations allow you to
create groups of AWS accounts and then
administer policies across all of them
from a single location
Consolidated billing is available in
both features set of AWS organizations
allowing you to set up a single payment
method in the organization master
account while still receiving invoices
for individual activity in each member
account
question 4 what advantages do you get
from using the AWS Cloud you have to
choose any queue
Option 1 trade Capital expense for
variable expense option 2 stop guessing
about capacity
option 3 increase capital expenditure
option 4 gain greater control of the
infrastructure layer and option 5 comply
with all local security compliance
programs
well the answer is Option 1 and 2.
you may pay on a variable Opex basis for
the resources you use and grow on demand
using public cloud services like AWS so
you never have to guess how much
resources you need to deploy
question 5 under the AWS shared
responsibility model what is the
customer responsible for choose to
Option 1 physical security of the data
center option 2 replacement and
disposable of this driver
option three configuration of security
groups option 4 patch management of
infrastructure and option 5 encryption
of customer data
the answer is option 3 and 5. customers
are responsible for creating security
groups Network ACLS updating their
operating systems and encrypting their
data while AWS is responsible for the
physical security of the DC the
replacement of obsolete describers and
infrastructure patch management
question 6 what are two ways an AWS
customer can reduce their monthly spend
fuse 2 option one
turn off your resources that are not
being used
option 2 use more power efficient
instance type
option 3 Reserve capacity where suitable
option 4 be efficient with the usage of
security groups and option 5 reduce the
amount of data in dress charges
the answer is Option 1 and 3.
turning of resources that aren't in use
can help you save money reserved
instances can also help you save money
on a monthly basis without committing to
a one or three year contract which is
ideal for predictable workloads
question seven
what are the advantages of availability
zones choose to option one we allow
Regional Disaster Recovery option two
they provide fault isolation
option 3 to enable the caching of data
for faster delivery to end users
option 4 we are connected by low latency
network connection
and option 5 they enable you to connect
to your on-premises networks to AWS to
form a hybrid cloud
the answer is option two and four
each AWS region has numerous
availability zones which are unique
locations each AZ is designed to be
isolated from other failures a data
center is an EZ and in some situations
an EZ is made up of numeral data centers
with an area here that provides low cost
low latency network connectivity to
other zones within that region this
allows you to synchronize lead duplicate
your data between data centers allowing
the failover to be automated and visible
to your users
question 8
which AWS support plans Provide support
via email chat and phone
option
for Global and option 5 Enterprise
the answer is option 2 and 5. only the
business and Enterprise programs offer
emails chat and phone assistance
now that we have discussed all the
important aspects required for AWS Cloud
practitioner exam let me give you some
tips you should remember while taking
the exam
first start by answering questions you
are familiar with to prevent spending
time on tough ones
it will give you confidence as well as
save you from wasting your time on
trying tough questions
second keep track of your time and learn
how to manage it in order to complete
all the questions distributed time
according to the time available to the
number of questions so that you can work
on each question precisely
third analyze the question first then
respond as there may be traps in the
questions focus on keywords and try to
read them two to three times for better
understanding sometimes it is said that
answer lies in the question itself
fourth because there may be numerous
answers to a question read and choose
your answers carefully if you feel there
is a chance that multiple answers are
correct for one question then do go for
it and analyze it wisely before coming
to that conclusion well this was all
about it today I'm going to tell you how
you can become an AWS Solutions
architect so who is an aw Solutions
architect now the main role of an AWS
Solutions architect is to help you
deploy your applications on the AWS
platform now this is nothing but a cloud
computing platform so it's not to say
that you can't deploy your applications
on the cloud computing platform yourself
it's just that when it comes to
organizations the applications that you
need to deploy become a whole lot more
complex that's where an aw Solutions
architect can help ever since cloud
computing became a thing companies
around the world have started migrating
their physical infrastructure onto the
cloud that's what an AWS Solutions
architect does they help you migrate
your physical infrastructure onto the
AWS Cloud companies around the world
work on a budget and an AWS Solutions
architect will help design a cloud
infrastructure based on the
organization's budget before that can be
done however an aw Solutions architect
has to create a design with an intricate
and detailed blueprint of the cloud
infrastructure that they plan to set up
now aw Solutions Architects also have to
focus mainly on non-functional
requirements like usability reliability
scalability and performance of the cloud
infrastructure they're also responsible
when it comes to minimizing risks that
an organization can face when it comes
to cloud computing platforms now they
could face risks like security leaks
calculation mistakes and application
downtimes and AWS Solutions architect
has to ensure that these don't happen
before we can talk about how you can
become an aw Solutions architect I have
some exciting news for you guys we've
launched our own YouTube Community
you'll get to see a lot of quizzes polls
offers and much more to make your
learning experience a whole lot more fun
you can find all of this on your
subscription feed or you can also click
on the top right corner right now to get
started and let's get back to how can
you become an aw Solutions architect now
to become an aw certified Solutions
architect you need to clear the aw
certified Solutions architect associate
level examination now here's some
details about it the exam score ranges
from 100 to thousand marks and the
minimum passing score is 720. however
there's a catch the passing marks are
actually set using statistical analysis
so they can be changed based on how
difficult the examination actually is
the exam fee is 150 dollars and you can
also take a practice examination which
costs 20 dollars now regardless of the
examination you take be IT solutions
architect sysops administrator or
developer any associate level
examination costs 150 dollars for the
professional level examination it's 300
now if you want to learn more about the
AWS certifications I suggest you click
on the top right corner and watch our
video aw certifications in 10 minutes to
learn more now the exam duration is of
130 minutes and you have two types of
questions multiple choice and multiple
answer now the multiple choice questions
have four options out which one of them
is right and you have multiple answer
where you have five options out of which
two of them are correct you can take the
examination in English Japanese Korean
and simplify Chinese now let's talk
about how you can schedule an
examination first let's go to Google and
search for AWS certifications
I don't know
click on the first link
so on this page we can go to the bottom
and find the different certifications
AWS provides click on architect
and select the AWS certified Solutions
architect associate certification
click on register now
and here you need to click on the aw
certification account
you can sign in with your Amazon account
or create a new one
I already have an Amazon account that
I'll be using for signing in here in
case you don't you can click on the
create your Amazon account button here
and create an account for yourself
now after that is done you can schedule
new examination
and you can scroll down to the bottom
here aw certified Solutions architect
associate level certification
click on schedule exam
press continue
and here you need to select your
language
which I'm assuming is English
I am from India so I don't need to
change anything here but you can change
your country or time zone and other
details based on your requirement and
select your preferred month I want to do
it in October
now select search for Exam Center
select the one that's closest to you
I'm going to select this one
and here you can select the day that you
want to take your test and the time
available I want to do it on 15
I select the time that I want
and press continue
now you can go through all the details
here change them if you want to
otherwise you can press continue
after this is done
close
and here we have the final step which is
the fees
you can enter your details here
and pay now to finish the process now
let's look at an outline of the exam
content now what you're going to see in
the exam are five major domains and here
you have each of the domains with their
respective weightages first you have a
34 designing resilient architectures at
24 you have both defining performant
architectures and specifying secure
applications and architectures a 10
person you have designing cost optimized
architectures and at six percent you
have defining operationally excellent
architectures now let's look at each of
these domains in detail now the first
domain or design resilient architectures
can be divided into four parts firstly
you need to know how you can choose
reliable or resilient storage using
services like AWS S3 AWS Glacier and AWS
EBS then you have how you can design
decoupling mechanisms using AWS Services
now this is possible with the help of
AWS SNS now these aren't the only
services that enable this these are just
some of the services then how you can
design a multi-tier architecture
solution now this is important because
you need to know how you can create a
solution that involves several other
services then you need to know how you
can design highly available and fall
tolerant architectures now for the
second domain defining performance
architectures first you have how to
choose performance storages and
databases services that are used are AWS
RDS AWS threadshift and AWS dynamodb the
second step is how you can apply caching
to improve performance a service that
can be used for this is AWS elastic
cache third how you can design Solutions
with elasticity and scalability you have
AWS Lambda AWS cloudwatch and AWS data
pipeline now for the third domain which
is specifying secure applications and
architectures you need to know how you
can secure applications using services
like AWS inspector AWS cloudtrail and
AWS iaf you need to know how to secure
data using Cloud HSM and AWS Macy and
and how you can Define the networking
infrastructure using cloudfront VPC and
elastic load balancer for the fourth
domain you have designing cost optimized
architectures firstly you need to know
how you can design cost optimized
compute Solutions you can use AWS ec2
plastic bean stock Lambda and AWS light
sail then you need to know how you can
design cost optimized Storage Solutions
using AWS S3 Glacier EBS and elastic
file system and the final domain to
define operationally excellent
architectures you need to set up design
features and solutions that enable
operational excellence now some of the
features are that you perform operations
as code you annotate documentation you
make frequent and small reversible
changes and anticipate and tactile
failures now let's have a look at the
job opportunities when it comes to AWS
Solutions architects now if you have a
look at this graph you can see that AWS
has always provided a lot more job
postings as compared to the other two
giants in the cloud computing domain
which are Microsoft Azure and Google
Cloud platform now if you do a little
bit of research you'll find out that
there's a significant lacking of
experienced Personnel when it comes to
AWS so I would suggest this is the best
time for you to get certified in AWS the
Amazon web services Cloud management
system now has over a million clients
and generates 10 billion dollar in
Revenue annually
Kellogg's Samsung Unilever Nokia Netflix
Adobe are the majority of international
corporation trust the platform
so hello everyone I'm shamli and I'm
going to take you through this new video
of Simply learn on top 10 reasons to
choose AWS
through the medium of this video I'm
going to give you top 10 reasons to
choose adapt learn and explore AWS
but before moving ahead let me give you
an overview of the concept of AWS for
your better understanding
Amazon web services AWS is a cloud
computing platform that manages and
maintains hardware and infrastructure
reducing the expense and complexity of
purchasing and running resources on site
for businesses and individuals
these resources are available for free
or for a free per usage
it is a comprehensive and simple to use
Computing platform offered by Amazon
infrastructure as a service platform as
a service and package software as a
service are all used to build the
platform
so let's move ahead and explore the top
10 reasons to choose AWS starting with
the first one which is flexible pricing
options
the most popular motivation for learning
AWS is because of its cost effective
pricing alternatives
AWS provides a year of free access to
the free tire
which is idle for novices who wish to
learn about AWS technology for beginners
we believe this is plenty AWS is highly
customizable with pay as you go options
to match any company need flexible
pricing is a great approach to scale up
or down the architecture as needed
the AWS tariffs are comparable to those
for water and electricity
second reason to proceed with is the
global architecture
one of the most appealing features of
AWS is that it is offered in 24
different zones across 16 different
countries
you can connect to the server from any
country you may use all of the
advantages by hiring an AWS developer
from anywhere there are no restrictions
on who can use the reserves based on the
location
third is automated multi-region backups
AWS offers a variety of backup options
including Amis and EPS snapshots
aws's decentralized design Global reach
makes storing crucial data in several
geographical locations simple and cost
effective as a result
if your main production environment is
brought down by a natural or man-made
Calamity your backup data will be
unaffected third-party Solutions can
also let businesses automate backups
between AWS sites without the need for
internal scripting fourth reason that we
will come across is flexibility and
scalability
one of the most valuable AWS features is
its flexibility it lets you pick your
operating system programming language
web application platform database and
other services
AWS provides a virtual environment in
which your application May load all of
the services it requires
it also makes data migration easier
when demand increases or drops Auto
scaling an elastic load balancing
techniques are automatically scaled up
or down via AWS
AWS approaches are useful for dealing
with unpredictably high or unpredictable
loads
organizations gain from lower cost and
more user satisfaction as a result of
this
moving on Fifth is streamlined recovery
even a small bit of downtime or data
loss might be disastrous for some
services for others the cost of downtime
data loss is not greater than the
expense of maintaining a multi-site hot
standby recovery strategy regardless of
your organization's resilience to
downtime or data loss AWS adaptable
platform can provide you with the tools
you need to implement a disaster
recovery strategy from the event of a
disaster AWS Disaster Recovery that
restores your data fast across several
locations
sixth one is
pass offering
the most prevalent model is platform as
a service which has a market share of
roughly 32 percent and is likely to rise
in the next few years
a platform as a service solution
provides a platform for developers to
construct unique configurable software
and is a popular choice for Enterprises
looking to create unique apps without
spending a fortune or shouldering all of
the responsibilities seventh reason I
will take you through is customization
customization for AWS control tar is a
solution that combines AWS control tar
with other highly available trusted AWS
services to assist customers set up a
secure multi-account AWS environment
faster and more efficiently using AWS
best practices
customers must first have an AWS
controlled Landing Zone set up in the
account before installing the solution
eighth one is scheduling
AWS clients can start and stop services
at any moment
thanks to its service scheduling feature
you can keep track of your assets pay
for the service you use and run them at
a certain time
it enables you to create sample planners
to test consumers viability and schedule
Services as required
consider elastic Computing cloud ec2 and
relational database Service as examples
of scheduling services
you are not required to conduct these
services on a daily basis set the
timetable for the service based on the
day of the week you utilize it
ninth is security
AWS provides the same level of
world-class security to small one
startups as well as Enterprise size
behemoths
the commercial data center meets the
highest conceivable standards saving you
the trouble of needing to defend your
own data centers AWS also maintains a
large number of compliance processes in
its infrastructure as well as a large
security help network that may provide
real-time information about suspicious
tasks and potential vulnerabilities and
the last one the 10th reason is third
party apis
API implies that you can manage your
cloud-based facilities in a variety of
languages in line with the platform's
entire versatility
it also means that third-party Solutions
such as yours can take advantage of all
of AWS time and cost savings feature
you can automate ordinary but necessary
AWS processes without the use of coding
from automated backups to ec2 and RDS
instances
well this brings us to the end of this
video so how can simply learn help you
with your AWS certifications now we are
on the sampling on home page
and here we have the AWS Solutions
architect course
on this course which is the AWS
Solutions architect certification
training course we have 36 hours of
instructor-led training 20 hours of
self-paced learning 16 live demos 3
simulation exams three Hands-On practice
projects and so much more here our main
emphasis is on making sure that you have
enough hands-on experience with the
services that you crack the examination
at your first attempt
so here we also go through some of the
more important services that you need to
learn like IAM VPC ec2 S3 and so much
more we'll also talk about databases
application Services security practices
disaster recovery and so on we even have
practice examinations to help you
prepare for the real one choose from
over 300 in-demand skills and get access
to 1 000 Plus hours of video content for
free visit skillup by simply learn click
on the link in the description to know
more I'm here to walk you through some
of the AWS interview questions which we
find are important and our hope is that
you would use this material in your
interview preparation and be able to
crack that cloud interview and step into
your dream Cloud job by the way I'm and
Cloud technical architect trainer and an
interview panelist for cloud Network and
devops so as you progress in watching
are you gonna see that these questions
are practical scenario based questions
that tests the depth of the knowledge of
a person in a particular AWS product or
in a particular AWS architecture so why
wait let's move on all right so in an
interview you would find yourself with a
question that might ask you define and
explain the three basic types of cloud
services and the AWS products that are
built based on them see here it's a very
straightforward question just explain
three basic types of cloud service and
when we talk about basic type of cloud
service it's compute obviously that's a
very basic service storage obviously
because you need to store your data
somewhere and networking that actually
connects a couple of other services to
your application these basic will not
include monitoring these basic will not
include analytics because they are
considered as optional they are
considered as Advanced Services you
could choose a non-cloud server service
or a product for monitoring of and for
analytics so they're not considered as
basic so when we talk about Basics they
are compute storage and networking and
the second part of the question says
explain some of the AWS products that
are built based on them of course
compute ec2 is a major one that's that's
the major share of the compute resource
and then we have platform as a service
which is elastic bean stock and then
function as a service which is Lambda
Auto scaling and light cell are also
part of compute services so the compute
domain it really helps us to run any
application and the compute service
helps us in managing the scaling and
deployment of an application again
Lambda is a compute service so the
compute service also helps in running
event initiated stateless applications
the next one was storage a lot of
emphasis is on storage these days
because if there's one thing that grows
in a network on a daily basis that
storage every new day we have new data
to store process manage so storage is
again a basic and an important cloud
service and the products that are built
based on the storage services are S3
object storage Glacier for archiving EBS
elastic block storage as a drive
attachment for the ec2 instances and the
EFS file share for the ec2 instances so
the storage domain helps in the
following aspects it holds all the
information that the application uses so
it's the application data and we can
also archive old data using storage
which would be Glacier and any object
files and any requirement for Block
storage can be met through elastic Block
store and S3 which is again an object
storage talking about networks it's just
not important to answer the question
with the name of the services and the
name of the product it'll also be good
if you could go in depth and explain how
they can be used right so that actually
proves you to be a person knowledgeable
enough in that particular service or
product so talking about networking
domain VPC networking can't imagine
networking without VPC in in the cloud
environment especially in AWS Cloud
environment and then we have Route 53
for domain resolution or for DNS and
then we have cloudfront which is an edge
caching service that helps customers get
our customers to read their application
with low latency so networking domain
helps with some of the following use
cases it controls and manages the
connectivity of the AWS services within
our account and we can also pick an IP
address range if your network engineer
or if you are somebody who works in
networks or planning to work a network
you will soon realize the importance of
choosing your own IP address for easy
remembering so having an option to have
your own IP address in the cloud own
range of IP address in the cloud it
really helps really really helps in
Cloud networking the other question that
get asked would be the difference
between the availability Zone and the
region actually the question generally
gets asked so to test how well you can
actually differentiate and also
correlate the availability Zone and the
region relationship right so a region is
a separate geographic area like the
us2s1 I mean which represents not
California or the AP South which
represents Mumbai so regions are a
separate geographic area on the contrary
availability Zone resides inside the
region you shouldn't stop there you
should go further and explain about
availability zones and availability
zones are isolated from each other and
some of the services will replicate
themselves within the available the d
zone so availability Zone does
replication within them but regions they
don't generally do replication between
them the other question you could be
asked is what is order scaling what have
we achieved by Auto scaling so in short
Auto scaling it helps us to
automatically provision and launch new
instances Whenever there is an demand it
not only helps us meeting the increasing
demand it also helps in reducing the
resource usage when there is low demand
so Auto scaling also allows us to
decrease the resources or resource
capacity as per the need of that
particular R now this helps business in
not worrying about putting more effort
in managing or continuously monitoring
the server to see if they have the
needed resource or not because Auto
scaling is gonna handle it for us so
business does not need to worry about it
and auto scaling is one big reason
reason why people would want to go and
pick a cloud service especially in AWS
service the ability to increase and
Shrink based on the need of that art
that's how powerful is auto scaling the
other question you could get asked is
what's geo-targeting in Cloud front now
we know that cloudfront is caching and
it caches content globally in the Amazon
caching service Global wide the whole
point is to provide users worldwide
access to the data from a very nearest
server possible that's the whole point
in using or going for cloudfront then
what do you mean by Geo targeting Geo
targeting is showing customer and
specific content based on language we
can customize the content based on
what's popular in that place we can
actually customize the content the URL
is the same but we could actually change
the content a little bit not the whole
content otherwise it would be dynamic
but we can change the content a little
bit a specific a file or a picture or a
particular Link in a website and show
customized content to users who will be
in different parts of the globe so how
does it happen cloudfront will detect
the country where the viewers are
located and it will forward the country
code to the origin server and once the
origin server gets the specialized or a
specific country code it will change the
content and it will send to the caching
server and it get cached there forever
and the user gets to view a Content
which is personalized for them for the
country they are in the other question
you could get asked is the steps
involved in using cloud formation or
creating a cloud formation or a backing
up an environment within cloud formation
template we all know that if there is a
template we can simply run it and it
Provisions the environment but there is
a lot more going into it so the first
step in moving towards infrastructure as
a code is to create the cloud formation
template which as of now supports Json
and yaml file format so first create the
cloud formation template and then save
the code in an S3 bucket S3 bucket
serves as the repository for our code
and then from the cloud formation call
the file in the S3 bucket and create a
stack and now cloud formation uses the
file reads the file understands services
that are being called understands the
order understands how they are connected
with each other cloud formation is
actually an intelligent service it
understands the relation based on the
code it would understand the
relationship between the different
services and it would set an order for
itself and then would provision the
services one after the other let's say a
service has a dependency and the
dependent service the other service
which this service let's say it serves A
and B service B is dependent on service
a let's say cloud formation is an
intelligent service it would provision
the resource a first and then would
provision resource B what happens if we
inverse the order if we inverse the
order resource B first gets provision
and because it does not have dependency
chances that the cloud formation's
default behavior is that if something is
not permissioned properly if something
is not healthy it would roll back
chances that the environment
provisioning will roll back so to avoid
that cloud formation first Provisions
all the services that has or that's
dependent on that's depended by another
service so it Provisions those service
first and then Provisions the services
that has dependencies and if you are
being hired for a devops or you know if
the interviewer wanted to test your
skill on systems aside this definitely
would be a question in his list how do
you upgrade or downgrade the system with
near zero downtime now everybody is
moving towards zero downtime or near
zero downtime all of them want their
application to be highly available so
the question would be how do you
actually upgrade or downgrade a system
with near zero downtime now we all know
that I can upgrade an ec2 instance to a
better ec2 instance by changing the
instance type stopping and starting but
stopping and starting is gonna cause a
downtime right so that's you should be
answering or you shouldn't be thinking
in those terms because that's the wrong
answer specifically the interviewer
wants to know how do you upgrade a
system with zero downtime so updating
system with zero downtime it includes
launching another system parallelly with
the bigger ec2 instance type over the
bigger capacity and install all that's
needed if you are going to use an Ami of
the old machine well and good you don't
have to go through installing all the
updates and installing in all the
application from the Ami once you have
launched it in a bigger instance locally
test the application to see if it is
working don't put it on production yet
test the application to see if it is
working and if the application works we
can actually swap if your server is
behind and behind a Route 53 let's say
all that you could do is go to Route 53
update the information with the new IP
address new IP address of the new server
and that's going to send traffic to the
new server now so the cutover is handled
or if you're using static IP you can
actually remove the static IP from the
old machine and assign it to the new
machine that's one way of doing it or if
you are using elastic Nik card you can
actually remove any card from the old
machine and attach the new cards to the
new machine so that way we would get
near zero downtime if you're hired for
an architect level you should be
worrying about cost as well along with
the technology and this question would
test how well you manage cost so what
are the tools and techniques we can use
in AWS to identify and correct identify
and know that we are paying the correct
amount for the resources that we are
using or how do you get a visibility of
your AWS resources running one way is to
check the billing there's a place where
you can check the top services that were
utilized it could be free and it could
be paid Service as well top services
that can be utilized it's actually in
the dashboard of the cost Management
console so that table here shows the top
five most used services so looking at it
you can get it all right so I'm using a
lot of storage I'm using a lot of ec2
why is storage high you can go and try
to justify that and you will find if you
are storing things that shouldn't be
storing then you clean it up why is
compute capacity so high why is data
transfer so high so if you start
thinking in those levels you'll be able
to dig in and clean up unnecessary and
be able to save your bill and there are
cost Explorer services available which
will help you to view your usage pattern
or view your spending for the past 13
months or so and it will also forecast
for the next three months now how much
will you be using if your pattern is
like this so that will actually help and
will give you a visibility on how much
you have spent how much you will be
spending if the trend continues budgets
are another excellent a way to control
cost you can actually set up budget all
right this is how much I am willing to
spend for this application for this team
or for this month for this particular
resource so you can actually put a
budget Mark and anytime it exceeds
anytime it's nearing you would get an
alarm saying that well we're about to
reach the gated budget amount stuff like
that that way you can go back and know
and you know that how much the bill is
going to be for that month or you can
take steps to control bill amount for
that particular month so AWS budget is
another very good tool that you could
use cost allocation tags helps in
identifying which team or which resource
has spent more in that particular month
instead of looking at the bill as one
list with no specifications into it and
looking at it as an expenditure list you
can actually break it down and tag the
expenditure to the teams with cost
allocation tags the dev team has spent
so much the production team has spent so
much the training team has spent more
than the dev and the production team why
is that you'll be able to you know think
in those levels only if you have cost
allocation tags now cost allocation tags
are nothing but the tags that you would
put when you create a resource so for
Production Services you would put as a
production you would create a production
tag and you would associate that
resources to it and at a later point
when you actually pull up your bill
that's going to show a detailed a list
of this is the owner this is the group
and this is how much they have used in
the last month and you can move forward
with your investigation and encourage or
stop users using more services with the
cost allocation tax the other famous
question is are there any other tools or
is there any other way of accessing AWS
resource other than the console console
is GUI right so in other words other
than GUI how would you use the AWS
resource and how familiar are you with
those tools and Technologies the other
tools that are available that we can
leverage and access the AWS resource are
of course fully you can configure Puri
to access the AWS resources like log
into an ec2 instance an ec2 instance
does not always have to be logged in
through the console you could use putty
to log into an ec2 instance and like the
jump box like the proxy machine and like
the Gateway machine and from there you
can actually access the rest of the
resources so this is an alternative to
the console and of course we have the
AWS CLI in any of the Linux machines or
Windows machines we can install so
that's 0.23 and 4 we can install AWS CLI
for Linux Windows also for Mac so we can
install them and from there from a local
machine we can access run AWS commands
and access provision monitor the AWS
resources the other ones are we can
access the AWS resource programmatically
using AWS SDK and Eclipse so these are a
bunch of options we have to use the AWS
resource other than the console if you
are interviewed in a company or buy a
company that focuses more on security
and want to use AWS native services for
their security then you will come across
this question what services can be used
to create a centralized logging solution
the basic Services we could use are
cloudwatch logs store them in S3 and
then use elasticsearch to visualize them
and use Kinesis to move the data from S3
to elasticsearch right so log management
it actually helps organizations to track
the relationship between operational and
security changes and the events that got
triggered based on those logs instead of
logging into an instance or instead of
logging into the environment and
checking the resources physically I can
come to a fair conclusion by just
looking at the logs every time there's a
change the system would scream and it
gets tracked in the cloud watch and then
Cloud watch pushes it to S3 Kinesis
pushes the data from S3 to elasticsearch
and I can do a Time based filter and I
would get an a fair understanding of
what was going on in the environment for
the past one hour or whatever the time
window that I wanted to look at so it
helps in getting a good understanding of
the infrastructure as a whole all the
logs are getting saved in one place so
all the infrastructure logs are getting
saved in one place so it's easy for me
to look at it in an infrastructure
perspective so we know the services that
can be used and here are some of the
services and how they actually connect
to each other it could be logs that
belongs to One account it could be logs
that belongs to multiple accounts it
doesn't matter you know those three
services are going to work fairly good
and they're gonna inject or they're
gonna like suck logs from the other
accounts put it in one place and help us
to monitor so as you see you have Cloud
watch here that actually tracks the
metrics you can also use cloud trail if
you want to log API calls as well push
them in an S3 bucket so there are
different types of blog flow logs are
getting captured in an instance
application logs are getting captured
from the same VPC from a different VPC
from the same account from a different
account and all of them are analyzed
using elasticsearch using the kibana
client so step one is to deploy the ECS
cluster step two is to restrict access
to the ECS cluster because it's valid
data you don't want anybody to put their
hands and access their data so restrict
access to the ECS dashboard and we could
use Lambda also to push the data from
cloud watch to The elasticsearch Domain
and then kibana is actually the
graphical tool that helps us to
visualize the logs instead of looking at
log as just statements or a bunch of
characters a bunch of files kibana helps
us to analyze the logs in a graphical or
a chart or a bar diagram format again in
an interview the interview is more
concerned about testing your knowledge
on AWS security products especially on
the logging monitoring even management
or Incident Management then you could
have a question like this what are the
native AWS security logging capabilities
now most of the services have their own
logging in them like have their own
logging like S3 S3 has its own login and
cloudfront has its own logging DS has
its own logging VPC has its own logging
in additional there are account level
logins like a cloudtrail and AWS config
services so there are variety of logging
options available in the AWS like Cloud
soil config cloudfront redshift logging
RDS logging VPC flow logs S3 object
logging S3 access logging stuff like
that so we're going to look at two
servers in specific cloudtrail now this
cloud trail the very first product in
that picture we just up the cloudtrail
provides an very high level history of
the API calls for all the account and
with that we can actually perform a very
good security analysis a security
analysis of our account and these logs
are actually delivered to you can
configure it they can be delivered to S3
for long time archivals and based on a
particular event it can also send an
email notification to us saying hey just
got this error thought I'll let you know
stuff like that the other one is config
service now config service helps us to
understand the configuration changes
that happened in our environment and we
can also set up notifications based on
the configuration changes so it records
the cumulative changes that are made in
a short period of time so if you want to
go through the lifetime of a particular
resource what are the things that
happened what are the things it went
through they can be looked at using aw
us config all right the other question
you could get asked is if you know your
role includes taking care of cloud
security as well then the other question
you could get asked is the native
services that Amazon provides
automaticate DDOS which is denial of
service now not all companies would go
with Amazon native services but there
are some companies which want to stick
with Amazon native Services just to save
them from the headache of managing the
other softwares or bringing in another
tool a third-party tool into managing
DDOS they simply want to stick with
Amazon proprietary Amazon native
services and a lot of companies are
using Amazon service to prevent DDOS
denial of service now denial of service
is if you already know what denial of
service is well and good if you do not
know then let's know it now denial of
service is a user trying to or
maliciously making attempt to access a
website or an application the user would
actually create multiple sessions and he
would occupy all the sessions and he
would not let legimate users access the
servers so he's in turn denying the
service for the user a quick picture
review of what denial of services now
look at it these users instead of making
one connection they are making multiple
connections and there are cheap software
programs available that would actually
trigger connections from different
computers in the internet with different
Mac addresses so everything kind of
looks legitimate for the server and it
would accept those connections and it
would keep the sessions open the actual
users won't be able to use them so
that's denying the service for the
actual users denial of service all right
and distributed denial of service is
generating attacks from multiple places
you know from a distributed environment
so that's distributed denial of service
so the tools the native tools that helps
us to provide event the denial of
service attacks in AWS is cloud shield
and web access firewall AWS web now they
are the major ones they are designed to
mitigate a denial of service if your
website is often bothered by denial of
service then we should be using AWS
Shield or AWS Waf and there are a couple
of other tools that all showed us when I
say that also does denial of service is
not their primary job but you could use
them for denial of service route 53's
purpose is to provide DNS cloudfront is
to provide caching elastic load balancer
elb's work is to provide load balancing
VPC is to create and secure a virtual
private environment but they also
support mitigating denial of service but
not to the extent you would get in AWS
shield and AWS web so AWS shield and Waf
are the primary ones but the rest can
also be used to mitigate distributed
denial of service the other tricky
question is this actually will test your
familiarity with the region and the
services available in the region so when
you're trying to provision a service in
a particular region you're not seeing
the service in that region how do we go
about fixing it or how do we go about
using the service in the cloud it's a
tricky question and if you have not gone
through such situation you can totally
blow it away you really need to have a
good understanding on regions the
services available in those regions and
what a particular service is not
available how to go about doing it the
answer is not all services are available
in all regions anytime Amazon announces
a new service they don't immediately
publish them on all regions they start
small and as in when the traffic
increases as and when it becomes more
likeable to the customers they actually
move the service to different regions so
as you see in this picture within
America North Virginia has is more
services compared to Ohio or Compared to
North California So within not America
itself North Virginia is the preferred
one so similarly there are preferred
regions within Europe Middle East and
Africa and prefer regions within Asia
Pacific so anytime we don't see a
service in a particular region chances
that the service is not available in
that region yet we got to check the
documentation and find the nearest
region that offers that service and
start using the service from that region
now you might think well if I'm looking
for a service in Asia let's say in
Mumbai and if it is not available why
not simply switch to North Virginia and
start using it you could but you know
that's going to add more latency to your
application so that's why we need to
check for application which is check for
region which is very near to the place
where you want to serve your customers
and find nearest region instead of
always going back to North Virginian
deploying an application in North
Virginia again let's is a place there's
a link in aws.com that you can go and
look for services available in different
region and that's exactly what you're
seeing here and if your service is not
available in a particular region switch
to the other region that provides your
service the nearest other region that
provides that service and start using
service from there with the coming up of
cloud a lot of companies have turned
down their monitoring team instead they
want to go with the monitorings that
cloud provides you know nobody wants to
or at least many people don't want to go
through the hassle of at least new
startups and new companies that are
thinking of having a monitoring
environment that they don't want to go
with traditional not monitoring instead
they would like to leverage AWS
monitorings available because it
monitors a lot of stuff not just the
availability but it monitors a lot of
stuff like failures errors it also
triggers emails stuff like that so how
do you actually set up a monitor to
website I mean how to set up a Monitor
to Monitor the website metrics in real
time in AWS the simple way anytime you
have a question about monitoring
cloudwatch should strike your mind
because cloudwatch is meant for
monitoring it is meant for collecting
metrics is is meant for providing
graphical representation of what's going
on in a particular Network at a
particular point of time so cloudwatch
cloudwatch helps us to monitor
applications and using cloudwatch we can
monitor the state changes not only the
state changes the auto scaling life
cycle events anytime there are more
services added there is a reduction in
the number of servers because of less
usage a very informative messages can be
received through cloudwatch any
cloudwatch can now support scheduled
events if you want to schedule anything
cloudwatch has an event that would
schedule an action all right schedule a
Trigger Time based not incident based
you know anything happening and then you
get an action happening that's incident
based on the other hand you can simply
schedule few things on time based so
that's possible with Cloud watch So This
Cloud watch integrates very well with a
lot of other services like notifications
for notifying the user or for notifying
the administrator about it and it can
integrate well with Lambda so to trigger
an action anytime you're designing an
auto healing environment This Cloud
watch can actually monitor and send an
email if we are integrating it with SNS
simple notification service or this
Cloud watch can monitor and based on
what's Happening it can trigger an event
in Lambda and that would in turn run a
function till the environment comes back
to normal so cloudwatch integrates well
with a lot of other AWS Services all
right so cloudwatch has three statuses
green when everything is going good
hello when the service is degraded and
red when the service is not available
green is good so we don't have to do
anything about it but anytime there's an
Lo the picture that we're looking at
it's actually calling an Lambda function
to debug the application and to fix it
and anytime there's a red alert it
immediately notifies the owner of the
application about well this service is
down and here is the report that I have
here is the metrics that I've collected
about the service stuff like that if the
job role requires you to manage the
servers as well there are certain job
roles which are on the system side there
are certain job roles which is
development plus system side now you're
responsible for the application and the
server as well so if that's the case you
might be tested with some basic
questions like the different types of
virtualization and AWS and what are the
difference between them all right the
three major types of virtualization are
hvm which is Hardware virtual machine
the other one is PV para virtualization
and the third one is PV on hvm para
virtualization on Hardware virtual
module all right the difference between
them are actually describing them is
actually the difference between them hvm
it's actually a fully virtualized
Hardware you know the whole Hardware is
virtualized and all virtual machines act
separate from each other and these VMS
are booted by executing Master boot
record in the root block and when we
talk about para virtualization paragraph
is actually the special boot loader
which boots the PV Amis and when we talk
about PVR hvm it's it's actually the
marriage between hvm and PV and this
para virtualization on hvm in other
words PV on hvm it actually helps
operating system take advantage in
storage and the network input output
available through the host another good
question is name some of the services
that are not region specific now you've
been thought that all services are
within a region and some services are
within an availability zone for example
ec2 is within an availability Zone EBS
is within an availability Zone S3 is
region specific dynamodb is region
specific stuff like that VPC is both
availability and region specific meaning
you know subnets are availability Zone
specific and vpc's region specific stuff
like that so you might have thought you
might have learned in that combination
but that could be some tricky questions
that tests you how well you have
understood the region non-region and
availability non-availability Services I
should say there are services that are
not region specific that would be IAM so
we can't have IM for every availability
Zone and for every region which means
you know users will have to use one
username and password for one region and
anytime they switch to another region
they will have to use another username
and password that that's more work and
that's not a good design as well
authentication has to be Global so IM is
a global Service and which means it's
not region specific on the other hand
Route 53 is again a regional Pacific so
we can't have Route 53 for every region
Route 53 is not a region specific
service it's a global Service and it's
one application users access from
everywhere or from every part of the
world so we can't have one URL or one
DNS name for each region if your
application is a global application and
then web application firewall works well
with cloudfront then cloudfront is a
region based service so the web
application firewall it's not region
specific service it's a global Service
and cloudfront is again a global Service
though you can you know cache content on
a continent and country basis it's still
considered a global Service all right
it's not bound to any region so when you
activate cloudfront you're activating it
away from region or availability zone so
when you're activating a web application
firewall because it's not a region
specific service you're activating it
away from availability Zone and regions
so a quick recap I am users groups roles
and accounts they are Global Services
they can be used globally around 53
services are offered at Edge locations
and they are Global as well web
application firewall a service that
protects our web application from common
web exploits they are Global Services
well cloudfront cloudfront is global
content delivery Network CDN and they
are offered at Edge locations which are
a global Service in other words
non-region specific service or Beyond
region service all right this is another
good question as well in the project
that you are being interviewed if they
really want to secure their environment
using Nat or if they are already
securing their environment using Nat by
any of these two methods like Nat
Gateway or Nat instances you can expect
this question what are the difference
between a Nat Gateway and Nat instances
now they both saw the same thing right
so they're not two different Services
trying to achieve two different things
they both serve the same thing but still
they do have differences in them all
right on a high level they both achieve
providing nothing for the service behind
it but the difference comes when we talk
about the availability of it not Gateway
is a managed service by Amazon whereas
Nat instance is managed by us now I'm
talking about the third Point
maintenance here not Gateway is managed
by Amazon that instance is managed by us
and availability of nat Gateway is very
high and availability of nat instance is
less compared to the NAT Gateway because
it's managed by us you know it's on an
ec2 instance which could actually fail
and if it fails we'll have to relaunch
it but if it is not Gateway if something
happens to that service Amazon would
take care of reprovisioning it and
talking talking about bandwidth it can
burst up to 75 gigabits now traffic
through the night Gateway can burst up
to 75 gigabits but for that instance it
actually depends on the server that we
launch and if we are launching a T2
micro it badly gets any bandwidth so
there's a difference there and the
performance because it's highly
available because of the bigger pipe 75
gigabits now the performance of the NAT
Gateway is very high but the performance
of the NAD instance is going to be
average again it depends on the size of
the NAT instance that we pick and
billing a billing for Nat Gateway is the
number of gateways that we provision and
the duration for which we use the NAT
Gateway but billing for Nat instance is
number of instance and the type of
instance that we use of course number of
instance duration and the type of
instance that we use security not
Gateway cannot be assigned meaning it
already comes with full backed security
but but in that instance security is a
bit customizable I can go and change the
security because it's a server managed
by me or managed by us I can always
change the security well allow this
allow don't allow this stuff like that
size and load of the NAT Gateway is
uniform but the size and the load of the
NAT instance changes as per around
Gateway is a fixed product but in that
instance can be small instance can be a
big instance so the size and the load
through it varies
right the other question you could get
asked is what are the difference between
stopping and terminating an ec2 instance
now you will be able to answer only if
you have worked on environments where
you have your instance stopped and where
you have your instance terminated if you
have only used lab and are attending the
interview chances are that you might you
always lost when answering this question
it might look like both are the same
well stopping and terminating both are
the same but there is a difference in it
so when you stop an instance it actually
performs a normal shutdown on the
instance and it simply moves the
instance to the stop state but when you
actually terminate the instance the
instance is moved to this stop State the
EBS volumes that are attached to it are
deleted and removed and will never be
able to recover them again so that's a
big difference between stopping and
terminating an instance if you're
thinking of using the instance again
along with the data in it you should
only be thinking of stopping the
instance but you should be terminating
the instance only if you want to get rid
of that instance forever
if you are being interviewed for an
architect level position or a junior
architect level position or even an
Cloud consultant level position or even
in an engineering position this is a
very common question that get asked what
are the different types of ec2 instances
based on their cost or based on how we
pay them right they're all compute
capacity for example the different types
are on-demand stances Port instances and
reserved instances it kind of looks the
same they all provide the compute
capacity they all provide the same type
of Hardwares for us but if you are
looking at Cost saving or optimizing
cost in our environment we're going to
be very careful about which one are we
picking now we might think that well
I'll go with on-demand instance because
I pay on a per hour basis which is cheap
you know I can use them anytime I want
and anytime I don't want I can simply
get rid of it by terminating it you're
right but if the requirement is to use
the service for one year the requirement
is to use the service for three years
then you'll be wasting a lot of money
buying on-demand instances you'll be
wasting a lot of money paying on an
hourly basis instead we should be going
for reserved instance where we can
reserve the capacity for the complete
one year or complete three years and
save huge amount in buying reserved
instances alright so on-demand is cheap
to start with if you are only planning
to use it for a short while but if
you're planning to run it for a long
while then we should be going for
reserved instance that is what is cost
efficient so spot instance is cheaper
than on-demand instance and there are
different use cases for spot instance as
well so let's look at one after the
other the on-demand instance the
on-demand instances purchased at a fixed
rate per hour this is very short term
and irregular workloads and for testing
for development on demand instance is a
very good use case we should be using
on-demand for production spot instance
spot instance allows users to purchase
ec2 at a reduced price and anytime we
have more instances we can always go and
sell it in spot instances I'm referring
to anytime we have more reserved
instances we can always sell them in
spot instance catalog and the way we buy
Spot instance is we actually put a
budget this is how much I'm willing to
pay all right would you be able to give
service within this cost so anytime the
price comes down and meets the cost that
we have put in will be assigned an
instance and anytime the price shoots up
the instance will be taken away from us
but in case of on-demand instances we
have bought that instance for that
particular R and it stays with us but
with spot instances it varies based on
the price if you meet the price you get
the instance if you don't meet the price
it goes away to somebody else and the
spot instance availability is actually
based on supply and demand in the market
there's no guarantee that you will get
support instance at all line all right
so that's a caveat there you should be
familiar with that's a caveat there you
should be aware when you are proposing
somebody that we can go for spot
instance and save money it's not always
going to be available if you want your
spot instance to be available to you
then we need to carefully watch the
history of the price of the spot
instance now how much was it last month
and how much was it how much is it this
month so how can I code or how much can
I code stuff like that so you got to
look at those history before you propose
somebody that well we're gonna save
money using spot instance on the other
hand reserved instance provide cost
savings for the company or we can opt
for reserved instances for you know one
year or three years there are actually
three types of reserved instances light
medium and heavy reserved instances they
are based on the amount that we would be
paying and cost benefit also depends
with reserved instance the cost benefit
also depends based on are we doing all
upfront or no upfront or partial payment
then split the rest as monthly payments
so there are many purchase options
available but overall if you're looking
at using an application for the next one
year and three years you should not be
going for on-demand instance you should
be going for reserved instance and
that's what gives you the cost benefit
and in an AWS interview sometimes you
might be asked you know how you interact
with the AWS environment are you using
CLI are you using console and depending
on your answer whether console or a CLI
the panelist put a score okay this
person is CLI specific this person is
console specific or this person has used
AWS environment through the SDK stuff
like that so this question tests whether
you are a CLI person or a console person
and the question goes like this how do
you set up SSH agent forwarding so that
you do not have to copy the key every
time you log in if you have used Puri
anytime if you want to log into an ec2
instance you will have to put the IP and
the port number along with that you will
have to map or we will have to map the
key in the Puri and this has to be done
every time that's what we would have
done in our lab environments right but
in production environment using the same
key or mapping the same key again and
again every time it's actually an hassle
it's concerned as a blocker so you might
want to Cache it you might want to
permanently add it in your putty session
so you can immediately log in and start
using it so here in the place where you
would actually map the private key
there's a quick button that actually
fixes or that actually binds your SSH to
your pretty instance so we can enable
SSH agent forwarding and that will
actually bind our key to the SSH and
next time when we try to log in we don't
have to always go through mapping the
key and trying to log in all right this
question what are Solaris and ax
operating systems are they available
with AWS that question generally gets
asked to test how familiar are you with
the Amis available how familiar are you
with ec2 how familiar are you with the
ec2 Hardwares available that basically
tests that now the first question or the
first thought that comes to your mind is
well everything is available with AWS
I've seen Windows I've seen in Ubuntu
I've seen Red Hat I've seen Amazon Amis
and if I don't see my operating system
there I can always go to Marketplace and
try them if I don't final Marketplace I
can always go to community and try them
so a lot of Amis available there are a
lot of operating systems available I
will be able to find Solaris and ax but
that's not the case Solaris and ax are
not available with AWS that's because
Solaris uses a different I mean Solaris
does not support the architecture does
not support public Cloud currently the
same goes for ax as well they run on
power CPU and not on Intel and as of now
Amazon does not provide Power machines
this should not be confused with HPC
which is a high performance Computing
should not be confused with that now
these are different Hardwares different
CPU itself that the cloud providers did
do not provide yet another question you
could get asked in organizations that
would want to automate the their
infrastructure using Amazon native
Services would be how do you actually
recover an ec2 instance or Auto recover
an ec2 instance when it fails well we
know that ec2 instances are considered
as immutable meaning irreparable we
don't spend time fixing bugs in an OS
stuff like that you know once and ec2
instance crashes like it goes on a OS
Panic or there are various reasons why
it would fail so we don't have to really
worry about fixing it we can always
relaunch that instance and that would
fix it but what if it happens at two
o'clock in the night what if it happens
that during a weekend when nobody is in
office now looking or monitoring those
instances so you would want to automate
that not only on a weekend or during
midnights but it's general practice good
to automate it so you could face this
question how do you actually automate an
ec2 instance once it fails and the
answer to that question is using Cloud
watch we can recover the instance so as
you see there is an alarm threshold a
set in Cloud watch and once the
threshold is met meaning if there is an
error if there is a failure if the ec2
instance is not responding for a certain
while we can set an alarm and once the
alarm is met let's say the CPU
utilization stayed high for 5 minutes
all right it's not taking any new
connections or the instance is not
pinging for five minutes or in this case
it's two minutes it's not pinging so
it's not going to respond connection so
in those cases you would want to
automatically recover that ec2 instance
by rebooting the instance all right now
look at this the take this action
section under the action so there we
have a bunch of options like recover
this instance meaning reboot the
instance so that's how we would recover
the other two options are beyond the
scope of the question but still you can
go ahead and apply just like I'm gonna
do it so the other option to stop the
instance that's very useful when you
want to stop instances that are having
low utilizations nobody's using the
system as of now you don't want them to
be running and wasting the cloud
expenditure so you can actually set an
alarm that stops the ec2 instance that's
having low utilization so somebody was
working in an instance and they left it
without or they forgot to shut down that
instance and it gets I mean they will
only use it again the next day only so
in between there could be like 12 hours
that the system is running idle nobody's
using it and you're paying for it so you
can identify such instances and actually
stop them when the CPU utilization is
low meaning nobody is using it the other
one is to terminate let's say you want
to give system to somebody temporarily
and you don't want them to hand the
system back to you all right this is
actually an idea in other words this is
actually the scenario so you hand over a
system to somebody and when they're done
they're done we can actually terminate
the system so you could instruct the
other person to terminate the system
when they're done and they could forget
and the instance could be running
forever or you can monitor the system
after the specified time is over and you
can terminate the system or best part
you can automate the system termination
so you assign a system to somebody and
then turn on this cloudwatch action to
terminate the instance when the CPU is
low for like two hours meaning they've
already left or CPU is low for 30
minutes meaning they've already left
stuff like that so that's possible and
if you're getting hired for an system
side architect or even on the sysop site
you could face this question what are
the common and different types of Ami
designs there are a lot of Ami designs
the question is the common ones and the
difference between them so the common
ones are the full backed Amis and the
other one is just enough OS Ami j e o s
m i and the other one is hybrid type
Amis so let's look at the difference
between them the full backed Ami just
like the name says it's fully baked it's
ready to use Ami and this is the
simplest Ami to deploy can be a bit
expensive it can be a bit a cumbersome
because you'll have to do a lot of work
beforehand you could use the Amis a lot
of planning a lot of thought process
will go into it and the Ami is ready to
use right you hand over the Ami to
somebody and it's ready to use or if you
want to reuse the Ami it's already ready
for you to use so that's full baked Ami
the other one is just enough operating
system Ami just like the name says it
has I mean as you can also see in the
diagram or in the picture it covers a
part of the OS all bootstraps are
already packed properly and the security
monitoring logging and the other stuff
are configured at the time of deployment
or at the time you would be using it so
not much thought process will go in here
the only focus is on choosing the
operating system and what goes the
operating system specific agents or
bootstraps that goes into the operating
system that's all we worry about the
advantage of this is it's flexible
meaning you can choose to install
additional softwares at the time of
deploying but that's going to require an
additional expertise on the person who
will be using the Ami so that's another
overhead there but the advantage is that
it's kind of flexible I can change the
configurations during the time of
deployment the other one is the hybrid
Ami now the hybrid Ami actually falls in
between the fully baked Ami and just
enough operating system options so these
Amis have some features of the big type
and some features of the just enough OS
type so as you see the security
monitoring logging are packed in that
Ami and the runtime environments or
insta stalled during the time of a
deployment so this is where the strict
company policies would go into the Ami
company policies like you got to lock
this you got to monitor this these are
the ports that generally gets open in
all the systems stuff like that so they
strictly go into the Ami and sits in an
AMF format and during deployment you
have the flexibility of choosing the
different runtime and the application
that sits in an ec2 instance another
very famous question you would face in
an interview is how can you recover
login to an ec2 instance to which you
lost the key well we know that if the
key is lost we can't recover it there
are some organizations that integrate
their ec2 instances with an ad that's
different all right so you can go and
reset the password in the ad and you
will be able to log in with the new
password but here the specific tricky
question is you are using a key to log
in and how do you recover if you have
lost the key generally companies would
have made a backup of the key so we can
pick from the backup but here the
specific question is we have lost the
key literally no backups on the key at
all so how can we login and we know that
we can't log into the instance without
the key present with us so the steps to
recover is that make the instance use
another key and use that key to log in
once the key is lost it's lost forever
we won't be able to recover it you can't
raise a ticket with Amazon not possible
they're not going to help it's beyond
the scope so make the instance use
another key it's only the key that's the
problem you still have valid data in it
you got to recover the data it's just
the key that's having the problem so we
can actually focus on the key part alone
and change the key and that will allow
us to log in so how do we do it step by
step procedure so first verify the ec2
config service is running in that
instance if you want you can actually
beforehand install the ec2 config in
that service or you can actually make
the easy to config run through the
console just couple of button clicks and
that will make the easy to configure one
in that easy to instance and then it
detach the root volume for that instance
of course it's going to require a stop
and start to detach the root volume from
the instance attach the root volume to
another instance as a temporary volume
or it could be a temporary instance that
you've launched only to fix this issue
and then login to that instance and to
that particular volume and modify the
configuration file configuration file
modify it to use the new key and then
move the root volume back to its
original position and restart the
instance and now the instance is going
to have the new key and you also have
the new key with which you can log in so
that's how we go ahead and fix it now
let's move on to some product specific
or S3 product specific questions a
general perception is S3 and EBS can be
used interchangeably and the interviewer
would want to test your knowledge on S3
and EVS well EBS uses S3 that's true but
they can't be interchangeably used so
you might face this question what are
some key differences between AWS S3 and
EBS well the differences are S3 is an
object store meaning you can't install
anything in it you can store try files
but you can't actually install in it
it's not a file system but ABS is a file
system you can install Services I mean
install applications in it and that's
going to run stuff like that and talking
about performance S3 is much faster and
abs is super faster when accessing from
the instance because from the instance
if you need to access S3 you'll actually
have to go out through the internet and
access the S3 or S3 is an external
service very external service you'll
have to go through or you'll have to go
outside of your VPC to access S3 S3 does
not come under a VPC but EBS comes under
a VPC it's on the same VPC so you would
be able to use it kind of locally
compared to S3 EBS is very local so that
way it's going to be faster and
redundancy talking about redundancy of
S3 and abs S3 is replicated the data in
S3 is replicated across the data centers
but EBS is replicated within the data
center meaning S3 is replicated across
availability zones EBS is within an
availability zone so that way redundancy
is a bit less in EVS in other words
blurrency is higher in S3 than EPs and
talking about security of S3 is3 can be
made private as well as public meaning
anybody can access S3 from anywhere in
the internet that's possible with S3 but
EBS can only only be accessed when
attached to an ec2 instance right just
one instance can access it whereas S3 is
publicly directly accessible the other
question related to S3 security is how
do you allow access to a user to a
certain a user to a certain bucket which
means this user is not having access to
S3 at all but this user needs to be
given access to a certain bucket how do
we do it the same case applies to
servers as well in few cases there could
be an instance where a person is new to
the team and you actually don't want
them to access the production service
now he is in the production group and by
default he or she is granted access to
that server but you specifically want to
deny access to that production server
till the time he or she is matured
enough to access or understand the
process understand the do's and don'ts
before they can put their hands on the
production server so how do we go about
doing it so first we would categorize
our instances well these are critical
instances these are normal instances and
we were actually put a tag on them
that's how we categorize right so you
put a tag on them put attack saying well
they are highly critical they are medium
critical and they are not critical at
all still they're in production stuff
like that and then you would pick the
users who wants to or who should be or
should not be given access to a certain
server and you would actually allow the
user to access or not access servers
based on a specific tag in other words
you can use actually tags in in the
previous step we put tags on the
critical server right so you would
Define that this user is not going to
use this tag all right this user is not
allowed to use the resources for this
stack so that's how you would make your
step forward so you would allow or deny
based on the tags that you have put so
in this case he or she will not be
allowed to servers which are TAG
critical servers so that's how you allow
deny access to them the same goes for
bucket as well or if an organization is
excessively using S3 for their data
storage because of the benefit that it
provides the cost and the durability you
might get asked this question which is
organizations would replicate the data
from one region to another region for
additional data durability and for
having data redundancy not only for that
they would also do that for Dr purposes
for Disaster Recovery if the whole
region is down you still have the data
available somewhere else and you can
pick and use it some organizations would
store data in different regions for
compliance reasons to provide low
latency access to their users who are
local to that region stuff like that so
when companies do replication how do you
make sure that there is consistency in
the replication how do you make sure
that a replication is not failing and
the data gets transferred for sure and
there are logs for that replication this
is something that the companies would
use where they are excessively using S3
and they are fully relying on the
replication in running their business
and the way we could do it is we can set
up a replication monitor it's actually
set of tools that we could use together
to make sure that the cloud replication
a region level replication is happening
properly so this is how it happens now
on this side on the left hand side we
have the region one and on the right
hand side we have Region 2 and region
one is the source bucket and region two
is the destination bucket all right so
object is put in the source bucket and
it has to go directly to the region to
bucket or made a copy in the region 2
bucket and the problem is sometimes it
fails and there is no consistency
between them so the way you would do it
is connect these Services together and
create an cross replication or cross
region replication monitor that actually
monitors that actually monitors your
environment so there are cloudwatch that
make sure that the data is moved no data
is failing again there's Cloud watch on
the other end to make sure that the data
is moving and then we have the logs
generated through cloudtrail and that's
actually written in dynamodb and if
there is an error if something is
failing you get notified through an SMS
or you get notified through an email
using the SNS service so that's how we
could leverage these tools and set up
and cross region replication monitor
that actually monitors your data
replication some common issues that
company companies face in VPC is that we
all know that I can use Route 53 to
resolve an IP address externally from
the internet but by default the servers
won't connect to the other servers using
our custom DNS name that it does not do
that by default so it's actually a
problem there are some additional things
that as an administrator or as an
architect or as a person who uses it you
will have to do and that's what we're
going to discuss so the question could
be a VPC is not resolving the server
through the DNS you can access it
through the IP but not through the DNS
name and what could be the issue and how
do you go about fixing it and you will
be able to answer this question only if
you have done it already it's a quick
and simple step by default World VPC
does not allow that's the default
feature and we will have to enable the
DNS hostname resolution before now this
is for the custom DNS not for the
default DNS that comes along this is for
the custom DNS so we will have to enable
the DNS hostname resolution so our will
have to enable DNS hostname resolution
so they actually resolve let's say I
want to connect to a server one dot
simplylearn.com by default it's not
allowed but if I enable this option then
I will be able to connect to server one
simplylearn.com if a company has vpcs in
different regions and they have head
office in a central place and the rest
of them are Branch offices and they are
connecting to the head office for Access
or you know for saving data or for
accessing certain files or certain data
or storing data all right so they would
actually mimic The Hub and spoke
topology where you have the VPC which is
centrally in an accessible region a
centrally accessible region and then you
would have a local vpcs or Branch
offices in different other regions and
they get connected to the VPC in the
central location and the question is how
do you actually connect the multiple
sites to a VPC and make communication
happen between them by default it does
not do that we know that vpcs they need
to be paired between them in order to
access the resources let's look at this
picture right so I have like a customer
Network or Branch offices in different
parts and they get connected to a VPC
that's fine so what we have achieved is
those different offices the remote
offices they are connecting to the VPC
and they're talking but they can't
connect or they can't talk to each other
that's what we have built but the
requirement is the traffic needs to or
they should be able to talk to each
other but they should not have direct
connection between them which means that
they will have to come and hit the VPC
and then reach the other customer
Network which is in Los Angeles or which
is in New York right that's the
requirement so that's possible with some
architecting in the cloud so that's
using VPN Cloud Hub you look at this
dotted lines which actually allows
customers or which actually allows the
corporate networks to talk to each other
through the VPC Again by default it
doesn't happen cloudhub is an
architecture that we should be using to
make this happen and what's the
advantage of it as a central office or
as the headquarters office which is in
the VPC or headquarters data center
which is in the VPC you have control or
the VPC has control on who talks to who
and what traffic can talk to I mean what
traffic can be routed to the other head
office stuff like that that centralized
control is on the VPC the other question
person you could get asked is name and
explain some security products and
features available in VPC well VPC
itself is a security service it provides
a security service to the application
but how do you actually secure the VPC
itself that's the question and yes there
are products that can actually secure
the VPC or the VPC delivers those
products to secure the application
access to the vpcs restricted through a
network access control list all right so
that's and security product in VPC and a
VPC has security groups that protects
the instances from unwanted inbound and
outbound traffic and network access
control list protects the subnets from
unwanted inbound and outbound access and
there are flow logs we can capture in VP
scene that captures incoming and
outgoing traffic through a VPC which
will be used for later analysis as in
what's the traffic pattern what's the
behavior of the traffic pattern stuff
like that so there are some security
products and features available in VPC
now how do you monitor VPC VPC is a very
important concept very important Service
as well everything sits in a VPC most of
the service sits in a VPC except for
Lambda and S3 and dynamodb and couple of
other services most of them sit in a VPC
for security reasons so how do you
monitor your VPC how do you gain some
visibility on your VPC well we can gain
visibility on our VPC using VPC flow log
that's the basic Service as you see it
actually captures what's allowed what's
not allowed stuff like that which IPS
allowed which IP is not allowed stuff
like that so we can gather it and we can
use that for analysis and the other one
is cloud watch and Cloud watch logs the
data transfers that happen so this is
you know who gets allowed and who does
not get allowed I mean the flow logs is
who is allowed and who's not allowed
that kind of detail and Cloud watch
gives information about the data
transfer how much data is getting
transferred we can actually pick unusual
data transfers if there is a sudden hike
in the graph there's a sudden hike and
something happens at 12 on a regular
basis and you weren't expecting it
there's something suspicious it could be
valid backups it could be a malicious
activity as well so that's how you know
by looking at cloudwatch logs and
cloudwatch dashboard now let's talk
about multiple choice questions when
going for an interview you might
sometimes find yourself that the company
is conducting an online test based on
the score they can put you to a panelist
and then they would take it forward so
we thought it will also include multiple
choice questions to help you better
handle such situation if you come across
all right when you find yourself in such
situation and the key to clear them is
to understand the question properly read
between the lines that's what they say
you know there can be like a big
paragraph with three lines or ten lines
you really got to understand what the
question is about and then try to find
answer for that question so that's a
come rule number one and then the second
rule is try to compare and contrast the
services mentioned or try to compare and
contrast the answers you can easily read
out one or two answers and then you will
be left with only two answers to decide
from you know so that also helps you
with time and that's how that also helps
you with some Precision in your answer
so number one read between the lines
number two compare and contrast the
services and you'll be able to easily
read out the wrong ones so let's try
answering this question suppose you are
a game designer and you want to develop
a game with a single digit millisecond
latency which of the following database
Services would you choose so we know
that the following are database services
are good enough all right and it talks
about millisecond latency that's the key
point and the third thing is it's a game
it could be a mobile game it's a game
that you are trying to design and you
need the millisecond latency and it has
to be a database all right so let's talk
about the options available RDS RDS is
the database for sure is it good for
game design we'll come back to that
Neptune Neptune is a graph a database
service in Amazon so that's kind of out
of the equation and snowball is actually
a storage all right it's it's a
transport medium I would say so that's
again out of the equation so the tie is
between RDS and dynamodb so if we need
to talk about RDS RDS is an a platform
as a service it provides cost efficient
resizable capacity but it's an SQL
database meaning the tables are kind of
strict you know it's good for Banking
and other type of applications but not
really good for anything that has to do
with the gaming so the only option left
is dynamodb again it's the right answer
dynamodb is actually an fast and
flexible nosql database service and it
provides a single digit millisecond
latency at any scale and it's a database
at the same time it's a key Value Store
model database so the right answer is
dynamodb all right let's look at the
next question if you need to perform
real-time monitoring of AWS services and
get actionable insights which service
would you use all right let's list the
services so it talks about real-time
monitoring a firewall manager what does
it provide now firewall manager is not
really a monitor just like the name says
it's a manager it manages multiple
firewalls and AWS guard duty is and
thread detection service it does
monitoring it dust continuously monitor
our environment but it monitors for
threats all right only threats now let's
talk about Cloud watch a cloud watch is
a service that helps to track metrics
it's a service that is used to
monitor the environment and give us a
system-wide visibility and also it helps
us to store logs so at the moment it
kind of looks like that could be the
right answer we don't know that yet but
I mean we have one more option left
that's EBS so what's EBS EBS is a block
storage elastic Block store if we
abbreviate EBS it's elastic Block store
so all three of them are easily out of
the question the first one is to manage
second one is to find threats of course
it does monitoring so there's I mean if
there is one relation between Cloud
watch and guard Duty that's monitoring
so easily we can actually find ourselves
slipped towards picking guard duty but
know that God Duty is only for gaining
security inside but not about gaining
AWS service inside so cloudwatch is a
service that helps us to get a system
wide or an AWS wide or an account wide
and it has number of metrics we can
monitor and get a very good Insight of
how a service is performing be it CPU be
it Ram beat Network utilization beat the
connection failures Cloud watch is a
service that helps us perform a
real-time monitoring and get some
actionable insights on the services all
right let's talk about this 33rd
question as a web developer you are
developing an app especially for the
mobile platform all right there is a
mention that this is especially for the
mobile platform so a lot of services
gets filtered out mobile platform right
which of the following lets you add user
sign up sign in and access control to
your web and mobile app quickly and
easily alright so this is all about
signing in to your mobile app so if we
need to read between the the lines
that's how we can read sign up or sign
in into a mobile platform all right so
we have like four options here Shield
AWS Massey AWS inspector Amazon Cognito
so let's try to weed out Services which
are not relevant to it so what's AWS
Shield AWS Shield is actually a service
that provides a DDOS mitigation or DDOS
protection denial of service protection
it's a security feature let's talk about
the second option AWS Maxi is again a
security service that uses machine
learning to automatically discover and
classify the data it again talks about
security and this security is all about
encrypting or saving the data does not
come close with signing up an mobile
platform all right let's talk about the
other one AWS inspector now AWS
inspector has something to do with apps
it definitely has something to do with
apps so kind of looks like that's right
relevant as of now so it actually helps
with improving the security and
compliance of the apps that we deploy in
the cloud so kind of looks like it could
be because it has to do with apps the
last one Cognito now Cognito is a
service that actually lets the
administrator to have control access
over web and mobile apps and it's a
service that helps us to sign up and
assign in to an mobile and web app so
that very much looks like we found the
answer so cognitive service that helps
web app and mobile app for sign up and
signing in and also gives the
administrator to have control over who
is I mean access control over the web
and the mobile app pretty much we found
it so it's cognitive cognitive is a
service that helps us to set up sign up
sign in and have access control over the
users who will be using our mobile and
web app all right how about this
question you are an ml engineer or a
machine learning engineer who is on the
lookout for a solution that will
discover sensitive information that your
Enterprise stores in AWS and then uses
NLP to classify that data and provide
business related insights which among
the following Services would you choose
so we have a bunch of services that's
going to help us achieve or one of it is
going to help us achieve the above
requirement so it's a service that deals
with machine learning you are a machine
learning engineer who's looking for a
service that will help you to discover
information at your Enterprise store so
we're talking about storage discover
information in store and then classify
the data depending on severity the
sensitivity classify the data so which
service is that so firewall manager just
like the name says it's a manager and
the AWS IAM if we abbreviate it it's a
identity and access manage months its
identity and access management nothing
to do with identifying sensitive data
and managing it so the first two is
already out of the equation then the AWS
is massive we already had a quick
definition description for AWS Mass see
that it's actually a security service
that uses machine learning kind of looks
like it could be it it's a security
service that uses machine learning and
it discovers and classifies the
sensitive information not only that it
does not stop there it goes beyond and
protects the sensitive data AWS Mass it
kind of looks like but we still have one
more option to look at which is cloud
HMS Cloud HMS is also a security service
kind of looks like that could be the
answer as well and it enables us to
generate encryption keys and save the
data so kind of 50 of it it's a security
service it encrypts helps us protect the
data but AWS Maxi is right on spot it's
a machine learning service it helps us
to classify the data and also to protect
the data so the answer for this question
would be AWS Massey so hope you kind of
get it how this is going so first we
apply the thumb rule identify the
question that's being asked read between
the lines and then try to find the
service that meets your requirement then
finding the service is by verse reading
out at the wrong ones a recollect
everything that you've learned about the
service and see how well that matches
with those hints that you have picked up
and if that doesn't match weed that out
then you'll end up with two just two to
decide from at some point and then it
becomes easy for you to decide click on
the question submit it and then move on
to the other question in your interview
alright so how about this one you are a
system administrator in your company
which is running most of its
infrastructure on AWS you are required
to track your users and keep a look on
how your users are being authenticated
all right so this is where the problem
statement starts right you need to keep
track of how your users are being
authenticated and you wish to create and
manage AWS users and use permissions to
allow and deny their access to the AWS
resources right you are to give them
permission number one and then I mean if
we put them in the right order first
giving them permissions and then
tracking their usage let's see which of
the service will help us achieve it IAM
is a service that helps us to looking at
the permissions we can actually predict
whether the user or the group will have
servers or not so that helps us to get a
track of who's able to use who's not
able to use certain servers and all that
stuff so it kind of looks like but we
have other three options left let's look
at AWS firewall manager just like the
name says it's actually a firewall
manager it helps us to manage multiple
firewalls simple as that and shield is a
service it's a service that's used to
protect denial of service or distributed
denial of service an API Gateway is a
service that may makes it easy for
developers to create publish maintain
and monitor and secure API so I mean
it's completely on the API side very
Less on user and how you authenticate
your user we can get that by looking at
the name itself right if you abbreviate
it or if we if you try to find a
definition for the name API Gateway you
would get it it has to do with API but
if we upload AWS IEM it's identity and
access management pretty much meets the
requirement but for the problem
statement about it's AWS identity and
access management that's the right
answer all right let's look at this one
if you want to allocate various private
and public IP address in order to make
them communicate with the internet and
other instances you will use this
service which of the following is this
service so it talks about using public
and private IP address so this service
uses IP address and then this service
helps us to allow and deny connections
to the internet and to the other
instances so you get the question is it
let's pick the service that helps us
achieve it Route 53 Route 53 is actually
a DNS service right so it's not a
service that's used to allow or deny no
it does not do that VPC VPC uses public
and private IP address yes so kind of
looks like a VPC helps us to allow I
mean the security in VPC the security
group the network access control list in
a VPC the routing table in a VPC that
actually helps us to allow or deny a
connection to a particular IP address or
to a particular service within the VPC
or outside of the VPC so as of now it
kind of looks like it could be but let's
look at the other services what if if we
find a service that closely matches to
the above requirement than the Amazon
VPC Gateway API Gateway we know that
it's a managed service that makes it
easy for developers to create publish
maintain and monitor apis and secure API
so that has completely do with API not
with IP cloudfront we know about
cloudfront that it's a Content delivery
Network and it provides global
distribution of servers where our
content can be cached it could be video
or or bulk Media or anything else they
can be cached locally so users can
easily access them and download them
easily alright so that's Cloud front now
at this point after looking at all four
it looks like VPC is the right answer
and in fact VPC is the right answer VPC
has public IP address VPC can help us
with the private IP address VPC can be
used to allow deny connection based on
the security group Access Control list
and routing table it has so that's right
answer is VPC all right how about this
one this platform as a service or
platform as a DB service provides us
with the cost of and resizable capacity
while automating time consuming
administrative tasks so this question is
very clear it's a DB service we gotta
look for and it's a service that can
provide automating some of the time
consuming tasks it has to be resizable
at the same time so let's talk about
Amazon rational database it's a database
kind of matches the requirement we can
resize it as and when needed
all right looks like it's a fit as of
now it actually automates some of the
time consuming work looks like it's a
fit as of now let's move on to elastic
cache and then try to see if uh that
matches the definition that we've
figured out about elastic cache it's
actually a caching service it's again an
in-memory data store which helps in
achieving high throughput and low
latency in memory data store so it's not
a full-blown database and it does not
come with any Amazon provisioned
Automation in it for automating any of
the administration tasks now it does not
come up with anything like that yeah we
can resize the capacity as and when
needed but automation it's not there yet
and moreover it's not a database so
that's out of the equation VPC is not a
resizable one you know once we have
designed VPC it's fixed it can be
resized so that's out of the equation
and Amazon Glacier Glacier is a storage
but not a database all right so that's
again of the equation so the tie is kind
of between Amazon rational database
service and Amazon elastic cache because
they both Aid the database service but
elastic cache is not a full-blown
database it actually helps the database
but it's not a full-blown database so
it's Amazon relational database so
that's the one which is a platform as a
service it's the one which can be
resized it's the one which can be used
to automate the time consuming
administrative tasks all right let's
talk about this one which of the
following is a means for accessing human
researchers or Consultants to help solve
a problem on a contractual or a
temporary basis all right let's read the
question again which of the following is
a means for accessing human researchers
or consultant to help solve problems on
a contractual or a temporary basis it's
like assigning task or hiring AWS
exports for a temporary job so let's try
to find that kind of service in the four
services that are listed Amazon elastic
map reduce map reduce is actually an
framework service that makes it easy and
cost effective to analyze large amount
of data but that has nothing to do with
accessing human researchers all right
let's talk about mechanical terms it's a
web service that provides a human
Workforce that's the definition for it
for example automation is good but not
everything can be automated for
something to qualify for automation it
has to be a repeated task a one-time
task can't be automated or the time and
money that you would be spending in
automation is not worth it instead you
could have done it manually so that does
not qualify for Automation and anything
that requires intelligence or anything
that's a special case all right
automation can do reperative tasks
automation can do precise work but it
has to be repeated tasks scenario you
know it should have been there already
only then that can be executed but if
it's a new scenario and it requires
appropriate addressing then it requires
human thoughts so we could hire
researchers and Consultants who can help
solve a problem using Amazon Mechanical
Turk the other two are already out of
the equation now Dev pay is actually a
payment system through Amazon and
multi-factor authentication as it says
it's an authentication system so the
right answer is Amazon Mechanical Turk
all right this sounds interesting let's
look at this one this service is used to
make it easy to deploy manage and scale
containerized applications using
kubernetes on AWS which of the following
is this AWS service so it's a service to
deploy manage and scale containerized
applications so it deals with containers
so it also should have the ability to
use kubernetes which is a container
orchestration service all right the
first one an Amazon elastic container
service kind of looks like it's the one
the name itself has the word and the
relation we're looking for elastic
container service so this container
service is an highly scalable high
performance container orchestration
service let's look at the other one AWS
batch it's a service that enables it
professionals to schedule and execute
batch processing I mean the name itself
says that that's meant for batch
processing elastic bean stock that's
another service that helps us to deploy
manage and scale but it helps us with
easy to instances not with containerized
instances so that's again out of the
equation would light scale be a good tie
for elastic container service what's
slide sale now light sale is a service
it's called as virtual private server
without a VPC it's called as a virtual
private server it comes with a
predefined in the compute storage
networking capacity it's actually a
server not a container right so at this
point that also becomes out of the
equation so it's Amazon elastic
container service that's the one that
helps us to easily deploy manage scale
container services and it helps us
orchestrate the containers using
kubernetes all right how about this one
all right this service lets us to run
code without provisioning or managing
servers so no servers run code select
the correct service from the below
option all right so no servers but we
should be able to run code Amazon ec2
Auto scaling easy to order scaling ec2
is elastic compute Cloud which is a
server and auto scaling is a service
that helps us to achieve scaling the
server so that's the definition for it
could be that's out of the equation AWS
Lambda now Lambda is a service It's
actually an event driven serverless
Computing platform and Lambda runs code
in response to the event that it
receives and it automatically manages
the compute resource that's required for
that code as long as we have uploaded a
code that's correct and setup events
correctly to map to that code it's going
to run seamlessly so that's about Lambda
it kind of looks like it could be the
answer because Lambda runs code we don't
have to manage servers it manages
servers by itself but we can't conclude
as of now we have other two servers to
talk about AWS badge all right batch is
service that enables ID professionals to
run batch job we know that and about
inspector Amazon inspector it's actually
a service that helps us to increase and
identify any security issues and align
our application with compliance well
that's not the requirement of the
question the requirement and the
question was a run code without
provisioning a server and without any
more space for confusion AWS Lambda is a
service or is the service that runs code
without provisioning and managing
Services right the right one would be
AWS Lambda I'm very excited that you're
watching this video and I'm equally glad
that we were able to provide you a
second part in AWS interview questions
all right let's get started so in an
environment where there's a lot of
automation infrastructure automation
you'll be posted with this question how
can you add an existing instance to a
new auto scaling group now this is when
you are taking an instance away from the
auto scaling group to troubleshoot to
fix a problem you know to look at logs
or if you have suspended the auto
scaling you know you might need to
re-add that instance to the auto scaling
group only then it's going to take part
in it right only then the auto scaling
is going to count it as part of it it's
not a straight procedure you know when
you remove them you know it doesn't get
automatically re-added I've had worked
with some clients when their developers
were managed managing their own
environment they had problems adding the
instance back to the auto scaling group
you know irrespective of what they tried
the instance was not getting added to
the order scaling group and whatever
they fixed that they were provided or
whatever fix that they have provided
were not you know getting encountered in
the auto scaling group so like I said
it's not a straight you know a click
button procedure there are ways we'll
have to do it so how can you add an
existing instance to the auto scaling
group there are a few steps that we need
to follow so the first one would be to
under the ec2 instance console right
under the instance under actions in
specific you know there's an option
called attach to Auto scaling group
right if you have multiple Auto scaling
groups in your account or in the region
that you're working in then you're going
to be posted with the different Auto
scaling groups that you have in your
account let's say you have five Auto
Skilling groups for five different
application you know you're going to be
posted with five different or scaling
groups and then you would select the
auto scaling the appropriate Auto
scaling group and attach the instance to
that particular or a scaling group while
adding to the auto scaling group if you
want to change the instance type you
know that's possible as well sometimes
when you want to add the instance back
to the auto scaling group there would be
requirement that you change the instance
type to a better one to a better family
to the better instance type you could do
that at that time and after that you are
or you have completely added the
instance back to the auto scaling group
so it's actually an seven step up
process adding an instance back to the
auto scaling group in an environment
where they're dealing with migrating the
instance or migrating an application or
migrating an instance migrating and VM
into the cloud you know if the project
that you're going to work with deals
with a lot of migrations you could be
posted this question what are the
factors you will consider while
migrating to Amazon web services the
first one is cost is it worth moving the
instance to the cloud given the
additional bills and whistles features
available in the cloud is this
application going to use all of them is
moving into the cloud beneficial to the
application in the first place you know
beneficial to the users who will be
using the application in the first place
so that's a factor to think of so this
actually includes you know cost of the
infrastructure and the ability to match
the demand and Supply transparency is
this application in high demand you know
is it going to be a big loss if the
application becomes unavailable for some
time so there are few things that needs
to be considered before we move the
application to the cloud and then if the
application does the application needs
to be provisioned immediately is there
an urge is there an urge to provision
the application immediately that's
something that needs to be considered if
the application requires to go online if
the application needs to hit the market
immediately then we would need to move
it to the cloud because in on-premises
procuring or buying an infrastructure
buying the bandwidth buying the
switchboard you know buying an instance
you know buying their software so buying
the license related to it it's going to
take time at least like two weeks or so
before you can bring up an server and
launch an application in it right so if
the application cannot wait you know
waiting means you know Workforce
productivity loss is it so we would want
to immediately launch instances and put
application on top of it in those case
if your application is of that type if
there is the urge in making the
application go online as soon as
possible then that's a candidate for
moving to the cloud and if the
application or if the the software or if
the product that you're launching it
requires Hardware it requires an updated
Hardware all the time that's not going
to be possible in on-premises we try to
deal with Legacy see infrastructure all
the time in on-premises but in the cloud
they're constantly upgrading their hard
ways only then they can keep themselves
up going in the market so they
constantly the cloud providers are
constantly updating their hard ways and
if you want to be benefited of your
application wants to be benefited by the
constant upgrading of the Hardwares
making sure the hardware is as latest as
possible the software version the
licensing is as latest as possible then
that's a candidate to be moved to the
cloud and if the application does not
want to go through any risk if the
application is very sensitive to
failures if the application is very much
stacked to the revenue of the company
and you don't want to take a chance in
you know seeing the application fail and
you know seeing the revenue drop then
that's a candidate for moving to the
cloud and business agility you know
moving to the cloud at least half of the
responsibility is now taken care by the
provider in this case it's Amazon at
least half of the responsibility is
taken care by them like if the hardware
fails Amazon makes sure that they're
fixing the hardware immediately and
notifications you know if something
happens you know there are immediate
notifications available that we can set
it up and make yourself aware that
something has broken and we can
immediately jump in and fix it so you
see there are the responsibility is now
being shared between Amazon and us so if
you want to get that benefit for your
application for your organization for
the product that you're launching then
it needs to be moved to the cloud so you
can get that benefit from the cloud the
other question you could get asked is
what is RTO and RPO in AWS they are
essentially Disaster Recovery terms when
you're planning for Disaster Recovery
you cannot avoid planning disaster
recovery without talking about RTO and
RPO now what's the RTO what's the RPO in
your environment or how do you define
RTO how do you define RPO or some
general questions that get asked RTO is
recovery time objective RTO stands for
the maximum time the company is willing
to wait for the recovery to happen or
for the recovery to finish when an
disaster strikes so RTO is in the future
right how much time is it going to take
to fix and bring everything to normal so
that's RTO on the other hand RPO is
recovery Point objective which is the
maximum amount of data loss your company
is willing to accept as measured in time
RPO always refers to the backups the
number of backups the the frequency of
the backups right because when an outage
happens you can always go back to the
latest backup right and if the latest
backup was data storage right so RPO is
the acceptable amount if the company
wants less RPO RPO is 1R then you should
be planning on taking backups everyone
up if RPO is 12 hours then you should be
planning on taking backups every 12
hours so that's how RPO and RTO you know
helps Disaster Recovery the fourth
question you could get asked is if you'd
like to transfer huge amount of data
which is the best option among a
snowball snowball Edge and snowmobile
again this is a question that get asked
if the company is dealing with a lot of
data transfer into the cloud or if the
company is dealing with the migrating
data into the cloud I'm talking about a
huge amount of data data in petabytes
snowball and all of the snowball series
deals with the petabyte sized data
migrations so there are three options
available as of now AWS snowball is an
data transport solution for moving high
volume of data into and out of a
specified AWS region on the other hand
AWS snowball Edge adds additional
Computing functions snowball is simple
storage and movement of data and
snowball Edge has a compute function
attached to it a snowmobile on the other
hand is an exabyte scale migration
service that allows us to transfer data
up to 100 petabytes that's like 100
000 terabytes so depending on the size
of data that we want to transfer from
our data center to the cloud we can hire
we can rent any of these three services
let's talk about some cloud formation
questions this is a classic question how
is AWS cloud formation different from
AWS elastic bean stock you know from the
surface they both look like the same you
know you don't go through the console
provisioning resources you don't you
know you don't go through CLI and
provision resources both of them
provision resources through click button
right but underneath they are actually
different Services they support they aid
different services so knowing that is
going to help you understand this
question a lot better let's talk about
the difference between them and this is
what you will be explaining to the
interviewer or the panelist so the cloud
formation the cloud formation service
helps you describe and provision all the
infrastructure resources in the cloud
environment on the other hand elastic
bean stock provides an simple
environment to which we can deploy and
run application cloud formation gives us
an infrastructure and elastic bean stock
gives us an small contained environment
in which we can run our application and
cloud formation supports the
infrastructure needs of many different
types of application like the Enterprise
application the Legacy applications and
any new modern application that you want
to have in the cloud on the other hand
the elastic bean stock It's a
combination of developer tools they are
tools that helps manage the life cycle
of as single application so cloud
formation in short is managing the
infrastructure as a whole an elastic
bean stock in short is managing and
running an application in the cloud and
if the company that you're getting hired
is using cloud formation to manage their
infrastructure using or if they're using
infrastructure or any of the
infrastructure as a code Services then
you would definitely face this question
what are the elements of an AWS cloud
formation template so it has a four or
five basic elements right and the
template is in the form of Json or in
yaml format right so it has parameters
it has outputs it has data it has
resources and then the format or the
format version or the file format
version for the cloud formation template
so parameter is nothing but it actually
lets you to specify the type of ec2
instance that you want and the type of
RDS that you want all right so ec2 is an
umbrella RDS is an umbrella and
parameters within that ec2 and
parameters within that rdas are the
specific details of the ec2 or the
specific details of the RDS service so
that's what parameters in a cloud
formation template and then the element
of the cloud formation template is
outputs for example if you want to
Output the name of an S3 bucket that was
created if you want to Output the name
of the ec2 instance if you want to
Output the name of some resources that
have been created instead of looking
into the template instead of you know
navigating through in the console and
finding the name of the resource we can
actually have them outputted in the
result section so we can simply go and
look at all the resources created
through the template in the output
section and that's what output values or
output does in the cloud formation
template and then we have so sources
resources are nothing but what defines
what are the cloud components or Cloud
resources that will be created through
this cloud formation template now ec2 is
a resource RDS is a resource and S3
bucket is a resource elastic load
balancer is a resource and Nat Gateway
is a resource VPC is a reserve so you
see all these components are the
resources and the resource section in
the cloud formation defines what are the
AWS Cloud resources that will be created
through this cloud formation template
and then we have version a version
actually identifies the capabilities of
the template you know we just need to
make sure that it is of the latest
version type and the latest version is
0909 2010 that's the latest version
number you'll be able to find that on
the top of the cloud formation template
and that version number defines the
capabilities of the cloud formation
template so just need to make sure that
it's the latest all the time still
talking about cloud formation this is
another classic question what happens
when one of the results in a stack
cannot be created successfully well if
the resource in a stack cannot be
created the cloud formation
automatically rolls back and terminates
all the resources that was created using
the cloud formation template so whatever
resources that were created through the
cloud formation template from the
beginning let's say we have created like
10 resources and the 11th resource is
now failing cloud formation will roll
back and delete all the 10 resources
that were created previously and this is
very useful when the cloud formation
cannot you know go forward cloud
formation cannot create additional
resources because we have reached the
elastic IP limits elastic IP limit per
region is five right and if you have
already used five IPS and a cloud
formation is trying to buy three more
IPS you know we've hit the soft limit
till we fix that with Amazon cloud
formation will not be able to you know
launch additional you know resources and
additional IPS so it's going to cancel
and roll back everything that's true
with a missing ec2 Ami as well if an Ami
is included in the template and but the
Ami is not actually present then cloud
formation is going to search for the Mi
and because it's not present it's going
to roll back and delete all the
resources that it created so that's what
cloud formation does it simply rolls
back all the resources that it created I
mean if it sees a failure it would
simply roll back all the resources that
it created and this feature actually
simplifies the system administration and
layout Solutions built on top of AWS
cloud formation so at any point we know
that there are no orphan resources in
the in in our environment you know
because something did not work or
because there was an you know cloud
formation executed some there are no
orphanage sources in our account at any
point we can be sure that if cloud
formation is launching a resource and if
it's going to fail and it's going to
come back and delete all the resources
it's created so there are no orphan
resources in our account now let's talk
about some questions in elastic Block
store again if the environment deals
with a lot of automation you could be
thrown this question how can you
automate easy to backup using EBS it's
actually a six step process to automate
the ec2 backups we'll need to write a
script to automate the below steps using
AWS API and these are the steps that
should be found in the scripts first get
the list of instances and then and then
the script that we are writing should be
able to connect to AWS using the API and
list the Amazon ABS volumes that are
attached locally to the instance and
then it needs to list the snapshots of
each volume make sure the snapshots are
present and it needs to assign a
retention period for the snapshot
because over time the snapshots are
going to be invalid right once you have
some 10 latest snapshots any snapshot
that you have taken before that 10
becomes invalid because you have
captured the latest and 10 snapshot
coverage is enough for you and then the
fifth point is to create a snapshot of
each volume create a new snapshot of
each volume and then delete the old
snapshot anytime a new snapshot gets
created the oldest snapshot the list
needs to go away so we need to include
options we need to include scripts in
our script lines in our script that make
sure that it's deleting the older
snapshots which are older than the
retention period that we are mentioning
another question that you could see in
the interview be it written interview
beat online interview or beat and
telephonic or face-to-face interview is
what's the difference between EBS and
instance store let's talk about EBS
first EBS is kind of permanent storage
the data in it can be restored at a
later point when we save data in EBS the
data lives even after the lifetime of
the ec2 instance for example we can stop
the instance and the data is still going
to be present in EBS we can move the EBS
from one instance to another instance
and the data is simply going to be
present there so ABS is kind of
permanent storage when compared to
instance on the other hand instance
store is a temporary storage and that
storage is actually physically attached
to the host of the machine abs is an
external storage and instant store is
locally attached to the instance or
locally attached to the host of The
Machine we cannot detach an instant
store from one instance and attach it to
another but we can do that with EBS so
that's a big difference one is permanent
data and another one is EBS permanent
instant store is a volatile data and
instant store with instant store we
won't be able to detach the storage and
attach it to another instance and
another feature of instant store is data
in an instance store is lost if the disk
fails or the instance is stopped or
terminated so instant store is only good
for storing cache data if you want to
store permanent data then we should
think of using EBS and not instant store
while talking about storage on the same
lines this is another classic question
how can you take backups of EFS like EBS
and if you can take backup how do you
take that backup the answer is yes we
can take EFS to EFS backup solution EFS
does not support snapshot like EBS EFS
does not support snapshot snapshot is
not an option for EFS elastic file
system right we can only take backup
from one EFS to another EFS and this
backup solution is to recover from
unintended changes or deletions of the
EFS and this can be automated but any
data that we store in EFS can be
automatically replicated to another EFS
and once this EFS goes down or gets
deleted or data gets deleted or you know
the whole EFS is for some reason
interrupted or deleted we can recover
the data from we can use the other EFS
and bring the application to consistency
and to achieve this it's not an One Step
configuration it's a cycle there are
series of steps that's involved before
we can achieve EFS to EFS backup the
first thing is to sign in to the AWS
Management console and under EFS or
click on EFS to EFS restore button from
the services list and from there we can
use the region selector in the console
navigation bar to select the actual
region in which we want to work on and
from there ensure that we have selected
the write template you know some of the
templates would be you know EFS to EFS
backup granular backups incremental
backups right so there are some
templates the kind of backups that you
want to take do you want to take
granular do you want to take increment
backups stuff like that and then create
a name to that solution the kind of
backup that we have created and finally
review all the configurations that you
have done and click on Save and from
that point onwards the data is going to
be copied and from that point onwards
any additional data that you put is
going to copy it and replicate it now
you have an EFS to EFS backup this is
another classic question in companies
which deals with a data management there
are easy options to create snapshots but
deleting snapshots is not always an you
know click button or a single step
configuration so you might be facing a
question like how do you Auto delete or
old snapshots and the procedure is like
this as best practice we will take
snapshots of EBS volume to S3 all
snapshots get stored in S3 we know that
now and we can use AWS Ops automator to
automatically handle all snapshots the
Ops automator service it allows us to
create copy delete EBS snapshots so
there are cloud formation templates
available for AWS Ops automator and this
automator template will scan the
environment and it would take snapshots
it would you know copy the snapshot from
one region to another region if you want
I know if you're setting up a Dr
environment and not only that based on
the retention period that we create it's
going to delete the snapshots which are
older than the retention period so life
or managing snapshot is made lot easier
because of this Ops automator cloud
formation template moving into questions
and elastic load balancer this again
could be an a question in the interview
what are the different types of load
balances in AWS and what's their use
case what's the difference between them
and as of now as we speak there are
three types of load balances which are
available in AWS the first one being
application load balancer just like the
name says the application load balancer
works on the application layer and deals
with the HTTP and https request and it
it also supports part based routing for
example simplylearn.com
some web page simplylearn.com another
website so it's going to direct the path
based on the slash value that you give
in the URLs that's path based routing so
it supports that and not only that it
can support a port based a colon 8080
colon 8081 or colon 8090 you know based
on that Port also it can take a routing
decision and that's what application
load balancer does on the other hand we
have Network load balancer and the
network load balancer makes routing
decisions at the transport level it's
faster because it has very less thing to
work on it works on Lower OSI layer it
works on a lower layer so it has very
less information to work with than
compared with application layers so
comparatively it's a lot faster and it
handles millions of requests per second
and after the load balancer receives the
connection it selects a Target group for
the default rule using the flow hash
routing algorithm it does simple routing
right it does not do path based or Port
based routing it does simple routing and
because of it it's faster and then we
have classic load balancer which is kind
of expiring as we speak Amazon is
discouraging people using classic load
balancer but there are companies which
are still using classic load balancer
they are the ones who are the first one
to step into Amazon when classic load
balancer was the first load balancer or
the only load balance answer available
at that point so it supports HTTP https
TCP SSL protocol and it has a fixed
relationship between a load balance
report and the container Port so
initially we only have classic load
balancer and then after some point
Amazon said instead of having one load
balancer address all type of traffic
we're gonna have a two load balances
called as the child from the classic to
load balancer and one is going to
specifically address the application
requirement and one is going to
specifically address the network
requirement and let's call it as
application load balancer and network
load balancer so that's how now we have
two different load balances talking
about load balance another classic
question could be what are the different
uses of the various load balancer in AWS
elastic or load balancing there are
three types of load balancer we just
spoke about it application load balancer
is used if we need a flexible
application management and a TLS
termination and network load balancer if
we require Extreme Performance and the
load balancing should happen on based on
static IPS for the application and
classic load balancer is an old load
balancer which is for people who are
still running their environment from EC
to Classic Network now this is an older
version of VPC or this is what was
present before VPC was created easy to
Classic network is what was present
before ec2 was created so they are the
three types and they are the use cases
of it let's talk about some of this
security related questions you would
face in the interview when talking about
security and firewall and AWS we cannot
avoid discussion talking about Waf web
application firewall and you would
definitely see yourself in this
situation where you've been asked how
can you use AWS Waf in monitoring your
AWS applications Waf or web application
firewall protects our web application
from common web exploits and Waf helps
us control which traffic Source your
application should be be allowed or a
Blog which traffic from certain Source
you know which source or which traffic
from a certain Source should be allowed
or blocked your application with Waf we
can also create custom rules that blocks
common attack patterns you know if it is
a banking application it has a certain
type of attacks and if it is simple data
management data storage application it
has I mean content management
application it has a separate type of
attack So based on the application type
we can identify a pattern and create
rules that would actually block that
attack based on the rule that we create
and Waf can be used for three cases you
know the first one is allow all requests
and then block all requests and count
all requests for a new policy so it's
also an monitoring and Management
Service which actually counts all the
policies or counts all the requests that
matches a particular policy that we
create and some of the characteristics
we can mention in AWS web or the the
origin IPS and the strings that appear
in the request we can allow block based
on Origin IP allow block based on
strings that appear in the request we
can allow block or count based on the
origin country length of the request
yeah we can block and count the presence
of malicious scripts in an connection
you know we can count the request
headers or we can allow block a certain
request header and we can count the
presence of a malicious SQL code in a
connection that we get and that want to
reach our application still talking
about security what are the different
AWS IM categories we can control using
AWS IAM we can do the following one is
create and manage IM users and once the
user database gets bigger and bigger we
can create and manage them in groups and
in IM we can use it to manage the
security credentials kind of setting the
complexity of the password you know
setting addition National
authentications you know like MFA and
you know rotating the passwords now
resetting the password there are a few
things we could do with the IAM and
finally we can create policies that
actually grants access to AWS services
and resources another question you will
see is what are the policies that you
can set for your users password so some
of the policies that we can set for the
user password is at the minimum length
or you know the complexity of the
password by at least having one number
or one special characters in the
password so that's one and then the
requirement of a specific character
types including you know uppercase
lowercase number and non-alphabetic
characters so it becomes very hard for
somebody else to guess what the password
would be and and try to hack them so we
can set the length of the password we
can set the complexity in the password
and then we can set an automatic
expiration of the password so after a
certain time the user is forced to
create a new password so the password is
not still old and easy to guess in the
environment and we can also set settings
like the user should contact the admin I
mean when the password is about to
expire so you know you can get a hold of
how the user is setting their password
is it having good complexity in it is it
meeting company standards or there are
few things that we can control and set
for the users when the users are setting
or recreating the password another
question that could be posted in an
interview so to understand your
understanding of IEM is what's the
difference between an IM role and an IM
user let's talk about IM user let's
touch small and then go big or let's
start simple and then talk about the
complex one the IM user has a permanent
long term credential and it's used to
interact directly with AWS services and
on the other hand IEM role is an IM
entity that defines a set of permissions
for making AWS service request so IM
user is an permanent credential and role
are temporary credentials and IM user
has full access to all AWS IEM
functionalities and with role trusted
entities such as IEM users application
or AWS Services assume are the role so
when an IM user is given an a permission
you know it sticks within the IEM user
but with roles we can give permissions
to Applications we can give permissions
to users in the same account in a
different account the corporate ID we
can give permissions to ec2 S3 RDS VPC
and lot more role is wide and IM user is
is not so wide you know it's very
constrained only for that IM user let's
talk about manage policies in AWS manage
policies there are two types personal
customer managed and Amazon managed so
managed policies are IM resources that
Express permissions using the IAM policy
language and we can create policies edit
them manage them manage them separately
from the IM user group and roles which
they are attached to so they are
something that we can do to managed
policies if it is customer managed and
we can now update policy in one place
and the permissions automatically extend
to all the attached entries so I can
have like three services four Services
point to a particular policy and if I
edit that particular policy it's going
to reflect on those three or four
services so anything that I allow is
going to be allowed for those four
Services anything that I denied is going
to be denied for the four Services
imagine what would be without the IM
managed policy will have to go and
specifically allow deny on those
different instances four or five times
depending on the number of instances
that we have so like I said there are
two types of managed policies one is
managed by us which is customer managed
policies and then the other is managed
by AWS which is AWS managed policy this
question can you give an example of an
IM policy and a policy summary this is
actually to test how well-versed are you
with the AWS console the answer to that
question is look at the following policy
this policy is used to Grant access to
add update and delete objects from a
specific folder now in this case name of
the folder is example folder and it's
present in a bucket called example
bucket so this is an IAM policy on the
other hand the policy summary is a list
of access level resource and conditions
for each service defined in a policy so
IM policy is all about one particular
resource and the policy summary is all
about multiple resources with IM policy
it was only talking about S3 bucket and
one particular S3 bucket here talks
about cloud formation template Cloud
watch logs ec2 elastic bean stock
Services summary summary of resources
and the permissions and policies
attached to them that's what policy
summary is all about another question
could be like this what's the use case
of IAM and how does IM help your
business two important or primary work
of IM is to help us manage IAM users and
their access it provides a secure access
to multiple users to their appropriate
AWS resources so that's one it does and
the second thing it does is manage
access for Federated users Federated
users or non-iam users and through IAM
we can actually allow and provide a
secure access to resources in our AWS
account to our employees without the IM
user no they could be authenticated
using the active directory they could be
authenticated using the Facebook
credential Google credential Amazon
credential and a couple of other
credentials third party identity
management right so we could actually
trust them and we could give them access
to our account based on the trust
relationship that we have built with the
other identity systems all right so two
things one is manage users and their
access for manage IAM user and their
access in our AWS environment and second
is manage access for Federated users who
are non-iam users and more importantly
IM is a free service and with that will
only be charged for the use of the
resources not for the IM username and
password that we create all right let's
now talk about some of the questions in
Route 53 one classic question that could
be asked in an interview is what is the
difference between latency based routing
and Geo DNS or jio based DNS routing now
now the geo-based DNS routing takes
routing decisions on the basis of the
geographic location of the request and
on the other hand the latency based
routing utilizes latency measurements
between networks and data centers now
latency based routing is used where you
want to give your customers the lowest
latency as possible so that's when we
would use latency based routing and on
the other hand a geo-based routing is
when we want to direct customers to
different websites based on the country
they are browsing from you know you
could have you know two different or
three different websites for the same
URL you know take Amazon the shopping
website for example when we go to
amazon.com from in the US it directs us
to the US web page where the products
are different the currency is different
right and the flag and and a couple of
other advertisements that shows up are
different and when we go to amazon.com
from India it gets directed to the
amazon.com Indian site where again the
currency the product and the
advertisements they're all different
right so depending on the country
they're trying to browse if you want to
direct customers to two or three
different websites we would use a
geo-based routing another use case of
geo-based routing is if you have a
compliance that you should handle all
the DNS requests sorry if you should
handle all the requests you know from a
country within the country then you
would do geo based routing now you
wouldn't direct the customer to a server
which is in another country right you
would direct the customer to a server
which is very local to them that's
another use case of geo-based routing
and like I said for latency based
routing the whole goal or aim is to
achieve minimum end user latency if you
are hired for the arcade detect role and
if that requires working a lot on the
DNS then you could be posted with this
question what is the difference between
domain and a hosted Zone a domain is
actually a collection of data describing
a self-contained administrative and
Technical unit on the internet right so
for example you know simplylearn.com is
actually a domain on the other hand
hosted zone is actually an container
that holds information about how you
want to Route traffic on the internet to
a specific domain for example
lms.simplyleon.com is an hosted Zone
whereas simplylearn.com is an domain so
in other words in hosted Zone you would
see the domain name plus and a prefix to
it LMS is a prefix here FTP is a prefix
mail.simplyron.com is a prefix so that's
how you would see a prefix in hosted
zones another question from another
classic question from Route 53 would be
how this Amazon Route 53 provide High
availability and low latency the way
Amazon Route 53 provides High
availability and low latency is by
globally distributed DNS servers Amazon
is a global Service and they have DNA
Services globally any customer doing a
query from different parts of the world
they get to reach an DNS server which is
very local to them and that's how it
provides low latency now this is not
true with all the DNS providers there
are DNS providers who are very local to
a country who are very local to a
continent so they don't they generally
don't provide low latency service right
it's always high latency it's low
latency for local users but anybody
browsing from a different country or a
different continent it's going to be
high latency for them but that's not
again true with Amazon Amazon is a
globally distributed DNS provider it has
DNS service Global wide and like I said
it has optimal locations it has got
global Service or in other words it has
got servers around the globe different
parts in the globe and that's how they
are able to provide High availability
and because it's not running on just one
server but on many servers they provide
High availability and low latency if the
environment that you're going to work on
is going to take a lot of configuration
backups environmental backups then you
can expect questions in AWS config a
classic question would be how does AWS
config work along with AWS cloudtrail
AWS cloudtrail actually records user API
activity on the account and you know any
HTTP https access or any any sort of
access I know that's made to the cloud
environment that's recorded in the cloud
trail in other words any APA calls the
time is recorded the type of call is
recorded and what was the response given
was it a failure was it successful they
also get recorded in cloudtrail it's
actually log it actually records the
activity in your Cloud environment on
the other hand config is an a point in
time configuration details of your
resources for example at a given point
what are all the resources that were
present in my environment what are all
the resources or what are the
configuration in those resources at a
given point they get captured in AWS
config all right so with that
information you can always answer the
question what did my AWS resource look
like at a given point in time that
question gets answered when we use AWS
config on the other hand with cloudtrail
you can answer the question I mean by
looking at the cloud trail or with the
help of cloudtrail you can easily answer
the question who made an APA call to
modify this resource that's answered by
cloudswail and with cloudtrail we can
detect if a security group was
incorrectly configured and who did that
configuration let's say there happened
to be in downtime and you want to
identify let's say there happened to be
a downtime and you want to identify who
made that change in the environment you
can simply look at cloudtrail and find
out who made the change and if you want
to look at how the environment looks
like before the change you can always
look at AWS config can AWS configure or
AWS config aggregate data across
different AWS accounts yes it can now
this question is actually to test
whether you have used AWS config or not
I know some of the services are very
local is it some of these services are
availability Zone specific some of them
are Regional specific and some of them
are Global Services in Amazon and though
some of the services are region Services
you still can do some changes you know
add some configuration to it and collect
Regional data in it for example S3 is a
regional service but still you can
collect logs from all of the regions
into an S3 bucket in one particular
region that's possible and cloud trail
is and Cloud watch is a regional service
but still you can with some changes to
it with some adding permissions to it
you can always monitor the cloud watch
that belongs to Cloud watch logs that
belongs to other regions you know
they're not Global by default but you
can do some changes and make it Global
similarly AWS config is a service that's
a region-based service but still you can
make it act globally we can aggregate
data across a different region and
different accounts in an AWS config and
deliver the updates from different
accounts to one S3 bucket and can access
it from there AWS config also works or
integrates seamlessly with SNS topic so
you know anytime there is a change
anytime a new data gets collected you
can always notify yourself or notify a
group of people about the new log or the
new config or new edit that happened in
the environment let's look at some of
the database questions you know database
should be running on reserved instances
right so whether you know that fact or
not the interviewer wants to understand
how well you know that fact by asking
this question how are reserved instances
different from on-demand DB instances
reserved instances and on-demand
instances are exactly the same when it
comes to their function but they only
differ based on how they are built
reserved instances are purchased for one
year or three year reservation and in
return we get a very low per hour
pricing because we're paying up front
it's generally said that reserved
incidence is 75 percentage cheaper than
on-demand instance and Amazon gives you
that benefit because you know you're
committing for one year and sometimes
you're paying in advance for the whole
year on the other hand on-demand
instances are built on an hourly hourly
price talking about Auto scaling how
will you understand the different types
of scaling the interviewer might ask
this question which type of scaling
would you recommend for RDS and why the
two types of scaling as you would know
now vertical and horizontal and in
vertical scaling we can vertically scale
up the master database with a couple of
clicks all right so that's vertical
scaling vertical scaling is keeping the
same node and making it bigger and
bigger if previously it was running on
T2 micro now we would like to run it on
M3 two times large instance previously
it had one virtual CPU one gigabit now
it's gonna have eight virtual CPU and 30
gigabit of ram so that's vertical
scaling on the other hand the horizontal
scaling is adding more nodes to it
previously it was running on one VM now
it's going to run on 2 3 10 VMS right
that's horizontal scaling so database
can only be scaled vertically and there
are 18 different types of instances we
can resize our rds2 all right so this is
true for ideas MySQL postgres SQL
mariadb Oracle Microsoft SQL servers
there are 18 type of instances we can
vertically scale up to on the other hand
horizontal scaling are good for replicas
so they are read-only replicas we're not
going to touch the master database we're
not going to touch the primary database
but I can do horizontal scaling only
with Amazon Aurora and I can add
additional read replicas I can add up to
15 Read replicas for Amazon Aurora and
up to five a read replicas for RDS MySQL
postgres SQL and mariadb RDS instances
and when we add replica we are
horizontally scaling adding more nodes
right I read only nodes so that's
horizontal scaling so how do you really
decide between vertical and horizontal
scaling if you're looking into increase
the storage and the processing capacity
we'll have to do a a vertical scaling if
you are looking at increasing the
performance or of the read heavy
database we need to be looking for
horizontal scaling or we need to be
implementing horizontal scaling in our
environment still talking about database
this is another good question you can
expect in the interview what is the
maintenance window and Amazon RDS will
your DB instance be available during the
maintenance event right so this is
really to test how well you have
understood the SLA how well you have
understood the Amazon rdas the failover
mechanism of Amazon rdas stuff like that
so RDS maintenance window it lets you
decide when ADB instance modification a
database engine upgrades or software
patching has to occur and you you
actually get to decide should it happen
at 12 in the night or should it happen
at afternoon should it happen early in
the morning should it happen in the
evening you actually get to decide an
automatic scheduling by Amazon is done
only for are patches that are security
and durability related sometimes Amazon
takes down and does automatic scheduling
if you know if there is a need for a
patch update that deals with security
and durability and by default the
maintenance window is is for 30 minutes
and the important point is the DB
instance will be available during that
event because you're going to have
primary and secondary rights so when
that upgrade happens Amazon would shift
the connection to the secondary do the
upgrade and then switch back to the
primary another classic question would
be what are the consistency models in
dynamodb in dynamodb there is eventual
consistency read this eventual
consistency model it actually maximizes
your read throughput and the best part
with eventual consistency is all copies
of data reach consistency within a
second and sometimes when you write and
when you're you know trying to read
immediately chances that you would still
be reading the old data that's eventual
consistency on the other hand there is
another consistency model called the
strong consistency you are strongly
consistent read where there is going to
be a delay in writing the data you know
making sure the data is written in all
places but it guarantees one thing that
is once you have done a write and then
you're trying to do a read it's going to
make sure that it's going to show you
the updated data not the old data now
you can be guaranteed of it that it is
going to show the updated data and not
the old data that's strongly consistent
still talking about database talking
about nosql dynamodb or nosql database
which is dynamodb and Amazon you could
be asked this question what kind of
query functionality does dynamodb
support dynamodb supports get and put
operation dynamodb supports or dynamodb
provides a flexible querying by letting
in your query on non-primary key
attributes using Global secondary index
and local secondary indexes a primary
key can be either a single attribute
partition key or a composite partition
sort key in other words a dynamodb
indexes a composite partition sort key
as a partition key element and a sort
key element and by holding the partition
key you know when doing a search or when
doing a query by holding the partition
key element constant we can search
across the sort key element to retrieve
the other items in that table and at the
composite partition sort key should be a
combination of user ID partition and a
timestamp so that's what the composite
partition sort key is made of let's look
at some of the multiple choice questions
you know sometimes some companies would
have an written test or an McQ type
online test before they call you for the
first level or before they call you for
the second level so these are some
classical quests instance that companies
asked or companies ask in their multiple
choice online questions let's look at
this question as a developer using this
pay-per-use service you can send store
and receive messages between software
components which of the following is
being referred here let's look at it
right we have AWS step functions Amazon
mq Amazon simple Q service Amazon simple
notification service let's read the
question again as a developer using this
pay-per-view service so the service that
we are looking for is a paper or your
service you can send store and retrieve
messages between two software components
kind of like a queue there so what would
be the right answer it would be Amazon
simple Q service now Amazon simple queue
service is the one that's used to
decouple the environment it breaks the
tight coupling and then it introduces
decoupling in that environment by
providing a queue or by inserting a
queue between two software components
let's look at this other question if you
would like to host a real-time audio and
video conferencing application on AWS
right it's an audio and video
conferencing application on AWS this
service provides you with a secure and
easy to use application what is this
service let's look at the options they
are Amazon chime Amazon workspace Amazon
mq Amazon appstream all right you might
tend to look at Amazon appstream because
it's real time and video conference but
it's actually for a different purpose
it's actually Amazon chime that lets you
create chat and create a chat board and
then collaborate with the security of
the AWS services so it lets you do the
audio it's lets you do the video
conference all supported by AWS security
features it's actually Amazon China
let's look at this question as your
company's aw solution architect you are
in charge of Designing thousands of
individual jobs which are similar which
of the following service best serves
your requirement AWS ec2 Auto scaling
AWS snowball AWS fargate AWS batch let's
read the question again as your
company's AWS solution architect you are
in charge of Designing thousands of
individual jobs which are similar it
looks like it's batch service let's look
at the other options as well AWS
snowball is actually an storage
Transport service ec2 auto scaling is
you know in introducing scalability and
elasticity in the environment and AWS
far gate is container services AWS batch
is the one is being referred here that
actually runs thousands of individual
jobs which are similar AWS batch it's
the right answer let's look at the other
one you are a machine learning engineer
and you're looking for a service that
helps you build and train machine
learning models in AWS which among the
following are we referring to so we have
Amazon sagemaker and AWS deep lens
Amazon on comprehend AWS device form
let's read the question again you are a
machine learning engineer and you're
looking for a service that helps you
build and train machine learning models
in AWS which among the following are
referred here the answer is Sage maker
it provides every developer and data
scientist with the ability to build
train and deploy machine learning models
quickly that's what sagemaker does now
for you to be familiar with you know the
products I would recommend you to you
know simply go through the product
description you know there's one page
available on Amazon that explains all
the products a quick neat and simple and
that really helps you to be very
familiar with you know what the product
is all about and what it is capable of
you know is it the DB service is it a
machine learning service or is it a
monitoring service is it a developer
service stuff like that so get that
information get that details before you
attend an interview and that should
really help to answer or face such
questions with great confidence so the
answer is Amazon Sage maker because
that's the one that provides developers
and a data scientist the ability to
build the train and deploy machine
learning models quickly as possible all
right let's look at this one let's say
that you are working for your company's
ID team and you are designated to adjust
the capacity of the AWS resource based
on the incoming application and network
traffic how do you do it so what's the
service that's actually helps us to
adjust the capacity of the AWS resource
based on the incoming application let's
look at it Amazon VPC Amazon IAM Amazon
inspector Amazon elastic load balancing
Amazon VPC is a networking service
Amazon IAM is an username password
authentication Amazon inspector is a
service that actually does a security
audit in our environment and Amazon
elastic load balancer is a service that
helps in scalability that's in one way
you know indirectly that helps in yeah
increasing the availability of the
application right and monitoring it
monitoring you know how much requests
are coming in through the elastic load
balancer we can actually adjust the
environment that's running behind it so
the answer is going to be Amazon elastic
load balancer all right let's look at
this question this cross-platform video
game development engine that supports PC
Xbox PlayStation IOS and Android
platforms allows developers to build and
host their games on Amazon's servers so
we have Amazon game lift Amazon green
grass Amazon Lumberyard Amazon Sumerian
let's read the question again this
cross-platform video game development
engine that supports PC Xbox PlayStation
IOS and Android platforms allows
developers to build and host their games
on Amazon servers the answer is Amazon
lumber yard this lumber yard is an free
AAA gaming engine deeply integrated with
the AWS and twitch with full source this
lumba yard provides a growing set of
tools that helps you create an highest
game quality applications and they
connect to a lot of games and vast
compute and storage in the cloud so it's
that service they are referring to let's
look at this question you are the
project manager of your company's Cloud
architect team you are required to
visualize understand and manage your AWS
cost and usage over time which of the
following service will be the best fit
for this we have AWS budgets we have AWS
cost Explorer we have Amazon workmail we
have Amazon connect and the answer is
going to be cost Explorer now cost
Explorer is an option any of the Amazon
console that helps you to visualize and
understand and even manage the AWS cost
over time who's spending more who's
spending less and what is the trend what
is the projected cost for the coming
month all these can be visualized in AWS
cost Explorer let's look at this
question you are a chief Cloud architect
at your company and how can you
automatically Monitor and adjust
Computer Resources to ensure maximum
performance and efficiency of all
scalable resources so we have a cloud
formation as a service we have AWS
Aurora as a solution we have AWS Auto
scaling and Amazon API Gateway let's
read the question again you're the chief
Cloud architect at your company how can
you automatically Monitor and adjust
Computer Resources how can you
automatically Monitor and adjust
Computer Resources to ensure maximum
performance and efficiency of all
scalable resources this is an easy
question to answer the answer is auto
scaling all right that's a basic service
and solution architect course is it auto
scaling is the service that helps us to
easily adjust Monitor and ensure the
maximum performance and efficiency of
all scalable resources it does that by
automatically scaling the environment to
handle the inputs let's look at this
question as a database administrator you
will use a service that is used to set
up and manage databases such as MySQL
mayodb and postgres SQL which service
are we referring to Amazon Aurora Amazon
elastic cache AWS RDS AWS database
migration service Amazon Aurora is
Amazon's flavor of the RDS service and
elastic cache is is the caching service
provided by Amazon they are not
full-fledged database and database
migration service just like the name
says it helps to migrate the database
from on-premises to the cloud and from
one a database flavor to another
database flavor Amazon RDS is the
service is the console is the service is
the umbrella service that helps us to
set up manage our databases like MySQL
maridb and postgres SQL it's Amazon RTS
let's look at this last question a part
of your marketing work required is you
to push messages to onto Google Facebook
Windows and Apple through apis or AWS
Management console you will use the
following service so the options are AWS
cloudtrail AWS config Amazon chime AWS
simple notification service all right it
says a part of your marketing work
requires you to push messages it's
dealing with pushing messages to Google
Facebook Windows and Apple through apis
or AWS Management console you will use
the following service it's simple
notification service simple notification
service is a message pushing service and
sqs is pulling similarly SMS is pushing
right here it talks about a pushing
system that pushes messages to Google
Facebook Windows and Apple through API
and it's going to be a simple
notification system or a simple
notification service thank you for
joining us in this Amazon web service
bootcamp video as you can see AWS is a
highly
with a wide range of topics and
applications to learn our simply learns
AWS Cloud architect master program will
provide with all the necessary skills
and knowledge to become a successful AWS
professional if you have any questions
or want to learn more please feel free
to leave a comment below and we will be
happy to help you out thank you for
watching and we will see you in the next
video
foreign
hi there if you like this video
subscribe to the simply learned YouTube
channel and click here to watch similar
videos turn it up and get certified
click here