welcome to this new video by simply
learn kubernetes is an open source
container orchestration platform it
automates the deployment and management
of containerized applications key
features include cluster architecture
ports for grouping containers Auto
scaling service Discovery self-healing
and extensibility kubernetes simplifies
modern application deployment making it
efficient scalable and reliable as
growth of Technology increases demand of
these professional is also growing the
average devops kubernetes engineer
salary in USA is 146 000 per year on
that note if you are looking to learn
kubernetes then get certified in devops
with postgraduate program in devops by
simply learn this program offers Blended
learning that combines live online
devops certification classes with
interactive Labs that gives you hands-on
experience you will gain expertise in
tools like terraform kubernetes Maven
ansible Jenkins Docker junit get and
more we prefer one year of Prior
experience to enroll in this course so
why wait enroll the course from the link
in the description box before we begin
consider subscribing to Simply learn and
hit the Bell icon to never miss any
updates from us
we're going to break up this
presentation into four key areas we're
going to talk about life before
kubernetes which some of you are
probably experiencing right now what is
kubernetes the benefits that kubernetes
brings to you particularly if you are
using containers in a devops environment
and then finally we're going to break
down the architecture and working
infrastructure for kubernetes so you
understand what's happening and why the
actions are happening the way that they
are so let's jump into our first section
of life before kubernetes so the way
that you have done work in the past or
you may be doing work right now is
really building out and deploying
Solutions into two distinct areas one is
a traditional deployment where you're
pushing out code to physical servers in
a Data Center and you're managing the
operating system and the code that's
actually running on each of those
servers another environment that you may
potentially be using is to Blind code
out to Virtual machines so let's go
through and look at the two different
types of deployment that you may be
experiencing when you have applications
running on multiple machines you run
into the potential risk that the setup
and configuration of each of those
machines isn't going to be consistent
and your code isn't going to work
effectively and there may be issues with
uptime and errors within the
infrastructure of your entire
environment there's going to be around
problems with resource allocation and
you're going to have error issues where
applications may be running effectively
and not look not effectively and not
load balance effectively across the
environment the problem that you have
with this kind of infrastructure is that
it gets very expensive you can only
install one piece of software one
service on one piece of Hardware so your
Hardware is being massively
underutilized this is where virtual
machines have become really popular with
a virtual machine you're able to have
better resource utilization and
scalability a much less cost and this
allows you to be able to run multiple
virtual machines on a single piece of
Hardware the problem is is that VMS are
for virtual machines are not perfect
either some of the challenges you run
with VMS is that the actual hardware and
software need needed to manage the VM
environment can be expansive there are
security risks with virtual with VMS
there are security risks with VMS there
have been data breaches recorded about
solutions that run in virtualized
environments you also run into an issue
of availability and this is largely
because you can only have a finite
number of virtual machines running on a
piece of hardware and this results in
limitations and restrictions in the
types of environment you want to be
running and then finally setting up and
managing a virtualized environment is
time consuming it can take a lot of time
and it can also get very expensive so
how about kubernetes well kubernetes is
a tool that allows you to manage
containerized deployment of solutions an
inherently kubernetes is a tour that is
really a Next Level maturity of
deployment so if you can think of your
maturity curve as deploying code in
directly to Hardware in a Data Center
and then deploying your solutions to
Virtual machines the next evolution of
that deployment is to use containers and
kubernetes so let's kind of go through
and look at the differences between a
virtual machine and kubernetes and we've
got a few here that we want to highlight
and you'll get an understanding of what
the differences are between the two so
first of all with virtual machines there
is inherently security risks and what
you'll find as we get dig through the
architecture later in the presentation
is that kubernetes is inherently secure
and this is largely because of the
Legacy code the Legacy see of kubernetes
and where it came from we will talk
about that in just a moment but
kubernetes is inherently secure virtual
machines are not easily portable now
with that said they they are technically
portable they're just not very easily
portable whereas with kubernetes it's
working with Docker container Solutions
it is extremely portable that means that
you can actually spin up and spin down
and manage your infrastructure exactly
the way that you want it to be managed
and scale it on the demands of the
customers as they're coming in to use
the solution from a time consuming point
of view kubernetes is much less time
consuming than with a virtual machine a
few other areas that we want to kind of
highlight from differences virtual
machines use much less isolation when
building out the encapsulated
environment then kubernetes does for
instance with a virtual machine you have
to run hypervisor on top of the OS and
hardware and then inside of the a
virtual machine you also have to have
the operating system as well whereas in
contrast on a kubernetes environment
because it's leveraging a Docker
container and or container-like
Technologies it only has to have the OS
and the hardware and then inside of each
container it doesn't need to have that
additional iOS layer it's able to
inherit what it needs to be able to run
the application this makes the whole
solution much more flexible and allows
you to run many more containers on a
piece of Hardware than versus running
virtual machines on a single piece of
Hardware so as we highlighted here our
VMS are not as portable as q and
kubernetes and kubernetes is portable
directly related to the use of
containerization and because kubernetes
is built on top of containers it is much
less time consuming because you can
actually script and automatically
allocate resource to nodes within your
kubernetes environment because it allows
the infrastructure to run much more
effectively and much more efficient so
this is why if we look at our evolution
of the land of time before kubernetes
while we are running into a solution
where kubernetes had to come about
because the demand for having more
highly scalable solutions that are more
efficient was just really a natural
evolution of this software deployment
model that started with pushing out code
to physical hardware and then pushing
code out to Virtual machines and then
needing to have a solution much more
sophisticated kubernetes would have come
about at some point in time I'm just
really glad it came back when it did so
what is kubernetes let's dig into the
history of kubernetes and how it came
about so in essence kubernetes is an
open source platform that allows you to
manage and deploy and maintain groups of
containers and a container is something
like Docker and if you're a developing
code you're probably already using
Docker today consider kubernetes as the
tool that manages multiple Docker
environments together now we talk a lot
about DACA and as a container solution
with kubernetes the reality is is that
kubernetes can actually use other
container tools out there but Docker
just simply is the most popular
container out there both these tools are
open source that's why they're so
popular and they just allow you to be
able to have flexibility in being able
to scale up your Solutions and they were
designed for the post-digital world that
we live and exist in today so a little
bit of background a little bit of trivia
around kubernetes so kubernetes was
originally a successor to a project at
Google and the original project was
Google ball um Google Borg it does
exactly what kubernetes done does today
but kubernetes was Rewritten from the
ground up and then released as an open
source project in 2014 so that people
outside of Google could take advantage
of the power of kubernetes
containerization management tools and
today it is managed by the cloud native
Computing foundation and there are many
many companies that support and manage
kubernetes so for instance if you're
signing up for Microsoft Azure AWS
Google Cloud all of them will leverage
kubernetes and it's just become the the
de facto tool for managing large groups
of containers so let's kind of Step
through some of the key benefits that
you'd experience from kubernetes and so
we have nine key benefits the first it
is highly portable and is 100 open
source code and this means that you can
actually go ahead and contribute to this
code project if you want to through
GitHub the ability to scale up the
solution is incredible
um what's the the history of kubernetes
being part of a Google project for
managing the Google network and
infrastructure kind of really sets the
groundwork for having a surgeon that is
highly scalable the out of the high
scalability also comes the need for high
avail ability and this is the desire to
be able to have a highly efficient and
highly energized environment that also
you can really rely on so if you're
building outer kubernetes management
environment you know that it's going to
be available for the solutions that
you're maintaining and it's really
designed for deployment so you can
script out the environment and actually
have it as part of your devops model so
you can scale up and meet the demands of
your customer then what you'll find is
that the load balancing is extremely
efficient and it allows you to
distribute the load efficiently across
your entire network so your network
remains stable and then also the tool
allows you to manage the orchestration
of your storage so you can have local
storage such as an SSD on the hardware
that the kubernetes is maintaining or if
the kubernetes environment is pulling
storage from a public Cloud such as
Azure or AWS you can actually go ahead
and make that available to your our
system and you can inherit the security
that goes back and forth between the
cloud environments and one of the things
you'll find consistent with kubernetes
is that it is designed for a cloud-first
environment um kubernetes as well is
that it's it's really a self-healing
environment so if something happens or
something fails uh kubernetes will
detect that failure and then either
restart the process kill the process or
replace it and then because of that you
also have automated rollouts and
rollbacks in case you need to be able to
manage the state of the environment and
then finally you have automatic bin
packaging so you can actually specify
the compute power that's being used from
CPU and ramp for each container so let's
dig into the final area which is the
actual kubernetes architecture I'm going
to cover this at a high level there's
actually another video that you can that
simply learn has developed which digs
deeper into the kubernetes architecture
and so the kubernetes architecture is a
constant based architecture and it's
really about two key areas you have the
kubernetes master which actually
controls
um all of the activities within your
entire kubernetes infrastructure and
then you have nodes that actually are
running on Linux machines
um outs that are controlled by the
master so let's kind of go through some
of these areas so if we look at the
kubernetes master to begin with
um we'll start with uh Etc is this is a
tool that allows for the configuration
and the information and the management
of nodes within your cluster and one of
the key features that you'll find with
all of the tools that are managed within
either a the master environment or
within a node is that they are all
accessible via the API server and what's
interesting about the API server is that
it's a restful based infrastructure
which means that you can actually secure
each connection with SSL and other um
security models to ensure that your
entire infrastructure and the
communication going back and forth
across your infrastructure is tightly
secured scheduler goes ahead and
actually as you'd expect actually
manages the schedule of activities
within the actual cluster and then you
have the control and the controller is a
Daemon server that actually manages and
pushes out the instructions to all of
your nodes so the other tools really are
the the infrastructure and you can
consider them the administration site um
of the master whereas controller is the
management it actually pushes out all of
the controls via the API server so let's
actually dig into one of the actual
nodes themselves and there are three key
areas of the nodes one is the docker
environment which actually helps and
manage and maintain the container that's
actually inside of the node and then you
have the Kubler which is responsible for
information that goes back and forth and
it's going to do most of the
conversation with the API server on the
actual health of that node and then you
have of the actual kubernetes proxy
which actually runs the services
actually inside of the node so as you
see all of these infrastructures are
extremely lightweight and designed to be
very efficient and very available for
your infrastructure and so here's a
quick recap of the different tools that
are available and it really breaks down
into two key areas you have your
kubernetes master and the kubernetes
node the kubernetes Mazda has the
instructions of what's going to happen
within your kubernetes infrastructure
and then it's going to push out those
instructors to an indefinite number of
nodes that will allow you to be able to
scale up and scale down your solution in
a dynamic way
kubernetes is an open source platform
used for deployment and management of
containers so let's get started now the
first step is to install the necessary
dependencies using two of the commands
first is sudo app get update which gets
all the updates
and then you have to enter a password
so this will take a few seconds
that's done the next step is to install
app transport https this is basically
used to make repositories via https so
let's go ahead with that sudo
okay that's done going ahead the next
thing we have to do is to install the
docker dependency using the command sudo
apt install.co.io
you have to choose why
and this will take a few minutes
okay that's done so after installing the
docker we have to start and enable the
docker using the command sudo systemctl
start Docker and sudo system CTU
enabledoc
so now the docker is enabled so now we
are done with the first step of
installation the next step is we have to
install the necessary components for
kubernetes before that we have to
install the curl command because curl
command is used to send data using a URL
syntax let's install the curl command
using the command sudo apt-get install
code
so you have to select Y and this will
take few minutes
so moving on the next step we have to do
is download an add key for kubernetes
installation from a URL so let's go
ahead with that pseudo curl
so this is where we get the key and then
we have to add it using sudo app key add
and it's done so the next step is we
have to add a repository in a certain
location so before doing that we have to
change the permission of that file so we
do that using C or sudo CH mode command
okay so the permission has changed now
let's go ahead and save this file in
that location first we have to enter
this command into the file
this is a URL
save as
so we have to save it in this location
let's rename this to
kubernetes
dot list
now let's see
okay so here it is now moving forward
now we have to check for any updates
available
in the next command we are going to
install the kubernetes components that
is cubelet cubadian Q vector and
kubernetes cni so let's go ahead with
that
so this is going to take a few minutes
now that's done so the next step we have
to initialize the master node and to do
this we have to first use a swap of
command to disable the swapping on other
devices so let's do that sudo swap off
hyperly
now let's go ahead with the
initialization sudo
Cube EDM in it
this is going to take a few minutes
okay so before going to the next step to
start the cluster we have to use these
three commands let's just copy paste it
okay that's done
so moving forward the next step is to
deploy Parts using the following command
now the pods have been deployed to our
Network to see all the pods you have to
use the command pseudo Q Vector get
parts
there you can see the posts that have
been deployed
so we're going to break out the
presentation into the following areas
we're going to cover what kubernetes is
and why you'd want to be using it we're
going to introduce the actual
architecture and provide an overview of
how it contains the containers and the
other components that you'd have within
the architecture will also compare
kubernetes with Docker swarm and one of
the things that you hear with kubernetes
almost in the same breath as Docker and
we have to be careful not to confuse
Docker from a container point of view
and Docker swarm which is a container
management tool and kubernetes is also a
container management tool so compare the
two of those and then we'll look at the
hardware and software components and
then finally do a deep dive into the
architecture of kubernetes and provide a
case study of where kubernetes is being
used successfully in the past so let's
jump into this so what actually is
kubernetes so kubernetes is a tool that
was designed to actually help manage and
contain large groups of containers it
was developed originally by Google out
of the Google Borg solution and then
opens Source by Google the actual
environment allows you to manage large
groups of containers so if you are a
developer and devops and you're working
with Docker then you're used to the
concept of the container and you'll also
know that containers and Cloud
Technologies tend to be almost breathed
in the same breath so the work that
you're doing with kubernetes is to be
able to manage large groups of
containers inside of the cloud the thing
that's great about working with
kubernetes is that it is incredibly
flexible and allows you to have
incredibly complex applications run
efficient efficiently so let's step into
why you'd want to use kubernetes so a
couple of key points kubernetes itself
is an open source solution originally
developed by Google but it is available
on most Cloud platforms today so AWS
Google Cloud Microsoft Azure all of them
support kubernetes and what you'll find
is as you're setting up your
infrastructure particularly if you're
using containers you'll see that the
support for kubernetes is floated up
very very efficiently and effectively by
all three vendors it's extremely um
useful for being able to manage large
modular environments and that's really
kind of these I think one of the big
benefits of kubernetes is its modularity
is that it really breaks down uh
containers into their smaller parts and
once you do that it becomes much more
efficient for you as an administrator to
be able to manage that entire entire
environment the reproductibility of
kubernetes is extremely high you can
build out infrastructures very quickly
and have them being able to imagine and
have containers coming on and off um and
killed and created to be able to help
load balance your entire environment and
so again you know we keep talking about
containers but that's really what
kubernetes is all about is being able to
manage your applications that are
managed through containers and being
able to do that in a virtualized
infrastructure and the thing that's also
really good with kubernetes is it's
really easy to deploy Solutions you just
have to use a simple curl call and you
can actually push out your kubernetes
infrastructure so let's have an overview
of the kubernetes architecture so
kubernetes has really broken up into
three key areas you have your
workstation where you develop your
commands and you push out those commands
to your master and the master is
comprised of four key areas which
essentially control all of your nodes
and and the node contains multiple parts
and each pod has your Docker container
built into it so consider that you could
have a really almost an infinite number
of PODS I'm sorry infinite number of
nodes are being managed by the master
environment so you have your cluster
which is a collection of servers that
maintain the availability and the
compute power such as RAM CPU and disk
utilization and you have the master
which is really components that control
and schedule the activities of your
network and then you have the node which
actually hosts the actual Docker virtual
machine itself and be able to actually
control and communicate back to the
master the health of that pod and we'll
get into more detail on the architecture
later in the presentation so you know
you keep hearing me talk about
containers but they really are the
center of the work that you're doing
with kubernetes and can the concept
around kubernetes and containers is
really just a natural evolution of where
we've been with internet and digital
Technologies over the last uh 10 15
years so before kubernetes you had tools
where you either running virtual
machines or you're running data centers
that had to maintain and manage and
notify you of any interruptions in your
network kubernetes is the tool that
actually comes in and helps address
those interruptions and manages them for
you so the solution to this is the use
of containers so you can think of
containers as that Natural Evolution
from you know uh 15 20 years ago you
would have written your code code and
posted it to a data center more recently
you probably posted your code to a
virtual machine and then move the
virtual machine and now you actually
just work directly into a container and
everything is self-contained and can be
pushed out to your environment and the
thing that's great about containers
they're they're isolated environments
very easy for developers to work on them
but it's also really easy for operations
teams to be able to move a container
into production so let's kind of step
and back and look at a competing product
to kubernetes which is Docker swarm now
one of the things we have to remember is
that darker containers which are
extremely popular
um built by the company Docker and made
open source and Docker actually has
other products one of those other
products is Docker swarm and Docker
swarm is a tool that allows you to be
able to manage multiple containers so if
we look at some of the uh the benefits
of using Docker swarm versus kubernetes
and one thing that you'll find is that
both tools have strengths and weaknesses
but it's really good that they're both
out there because it helps keep they
really kind of justifies the importance
of having these kind of tools so
kubernetes was designed originally from
the ground up to be Auto scaling whereas
duka swarm isn't the load balancing is
automatic on Docker Swan whereas with
kubernetes you have to manually
configure load balancing across your
nodes the installation for Docker swarm
is really fast and easy I mean you can
be up and running within minutes
kubernetes takes a little bit more time
is a little bit more complicated
eventually you'll get there um I mean
it's not like it's going to take you
days and weeks but it's it is a tool
that's a when you compare the two Docker
swarms much easier to get up and running
now what's interesting is that
kubernetes is incredibly scalable and
it's you know that's its its real
strength is its ability to have strong
clusters whereas with Docker swarm it's
cluster strength isn't as strong when
compared to kubernetes now you compare
it to anything else on the market it's
really good um so this is kind of a
splitting hairs kind of comparison but
kubernetes really does have the
advantage here if you're looking at the
two compared to each other for
scalability I mean kubernetes was
designed for by Google to scale up and
support Google Cloud Network
infrastructure they both allow you to be
able to share um storage volumes with
Docking you can actually do it with any
container with uh that is managed by the
docker swamp whereas with kubernetes it
manages the storage with the pods and
our product can have multiple containers
within it but you can't take it down to
the level of the container interestingly
uh kubernetes does have a graphical user
interface for being able to control and
manage the environment the reality
however is that you're likely to be
using terminal to actually make the
controls and Commands to control your
either Docker swarm or kubernetes
environment and it's great that it has
is a GUI to get you started but once
you're up and running you're going to be
using terminal window for those fast
quick administrative controls that you
need to make so let's look at the
hardware components for kubernetes so
what's interesting is that kubernetes is
extremely light out of all the systems
that we're looking at it's extremely
lightweight
um it allows you to have you compare it
to like a virtual machine which is very
heavy at you know kubernetes is
extremely lightweight and other uses any
resources at all interesting enough
though is that if you are looking at the
usage of CPU it's better to actually
take it for
the cluster as a whole rather than
individual nodes because the nodes will
actually combined together to give you
that whole compute power again this is
why kubernetes works really well in the
cloud where you can do that kind of
activity rather than if you're running
in your own data center
um so you can have persistent volumes um
such as a local SSD or you can actually
attach to a cloud data storage again
kubernetes is really designed for the
cloud I would encourage you to use cloud
storage wherever possible rather than
relying on physical storage the reason
being is that if you connect to cloud
storage and you need to flex your your
storage the cloud will do that for you I
mean that's just an inherent part of why
you'd have cloud storage whereas if
you're connecting to physical storage
you're always restricted to the
limitations of the physical Hardware so
that's um kind of pivot and look at the
software components as compared to the
hardware components so the main part of
the components is the actual container
and all of the software running in the
container runs on Linux so if you have
documents stored as a developer on your
machine it's actually running inside of
Linux and that's what makes it so
lightweight and really one of the things
that you'll find is that most data
centers and Cloud providers now are
running predominantly on Linux inside of
the the container itself is then managed
inside of a pod and a part is really
just a group of containers bundles
together and the kubernetes scheduler
and proxy server then actually manage
what how the pods are actually pushed
out into your kubernetes environment the
the parts themselves can actually then
share resources both networking and
storage so pods aren't pushed out
manually they're actually managed
through a layer of abstraction and part
of their deployment and and this is the
strength of kubernetes you use to find
your infrastructure and then kubernetes
will then manage it for you and there
isn't that problem of manual management
of PODS if you have to manage the
deployment of them and you that's simply
taken away and it's completely automated
and the the final area of software
Services is on Ingress and this is
really the secure way of being able to
have communication from outside of the
cluster and passing of information into
the cluster and again this is done
security through SSL layers and allows
you to ensure that security is at the
center of the work that you have within
your kubernetes environment so this so
they're now into the actual architecture
before we start looking at a use case of
how kubernetes is being employed so
kubernetes again is we looked at this uh
diagram at the beginning of the
presentation and there were really three
key areas there's the workstation where
you develop your commands and then you
have your master environment which
controls the scheduling the
communication and the actual commands
that you have created and pushes those
out and manages the health of your
entire node Network and each node has
various pods so we like break this down
so the master node is the most vital
component with the master you have four
key controls you have Etc the controller
manager schedule and API server the
cluster store Etc this actually manages
the details and values that you've
developed on your local workstation and
then we'll work with the out the control
schedule on the API server to
communicate that out those instructions
of how your infrastructure should look
like to your entire network the control
manager is really an API server and
again this is all about security so we
use restful apis which can be packaged
in SSL to communicate back and forth
across your pods and the master and
indeed the services within each of them
as well so at every single layer of
abstraction the communication is secure
the schedule as the name would imply
really schedules when tasks get sent out
to the actual nodes themselves the nodes
themselves are are done nodes they just
have the applications
um running on them the master and the
scum is really doing all of the work to
make sure that your entire network is
running efficiently and then you have
the API server which has your risk
commands and the communication and back
and forth across your networks that is
secure and efficient so your node
environment is where all the work that
you do with your containers gets pushed
out too so a work is really a it's a
combination of containers and each
container will then logically run
together on that node so you'd have a
collection of containers uh on a node
that all make logical sense to have
together uh within each node um you have
uh Docker and this is your isolated
environment for running your container
you have your cubelet which is a service
for conveying information back and forth
um to the service about the actual
health of the kubernetes node itself and
then finally you have the proxy server
and the proxy server is able to manage
the nodes the volumes the the creation
of new containers and actually helps
pass the community the the health of the
container back up to the master to see
whether or not the containers should be
either killed stop started or updated so
finally let's look at see where
kubernetes is being used by other
companies so you know kubernetes is
being used by a lot of companies and
they're really using it to help manage
complex existing systems so that they
can have greater performance and with
the end goal of being able to Delight
the customer increase value to the
customer and enhance increased value and
revenue into the organization so your
example of this is a company called
BlackRock where they actually went
through the process of implementing
kubernetes and so they could so
BlackRock had a chance where they needed
to be able to have much more Dynamic
access to their resources uh they were
running complex installations on
people's desktops and it was just really
really difficult to be able to manage
their entire infrastructure so they
actually went and pivoted to using
kubernetes and this allowed them to be
able to be much more scalable and
expansive in the in the management of
their infrastructure and as you can
imagine kubernetes was then hooked into
their entire existing system and has
really become a key part of the success
that BlackRock is now experiencing of a
very stable infrastructure and the
bottom line is that BlackRock is now
able to have confidence in their
infrastructure and be able to give their
confidence as back to their customers
through the implementation and more
rapid deployment of additional features
and services on that note if you are
looking to learn kubernetes then get
certified in devops with postgraduate
program in devops by simplylearn this
program offers Blended learning that
combines live online devops
certification classes with interactive
Labs that gives you hands-on experience
you will gain expertise in tools like
terraform kubernetes Maven ansible
Jenkins Docker junit get and more we
prefer one year of Prior experience to
enroll in this course so why wait enroll
the course from the link in the
description box so what are
orchestration tools the application
development and the life cycle
management from the time the application
is developed connecting the source code
to your continuous integration to
testing them all along your development
process and eventually moving it down to
production managing your production
servers the dependencies that your
software has the requirement that it has
in terms of the hardware the features of
fault all runs self-healing capabilities
Auto scaling all this as complicated
over the last few years as a part of
devops one thing that everyone is
interested in is managing all these
application dependency or lifecycle
management using some or the other kind
of a tool so that's where these
orchestration tools have become very
very popular so what kind of features
that they provide is you'll have to just
let them know what are the kinds of tool
sets that is required what are the fault
tolerance mechanisms that it has to be
adopted to what kind of a self-healing
capabilities that application will have
to need and if at all there is any auto
scaling features that is required if at
all you can bake in all these specific
parameters into your tool your
orchestration tool becomes a One-Stop
shop for all your deployment and
configuration management needs that's
where this orchestration tools have
become a very very popular and relevant
specifically these days when people are
more and more interested in adopting
devops practices all right so having
said that about orchestration tools
let's concentrate on two of these
orchestration tools specifically for
container management Docker swarm and
kubuntus many of the applications that
are written for these kind of containers
are called or kind of fall into a
category of cloud native where the
application need not know much about the
underlying infrastructure or the
platform where these applications are
running so these two along with Apache
me Source there are many other container
management systems but I'm going to
topic Docker swarm and kubernetes for
comparing the features that is provided
by these two Frameworks for the sake of
comparison I picked up Docker swarm and
kubuntase just because of the reason
that both of them operate in the space
of container management they do
something very very similar to
Containers that's the similarity that
exists between these two orchestration
tools however there's a big difference
that exists when it comes to the types
of containers that both of them cater to
and also the capacity of workloads they
can be used for Docker swarm is a
cluster management solution that is
provided by Docker container Docker is
one of the very very popular containers
of recent times and in case you have
your applications that is totally
powered by only Docker containers if you
have a need where you want to run a
cluster of servers which are running
only Docker containers Docker swarm
should be your choice for your
orchestration tool kubernetes this on
the other hand can cater to any other
types of containers other than Docker
and including Docker as well so in case
you have your applications which have
got Docker in them which I've got rkt in
them which they have got Alex in them or
any other type of container kubernetes
should be a choice of orchestration too
a Docker swarm can manage up to 40 50 or
60 Max nodes so in case your application
is totally written in Docker containers
and the load or the expected cluster
size is about 50 60 nodes Docker swarm
should be a choice of orchestration tool
on the other hand kubernetes is
something that was open sourced by
Google and the kind of the scale of
operations that kubernetes can cater to
is Google scale so if in case your
applications are more than a thousand
nodes that is where kubernetes comes
into play that's the big difference that
exists between Docker swarm and
kubernetes putting those differences
aside let me compare docus warm and
kubernetes based upon other features
which are similar in nature the first
and important feature that kubernetes
provides is something called as Auto
scaling if at all the load on your
cluster is too high and an application
is experiencing more load so kubernetes
can add new nodes to your cluster of
course you'll have to configure
kubernetes in order to have those
capabilities where it can spin up new
VMS or new nodes if at all you do that
configuration correctly kubernetes has
got the capacity or it has got the
feature where it can bring up a new node
and add it to the cluster on the Fly
based upon the load that exists at that
moment on similar lines if at all the
load is not too high it can identify a
few of those nodes which have got less
number of replicas or less number of
application containers running on it it
can move them to some other node and
also scale down by deleting few nodes
from your cluster so that is the of less
of Auto scaling which is provided by
kubernetes and this unfortunately does
not exist in Docker swarm the other
feature of load balancing specifically
application node balancing so Docker
swarm gives you an application Level
Auto load balancing however kubernetes
gives you the flexibility of manually
configuring any other type of load
balancer of your choice for application
load balancing installation as I
mentioned earlier Docker swarm is a
loose cluster of containers which are
running on nodes it is very easy to spin
up a new node and then connect them to a
swarm and this can be done in a very
very loose coupled way where you can
create swarms of your choice notes are
allowed to connect to swarms and quit or
leave the Swarms on the fly so in and
all the installation is pretty easy and
fast kubernetes on the other hand the
configuration of kubernetes is the way
to spin up a big cluster specifying the
size of the node how many Master known
so many config planes that you want how
many nodes that you want it's a pretty
tough thing to bring up a kubernetes
scalability kubernetes strength is very
very strong they are very tightly
coupled and even they have the
capability where on the cluster size
things or the nodes can be increased on
the Fly based upon the requirement on a
Docker swarm the cluster strength is
weak as compared to kubernetes worker
nodes can be added to a swarm worker
notes can be asked to leave us form or
can be taken out of a swarm based upon
the requirement so this is kind of a
Loosely coupled architecture for Docker
swarm while kubernetes the cluster is
very tightly coupled since Docker swarm
runs only Docker containers containers
find it very easy to share data and a
lot of other things with each other
because they all have the same signature
they are all from the same family so
it's very easy for them to share not
just volumes but a lot of other things
however for kubernetes since it manages
containers of different types if
application as to share some data across
different containers there's a little
bit of a complexity in how you would
want your containers to share data also
when it comes to kubernetes kubernetes
groups containers in something called as
pods while a pod can have either one
container that's the preferred choice or
multiple containers and the idea of pod
is like each pod can run in any other
node it's not guaranteed that you would
want to have two or three parts to be
running on the same node that makes data
sharing a little bit of a different
thing compared to Dockers form uh GUI so
there's not a good enough UI tool in
Dockers form at least the CE edition of
it which will allow you to get to know
what is running on your containers what
is the size of the containers what is
the volume of the containers and all
that stuff there are some free and open
source tools like pertainer which gives
you a good visibility into your running
Docker containers however there's
nothing at a swarm level that is
provided by docker abilities gives you
an out of the box dashboard which is
easy to configure and set up you can
also Club it with some metrics services
and you can also get information about
the size of the cluster that is running
what is the load on the nodes and stuff
like that all this across your cluster
in a very beautiful dashboard
perspective so that way this is little
bit of a good UI for kubernetes file for
docus form there isn't anything that is
provided out of the box let me spend
some time in explaining a little bit
about the kubernetes architecture what
comprises of the kubernetes cluster what
resides in a master or a control plane
and what resides in a worker node this
is a very high level depiction of a
kubernetes cluster so this is the master
node which has got something called as a
cluster store a controller a scheduler
component and an EPA server and these
are the bunch of nodes that are
connected or administered by the master
node the master node can be 1 3 Phi
typically an odd number this is as per
the typical cluster management thing
where you would need the master nodes to
be in odd numbers so that whenever
there's any contention in terms of what
needs to be deployed where or where to
give the job to whom and stuff like that
all the Masters would cast their vote
and they would decide on the outcome so
that's the master node and there are a
bunch of nodes which can be connected
and administered by the master node
cubic CTL or the command from using
which anybody can assign some workloads
to the cluster can also be run either on
the same node or on a separate node so
Cube CTL is the command line tool that
we will be installing and using in order
to fire up our commands to our cluster
so if at all I have to put up a job on
our cluster if I have to give out any
specific commands to my cluster all this
is done using cubic CTL there's a bunch
of rest apis which is used by Q CTL and
Cube CTL will talk to the API server and
fire up the commands specific to my
cluster what runs in the master node
Master node without saying it's the most
important component of any of the
cluster if at all you're running
um Thruster with only one must node if
your master node goes down there's no
other way that you know you can or any
user can talk to the different nodes in
the cluster so Master node is the most
vital component responsible for the
complete kubernetes cluster there's
always one node that should be running
as a master node that's the bare minimum
requirement for your cluster so what are
the other components in the master node
the most important one is called etcd so
this is nothing but a data store a value
of key value pair which is stored which
contains all information about your
cluster so all the configuration details
which node is up which worker node is
down all this information is stored in
the cluster store so all the managers
would access this cluster store they 4
they go ahead and decide any other work
item or anything else that has to be
done specific to your cluster what's in
the controller as the name says
controller is something it's like a
Daemon server that is running in a
continuous loop all the time it is
responsible for controlling the set of
replicas the kind of the workload that
is running based upon the configuration
that the user has set in so if at all if
you are aware of something called as
replication controllers endpoint
controllers namespace controllers all
these controllers are managed by the
controller component if any user asked
for some three replicas of a particular
pod or a service to be running and if at
all any of the nodes goes down or the
Pod goes down for whatever reason the
controller is the one who wakes up and
assigns this particular job or the Pod
to some other available node by looking
up for the details using in the cluster
store that's the importance of the
control manager or the controller the
scheduler the scheduler assigns the
tasks based upon whoever is asked for
any job to be scheduled based upon a
time frame or based upon some criteria
it also tracks the working load as to
what what exactly is the load who is
running what in the cluster and places
the workload on whoever is the available
resource at that time all right APA
server this is one other important
component in in our kubernetes cluster
where how would the end user deploy or
give out any sort of a workload onto
your cluster all the requests come to
the API server so this is an entry point
for all your requests that come into
your cluster anybody who wants to deploy
something anybody want to scale up a
controller anybody who wants to bring
down few Services anybody wants to put
in a service all this will have to come
in as a part of a rest API endpoint and
API server is the entry point in case
you don't want want you know somebody to
access your cluster in case you want
only specific people or specific users
to be running some specific workloads
you can set all those role-based access
control for this API server so this is
the entry point for anyone who wants to
submit any job to your cluster so that's
a quick overview about the master node
what what are the important components
of a master node now let's go over to
what runs in a worker or a slave node as
I mentioned earlier slave nodes are
something where the job that is
submitted to your cluster is eventually
run as a pod as a service as a container
so in kubernetes World there is
something called as a pod a pod is
nothing but a combination of a container
it is a wrapper around your running
containers so slave nodes are the worker
nodes typically run these parts so that
is where the whole workload typically
gets run but there are a bunch of other
components in the in the node which also
manages what runs in the Pod who has to
have access to the Pod what is the state
of the Pod is it in a running state is
it going down for some reason and all
this stuff so let's quickly go over
those components that is there in the
slave node the most important component
in my opinion that should be on the on
the Node should be the container runtime
as I mentioned earlier kubernetes can
run any different types of containers
not just Docker so in case you want to
have a node which wants to have Docker
running on it rkt running or at LKC
running on it or any other container
environment that is running on it you
have to ensure that the specific
container runtime environment is
available on that specific node so that
whenever a job is submitted description
of what needs to be run as a part of the
Pod what should be the image with it
should be powered and what kind of a
container to spin up so that's what you
know when the job is finally assigned to
this particular node it would definitely
need a container runtime to be up and
running so that exists only if at all
the container runtime is installed and
running on your kubernetes worker node
okay now that we know what our
kubernetes node can run how would
somebody assign a job to our no that is
using something called as a cube alert
as the name says cubelet or cube alert
is a small subset which talks to the
cube API server so any node which has to
run any kind of a pod all these
instructions are passed around by the
qvpa server to the cubelet and cubelet
is capable of processing whatever job
that is assigned to it and ensuring that
so many parts and so many services are
spun up based upon the requirement the
last component that exists in the worker
node is the cube proxy or kubernetes
proxy this plays a very very important
role acting as a load balancer and a
network proxy so what typically happens
is whenever the pods are running in
nodes the parts can be typically running
in any node there is no um Affinity
towards any node on which these parts
are running because pods or containers
are something called as ephemeral they
can run anywhere so how would somebody
reach out to these applications that is
running in some pod which is running in
one container now and running in
probably another container another node
all together tomorrow so that is where
your kubernetes proxy contains a picture
and this component will ensure that any
container that is spun up or any pod
that is spun up it keeps track of all
these spots and kind of connects the end
points or acts like a DNS server so that
it knows when somebody is trying to
reach out to this particular service
which is the pod on which the service is
typically running so that plays a very
very important role by acting as a load
balancer and a network proxy now that at
a very high level we know what are all
the components that make up of our
kubernetes cluster let's go ahead and
create a small cluster we'll spin up
this cluster on AWS platform finally we
get to the demo section of my tutorial
now that you guys are aware as to what
is kubernetes what is the use of
kubernetes you also know at a very very
high level what are the components of a
cluster what is a master node or a
control plane what are the components of
a master node and what should be
existing in a worker node probably
you're thinking that is maybe pretty
hard to kind of set this up so let me
demystify that by running you with a
quick demo of how to set up a simple
free node kubernetes cluster using a
tool called cops and I will be doing
this whole setup on my AWS Cloud okay so
what is cops cops is nothing but a
simple tool this is an admin tool that
allows you to bring up production grade
kubernetes environments as I said
setting up a kubernetes cluster on a
bare metal is little challenging so that
is why I would use something called as a
cops and I will use this on my cluster
which I'll spin up on my AWS instance so
I have an AWS account and what I'll do
first is I I will first pin up an ec2
server I will power this with one of the
Linux Amis and I will install cops on it
now this will be my starting point from
where I'm going to trigger running up a
big cluster in our case I'll try to keep
it little small I don't want too many
nodes in my cluster I'm going to have
one master node and two worker nodes
this whole operation would be done
programmatically by installing this cops
admin tool so as you may know AWS is
little hard for me to programmatically
run other components or rather bring up
servers it's not pretty simple so what I
would do is I will create an IAM role
which will attach to my S2 server so
that my ec2 server gets powered with all
the required role so that it can go
ahead and spin up cluster based upon my
configuration that I am going to specif
all right so once I have my cluster set
up I will also install cubic CTL which
uh you'd probably know by now is nothing
but a command line utility using which I
can kind of connect to my cluster give
out some jobs put some parts and stuff
like that so I will use qbctl I will
also have a pair of SSH keys so that
from this machine I would be able to
successfully get onto the master node
and submit jobs on my behalf so let me
Begin by first logging into my AWS
account all the steps that I will be
following to bring up my AWS cluster
kubernetes cluster will be documented
and I'll be sharing you this document in
a GitHub repository so that in case
anybody wants to try you'll be more than
happy to find all the required
instructions in one place okay now the
first thing that I would need is to
launch an AWS ec2 instance now this is
an instance where I am going to install
cops and I will use it as a base server
from where I'm going to find some
commands which will bring up my
kubernetes cluster so let me log into my
AWS account and let me stick to one
particular region and let me bring up
some a one ec2 instance all right so let
me launch my instance let me choose
um it doesn't matter in the ways but in
case you want to try it on your free
tire I would recommend choose free tire
only and choose an Ami of your choice
let me choose this instance I will make
a T2 micro that is good for me
configuration instance details all right
I don't want to change anything here
that looks good maybe I will change this
30 gig that's good for me let me add a
tag to my instance I'll name this as my
cups server all right I would need a
bunch of ports to be opened up so what I
will do is I'll create a new security
group called open to the word I would
not recommend that you do this since I
don't want to it will take a long time
for me to specify quickly pick and
choose the ports that has to be open for
running my cluster I don't want to do
that I will open up all the HTTP and
https ports so that it's quicker so I
will say all TCP I would save from
anywhere just to be on the safe side
also open HTTP and https and I will make
this anywhere and anywhere this is good
for me I will say review on launch
alright so I chose a T2 micro instance
just for the sake of easiness I opened
up all the ports and the instant details
are you've seen what is that I've chosen
all right now important part is like I
would need a keypad I need to create a
new keypad I will call this as simply
learn keys I will download this key pair
and then I will launch the instance it
will take some time for my ec2 instance
to come up in the meanwhile what I'll do
is I will convert I have this Pim file
which is the pair of keys the SSH keys
with which I need to log into my ec2
instance so I'll need to convert this
because I'm trying to connect from my
Windows box I would need to convert this
into a PPK file so I have put engine I'm
going to load my set of keys here and
convert that key into a ppk3 all right
open all right I would say save and save
privately yes I would say simply learn
or simply learn private private key and
save this here done that that's all so
this is my public key which is the pem
file and this is my APK or my private
key Now using the private key I will log
into my ec2 instance okay now my ec2
instance is up and running it is running
in this Ohio region and this is the IP
address of my public IP address of my
machine so let me connect to it so I use
mobile exterm which is nothing but an
SSH emulator you can use putty or any of
these sessions which I allow you to do
on SSH this is my IP address of my
machine and since I've spun up an Amazon
Ami the user this is the default user I
would need to specify the private keys
for my session and this is the private
key for my session say okay I am in my
Amazon ec2 instance so let me look at
what are the other things that I need to
do all right so as I said I just brought
up my ec2 instance but I would need my
ec2 instance to run few things on my
behalf so that it can spin up easy to
other ec2 instances it can also talk to
an S3 bucket where I'm going to store
the instance date of my kubernetes
cluster also some sort of an auto
scaling group because I want to spin up
more and more instances I also want to
have a private hosted Zone using my
Route 53 so I would need my ec2 instance
to have all these sort of permissions
for my server so what I would do is I
will go and create a permission a role
in the ec2 or rather in the AWS IM role
and I will attach this I am role to my
ec2 instance all right so let me go to
my IM which is nothing but the identity
and access management I will create a
role called as possibly cops rule I will
create a role this role will be used by
my ec2 so I'm going to click on ec2 I'll
say next permissions they would need a
lot of permissions uh specific
permissions to be very honest and the
permissions are all listed out here S3
ec2 and all this stuff just to keep it
pretty simple what I would do is I'll
create a role with the administrative
axis all right so I don't want any tags
I will review I'll create a role name I
will say cops role for ec2 all right so
I'm going to create a rule which has got
administrative access rules I'm going to
create a role so this would ensure that
my ec2 instance from which I am going to
run a my cluster would have all the
prerequisite permission so that it can
go and spin up instances talk to S3
buckets and all that stuff all right now
let us have our running instance get
powered by this rules I will connect
this role to my running instance alright
so this is my ec2 instant that is
running I will say action attach attach
attach attach replace IM roles so there
is no rule as of now I would want my
cops role for easy to zero that I
created so I want this new rule to be
assigned to my ec2 all right great now
my ec2 instance which is my cop server
has got all the required permissions so
that he can create a cluster on my
behalf great now before this let me do a
sudo gem update iPhone y so that any of
the because this is a newly provisioned
VM I just want to ensure that all my
library system libraries are all updated
so that when I do install cop and Cube
CTL and all those things none of them
will fail because of some dependency or
package issues so I'm just running a
sudo update iPhone y so that all the
libraries are updated okay looks like it
is done so let me go ahead and install
all cops cops is a pretty simple
installation it is available in a
particular location I have to just copy
this and ensure that I do a curl and I
install this particular stuff all right
so it is fetching uh the cops tool it's
installing for me once it is copied down
let me change the mode so that I can
execute it and then also let me move it
to my user local bin so that it is
available for me in my path okay that's
pretty much the corpse installation let
me also do one other thing let me
install something called as qbctl so
this is what would be you know a tool
using which I'm going to be firing my
commands to my kubernetes cluster once
it comes up okay this is a pretty small
or executable so keep sitting here all
right so I have Cube CTL as well as cops
installed on my ec2 server now okay now
what I'll do next is I'm going to create
an S3 bucket in AWS so that the
equivalent is a cluster state is
persisted in this bucket so let me
create a simple bucket with this name S3
bucket with this name so let me go to S3
and let me create a simple bucket with
this name I will just say create a
bucket okay so this simply run dot
kubernetes is the bucket that I created
so this bucket will store all
information about my cluster let me now
go and create a DNS entry or a DNS zone
or AWS Route 53 hosted zone for my
cluster so this is required so that you
know I can give a cluster name for my
kubernetes cluster that I'm coming up so
I will create a private hosted Zone and
possibility possibly I will call this as
simply learn.in so this will be my name
of my private hosted Zone in case you
already have any other public hosted
zones in your name you can always put a
Route 53 hosted Zone specific for your
domain name since I don't have any one
I'm just going to create a simple
private hosted zone so let me head down
to Route 53 click on any of these and
you can get these hosted zones out here
so I'm going to create a hosted Zone
here I'm going to create a hosted Zone
click on create hosted Zone
um this would be simply learn.in I want
to create a public no I don't want
public I want a private hosted Zone and
I'm going to associate my VPC ID for
this hosted Zone since I'm going to try
out all my exercises in the Ohio region
I will associate the VPC of the Ohio
region for this all right so this is the
one that is specific to the Ohio region
so I will say this one I'll say create a
hosted so great now let me come back to
my ec2 box my ec2 is all powered with
whatever it needs in order to go out
with the installation of my cluster only
few things that I need to take care of
is that you know now that I put a name
for my cluster I also have an S3 bucket
that I've configured for my cluster I
will have to ensure that I need to put
in these two configurations as a part of
my cluster building activity so I will
open my bash RC file and I'm going to
export these two variables if at all you
remember well these two variables are
nothing but the cluster name which is
nothing but the public or sorry the
private hosted Zone that I created and
the S3 bucket where I'm going to create
or rather I'm going to store my cluster
state so let me copy these two and open
up my bash RC file all right so I will
just add these two and copy these and
Export them out as my our variable I'm
going to save this here now let me
ensure that this gets picked up all
right I've got these two that have
configured and I've also ensure that
this environment variables are set now
let me create a pair of SSH keys this is
to ensure that I can log into the box
that I'm going to be provisioning as a
part of my cluster SSH hyphen Keygen I
don't want to give any parameters let it
go to the home directory with the
default name and without any passphrase
all right so I created a bunch of my key
pair now I'm all set to go ahead and run
my cops hookups will ensure that this
will take some time for the cluster to
come up but this is exactly how I would
Define my cups command I had to copy
here so cops create cluster the state of
the cluster will be stored in this
particular variable which is what I have
exported out the number of node count is
2 node count is the worker node count if
at all I don't specif there's a
configuration for specifying the master
or the control pane as well if I don't
specify that the default one is one so
I'm going to create one primary or the
master node and to worker nodes uh size
of my master Note size of the worker
nodes uh what are the zones where I want
this to be created and where do I store
the cluster information and okay I've
already added this must account this is
actually not required but this is the
command that I'm going to fire off so
that I bring up my cluster all right
that went off very fast but this
actually did not create a cluster it is
just the definition of a cluster and
that's why this came out very fast now
that everything looks good with whatever
configuration that is specified now let
me go ahead and create the cluster by
seeing cops update cluster hyphen iPhone
8 yes now this will take some time a
good five to ten minutes for it too
because this will actually start
provisioning the servers and as I
mentioned earlier based upon my
configuration I'm going to come up with
one master server and two notes or these
worker nodes all right so this is the
command for me to validate the cluster
and I'm going to try it out first I'm
pretty sure that will fail because all
my servers are not up yet it is taking
some time the validation everything
failed but let me try to look at my ec2
instances and see how many servers do I
have running as of now if you see I had
only one server that had started which
was my cop server and automatically the
other three instances one is called
notes dot simply not in and this these
two are the nodes and this is the master
node so these 3 got automatically
provisioned by the cops command that I
ran all right so this may take a while
for it to get validated on good 5-10
minutes so what I'll do is I'll pause
the video now and I'll come back once
the server the cluster is up and running
it's been a good uh eight to nine
minutes since I started my cluster so
let me validate now okay seems good so
this is a master node uh minimum One Max
one some notes which are nothing but the
slave nodes or the worker nodes there
are two of them T2 micro uh subnet so my
clusters seem to be up and running and
my cluster name is simple learn.in so
what I would want to do is now let me
just log into the master and see if I
can run some pots so let me get back to
my installation steps here the
validation cluster is done so let me log
into my since my cluster name is simply
learn.n this is the way to log into my
box so let me get into my box so if you
see here this is the host name of the
machine that I'm currently in if you see
this this is nothing but our this cop
server this is now from here I'm trying
to get into the master box all right so
if you see the IP address has changed I
was in a different box I'm in a
different box now if I see host name
you'll find a different host name so
this is 153 which is nothing but the
master node yep that's 153 this is the
particular house so I'm getting into
this machine now so I started the
cluster from my cop server here it ran
and brought up three nodes so I'm
actually getting into my master node and
see if I can run some parts on it
alright so let me try Cube CTL get
cluster info right Cube CTL get nodes
all right so there are three nodes here
Master node and one master node and two
worker nodes so let me see if I have
some pods here give the CTL get pods
there's no pod as of now so what I'll
try to do is let me just spin up a very
very simple Pawn just to check if my
connections is everything is correct or
not correct I have a simple node.js
application that I have built and I've
got a container for that this is already
pushed to the docker up registered is
called simply run Docker Hub and the
image name that I have here is called my
node app and I will use this image to
power one of my parts that I will run so
this is the way in which I am going to
launch my pod let me show you this
commands Cube CTL run the name for my
deployment that I'm going to run hyphen
iPhone image and this is the image that
is going to be powering my container so
this is simply learn Docker Hub forward
slash my node app hyphen F1 replica is
equal to I want two replicas to be
running and the port that I want to
expose this particular part is on 8080.
so let me run this as soon as this is
run it creates something called as a
deployment all right now let me just say
I'm not really interested in the
deployment I'm interested in checking if
at all the parts are up and running so I
will say Cube CTL get bot all right so
that was pretty fast so I have two parts
that are running here these are two
replicasback because I asked for two
replicas these parts are running so I
can run Cube CTL describe pod pickup all
right I can pick up any of the Pod name
and see what is the Pod what is the
information does it contain from what is
the image that is pulling what is the
image image Name ID this is actually
running my thought which is actually
spinning up a container great so for so
good so in kubernetes the pods can are
ephemeral so they can be there at any
time cannot be runtime if I need to
expose my pod outside I will have to
create a service for my pod so what I'll
do is I'll create a very very simple
service by exposing this particular
deployment before that you check Cube
CDL get deployment so there's a
deployment that is created the
deployment name is simply in an app so I
will expose my deployment simply learn
app as
um yeah I'll just expose this let me see
all right I'm not specifying what type
and all I don't want to get into the
complications of um what are the
different types of exposing this service
and all that stuff so if I don't specify
find anything it gives me something
closer cluster ID so this is where my
pod is actually exposed so let me just
check if at all this part is up and
running I'll just try a simple curl
command uh curl HTTP colon this is my
cluster IP and the port is 8080 if at
all I hit this it's actually hitting my
application and giving me uh whatever I
put a simple uh sys dot out kind of a
thing where I'm just printing
um the container ID of whatever pod is
serving uh being served out of so I'm
actually heating my container and I'm
getting this output from the content so
everything looks good my content is up
and running so let me just go and clean
it up I will say Cube CTL delete
deployment simply long app this should
get rid of all the parts that I created
Cube CTL get pod all right these parts
are in terminating things let me just
check Cube CTL get Services there's one
service I don't want this service let me
delete that keep CTL delete service on
this all right this sounds good let me
come back to my host my cop server from
where I'm running so I managed to
successfully verify this part see if
everything is up and running so what
I'll do is I'll just go ahead and
complete this demo by going and getting
rid of my cluster so cops delete cluster
hyphen iPhone yes all right so this will
ensure that it will clean up my complete
cluster that I created so I had three or
four running instances if you see them
all the three are shutting down because
you know the cop server which had cops
installed on it uh has now got a mandate
to go ahead and shut down and clean up
the whole instances and all those things
that are created as a part of my
deployment on that note if you are
looking to learn kubernetes then get
certified in devops with post graduate
program in devops by simply learn this
program offers Blended learning that
combines live online devops
certification classes with interactive
Labs that gives you hands-on experience
you will gain expertise in tools like
terraform kubernetes Maven ansible
Jenkins Docker junit get and more we
prefer one year of Prior experience to
enroll in this course so why wait enroll
the course from the link in the
description box
so let's go through a couple of
scenarios let's do one for kubernetes
and then one for Docker and we can
actually go through and understand what
the problem specific companies have
actually had and how they're able to use
the two different tools to solve them so
our first one is with Bose and Bose had
a large catalog of products that kept
growing and their infrastructure had to
change so the way that they looked at
that was actually establishing two
primary goals to be able to allow their
product groups to be able to easier more
easily catch up to the scale of their
business so after going through a number
of solutions they ended up coming up
with a solution of having kubernetes
running their iot platform as a service
inside of Amazon's AWS cloud service and
what you'll see with both these products
is they're very Cloud friendly but here
we have um Bose and kubernetes working
together with AWS to be able to scale up
and meet the demands of their product
catalog and so the result is that we
were able to increase the number of
number production deployments
significantly by taking the number of
services from being large bulky Services
down to small micro Services being able
to handle as many as 1250 plus
deployments every year an incredible
amount of time and value has been opened
through the use of kubernetes now let's
have a look at Docker and see what a
similar problem that people would have
so the problem is with PayPal and PayPal
and processes something in the region of
over 200 payments per second across all
of their products and PayPal doesn't
just have PayPal they have Braintree and
venmo so the challenge that PayPal was
really being given is that they had
different architectures which resulted
in different maintenance cycles and
different deployment times and an
overall complexity from having a decades
old architecture with PayPal through to
a modern architecture with a venmo
through the use of Docker PayPal was
able to unified application delivery and
be able to centralize the management of
all of the containers with one existing
group the net net result is that PayPal
was able to migrate over 700
applications into Docker Enterprise
which consists of over 200 000
containers this ultimately opened up a
50 percent increase in availability for
being able to add in additional time for
building testing and deploying of
applications just a huge win for PayPal
now let's dig into kubernetes and Docker
and so kubernetes is an open source
platform and it's designed for being
able to maintain a large number of
containers and what you're going to find
is that your argument for kubernetes
versus Docker isn't a real argument it's
kubernetes and Docker work together so
kubernetes is able to manage the
infrastructure of a containerized
environment and Docker is the number one
container management solution and so
with Docker you're able to automate the
deployment of your applications being
able to keep them in a very lightweight
environment and being able to create a
nice consistent experience so that your
developers are working in the same
containers that are then also pushed out
to production so with Docker you're able
to manage multiple containers running on
same Hardware much more efficiently than
you are with a VM environment the
productivity around Docker is extremely
high you're able to keep your
applications very isolated the
configuration for dark here is really
quick and easy you can be up and running
in minutes with Docker once you have it
installed and running on your
development machine or inside of your
devops environment so if we look at the
deployment between the two and the
differences kubernetes is really
designed for a company nation of PODS
and services in its deployment whereas
with Docker it's around about deploying
services in containers so the the
difference
um here is that kubernetes is going to
manage the entire environment and then
and that environment consisting of PODS
and inside of a pod you're going to have
all of your containers that you're
working on and those containers are
going to control the services that
actually power the applications that are
being deployed kubernetes is by default
an auto scaling solution it has it
turned on and is always available
whereas a Docker does not and that's not
surprising because Docker is a tool for
building out Solutions whereas
kubernetes is about managing your
infrastructure kubernetes is going to
run health checks on the liveness and
Readiness of your entire environment so
not just one container but tens of
thousands of containers whereas Docker
is going to limit the health check to
the services that it's managing within
its own containers now I'm not going to
kid you kubernetes is quite hard to set
up it's it's if of the tools that you're
going to be using in your devops
environment it's it's not an easy setup
for you to use and for this reason you
want to really take advantage of the
services within Azure and other similar
Cloud environments where they actually
will do the setup for you Docker in
contrast is really easy to set up you
can as I mentioned earlier you can be up
and running in a few minutes as you
would expect the fault tolerance within
kubernetes is very high and this is by
Design because the architecture of
kubernetes is built on the same
architecture that Google uses for
managing its entire Cloud infrastructure
in contrast Docker has lower fault
tolerance but that's because it's just
managing the the services within its own
containers what you'll find is that most
public Cloud providers will provide
support for both kubernetes and Docker
here we've highlighted Microsoft Azure
because they were very quick to jump on
and support kubernetes but the realities
is that today Google Amazon and many
other providers are having first level
support for kubernetes it's just become
extremely popular in a very very short
time frame the company is using both
kubernetes and Docker is vast and every
single day there are more and more
companies using it and you should be
able to look and see whether or not you
can add your own company to this list
us we're going to break up the interview
questions into three categories it's
going to be beginner intermediate and
advanced um the beginner questions are
really going to cover about three
quarters of the questions that we're
going to ask here and then the
intermediate and advanced will finalize
out the we'll finish out the final
quarter and and the reason is is that
quite typically most interviews um
you'll be asked you know intermediate to
beginner level questions mainly beginner
level questions just so you can
demonstrate your understanding of
kubernetes and then there's always one
Advanced question that gets thrown out
just to make sure that you really do
know what you're talking about and
you've done at least one or two
kubernetes environment setups to be able
to validate your experience so let's
kind of jump into that first section
which is you know why is kubernetes
widely used and your answer to this
really comes into the fact that
kubernetes is widely used because of
three distinct areas one is that
kubernetes is an open source platform
that is readily available for the
management into containers and allows
you to manage the deployment scaling and
management of those containers and so
the first area is that it allows you for
easy deployment of your containers the
ability to scale effectively within
kubernetes not just vertically but
horizontally for your containers and
then finally the kubernetes environment
itself was really designed to make it
easy to manage containers
um in small parts other key areas of why
kubernetes is popular is that kubernetes
is the single largest open source
project out there since its launched in
2014 is just grown in Leaps and Bounds
around that project there is just a huge
Community both an online and physical
Community there are just a large number
of online groups that are supporting
kubernetes and allow you to get fast
access to information but in addition to
that there are also a lot of local
meetup groups that cover specific
kubernetes weather as part of a devops
group or as part of a dedicated
kubernetes group the actual environment
itself is a really robust way of being
able to manage your containers and
containers are becoming the most popular
way of being able to deploy cloud-based
applications kubernetes is an effective
and persistent storage environment so
kubernetes is also able to effectively
manage persistent storage whether it's
local storage as a SSD drive or a local
hard drive or whether it's cloud-based
storage and because of the way that
kubernetes architecture is redesigned
for the cloud from the ground up so it's
a great way to get started in building
out your Cloud Solutions and then
finally the actual tools within
kubernetes are designed to help and
manage the health and monitoring of the
environment itself so what is Google
container engine so the Google container
engine is another open source solution
but it's designed for managing the
docker containers and clusters and so
kubernetes is a contain the management
solution and Docker is the most popular
container solution out there and this is
just a way of being able to bring the
two effectively with into each other so
that you can have Docker containers
manage the application and then
kubernetes managed the scale of your
applications as they're vertically and
horizontally scaled across your network
so what is the difference between
kubernetes and Docker and this is
important because kubernetes and Docker
are often dropped in the same sentence
and there's a lot of people that get
confused between the two so
fundamentally kubernetes is about taking
pods which contain containers which have
applications in and deploying them at
scale across a cloud Network Docker is
the container solution itself so the
actual application itself is put into
the docker container because of this
difference kubernetes is designed for
auto scaling is highly available whereas
Docker simply isn't because it doesn't
have to worry about that within the
container itself the kubernetes
environment will check for liveness and
Readiness of the health of the entire
infrastructure whereas with Docker it's
just really going to check the services
that are coming in and out of the actual
application itself within the container
kubernetes is complicated to set up it
really is just simply complicated setup
and what you'll find is a lot of cloud
services will actually do the setup for
you whereas Docker is very easy to set
up the as you would expect with the
tones ratio there is high fault
tolerance with the kubernetes whereas
with Dockers low foot tolerance again
this is fundamental in the two different
types of strategy that kubernetes and
Docker have they're both connected
tightly with each other but they both do
different things so what are the notable
features of kubernetes one is the
ability to do container balance
kubernetes always knows where to place a
container within its entire network
within the pods and with its
infrastructure the services are managed
for Security Network and storage it is
self-monitoring so it goes in and is
able to check its own no modes for
health and the containers within the
nodes to make sure that the applications
are always running effectively you can
do the scaling effectively both first
screen horizontally within kubernetes
the solution is open source so there's
no actual fee for the actual kubernetes
service itself and kubernetes allows for
storage orchestration what does it mean
by storage orchestration now storage
orchestration is that you're able to
access and share either local storage
such as a hard drive or SSD drive or a
cloud-based drive and my preferences
always go with cloud-based drive because
then you're really positioning Your
solution to be a true cloud-based
application so what are the main
advantages of kubernetes well you get
automatic rollback for changes that go
wrong the a you're able to automate a
lot of the processes that would have
been manual processes previously you're
able to scale the resources both
vertically and horizontally easily and
quickly and then the these three areas
the rollback automating roll Bank
automating services and being able to
scale indirectly allows you to save
money by reducing the amount of time and
people working on your operations
environment so let's break out the
architecture of kubernetes so you're
going to be explaining us to explain how
is kubernetes architecture structured
and there are three key areas there's
the application layer there's kubernetes
layer and then there's the
infrastructure layer we're going to
start off with the infrastructure layer
and so the infrastructure layers are
base layer and this is where all your
where you make your clusters for your
collections of storage and networking
resources within kubernetes and this is
where you would have if your databases
are your data stored locally on a hard
drive or in the cloud your core Network
and the VMS that you'll be building out
your infrastructure on these allow you
to group many sheens together into a
single unit so that you're able to scale
up and down very easily at to meet
customer demand so let's talk about the
middle layer which is the kubernetes
layer now so if we're looking at our
architecture for kubernetes layers
really broken down for the the core
operating system the docker container
environment and the kubernetes code
itself and this is where you're able to
take machines and pull them together as
a cluster so that they work within that
kubernetes system you have a master
controller within the kubernetes that
actually then we'll go ahead and manage
the scheduler the control manager the
API server and then you'll be able to
Cluster all of these kubernetes together
so that they're all working and
processing as one single unit and then
finally let's look at the application
layer where you actually have your apps
and so the application layer where you
have your pool and your services you
know kubernetes will use API to connect
in and out of this layer it's to run
your applications within containers and
use instructions that are written in
yaml or Json to actually actually set up
those containers and you actually have
the environment running on a plan that
examines these instructions so it keeps
the entire environment healthy API
ecosystem is used to interact with the
cluster it's so you don't have to do
that by yourself so make sure that
you're referencing that the apis are
secured and connecting in and out of the
system and so you have your scheduling
control manager actually from the
kubernetes environment doing the hard
work and then the workers themselves
execute the the commands they're coming
from kubernetes so you will be asked to
kind of dig into some of the key areas
such as explaining the master node and
listing its components so the master
note is comprised of four key components
there's the cluster store the controller
manager the scheduler and the API server
so four key components remember four
fingers four components and then you're
going to be asked to maybe dig deeper
into to what is a cluster within
kubernetes and so a customer is really a
combination of resources such as
machines that into a single node and
then those nodes are pulled together and
work as a single cluster as a whole
kubernetes will work with that whole
cluster and then once you're actually
working with that whole cluster if a
node fails or needs to be removed
kubernetes is able to do that quickly
and effectively so let's talk a little
bit about what are the different types
of controllers in kubernetes and there
are five types of control manager you
have the node controller the service
account the token controller the
endpoint controller and the replication
controller and so the no controller
controls and handles the node in the
system the service account enables
access controls in the system the token
controller cleans up any tokens for
non-existent service accounts the
endpoint controller joins services in
pods to the endpoint objects and the
replication controller manages the Pod
life cycle so what is the role of the
cloud controller manager within this
environment and so the role the cloud
control manager is really to help manage
persistent storage and so persistent
storage is storage that's either local
storage to the physical machine such as
an SSD or hard drive or cloud-based
storage and this allows the storage to
be shared across the entire network what
is the role of the cube API server and
the cube scheduler so the cube API
server responsible for the established
communication between the kubernetes
nodes and the master components and and
it does this through the through apis
the apis themselves are all commands
that are rest based commands and it
implements an interface which means
different tools and libraries can
communicate effectively with each other
so what is the role of the cube API
server and the cube scheduler so the
cube schedule actually does the
distribution of work across the entire
infrastructure so the actual yaml script
have written our executed appropriately
at the right time so you're going to be
asked to explain persistent volumes in
kubernetes so a persistent volume is
just a place where you can store data
again this can be local data as in a
hard drive local to the physical machine
or it can be cloud-based data so what is
a cubelet and a cube proxy so a cubelete
is a service responsible for conveying
information to and from the control plan
itself the process and the Q proxy
itself is responsible for maintaining
the work and the standards within the
node server in addition every node in
the cluster runs a simple Network proxy
and Cube proxy routes request to the
correct container in that node in
addition you'll have some basic and
primitive load balancing and management
of the node from the proxy itself and
there's additional layers of management
that get done outside of the proxy but
the proxy is also there to help do some
localized load management so what is
Cube CTL so Cube CTL is your command
line or terminal window interface that
allows you to execute commands and later
in this video we'll actually go through
some of those commands
etcd is the store that configures the
details and essential values such as key
value pairs that are distributed across
your entire network it also manages the
network rules and the post-forwarding of
activity so what are the disadvantages
of kubernetes well the first is it's
really hard to install and configure and
this is why Azure and many cloud
services such as Google Cloud will
actually do the installation and
configuration of kubernetes for you it
does take a long time to get up and
running with kubernetes not so much for
the actual software but your own
experience of being able to use
kubernetes it is not simple to manage
the service you actually have to have a
lot of experience and this is why people
get paid a lot for kubernetes skills
it's a difficult environment to get up
and running and be effective in that
management again kubernetes is just not
an easy platform to learn it takes a lot
of time and effort and one of the
disadvantages is that while kubernetes
itself is free because it is an open
source project getting the people who
are skilled you to using kubernetes is
expensive all right so those were kind
of the introductory questions that you
can expect to be asked in your interview
the next set of questions are really
going to be on an intermediate and
advanced level and these are the
questions that will be thrown out you
know you can see these as being the the
trick questions which are really there
to test that you have the skill set to
be able to implement and manage
kubernetes within a company so what
happens if a master node fails what
happens if a worker nerve fails how
would kubernetes manage that well
kubernetes is really designed for this
kind of scenario and so you can actually
look at it and the response would be is
that the Pod itself does not get
affected and kubernetes will actually
respond and fix the the failed node
whether it is uh needs to be repaired
but there needs to be restarted or needs
to be replaced so what is a service role
in kubernetes so a service role is an
extraction for all of the pods it
provides a virtual IEP address and it
allows clients to connect to Containers
running in the pods using that virtual
IP address and the command that you
would use to get that would use some of
these Cube CTL commands it's quite a
simple one it's Cube CTL get services
and you get a full list then of all the
services that are available so how do I
do a rollback in deployment and so
there's a couple of ways you can do the
rollback and the first is if you want to
see the rollout history deployment you
would write Cube CTL rollout history
deployment and then in square brackets
deployment if you want to just restore
to the previous rollout the command
would be Cube CTL rollout undo
deployment and then in square brackets
deployment and that would take you back
to the previous deployment so what is an
Ingress controller so an Ingress
controller really is a pod that manages
that inbound traffic and a prominent
feature that traffic is that it is
secured with an SSL termination and so
what we're going to do to now address
some of the key ways in which you can
provide API Securities on kubernetes and
we have eight of those so the first four
are implementing TL TLS for your
security the second would be API
authentication the third is to make the
Cubist protection the cubeless protected
via authorization mode equals webhook
command the fourth is to monitor rbac
failures the fifth is to remove those
default service account permissions six
would be to filter egress to Cloud API
metadata apis the seventh is to use pod
security policy to restrict container
policies and protect the node and the
eighth and final is to keep the cube at
the latest version all right let's go
into some of the more advanced commands
and so the advanced questions you're
going to get are all going to be
commands that you're going to be asked
to write now my advice for you for these
questions is to make sure that you have
some of these basic commands memorized
they don't have to be perfect when
you're in the interview environment but
it's good to be able to get up to a
whiteboard and actually write out what
these commands are just so you can
actually demonstrate that you know what
the commands are the interviewer isn't
going to ask you questions on how would
you apply kubernetes to the company that
you're being hired for because you
simply don't have that knowledge but
they do want to be able to see clearly
that you understand the commands and
terminal window instructions that will
be able to demonstrate how you would
actually go in and access some of that
information so from the first questions
you might be asked is how do you package
coupon entities applications and so you
would use the package manager within
kubernetes to be able to manage to and
configure and deploy applications into
your cluster and so here we have the the
helm command that you would use so what
are init containers and it container
gets executed for any other containers
can run on the Pod and so you want to be
able to illustrate what's the init
instructions would look like and here we
have an example and I would again use
this example and maybe memorize it for
when you have this question come up in
your interview so what is the difference
between a config map and a secret and
these are more uh fundamentals so a
config map just has the instructions
that are clearly available in plain text
on what the cube is supposed to do
whereas the secrets are actually
encrypted and these are like passwords
and other sensitive data that you don't
want to have easily available these are
encrypted information have to be
decrypted to access the content so if a
node is Tainted is there a way to
schedule the pods to that node and the
answer is no there isn't and so what you
have to do then is schedule the Pod to
be uh fixed and so that schedule the Pod
to fix the tainted node to allow for
tolerations to that and we have here the
example code that you would use to do
that so how do you deploy a feature with
zero downtime in kubernetes well
kubernetes is designed to be able to
roll out applications with zero downtime
and here we have four examples using the
rolling update as a strategy for zero
downtime deployments and kubernetes
again memorize one of these so you can
actually illustrate how you use rollout
to be able to deploy a zero downtime
solution so how do you monitor a pod
that is always running and so to do that
you would use the liveness probe to
check the health of the application
within the Pod and see whether or not
there are any failures or whether or not
the pot itself needs to be restarted so
how do you drain traffic from pod during
a maintenance so again this is a simple
command for you to remember but to be
able to drain an application so that
it's easy for you to be able to do
maintenance on the application you would
use the cube CTL drain and then in
square brackets the node name and that
would then temporarily drain that
particular pod so you can actually do
maintenance on it what you have to then
and this would probably give you bonus
points if you're in an interview is then
a reference the uncordant command so
that you can uncoordinate the actual
node itself so that the Pod itself can
come back up into live um and work as a
fully functioning solution so to be able
to tie services to a podder to a set of
Parts is that you can apply labels to
the actual Parts themselves and then use
the labels as selectors to be able to
glue services to that pod and again we
have here an example of how you'd
actually do that and again this is one
of those where you can probably won't be
asked to write this out but it'd be good
to have this memorized how do you get
all pods on a node and so the following
command will actually go ahead and bring
all the pods into a kubernetes cluster
this is quite lengthy and you know maybe
you want to like talk to how this would
be done but we have the instructions on
how to do that right here the important
part is to make sure that you are when
you're talking about the node name that
you reference to whoever you're
interviewing that you would what you
would switch out to reference the actual
nodes that you're trying to pull
together into the kubernetes cluster so
how do pods Mount NFS volumes and so in
these examples here we actually Define
the NFS server and then the Pod and how
you'd actually Mount two together and
again what you want to be able to do is
illustrate why you would use yaml and
how you that you then points to specific
IP addresses within your kubernetes
environment within either the production
or test environments to be able to
illustrate how you would Mount up those
NFS volumes so here we wrap up
kubernetes crash course if you like this
video consider subscribing to Simply
learn to stay updated with this
technology thanks for watching
staying ahead in your career requires
continuous learning and upskilling
whether you're a student aiming to learn
today's top skills or a working
professional looking to advance your
career we've got you covered explore our
impressive catalog of certification
programs in Cutting Edge domains
including data science cloud computing
cyber security AI machine learning or
digital marketing designed in
collaboration with leading universities
and top corporations and delivered by
industry experts choose any of our
programs and set yourself on the path to
Career Success click the link in the
description to know more
hi there if you like this video
subscribe to the simply learned YouTube
channel and click here to watch similar
videos turn it up and get certified
click here
thank you