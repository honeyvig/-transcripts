hey everyone welcome to the world of
advanced data analytics in this tutorial
we will embark on an exciting journey to
uncover hidden insights and patterns
within vast data as businesses and
industries become increasingly data
driven mastering Advanced Analytical
techniques has never been more crucial
throughout this tutorial we will explore
many powerful tools and methodologies
that will Empower you to extract
meaningful information from complex data
sets from predictive modeling and
machine learning to data visualization
and pattern recognition we will dwell
Into The Cutting Edge techniques that
drive today's data-driven decision
making whether you're a data scientist a
business analyst or simply an Enthusiast
eager to unlock the potential of data
this tutorial will equip you with the
skills and knowledge needed to transform
raw data into actionable intelligence
get ready to dive into the fascinating
world of advanced data analytics and
unleash the true power of data if these
are the type of videos that you would
like to watch then hit the Subscribe
button like and press the Bell icon to
never miss an update on our future
content if you're one of the aspiring
data analysts looking for online
training and certification from the best
universities or a profession who elects
to switch careers with data analytics by
learning from the experts then try
giving a shot to Simply runs close
graduate program in data analytics this
is from her University in collaboration
with IBM and this should be the right
choice the link in the description box
below should navigate you to the home
page where you can find a complete
overview of the program being offered
now over to our training experts online
analytical processing olap and online
transaction processing in short oltp are
two popular database systems that have
evolved separately to serve distinct
purposes while both systems offer data
storage and retrieval capabilities they
differ significantly in terms of their
architecture data flow and performance
characteristics olap is designed
specifically for data analysis and
Reporting while oltp is designed for
online transaction processing such as
online banking Inventory management and
order processing olap is optimized for
analyzing large volumes of data and
Reporting metrics while oltp is
optimized for quickly processing
individual transactions there are many
other differences between these two
crucial data processing systems so in
this video we'll be discussing the major
differences between these two and how
you can Implement these in large
transaction processing systems so
without any further Ado let's get
started also if you're an aspiring data
analyst looking for online training and
certifications from prestigious
universities and in collaboration with
leading experts then search no more
simply learn Sports Argent program and
data analysis from Purdue University in
collaboration with IBM should be your
right choice for more details use the
link in the description box below and
with that in mind over to our training
experts
approaches stand out in data processing
and data analysis online analytical
processing that is olap and online
transactional processing oltp although
they share the common goal of handling
data these methodologies differ
significantly in their purpose data
structure performance characteristics
and design principles olap is a powerful
tool for organizations seeking to
extract valuable insights from vast
volumes of historical data its
multi-dimensional data model organized
in cubes comprising dimensions and
measures allows for sophisticated
analysis and in-depth Exploration with a
focus on aggregating and summarizing
information
olap empowers users to identify Trends
patterns and correlations across
multiple Dimensions despite longer
response times which are acceptable due
to their analytical nature olap excels
in providing flexible ad hoc query
capabilities for inputs data
Explorations conversely oltp is designed
to handle real-time transactional
processing ensuring the integrity and
consistency of daily operational
activities by adopting a normalized data
model oltp optimizes the storage and
retrieval of individual records in high
speed environments its primary objective
revolves around efficient data
modification such as inserting updating
and deleting records to support
concurrent transactional operations with
a focus on rapid response times and
maintaining data accuracy at record
level oltp caters to the needs of time
sensitive business operations
understanding the distinctions between
olap and oltp is crucial for
organizations to choose the appropriate
data processing approach based on their
specific requirements today we will be
understanding the difference between
olap and oltp by going through the
following details mentioned in the
agenda we will get started with the
first one which is its purpose then data
structures data volume response time
query complexity data modification data
granularity concurrency data backup and
recovery and finally the system design
so with the briefing of olap and Olt we
discussed and the agenda for the session
discussed let's put the point of today's
session that is the major or top 10
differences between olap and oltp
firstly we will go through the purpose
open reporting enabling users to gain
insights from large volumes of
historical data it focuses on providing
aggregated and summarized views of the
data oltp oltp is designed for real-time
transaction processing handling
day-to-day operations such as inserting
updating and deleting individual records
its primary objective is to ensure data
integrating and
support high-speed transactional
operations
next is data structure olap uses a
multi-dimensional data model called a
queue it organizes data into Dimensions
such as time geography and product and
measures such as sales and profit to
facilitate multi-dimensional analysis
and drill down capabilities
then we have oltp oltp uses a normalized
data model with tables and relationships
aiming for efficient transaction
processing it minimizes the data
redundancy and ensures data consistency
through the use of normalization
techniques
next ahead we have the data volume
olap deals with large volumes of
historical data typically containing
years of data it focuses on analyzing
and summarizing this vast amount of
information
next is oltp oltp deals with relatively
smaller volumes of data usually
representing real-time transactions
happening within a shorter time frame
moving ahead we have response time olap
allows for longer response time since it
deals with complex queries and large
data sets users expect analytical
reports to be generated within minutes
or even hours
coming into oltp oltp requires vast
response times to support real-time
transaction processing users expect
quick responses usually in milliseconds
or seconds a lot faster than mobile AP
moving ahead we have query complexity
olap queries are usually complex
involving aggregations grouping
filtering and calculations across
multiple Dimensions users need flexible
advert querying capabilities to be found
data analysis moving ahead we have oltp
oltp queries are relatively simple
primarily focused on retrieving or
modifying individual records based on
specific transactional needs queries are
typically short and transaction oriented
moving ahead we have the sixth one that
is data modification olap is read-only
or minimally updated data is loaded into
all AP cubes periodically example daily
or weekly to update the analytical
database with the new information
coming into oltp oltp involves frequent
data modification including insertions
updates and deletions it ensures that
the transactional database remains up to
date and reflects the current state of
business
next in the docket we have data
granularity olap deals with aggregated
and summarized data providing with a
high level view of information across
various Dimensions it focuses on Trends
patterns and overall performance
analysis next we have oltp oltp operates
at a detailed level capturing individual
transactional data with a focus on
maintaining accuracy and integrity at a
record level
next we have the eighth point which is
about concurrency olap involves a low
level of concurrent users since users
typically perform separate analysis and
Reporting tasks that emphasizes on
analytical activities rather than
concurrent transactional processing next
oltp oltp requires a high level of
concurrency to handle multiple users
simultaneously accessing and modifying
the same data it focuses on maintaining
data consistency and isolation amongst
concurrent transactions nine point data
backup and Recovery olap is usually
derived from oltp system and backup and
Recovery are less critical it can be
generated from the transactional
database if necessary in case of oltp
oltp data is critical and backup and
Recovery processes are essential regular
backups are taken to ensure data
integrity and provide the ability to
store the system in case of failures
and lastly we have system design all AP
systems are typically designed with a
focus on read intensive operations they
employ specialized data storage and
indexing techniques optimized for
analytical queries and aggregations all
AP databases are often denormalized to
improve query performance and in case of
oltp oltp systems are designed to handle
a high volume of concurrent read and
write operations
they prioritize data consistency and
transaction Integrity often using
normalized database structures to
minimize redundancy and ensure data
accuracy in today's fast-paced digital
landscape businesses face a daunting
challenge extracting valuable insights
from massive amounts of data enter the
edl pipeline the backbone of data
processing and analytics in this
tutorial we will an accelerating Journey
unveiling the secrets of building a
powerful edl pipeline whether you are a
seasoned data engineer or just starting
your data-driven adventure this video is
your gateway to unlock the full
potential of your data together we will
demystify the ETL process step by step
we'll dive into the extract phase where
we retrieve data from multiple sources
ranging from databases to apis and then
we'll seamlessly transition into the
transformation phase where we clean
validate and reshape the data into a
consistent format but wait there's more
we will explore Cutting Edge techniques
for handling large data sets leveraging
cloud-based Technologies and ensuring
quality we aim to equip you with tools
and knowledge to create robust and
scalable ETL pipelines to handle any
radar challenge so buckle up and get
ready to revolutionize your data
workflow join us in this accelerating
journey to master the art of ETL
pipelines having said that if an
aspiring data analyst looking for online
training and certifications from
prestigious universities and in
collaboration with leading experts then
search no more simply learns
postgraduate program in data analytics
from Purdue University in collaboration
with IBM should be a right choice for
more details use the link in the
description box below with that in mind
over to our training experts hey
everyone so without further Ado let's
get started the PTL pipeline so ETL
basically stands for extract transform
and row so ETL pipelines fall under the
umbrella of data pipelines a data
pipeline is simply a medium of data
extraction filtration transformation X
protein and loading activities through
which the data is delivered from
producer to Consumer to make it a little
simpler the data is produced in two type
let's say you run a vehicle showroom and
you are being a data producer so the
data that you produce is very less and
that could be basically fit into an
Excel sheet this type of data might need
update once in 24 hours or based on your
audit cycle here we call it match data
and this data is processed using the
oltp model and the batch processing
tools but now let's say you're running
an entire vehicle manufacturing plant
now the data you're dealing with is
voluminous and includes various types of
data it can be structured data
semi-structured data ranging from space
inventory to all the way up to robotic
assembly sensors data based on
requirements this type of data needs
updates maybe every hour every minute or
even every subtle such type of data is
called real-time data and needs
real-time data streaming Frameworks and
the data is processed using o l a p
models now ETL is involved in both these
approaches now let's dive in and
understand what exactly is an ETL
pipeline ETL stands basically for
extract transform and load and
representing these three core steps in
data creation and transformation process
let's dive into each phase and explore
their significance firstly extract the
first step in detail pipeline is
extracting data from various sources
these sources can range from relational
databases data warehouses apis or even
streaming platforms the goal is to
gather raw data and bring it into
centralized location for further
processing tools like Apache Kafka
Apache nifi or even custom scripts can
be used to perform the extraction
efficiently next is transform once the
data is extracted it often requires a
significant cleaning validation and
restructuring this is transformation
phase the transformation phase ensures
that the data is consistent standardized
and ready for analysis Transformations
can include tasks such as data cleansing
filtering aggregating joining or
applying a complex and business rules
tools like Apache spark Talent OR python
libraries like pandas are commonly used
with these Transformations and lastly we
have the load phase the final step is
loading the transform data into Target
systems such as data warehouse data lake
or database optimized for analysis
this allows business users and analysts
to access and query the data easily
loading can import patch processing or
real-time streaming depending upon the
requirements of the Business
Technologies like Apache Hadoop Amazon
redshift or Google bigquery are often
employed for efficient data loading now
that we understood the core phases let's
explore some key Concepts and best
practices for building robust ETL
pipelines firstly data quality ensuring
data quality is crucial for Reliable
analysis implementing data validation
checks handling missing values and
resolving data and consistencies are
vital to maintaining data Integrity
throughout the pipeline next is
scalability as data volumes grow
exponentially scalability becomes
essential distributed computing
Frameworks like Apache spark enable
processing large data sets in parallel
allowing the pipelines to handle
increasing data loads efficiently
thirdly we have error handling and
monitor robust error handling mechanisms
such as retry logging and alerting
should be implemented to handle failures
gracefully additionally monitoring tools
can provide real-time insights into
pipeline performance allowing quick
identification and resolution of issues
we have incremental loading for
continuously evolving data sets
incremental loading strategies can
significantly improve pipeline
efficiency rather than processing the
entire data set each time only the new
or modified data is extracted and
transformed reducing processing time and
resource consumption and lastly we have
a data guidance and security
incorporating data gun practices and
adhering to security protocols is
crucial for protecting sensitive data
and ensuring compliance with regulations
like gdpr or hip AAA now that we have
covered what exactly is ETL and ATF
stages and also the best practices for
18 pipelines less proceeding with
understanding the popular ETL tools so
the first one amongst the popular ETL
tools as the Apache airflow Apache
airflow is an open source platform that
allows you to schedule Monitor and
manage complex workflows Apache airflow
provides a red set of operators and
connectors enabling seamless integration
with various data sources and
destinations next is styling a
comprehensive ETL tool that offers a
visual interface for Designing data
integration workflows Talent provides a
balanced array of pre-built connectors
Transformations and data quality
features making it ideal choice for
Enterprises and lastly we have
Informatica Informatica is a widely used
Enterprise grade ETL tool that supports
complex data integration scenarios Power
Center offers a robust set of features
like metadata management data profiling
and the data lineage and powering
organizations quantitative research is
the systematic analysis of measurable
data and the application of statistical
mathematical or computer methods with
the use of sampling techniques and the
distribution of online questionaries
polls and surveys quantitative research
gathers data from current and potential
clients now that we understood what is
the definition of quantitative data
let's discuss what are we going to
discuss today
we will we have already discussed about
what is quantitative research method
examples of quantitative research after
that what are the methods in
quantitative research and what are the
characteristics of quantitative research
advantages and disadvantages coming to
example a poll of teenagers was
performed to examine the effect of using
mobile phone on kids young people and
teenagers between the ages of 15 and 30
make up the survey sample
while the youngsters are being educated
via their mobile devices they are also
being spoiled because of the content is
inappropriate for young children so the
topic of whether it is Advantage is to
offer mobile phones to a children or not
must posed by 150 respondents of whom
100 were men and 50 were women the
response includes the seven point scale
option with seven denoting a strong
agreement six with agreement five with
mild disagreement neutral agreement and
three with moderate disagreement one
with strong disagreement so this is how
a quantitative survey is conducted this
data helps to gain insight into the
societal mindset and gain conclusion
according to that let us take up another
example every firm that wants to conduct
customer satisfaction survey can do so
by using customer satisfaction template
using the this type of survey a company
can gather quantifiable in information
and metrics on the perception that the
customers have of the brand or company
based on a variety of factors including
product quality cost and customer
experience regardless of the types of
research there is always one thing
common in every type of research that is
data data is omnipresent let me convey
how simply run can help you to master it
please check out the course on data
analytics in collaboration with IBM with
real-time projects and business case
studies link is given in the description
box below enroll now now comes important
types of quantitative research first is
correlational research survey research
descriptive research and experimental
research let's learn every one of them
in detail first is correlational
research in order to create a connection
between two related things and asserting
how one influences the other
correlational research is used a
researcher will need at least two
different groups to do this although
this kind of research doesn't not go as
far as to analyze the various patterns
it will be able to spot Trends and
patterns in the data survey research the
most basic tool for all quantitative
research methodology is survey research
This research primary goal is to
comprehensively explain the traits of a
specific population or group of
populations both small and large
businesses frequently use this study to
comprehend their clients needs and
opinions of their products and
merchandise this
the goal of descriptive research is to
describe how a variable is currently
behaving descriptive research seeks
would describe and interpret the current
state of individuals environments
customers circumstances or events and
the last one is experimental research as
its name implies experimental research
typically relies on one or more theories
it is predicated in one or more theories
known as real experimentation and it
applies to scientific method to
determine the cause and effect link
between a number of study related
variables moving on to quantitative
research characteristics first is
structure tool what is structure two
structure tools such as surveys polls or
questionaries are used to gather
quantitative data using such structured
methods helps collecting depth and
actionable data from the survey
responding
second is sample size quantitative
research is conducted on a significant
sample size that represents the target
market sampling methods have to be used
when deriving the sample to fortify the
research objective
measurable data typically non-numerical
forms such as tables charts and graphs
are used to portray quantitative data
this makes it simple to comprehend the
information that has been gathered and
to demonstrate the reliability of the
market treason these are the main
characteristics of a quantitative data
so what are the advantages of using
quantitative research first is Swift
data collection quantitative research
looks at a sample of people who
represent a wider population information
from these participants is gathered
using a survey or another quantitative
research technique the application of
statistics facilitates and accelerates
the process of data analysis and pattern
Discovery scope of data analysis this
study approaches or allows for a broad
range of data acquisition because of
statistics
gather and collect reliable data what do
you mean by that the results that are
produced through data analysis and
presentation in numerical form are quite
reliable they provide a clear accurate
and honest representation of the study
that was actually done quantitative
research is done in circumstances when
this researcher forces the controversy
the quantitative technique allows
researchers to concentrate on a
particular population wide fact that
they want to explode this approach is
also helpful when a number of data
points are highly sought after by a
specific population it is a procedure
that enables us to comprehend the social
context of the motivation guiding our
choices activities or behavior these are
the advantages now let's concentrate on
disadvantages of quantity research this
approach disregards the significance of
social phenomena a specific hypothesis
can be proved or refuted using the
quantitative approach replies to
specific queries it is unconcerned with
the reasons behind why people Express
their opinions or make decision
sometimes it can be misleading many
individuals believe that quantitative
research is more trustworthy or
scientific than observational
qualitative research since it is based
on numbers both type of study here I
mean qualitative and quantitative
research nevertheless have the potential
to be fascious or biased
more expensive this process is very
expensive compared to qualitative
research creating data for independent
research or surveys are highly expensive
we have already made a video on
qualitative research please check out
that people are frequently hired by
researchers to help with the process of
gathering and evaluating the data even
gaining access to enormous databases and
protecting secondary data can be costly
researchers who employ the quantitative
approach must proceed under the
presumption that every response they
receive from service test and
experiments are not alive it's very
difficult to predict right as there are
no face-to-face interactions while using
this method interviewers or researchers
cannot evaluate the validity of each
outcome after learning quantitative
research one thing is for sure that it
has many applications so what are the
applications
it is used in Psychology economics
demography sociology marketing health
and development gender studies and
political science by definition in
mathematical Sciences such as physics is
also quantitative however the context in
which the term is used here is different
early forms of research were conducted
in fields like biology chemistry and
physics and focused onto looking into
things we could observe and even measure
but we have come a long way qualitative
research is concerned with why people
behave the way they do how are opinions
formed how and why cultures have
developed in the way they are now before
we dive into the topic we must
understand what qualitative data is
first but before we move ahead let us
discuss the agenda for today's session
we'll start this tutorial with a quick
understanding of what is qualitative
data and then we'll understand what is
the qualitative research method and up
next we will discuss various methods
using this qualitative method and once
we get familiar with what qualitative
data is and its various methods let us
discuss some advantages of qualitative
research method and finally we'll wrap
up this session by discussing some
disadvantages of qualitative research
method but before we begin if you're new
to the channel and haven't subscribed
already consider getting subscribed to
our Channel simply learn to stay updated
with all the real estate content and hit
that Bell icon to never miss an update
from us now generally we categorize data
into qualitative
quantitative and mixed methods now
qualitative data focuses on text and
description whereas quantitative data
focuses on numbers and statistics which
can be called hard data but the mixed
method combines elements of both
quantitative as well as the qualitative
data now qualitative data is non-numeric
information such as videos notes images
survey results and text documents in
other words the data cannot be measured
or narrowed on with specific parameters
if it is not measurable then how it is
used what are the different methods in
it now for that we have to understand
qualitative research is about now
qualitative research aims to understand
ideas opinions or experiences by
gathering and evaluating non-numerical
data such as text video or even audio it
is used to discover complex details
about a topic or develop fresh research
Concepts now data is the new oil things
which are going to discover about
qualitative research are all related to
the data right so let me convey how
simply learn can help you master it
please check out the course on data
analytics in collaboration with IBM with
real life projects and business case
studies and also let us know have you
ever used any qualitative research
methods if yes please mention in the
comment section below where and where
you have used it now in general we have
five methods in qualitative research
first one is content analysis and then
we have discourse analysis next on the
list we have narrative analysis and
fourth on the list we have grounded
Theory and finally we have thematic
analysis so let us now talk about them
in detailed one by one let us discuss
about them in detail now first we have
content analysis now the research This
research method examines the presence of
certain words subjects and text in
videos images and audio files that are
completely non-numeric now this method
is conducted to examine or get customer
feedback about their experience and
opinion on the brand the method converts
qualitative data into quantitative one
and helps Brands to make reliable
conclusions now the most popular
application of content analysis nowadays
is in the analyzing social media field
now you will be surprised by the broad
range of content analysis software
Solutions today they assist you in
investigating nearly any sort of
business documents unstructured text
emails social media conversations
comments news blogs and much more we can
connect analysis manually too by using
tools like lexalytics to identify
communication patterns and create
conceptual analysis which is always
advisable
next let us discuss what is discourse
analysis the study of return or spoken
language with its social context is done
using the research method known as
critical discourse analysis it seeks to
comprehend how language is applied in
Practical context now discourse analysis
will appropriately interpret languages
in terms of any socio-political way they
are used in both primary as well as the
secondary data it can be used in
newspapers books government documents
broachers and advertisements as well now
it can be applied to oral language or
any non-verbal form of communication so
when to not use discourse analysis is
the main question now if a study is
narrowly focused and if you're not
concerned about the meaning of the
language used then it is the last choice
so let us now move ahead and discuss
what is narrative analysis is narrative
is omnipresent it is everywhere but it
depends on researchers on how to use the
narrative to their advantage now
researchers employ narrative analysis to
better understand how research
participants create stories and
narratives from their experiences hence
narrative analysis involves a second
layer of interpretation the research
subject first used storytelling to
analyze their own life the researcher
then analyzes how that narrative was put
together now the use of narratives in
narrative research might be taken from
journals letters dialogues
autobiographies in-depth interviews
transcripts focus group transcripts and
other various forms of narrative
qualitative research
so researchers mostly use this analysis
to write thesis papers on any narrative
now for instance although it is a human
nature to construct stories and
narratives when analyzing one's own life
some data Gathering techniques are
better suited to your research
participants of self-narrative
all right let us now move ahead and
discuss what is grounded Theory analysis
now a qualitative approach called
grounded theory is meant to Aid in
developing new hypothesis and inferences
now researchers use any preferred data
collection method and then evaluate the
data to develop Concepts now they
develop theories by contrasting these
ideas so they keep going until they hit
sample saturation after which no further
data will cause them to revise their
Theory then they present the concluding
hypothesis it enables you to investigate
a specific occurrence or process and
develop new hypothesis founded on
Gathering and examining empirical facts
so the human resource department often
uses its to collect data from employees
about what a company lacks so they might
study why employees are facing certain
issues in that segment
and finally we have the Thematic
analysis studying meaning patterns is
what thematic analysis is all about in
other words it involves looking at your
data and its data sets theme to find
significance of it more significantly
because your research questions guide
this method you don't need to find
everything in the data in thematic
analysis you will use codes so the
basically concentrate on the important
components connected to your research
questions researchers may use any
desired method of data collecting and
then they will analyze the data to
create concepts by opposing these
Notions they create theories so it is a
good approach when you want people to
review a brand knowledge and experience
from a data set so there are two types
of it which is inductive approach and
reductive approach the steps involved
are Step One is familiarization it is
used to get to know the data now the
second step is coding next step is to
code the data say it about the
perception of the brand among let's say
100 customers I and finally we have to
generate a theme so we look over the
codes we have and then identify patterns
among them so that was all about the
various methods that are used in
qualitative research I hope you
understood this now every process comes
with both advantages and disadvantages
now coming to advantages of qualitative
research first we have room for more
research it changes the customer's
attitude towards the company as
engagement increases by conducting more
research on customer experiences
now it is a very flexible approach if
useful information has been collected
researchers can immediately change the
method or any other room for improvement
is always available in qualitative
research it is also very cost effective
as it is a straightforward method but
information is collected from every
customer it also adds that it saves
money and produces faster results
now it acts as a Content generator as
well so what do you mean by content
generator here we have discussed how we
collect social economic data using
qualitative research so this raw data is
then turned into useful information to
create valuable content it also provides
perceptions that are specific to an
organization modern Brands can use
qualitative approaches generally to a
specific project for example you can
conduct research on clothing brand with
1000 participants or so
now what are the disadvantages of
qualitative research first one is
obviously loss of data now this is a
serious problem one faces during the
qualitative approach now data must be
collected and recognized initially by
the researchers researchers who cannot
see the data when they observe it will
lose the required data which lessens the
accuracy
it can be affected by buyers now you
might have heard about fake news most
frequently right so this term addresses
the bias that can be present in media
reporting and can be consciously or
subconsciously affect the data it can be
time consuming as well researchers take
longer to acquire data since they go in
many directions it also take time to
sort through all of the additional stuff
information as well so every data point
is analyzed subjectively and hence its
value is never certain which can be a
lot of time consuming process
sometimes it can create questionable
results because it depends on the
individual the researcher determines
what is included in the qualitative
research process and what is eliminated
because of this the data collection
procedure is quite subjective so these
are some of the advantages and
disadvantages of qualitative research do
you know what's common between
International Tech giants like meta
Google and a small size d2c startup that
is running from Jakarta is it the
product their services or their work
culture not really the biggest common
denominator between old tech companies
globally today's data and AI the
overarching need of utilizing data for
decision making and implementing the
almost necessary capabilities of AI in a
product or service has become the demand
of the hour all things considered
wouldn't it be amazing if you can
utilize AI to perform some of the most
complex processing and analytics task on
data I think your answer would be yes
with that being said hello everyone and
welcome to this extremely important
video in data analytics using AI in this
video we will discuss how you can
utilize AI for performing data analytics
which is the very first skill you must
learn to make a career in the field of
data analytics business analytics and
even become a data scientist but before
we begin with that make sure to
subscribe to our Channel and hit the
Bell icon for regular updates let's
quickly discuss the agenda of the video
before we begin if for someone who is
interested in building a career in data
analytics by graduating from the best
universities or a professional who
elicits to switch careers with data
analytics by learning from the experts
then try giving a short to Simply
learn's postgraduate program in data
analytics collaboration with IBM the
link is mentioned in the description box
and you should navigate to the course
page where you can find a complete
overview of the program being offered
and now moving back to our topic the
fear is in there whether it's marketing
automating building driving medical
practice or anything you name AI can do
it coming into data analytics which is
believed to be one of the promising
career options AI is getting integrated
and it is leveraging analytics by
enhancing and revolutionizing the way
data is analyzed interpreted and used to
derive meaningful insights here are some
key ways in which AI is leveraging
Analytics number one data processing and
preparation
AI algorithms automate and streamline
data processing tasks including data
cleaning data integration and data
transformation
this enables faster and more efficient
data preparation ensuring that high
quality data is available for analysis
the next is pattern recognition and
anomaly detection
AI powered analytics systems excellent
recognizing complex patterns in large
data sets machine learning algorithms
can identify hidden patterns Trends and
relationships that may not be apparent
to human analyst additionally a
algorithms can detect anomalies or
outliers in data helping to identify
potential issues or fraud land
activities and the next is Predictive
Analytics AI techniques such as
predictive modeling and forecasting
enable organizations to make accurate
predictions about future events or
outcomes based on historical data by
leveraging a algorithms business can
anticipate customer Behavior demand
Trends Market changes and more allowing
for proactive decision making
and the next we have is natural language
processing and text Analytics
AI powered NLP techniques enable the
analysis of unstructured data such as
text documents social media posts
customer reviews and emails by
extracting and analyzing textual
information organizations can gain
valuable insights into customer
sentiment market trends and emerging
tokens and the next we have is automated
insights and Reporting AI powered
analytics systems can automatically
generate meaningful insights and reports
Based on data analysis this reduces the
time and effort required for manual
analysis and Reporting allowing analysts
to focus on higher value tasks like
interpretation and strategy development
and the next we have is personalization
and recommendation systems a algorithms
are employed in recommendation systems
that provide personal suggestions to
users based on their preferences
behavior and historical data this is
particularly prevalent in e-commerce
streaming services and content platforms
where AI driven recommendations enhance
user experiences and rent customer
engagement
and next we have is optimization and
decision support
a algorithms can optimize complex
processes and decision making by
evaluating multiple variables
constraints and scenarios for example AI
power optimization models can assist in
Supply Chain management resource
allocation inventory planning and
pricing strategies that helps
organizations achieve better operational
efficiency and cost savings and the next
we have is continuous learning and
Improvement AI systems have the ability
to continuously learn and adapt based on
new data and feedback this allows
analytics models to improve their
accuracy and performance over time
making them more effective in generating
valuable insights and predictions
and by leveraging AI capabilities in
analytics organizations can unlock the
full potential of their data gain a
competitive advantage and drive
data-driven decision making across
various domains thus energy between Ai
and analytics is revolutionizing how
businesses abstract insights from data
leading to transformative outcomes and
Innovations
if you are watching this I believe you
are either an aspiring data analyst or a
professional already now the question of
the r is Will AI replace you should I
even care to proceed with a career in
data analytics don't worry we are here
to answer your questions and make things
less complicated or we believe we can
sort it out
now is designed to assist humans for
example chat Jeopardy bad and Bin the
smart AI language models can automate
the time consuming tasks such as writing
down emails performing data extraction
transformation and loading and
exploratory analysis and Reporting
however in the real time every
organization has specifically customized
needs and requirements a special set of
KRS and kpis to address you may have to
understand the business requirements and
build the custom dashboards you may have
to understand the real-time streaming of
data integrate AWS with power bi not
just these they are a gazillion tasks
that need human intervention and logics
AI still lacks the human cognition and
the ability acquiring the domain
knowledge which only a human can you as
a data analyst may need to understand
that AI can perform a redundant and
straightforward task but not the
critical operations which completely
rely on a human
only a human has the caliber to decode
the intentions of the client and
simplify the technical Concepts to known
technical client
has a strong focus on the business
outcomes most importantly try to unravel
the potential of AI and leverage it to
improvise and get ahead if you are
someone that not only performs analysis
but also is a torch Bearer the
organization with the right business
strategies in very short time then you
are someone which all major Tech joints
need so what is data analysis
data analysis is not just a single step
but a set of processes it is the process
of collecting data then cleaning it when
I say cleaning it simply means removing
the irrelevant data and then this data
is transformed into meaningful
information
we can simply relate this process to how
you make a jigsaw puzzle just like how
you gather all the pieces together and
fit them accordingly to bring out a
beautiful picture
data analysis also works on almost the
same grounds
to achieve the goals of data analysis we
use a number of data analysis tools
companies rely on these tools to gather
and transform their data into meaningful
insights so which tool should you choose
to analyze your data which tool should
you learn if you want to make a career
in this field we will answer that in
this session after extensive research we
have come up with these top 10 data
analysis tools here we will look at the
features of each of these tools and the
companies using them so let's start off
at number 10 we have Microsoft Excel all
of us would have used Microsoft Excel at
some point right it is easy to use and
one of the best tools for data analysis
developed by Microsoft Excel is
basically a spreadsheet program using
Excel you can create grids of numbers
text and formulas it is one of the
widely used tools be it in a small or
flat setup
the interface of Microsoft Excel looks
like this
let's now move on to the features of
excel
firstly Excel works with almost every
other piece of software in office we can
easily add Excel spreadsheets to Word
documents and PowerPoint presentations
to create more visually appealing
reports or presentations
the windows version of excel supports
programming through Microsoft's Visual
Basic for applications VBA
programming with VBA allows spreadsheet
manipulation that is difficult with
standard spreadsheet techniques
in addition to this the user can
automate tasks such as formatting or
date organization in VBA
one of the biggest benefits of excel is
its ability to organize large amounts of
data into orderly logical spreadsheets
and charts by doing so it's a lot easier
to analyze data especially while
creating graphs and other visual data
representations the visualization can be
generated from specified group of cells
those were few of the features of
Microsoft Excel
let's now have a look at the companies
using it most of the organizations today
use Excel few of them that use it for
analysis are the UK based company Ernest
and Young then we have Urban Pro Wipro
and Amazon
moving on to our next data analysis tool
at number nine we have rapidminer
a data science software platform
rapidminer provides an integrated
environment for data preparation
analysis machine learning and deep
learning
it is used in almost every business and
Commercial sector rapidminer also
supports all the steps of the machine
learning process
seen on your screens is the interface or
rapidminer
moving on to the features of rapidminer
firstly it offers the ability to drag
and drop it is very convenient to just
drag drop some columns as you are
exploring a data set and working on some
analysis
rapidminer allows the usage of any data
and it also gives an opportunity to
create models which are used as a basis
for decision making and formulation of
strategies
it has data exploration features such as
graphs descriptive statistics and
visualization which allows users to get
valuable insights
it also has more than 1500 operators for
every data transformation and Analysis
task
let's now have a look at the companies
using rapidminer we have the Caribbean
Airline Leeward Islands Air transport
next we have the United Health Group the
American online payment company PayPal
and the Austrian Telecom company
MobileComm so that was all about
rapidminer now let's see which tool we
have at number eight
we have talent at number eight
Talent is an open source software
platform which offers data integration
and management it specializes in Big
Data integration Talent is available
both in open source and premium versions
it is one of the best tools for cloud
computing and Big Data integration
the interface of talent is as seen on
your screens
moving on to the features of talent
firstly automation is one of the great
Boon's Talent offers it even maintains
the tasks for the users this helps with
quick deployment and development
it also offers open source tools Stalin
lets you download these tools for free
the development costs reduce
significantly as the process is
gradually speed up
Talent provides a unified platform it
allows you to integrate with many
databases SAS and other Technologies
with the help of the data integration
platform you can build flat files
relational databases and Cloud apps 10
times faster
those were the features of Talon the
companies using Talent are Air France
L'Oreal cab Gemini and the American
multinational Pisa restaurant chain
Domino's
next on the list at 7 we have nine
Constance information Miner on nime is a
free and open source data analytics
reporting and integration platform
it can integrate various components for
machine learning and data mining through
its modular data pipelining concept nime
has been used in pharmaceutical research
and other areas like CRM customer data
analysis business intelligence text
Mining and financial data analysis
here is how the interface of 9
application looks like
now coming to the nine features
9 provides an interactive graphical user
interface to create visual workflows
using the drag and drop feature use of
jdbc allows assembly of nodes blending
different data sources including
pre-processing such as ETL that is
extraction transformation loading for
modeling data analysis and visualization
with minimal programming
it supports multi-threaded in-memory
data processing 9 allows users to
visually create data flows selectively
execute some or all analysis steps and
later inspect the results models and
interactive views
9 server automates workflow execution
and supports team-based collaboration 9
integrates various other open source
projects such as machine learning
algorithms from Becca H2O carers Park
and our project
9 allows analysis of 300 million custom
addresses 20 million cell images and 10
million molecular structures
some of the companies hiring for nime
are United Health Group asml fractal
analytics atos and LEGO Group let's now
move on to the next tool we have SAS at
number six
SAS facilitates analysis reporting and
predictive modeling with the help of
powerful visualizations and dashboards
in SAS data is extracted and categorized
which helps in identifying and analyzing
data patterns
as you can see on your screens this is
how the interface looks like
moving on to the features of SAS
using SAS better analysis of data is
achieved by using automatic code
generation as our SQL
SAS allows you to access through
Microsoft Office by letting you create
reports using it and by Distributing
them through it
SAS helps with an easy understanding of
complex data and allows you to create
interactive dashboards and reports
let's now have a look at the companies
using SAS we have companies like genpact
iqia Accenture and IBM to name a few
that was all about SAS
so for all those who joined in late let
me just quickly repeat our list at
number 10 we have Microsoft Excel then
at number nine we have rapidminer at
number eight we have talent at number
seven we have nine and at number six we
have SAS so far you'll agree with this
list let us know in the comment section
below let's now move on to the next five
Tools in our list
so at number five we have both R and
python yes we have two of them in the
fifth position
R is a programming language which is
used for analysis as well it has
traditionally been used in academics and
research python is a high level
programming language which has a python
data analysis Library it is used for
everything starting from importing data
from Excel spreadsheets to processing
them for analysis
this is the interface of r
next up is the interface of the Python
Jupiter notebook
let's now move on to the features of
both R and python
when it comes to the availability of R
and python it is very easy both are in
Python are completely free hence it can
be used without any license
I used to compute everything in memory
and hence the computations were limited
but now it has changed both are in
Python have options for parallel
computations and good data handling
capabilities
as mentioned earlier as both R and
python are open in nature all the latest
features are available without any delay
moving on to the companies using R we
have Uber Google Facebook to name a few
python is used by many companies again
to name a few we have Amazon Google and
the American photo and video sharing
social networking service Instagram
that was all about RN python
at number four we have Apache Spark
Apache spark is an open source engine
developed specifically for handling
large-scale data processing and
Analytics
spark offers the ability to access data
in a variety of sources including Hadoop
distributed file system htfs openstack
Swift Amazon S3 and Cassandra
it allows you to store and process data
in real time across various clusters of
computers using simple programming
constructs
Apache spark is designed to accelerate
analytics on Hadoop while providing a
complete Suite of complementary tools
that include a fully featured machine
learning library a graph processing
engine and stream processing
so this is how the interface of Apache
spark looks like
now let's look at the important features
of Apache Spark
Sparks stores data in the ram hence it
can access the data quickly and
accelerate the speed of analytics spark
helps to run an application in a Hadoop
cluster up to 100 times faster in memory
and 10 times faster when running on disk
it supports multiple languages and
allows the developers to write
applications in Java Scala r or python
spark comes up with 80 high-level
operators for interactive querying spark
code for batch processing joint stream
against historical data or run ad hoc
queries on stream state
analytics can be performed better as
spark has a rich set of SQL queries
machine learning algorithms complex
analytics Etc Apache spark provides
fault tolerance through spark rdd spark
resilient distributed data sets are
designed to handle the failure of any
worker node in the cluster thus it
ensures that the loss of data reduces to
zero
Lockheed Martin and eBay are some of the
companies that use Apache spark on a
daily basis
at number 3 we have another important
growing data analysis tool that is Click
View
click view software is a product of
Click for business intelligence and data
visualization click view is a business
Discovery platform that provides
self-service bi for all business users
and organizations
with click view you can analyze data and
use your data discoveries to support
decision making
clickview is a leading business
intelligence and analytics platform in
Gartner magic quadrant
on the screen you can see how the
interface of Click view looks like
now talking about its features
clickview provides interactive guided
analytics with in-memory storage
technology during the process of data
Discovery and interpretation of
collected data the clickview software
helps the user by suggesting possible
interpretations
clickview uses a new patent in memory
architecture for data storage all the
data from the different sources is
loaded in the ram of the system and it
is ready to be retrieved from there
it has the capability of efficient
social and mobile data discovery
social data Discovery offers to share
individual Data Insights within groups
or out of it
a user can add annotations as an
addition to someone else's insights on a
particular data report clickview
supports mobile data Discovery within an
HTML file enabled touch feature which
lets the user search the data and
conduct data Discovery interactively and
explore other server-based applications
click view performs olap and ETL
features to perform analytical
operations extract data from multiple
sources transform it for usage and load
it to a data warehouse
the companies that can help you start
your career in Click view are
Mercedes-Benz cab Gemini Citibank
cognizant and Accenture to name a few
at number 2 we have power bi
power bi is a business analytics
solution that lets you visualize your
data and share insights across your
organization or embed them in your app a
website
it can connect to hundreds of data
sources and bring your data to life with
live dashboards and reports
power bi is the collective name for a
combination of cloud-based apps and
services that help organizations collate
manage and analyze data from a variety
of sources through a user-friendly
interface
power bi is built on the foundation of
Microsoft Excel and has several
components such as Windows desktop
application called Power bi desktop and
online software resist service called
Power bi service mobile power bi apps
available on Windows phones and tablets
as well as for IOS and Android devices
here is how the power bi interface looks
like as you can see there is a visually
interactive sales report with different
charts and graphs
moving on to the features of power bi
it has an easy drag and drop
functionality with features that make
data visually appealing you can create
reports without having the knowledge of
any programming language power bi helps
users see not only what's happened in
the past and what's happening in the
present but also what might happen in
the future
it offers a wide range of detailed and
attractive visualizations to create
reports and dashboards you can select
several charts and graphs from the
visualization pane power bi has machine
learning capabilities with which it can
spot patterns in data and use those
patterns to make informed predictions
and run what-if scenarios
power bi supports multiple data sources
such as Excel Tech CSV Oracle SQL Server
PDF and XML files the platform
integrates with other popular business
management tools like SharePoint Office
365 and Dynamics 365 as well as other
non-microsoft products like spark Hadoop
Google analytics sap Salesforce and
MailChimp
some of the companies using power bi are
Adobe AXA Carlsberg capture menai and
Nestle
moving on to the next tool so any
guesses as to what we have at number one
you can comment in the chat section
below
finally on the top of the pyramid we
have tableau
Gartner's magic quadrant of 2020
classified Tableau as a leader in
business intelligence and data analysis
Tableau interactive data visualization
software company was founded in Jan 2003
in Mountain View California
Tableau is a data visualization software
that is used for data science and
business intelligence it can create a
wide range of different visualization to
interactively present the data and
showcase insights
the important products of Tableau are
Tableau desktop Tableau public Tableau
server Tableau online and Tableau reader
this is how the interface of Tableau
desktop looks like
now coming to the features of tableau
data analysis is very fast with Tableau
and the visualizations created are in
the form of dashboards and worksheets
Tableau delivers interactive dashboards
that support insights on the Fly
it can translate queries to
visualizations and import all ranges and
sizes of data writing simple SQL queries
can help join multiple data sets and
then build reports out of it you can
create transparent filters parameters
and highlighters
Tableau allows you to ask questions spot
Trends and identify opportunities with
the help of Tableau online you can
connect with Cloud databases Amazon
redshift and Google bigquery
the company is using Tableau are
Deloitte Adobe Cisco LinkedIn and the
American e-commerce giant Amazon to name
a few
and there you go those are the top 10
data analysis tools
let's now have a question and answer
session please feel free to post your
queries in the comments section and
we'll respond in the chat
before the question answer session let's
recap quickly in the meanwhile you all
can post your questions in the comment
section below
so at number 10 we have Microsoft Excel
then at number nine we have rapidminer
at number eight we have talent at number
seven we have nine at number six we have
SAS R and python at number five Apache
spark at number four
click View at number three power bi at
number two and finally we have Tableau
topping the list at number one
try giving a shot to Simply Dance plus
strategy program in data analytics this
is from kurdy University in
collaboration with IBM and this should
be the right choice the link in the
description box below should navigate
you to the home page where you can find
a complete overview of the program being
offered we will learn about functions
and formulas we will learn about
conditional formatting data validation
pivot chart and keyboard table
now let's look at the scenario here so
one day in a startup
One Professional speaks that their
business is growing and they would need
an efficient way to work with the data
they would have to find a way to work
faster with storing and analyzing data
now to that another click response well
we can make use of Microsoft Excel to do
this job
the question is will excel be able to
cater to their business needs
now
the colleague response well we can make
use of excel in several ways and it also
is a cost efficient option
now in that case the colleague who posed
the question says let's go ahead with
Excel and let's train our employees in
Excel
and the suggestion is welcomed which
would make the job easier for them and
they would basically decide on using
Excel so they decide on taking a
training right away and basically
starting to learn Excel
now
before we move to excel one of the
question is why should we use Excel
so let's look at some of the points here
so Excel proves to a be a great platform
to perform various mathematical
calculation on large data sets which is
one of the biggest requirements of
various organizations these days various
features in Excel like searching sorting
filtering makes it easier for you to
play with the data and Excel also allows
you to beautify your data and present it
in the form of charts tables and data
bars
now when it comes to reporting reporting
accounting and Analysis can be performed
with the help of excel
it can help you with your task lists
your calendars and goal planning
worksheets
Excel also provides good security for
your data Excel files have the feature
of password protection this way your
information can be safe
now when we talk about what is Excel and
how it can be used so Excel or you might
have heard a spreadsheet can be
basically used for lot of different
tasks than just storing the information
in so-called tabular format
now Microsoft Excel is an application
that is used for recording analyzing and
visualizing data it is in the form of a
spreadsheet
let's have a look at few of the
functions and formulas used in Excel and
before we do that we can also quickly
take a small tour to understand how to
work with Excel now to do that what we
can do is we can type in in our search
say for example Excel and just select
your Excel app which is installed and
here you see you have lot of options
which says take a tour drop down list
get started with formulas make your
pivot table going forward with pie
charts and much more so we can click on
this one which says take a tour
and that basically pops up a window
which says welcome to Excel and if you
have always wanted to be better at Excel
you have this which can help you so
let's click on Create and that takes us
to the store window now that says
instructions for screen readers which
basically talks about 10 different steps
in which we can learn Excel and using
the spreadsheet app
so there are more than 11 sheets which
we see here at the bottom end and each
one gives us a simple example which we
can work on so for example if I click on
ADD now that takes me to this page which
says how do we add numbers now you might
be provided data which we can upload by
loading a file from our machine or
getting data from a web Source or even
connecting to a database so there are
various options which we will see in
some time so here we have an option
which is called Data you can click on
this one and this basically has
options where you can use existing
connections if you have created some you
can always click on from other sources
and you can get your data from SQL
Server from analysis services from odata
data feed you can get in from XML from
data connection Wizard or also from
Microsoft query you can be running in
different queries here which shows up in
the option which says new query there
are connections which you can use and
that basically will display all the
connections for this particular workbook
which we do not have as of now but we
can create them but let's look at simple
examples now you can follow these
instructions here which says basically
adding up the numbers and that could be
easily done by just placing your cursor
here and what you could do is either you
can type in the formula that is from
which row to which row you would want to
add the data so for example I could just
do a sum here and that shows up all the
different functions which are available
then we can open up a parenthesis and we
can say I would be interested in
totaling the amount
from
column D and I would select for example
D4 so I could be doing this and then I
could say D4 onwards till d 7 so that's
the data which I'm interested in you can
close your parenthesis and hit enter and
that gives you the total there is also a
shortcut for this which you can always
do is we can first delete this and you
can just place your cursor here and just
use your alt and equals that
automatically selects the numericals
which we can anytime expand or basically
collapse so I will basically select this
which says this function needs two
numbers which is number one and number
two and then you can hit on enter and
that gives you the total
so similarly we can be getting in the
data here by selecting all the fields so
here it also says that you can use a
shortcut now what we can also do is we
can add numbers over 50 by selecting the
yellow cell here and then giving a
condition
such as so I can basically use something
like sum if and then open a parenthesis
I can select I would be interested in
this row and then I can even drag and
drop
till here so that tells me the 11 to D15
you can then put in a comma here and you
can give your condition say for example
we would say I would be interested in
numbers only above 50 and we can select
this
close your codes and then just close
your parenthesis and that's your formula
so you can do this and that basically
gives me the total is hundred now
similarly we could do that for the
amount here I could select this now
there is also an option I can click on
home and I can go for something like
Auto sum so that's one more way of doing
it which anyway says sum is alt plus
equals so it automatically adds up your
values and I can try doing a auto sum
that automatically selects my rows and
then I can get my total now as per this
activity here it says try adding another
sum F Formula here but add amounts that
are less than 100 and the result should
be 160. so what we can do is we can
basically select all the numbers which
are lesser than 100 so the way we did
earlier here there can be always a
shortcut so you can always for example
if you would want to avoid typing in the
formula you can always copy it from here
and then just hit on enter so you are
back into this cell and then I can
basically go here and paste the number
and then as per the requirement we are
required to select anything which is
lesser than 100 so what I could do is I
could select here I could say let's say
G and then I can change this value to
g15 and that's one more way now we see
our selected rows have been changed so I
can hit on enter and I can check what is
the result so we would be interested in
looking for numbers which are lesser
than 100 so I will have to also change
this one to a lesser sign and that
basically gives me the total which is
160. so that's how you can simply add
numbers you can use autosum you can type
in the formula you can select the fields
or you can just place your cursor where
you would be looking for a sum and then
you can just do a alt equals and that
basically populates the sum
now let's look at some easy options of
filling your cells or automatically
populating the values in your cells
within your Excel sheet now here we have
an option which says 100 now we can
click on this and that basically says it
is making a sum of column C4 to D4 so if
I click on this one I can check that
this is row number four which shows up
here and I also know this is column C I
also have D so this equals is basically
giving me a sum of C4 to D4 now what we
can do is we can always place our cursor
here at the right corner and then we can
just drag and drop and this basically
gives me a total of all the numbers for
all different rows so this is one
shortcut which we can do to get the
total Excel will automatically give the
totals which we call as filling down now
what we can do is in the same way if we
would want to get the totals here we can
first check what is this 200 and this
tells me this C11 to c14 total so it is
totaling the rows from C11 so column C
and 11th row till 14th row and that's
the sum now what I can also do is I can
similarly like above we can do a filling
right which basically means bringing
your cursor here and then just dragging
and dropping it all the way where you
would need the totals and this basically
gives me the total there is one more
quick way to check if this is right so
the easiest option would be to select
this cell now what I can also do is I
can just select all of these fields by
just highlighting and selecting all the
fields once it is selected press on Ctrl
R and that gives you the total now if we
would be doing this stop down then I
could select all these rows for this
particular column and then I could do a
control D so that's your filling down
and this one was filling right so this
is an easier option of doing a fill when
you would want to have the formula
applied to every row as it occurs in the
first row or the last row we could test
this by for example selecting these
fields I could delete them and I have
here which says 130 I could just place
my cursor here and I could drag it all
the way up and that should also do the
same magic which we were seeing from top
down so this is a simple way wherein you
can fill up your cells and you can also
automatically really
propagate or move your computation to
all the cells
let's look at the split option which
basically helps us in splitting the data
when we have some kind of pattern or
when we have some kind of delimiters in
our data in say one particular column
and we would want to derive the values
out of it so we can always use the
splitting option now the easiest option
would be so for example we have our
email column which has the email IDs and
which we can clearly see has a first
name dot last name now I see that there
is a last name Smith filled up here
first name is empty so what I can also
do is I can just type in say Nancy here
now that's the first name I can again
start typing the second name and as soon
as you do that you would see a faded
list of numbers and that's your clue to
hit enter and once you do that you would
see all the first names have been filled
in here if you would want to maintain
the case sensitiveness you can just go
ahead and delete these and let's type in
as it occurs so let's say Nancy as the
first name go down to the next cell and
just type in Andy and there is your
grade list so just hit on enter and that
basically fills up your first name what
we can also do is
we can just select this particular field
and either we can type in control e
which basically fills up all the options
now I can just do a undo by typing in or
clicking Ctrl Z and that's basically
gone what I can also do is I can select
a particular field and then I can go
into home option and under home you have
an option here which says fill so you
can select this and then you can do a
Flash Fill which is what we are doing
here so click on Flash Fill and that
automatically fills up the values so in
this way you can work within your
spreadsheet and you can be filling up
the values where a delimiter by default
is understood and we can split the data
now however sometimes you might have
some data which has a different kind of
delimiter and there is again a smarter
way of splitting your data so you can
always scroll down here and that says
splitting a column based on delimiters
so we have some values in the data
column and these values in each row are
separated by comma so
select this your data is already
selected text to columns delimited comma
selected and now click on next so it
basically says what is the destination
let's select this one
and I can choose what would you want to
have so that shows me this would be my
data preview now I can basically select
this one I can say finished and say okay
and now if you see our data has been
placed in in the columns appropriately
so this is how you can split your data
based on a delimiter and then organize
your data in a better way now there are
some Advanced options which we can learn
later but this basically tells about
using a formula so this is something if
say if we have some name in one cell and
if you would want to split it into first
name your helper column your middle name
last name so that can also be done using
formulas and this basically tells how
would you extract characters from your
left cell and how would you place them
in your right cell so you can try this
activity which is a little more of
advanced option the benefit is that you
can always use this
wherein if you do some kind of
transformation using your formulas if
your original data gets updated then the
split data will also get updated and
that's the benefit of using formulas
where you can place values from one cell
into multiple cells based on execution
of your details in the formulas
how about using the transpose option now
you might have heard of situations where
you would want to switch or turn your
rows into columns and your columns into
rows and that's where transposing comes
into picture it might be useful when you
have your data in your X and Y axis or
as I would say in rows and columns and
you would want to switch your rows to
become the columns and columns to become
your rows so what we can do is the
simplest way is you can select all your
values so here we basically have six
columns and I would say two rows now I
can select all of these and then I can
select an empty field for example the
one which is highlighted here well you
can always do a control alt V that's a
shortcut what you can also do is once
you have selected all your Fields you
can just copy them so just do a Ctrl C
and then click on an empty cell and then
what you can do is you can do a special
paste or paste special so under your
home you have the paste option and here
you can go for paste special and once
you do that you need to select the
transpose option over here and click on
OK and now you will see that the columns
and the rows have been transposed so
your row name was item and that has
become the column heading you had row
name as a mount and that has become the
column heading and and all your values
have been transposed in this particular
format now there is another way of doing
that and again that's using your
formulas so what you can do is you can
transpose with a formula also and that
basically works when you have similar
kind of data so this has six columns and
basically two rows so you can basically
do this so you can select this and
earlier we were doing a copy but now
what we would want to do is we would
want to just look at the row numbers
which tells me it is c33 c34 and it
starts with c and ends in with your H
column so what we can do is we know that
we have six columns and two rows so
transposing that would actually give me
two columns and six rows so what we can
do is we can select two columns and six
rows in our Excel sheet I can then
basically start typing in the message or
I can just go to the address bar and
here I can say
transpose
let's go for c33 h34 it basically
selects my data and now I can just do a
control shift and enter and now if you
see all the values have been populated
now you can just place your cursor in
one of the cells but if you see the
address bar the formula Remains the Same
this is because this is an array formula
so we can read more about an array
formula here it's basically something
which performs calculations and on more
than one cell in an array and in the
example here the array is the original
data that is c33 to h34 so your
transpose is just changing the
horizontal orientation to the vertical
orientation so this is a very simple way
in which you can basically use the
excel's capability to transpose your
data and convert your rows into columns
and columns into rows apart from working
on
additions subtractions filling up your
data sorting the data or basically
splitting your data transposing your
data one of the other requirements is
sorting and filtering your data so that
can be very handy when you're working on
huge data and you would want to sort it
in a particular order say ascending or
descending or might be based on a
particular field or if that field was or
if the cell was highlighted with a
particular color sorting the data so
let's look at how Excel can be used for
sorting and filtering examples are
pretty simple here so let's check that
so if we're going to sort and filter and
say this is the data I have say for
example I would want to sort the values
in the department column alphabetically
so what I can do is I can select
Department column and I'm already in the
Home tab I can straight away go here
which says sort and filter here I can
then say sort A to Z and that's
basically alphabetically sorting your
department column and once once I do
this you would see the data has been
sorted but it's not just this data we
can just do a control Z and check what
are the values we have so here we have
meet which is beef and 90
000 hundred ten thousand the values then
you have Bakery which should ideally be
the first row if we sort it in an
alphabetical order which goes with
Bakery as desserts you have the values
so we can check this again so select
department and then just do a sort and
filter and let's say sort A to Z and if
you see the data has changed but it's
not just in changing your First Column
but then it has taken care of all the
data however the data has been sorted
based on the department column so you
have Bakery which aligns with deserts
which has the values and now we we have
all the data which has been filtered now
what we can also do is we can sort
December's amounts from largest to
smallest so what we can do is we can
basically click any cell in the December
column let's say 20
000 and then what I can do is I can go
into sort filter and then I can say sort
largest to smallest so if you see Bakery
breads is the row which has the smallest
data or maybe you have Delhi sandwiches
so that one looks also smaller so let's
do a larger to smallest and if I do this
you would see the values have been
shifted now so it is no more based on
the department column
because now the data is being sorted
based on the values in the December
column and you see Bakery which was
alphabetically the first one has become
second last
so either you can sort the data based on
a department column which goes based on
the values these are all string values
or words so it sorts alphabetically if
you have numbers might be you can give
some values and you can sort the data
you could anytime do a custom sort and
you could basically select if you would
want to select the data so I could do a
custom sort and then I could choose
which is the column which we would want
to use for sorting what is the Sorting
needs to be done is it cell values is it
cell color font color conditional
formatting and then you can also choose
the order so that's one more way to do
that now if you scroll down that also
shows how you can sort by date or a
color so for example if you would want
to sort based on the expense date so
there are different options so what I
can do is I can select this date field I
can just do a right click I can go into
sort and then I can choose I would want
to sort oldest to newest so since I
selected the date field it basically has
sorted all the data data and it has
taken the expense State into
consideration now there are these
filters which you see on the row
headings we could have also used those
so I could have selected this and that
basically says or mentions which are the
dates I would be interested in looking
at I also have sort by color here I can
do a sort oldest or newest or newest to
oldest so I could also use these filters
which have been applied here now we have
the data which is in color so if I would
want to basically select the color
columns or color cells I could select
this I can basically do a right click
here and when I go into sort I could
choose put selected color or cell color
on the top and that basically will make
sure that my data is sorted and it has
also sorted that in a descending order
so in this way you can sort or basically
filter your data what we can also do is
we can add filters so sometimes we can
go for formulas which we would want to
use what we can also do is we can
basically select the filter which has
been applied here now how does the
filter come in there so if I would
select a particular row I could select a
particular row and then I could decide
if I would want to just add a filter to
this one and that's how the filter has
come in so we have the filter what we
can do is we can basically click on this
drop down and then you have something
like number filters so we can always go
here we can basically choose one of
these so we can basically choose above
average so I could select this and then
basically it shows me the values we
could also delete the filter by clicking
on this one and we could say well I'm
not interested in this filter anymore so
I could clear the filter and that shows
me all the values or I could say
that let's click on some other field for
example food I can go in here I can go
into number filters and then I could say
well I'm interested in values which are
below average above average might be
greater than and then I can choose what
is the value so for example if we say I
am interested in food which is greater
than 25 dollars I could give a value
here I could say okay and now I have
applied the filter similarly you can
select this and then you can just clear
your filter and your data is back so
remember no data is lost it is just
hidden or basically based on the filter
not shown so that's good enough for us
and in this way you can sort and filter
the data so for more details obviously
in all the sheets you have the links
which point to more information on the
web and you can always refer to these so
this is simple way in which you can sort
and filter any amount of data which has
been stored within your Excel within a
particular sheet now that we have
learned about add fill split transpose
sorting and filtering it will also be
good to learn how to work with tables or
basically converting your data into a
tabular format and then doing some easy
computations so click on this tables
option here now here we see there is
some data which is in five columns and N
number of rows so I can basically select
this data and then what I can do is I
can insert choose the table option and
then it says my table as headers and
we'll be okay with that I'll say okay
and now if you see this is the table
created it basically has different
filters which we have learned earlier
how to use and this is basically my
table which is a collection of cells
which has some special features so we
can easily add rows to this table we can
add columns to this table and we can
even do some calculations so for example
here I can click on this one I can
basically enter some field and then I
can hit on enter and we see that this
row has been inserted wherein we can
easily fill in values for example I
would say chocolates
I could give some value here size 25
000 might be 35
000 and then basically I can given some
values here now what you can also do is
you can continue adding rows in this way
and say for example you would want more
columns so you can select this
option here in the top bottom right
corner you can just drag it towards the
right and that basically has
automatically created columns for my
next months wherein I can feed in the
data so this is a simpler way wherein
you can keep adding more rows and
columns to your data if that has been
converted in a tabular format now let me
just do a control Z that basically
deletes the columns again Ctrl Z deletes
the last row which we added and I can
stop here or I can even remove my values
by doing multiple control Z and removing
my rows so this is how I converted my
data into a table and then I can easily
work on this what I can also do is I can
do some calculations so what we can do
we have a table here we have a total
field and what we can do is we can just
select one cell here now as we have
learned earlier we can do a alt and then
equals and that basically says what is
this doing so it says it is calculating
the sum of the last three months and if
that's what you would want to do just
hit on enter and it has automatically
calculated the totals for all your rows
for these three columns so the sum
formula is getting filled up now I can
select any particular cell and I can
look in my address bar so it has already
given me the formula where it has
started calculating the sum from the
October column till the December column
and has given me the calculated values
of the columns what we can also do is we
can get total rows in the table now
that's a simpler option so what we can
do is we can select any l in this
particular table
and then we see that there is a table
tools design option showing up here now
I can select this and then it says well
let's get a total row so let's select
this and it automatically populates the
total here and if you would want the
average then we could select this and
from the drop down I can select what I'm
interested in so for example I would
want the average values and not the
total I could just select this and that
gives me the average of these values so
we can always do simpler computations
here by converting our data into table
format let's learn about one more
efficient way
of working with the data and that's
using your drop downs so let's see how
drop downs work here now say for example
you have this data which has the values
in the food column and department is
empty and say for example you would want
to enter the values in Department
however you would want to select the
department should either have produce or
meet and bakery and these are the only
three options which should be available
for any user to fill in the values how
do we do that so we can basically create
a table by pressing Ctrl D so what I can
do is under my department
here I can select one of the cells and
then I can do a control T that basically
converts this into a table I can say
okay and my table is created now what I
can do is once this part is done
we can select all the blank Fields here
where we would want this drop down to be
applicable now under your data tab you
can go in and select data validation and
this has an option called Data
validation click on this which basically
says allow any value so here I will
select I would want to give a list of
values and then I can type in my values
here which I can say produce
say for example meet and then say Bakery
now these are the values so we can click
on okay
and once we have done that we basically
have a drop down here next to Apples
which will only show us the values which
we can feed in under the department
column so I can go into every cell and
then I can basically choose what is the
department which handles this and then
basically I can select one of these from
the drop down so this is an easier
option of creating your drop down and
then feeding in the values from the set
of values which you have defined here on
the right so this is a simple example of
using your drop downs working with your
tables working with your sort and filter
transpose split filling up your data
adding in some data here and similarly
you can use Excel for more than one use
case using its inbuilt features to
easily work with your data let's see how
we can import data or bring in data into
our Excel from your local machine or
from an external web source so what we
can do is we can open up a blank Excel
sheet and say for example you have been
provided a text file or a CSV file and
you would want to import that data into
your Excel sheet that can be easily done
so right now I've opened an Excel sheet
now I can click on data and here I have
an option which says existing
connections from other data sources so
or you can click on connections if you
have already created some so we can
click on from other sources so this is
one option where you can connect to your
different data sources and you can get
the data from one of these what we can
also do is I can click on connections
now it says there is none I can click on
ADD it says well show the connections
where connection files or Network
connection files on this computer so I
can say let's get some files from this
computer now if that does not show up
something so say browse for more and
that basically shows you different
options so let's basically select a
folder where I have some data sets I'll
click in here and this is basically a
folder where I have some data sets now
let me select this particular file and I
know it is a CSV file so let's click on
open now if you would want to verify
this you could have gone and looked into
the properties of the file and it says
it is a DOT CSV file which is what we
are interested in so I'll take this file
I'll say open now this basically shows
me the text import wizard option which
says is the file delimited I'll say yes
click on next so I will select comma as
my delimiter I can say text qualifier is
none now this is my data so my data
preview is already showing me the data
is what is the data in the CSV file you
can click on next and then you have an
option which says data format is General
you can go for date format you can go
for advanced options so I'll just say
finish
and basically now this has been created
here so we basically have this and now I
can click on
close now once you have done that you
can click on existing connections it
shows me the data which we have here the
connection which we have created say
open and then it says do you want to
import this data or bring this data into
existing worksheet
you can also say add this to a data
model if you are doing some data
modeling so click on OK and now this
data is automatically inserted in my
Excel file I can basically save it
into
this particular sheet now what we can
also do is we can also start a new sheet
and that does not have a data and we can
get some other data from web so what I
can do is I can go into my GitHub and
let's say I would be interested in this
CSV file so I can select this and this
is my GitHub path a path on web so I can
click on draw and that basically gives
me the raw path where this particular
file is now you can select this copy
this particular path and here you can
come back to your Excel sheet we would
be interested in getting the data from
web might be from a text file where we
will have to specify the delimiters or
let's go to web and here I can given the
web path from where I would be
interested in getting the file let's
give the GitHub path which is publicly
available and then click on import now
once you click on import it tells me
that two Fields data and value
these are
within double quotes separated by comma
so first let's click on import
now once we do this it will basically
get the data from web and put in here it
says existing worksheet so we had
already created a new worksheet so let's
click on OK and now you have the data
coming in but then this basically shows
me in one particular field so what we
can also do is we can just do a control
a that selects all my columns here and
that's my data so we can then basically
filter this out so we can say text to
columns it's a delimited file click on
next we can select comma and let the
text qualifier be codes
it shows me data preview click on next
so you have the general format it shows
the destination that's the column click
on finish and now your data has been
split and you have the data which you
have imported from web so this is the
data which is coming in from web this is
the data which came in from my local
machine and similarly we can even create
connection with an existing database so
I can basically click on connections if
I would want to do that so I have an
option called connection here and it
says where the selected connections are
used I can basically click on ADD I can
basically choose if I would want to get
the files from Network or from computer
like we did earlier I can click on
browse for more which should show me
different other options to create
connections say you would want to create
a new SQL Server Connection you can
connect to a new data source coming in
from different place you could basically
choose what kind of connection would you
want so these are all the different
options which we can go for and we can
basically connect to a database for
example if I have some database and say
for example access database
I can see if there are some files with
that particular database and I can
import it so similarly we can also uh
click in here which says new query and
that also gives you an option of getting
the data from your files from all these
folders from databases so you can
basically click here and then you can
import data from a mySQL database
provided that is set up on your local
machine or on a particular server you
can go from cloud you can get it from
online services you can get it from
other sources which says from web from
your Hadoop file system from active
directory from a blank query and you can
even combine queries wherein you can run
a power query editor you can get the
data from different sources and then you
can bring it into your Excel so in this
way you can get your data from different
sources into your Excel into your
spreadsheet and then you can continue
working on those data sets
we have uh already learned some basic
operations which you can do in Excel and
let's Implement our knowledge by working
on this particular data which is coming
in from housing data set now here if we
see some fields we have agent date
listed area list price and this is
basically the data which has been sorted
in newest to oldest order of date listed
so how do we
arrive at this so what I can do is I can
just click on data listed and then I can
either go in here I can select sort I
can get into custom sort and then I can
choose the column based on which I would
want to sort the data so I would look
for the newest data to the oldest data
that means that would mean a descending
order of dates or you could say the
oldest state or the earliest month will
be towards the lower side of your sheet
so here we can select date listed now I
can say let it sort based on Cell values
and the order what we have here so we
have newest to oldest so let's select
this I can say OK and now if you see the
date has been sorted so we have your
10 18 2007 on the top so that seems to
be the latest date and as we go down we
will see an earlier month hour and
earlier month than that in the state
listed so we have sorted our data into
newest to oldest order and that's based
on your date column so the result shows
up here now what we can also do is we
can have different questions which we
would want to answer so for example I
would want to sort the data in ascending
order of area and descending order of
agent name how do we do that so let's
look into this so this is here I already
have the result here and how did I get
this so I'm looking for ascending order
of area and descending order of agent
name so we can start with any particular
column that does not matter so for
example if I look into this Excel sheet
I have my agent name select this which
we want in a descending order so we
could either do a sort and then go for
descending sort Z to A we could also use
the filter option here on the top right
and we could do it or I could just say
sort Z to A and then it has arranged the
data based on the agent column being in
descending order now I can go into area
and then I can again do a sort and I
wanted my area column to be used for
sorting the data and ascending to
descending and that basically not only
changes the order of this particular
column but for my complete data so let's
do that and now if you see we have the
data which has been sorted so we can see
how many values we have here and the
area values which we see and this is how
you get your result so I'll just do a
control Z and again and I'm back to my
original data and this is the sorted
result which we are seeing at similarly
we can answer other questions for
example sort the data according to the
following order of area that is we are
saying counties
Central and then your county so we can
basically choose in which particular way
we would want to do it so it is County
Central and then again County so if I
look into my sheet 3 so here I basically
have my data which is having some South
County then you have your Central and
then you have your North County so we
would want to sort the data to solar
problem which is according to the
following order so first we go for area
then we go for South County Central and
North County so what we can do is we can
basically have area field selected and I
would want to sort this particular data
so I have South County Central and North
County so I can basically go for custom
sort
and then I can choose which is the value
or column which I'm interested in let's
go for area
we will go for something like cell
values well you can also try to explore
conditional formatting icon if this is
what you would want to use or we can
basically go for just cell values and
here we can say if I would be interested
in first getting my values for South
County so for example I can say custom
list and then I can basically given the
new list here so I can say s
Dot County
and then I would want Central and then I
would want North County
so let's select this
as it exists we can basically say add
and that's basically the order which we
want to say okay and then say okay here
and now what we would want is we would
want our data so we can compare that
with the values which we see here it
starts with Kelly you have in the 12th
row something like Lang and that's what
we are doing so we can basically arrange
the data in a particular order by
choosing a custom list and then sorting
your data so that's one more simpler
task what we have done where we have
sorted the data in the order where
under our area column we first wanted
South County then we wanted Central data
and then we wanted North County so this
is how you can do it now let's look into
one more problem so it says find all the
houses in the central area and we would
want to basically apply a regular filter
let's let's see how do we do that so we
can click on this and here we have the
data so the problem statement is we
would want to find all the houses in
Central Area now how do we do that we
can do a sorting but we would want to
use the filter which you see here is
implemented so how do you do it so you
can select this area and say for example
I would want to apply filter I can just
go in here and I can say let's get a
filter on my first row and now I have
filters applied so we are interested in
looking into the Central Area houses
let's go in here it says all these
fields are selected that means it shows
everything wherein your area has all
these values let's unselect this and
then I will only be interested in the
central so let's say okay and then say
okay so now you see that the area filter
has been applied and we are looking at
the central column so what we have done
is we have applied a simple filter and
we are looking at our data at any point
of time if you do not want the filter
then what I can do is I can select this
and I can say well I'm interested in all
the data so I could do this or you can
clear the filters from area and you get
your data back so that's in one way you
can filter out your data so let's look
at an example of sort and filter where
we might have to filter the data based
on two columns or multiple columns with
different kind of values where it could
be and and or condition now say for
example this is the data I have and this
is the question which we need to answer
such as find the list of all houses in
the central region with pool and South
County without pool now if it was a
simple filter based on one cell I could
have just selected my header row I could
have then applied the filter
and once I have the filter I can look in
area where I have three regions so I'm
only interested in Central and South
County so I can get rid of North County
and that's fine but then we have two
different conditions here so we need the
data in central region to have the value
for pool being true
and for South County the value has to be
without pool now how do we do that so
what we can do is we can first create a
copy of these headers here so let me do
that now the area has to be South County
so basically I can either just select
this and I can choose this value
and then I can select this and then I
can choose Central so that's the
criteria which I have and the pool value
has to be so central region should be
with pool so then this one
basically has to be true
and this one has to be basically false
so south county is without pool so let's
select one of the values here and this
is my criteria now to get my result uh
we can always place your cursor here and
you can check this is M column and
eighth row okay so we would want the
result here so let's go ahead and now
click on data and then in filter you
have an option called Advanced and here
what I can do is I can say I would want
to filter the list in the place but
that's not what I would want to do so
I'll say copy to another location and
here if you see the list range will tell
me that this is the data so A1 to J 126
so a to J column selected and all the
rows
criteria range is basically based on
what I have given here so that is m
from 1
to V which is 3
so I am selecting these columns and then
I am saying all the way whatever
criteria I have given and copy 2 I am
saying M8
to V8 so that basically will give me my
filtered result so you could basically
just say okay and now I have my data
which has been filtered and I have my
area which is South County that is
without Pool Central with pool again
South County without pool and then if
you look at your central value that's
pool so this is an advanced filter which
we have applied where simply we have
filtered the data based on two columns
and then we have our result so in this
way you can have your customized filter
applied on two different columns and get
your data which can be either replacing
the existing content or in the same
sheet in different set of columns and
rows you can have your result let's look
at one more example of filtering where
you are trying to filter the data based
on a and condition
condition being met in two different
columns and then you would want to
filter out the data for only specific
columns so the situation is the agents
with a house in North County that should
be County area having two and a single
type family so we are talking about two
bedrooms and we would basically have a
single type family and here the criteria
is that we would want to only populate
these columns which is Agent area
bedroom and type now what you can do is
as I explained earlier that you can get
your result in the same sheet in a
different location so here I have
created these headers which says agent
area bedrooms and type now this is
basically a copy of all the columns what
we have here agent data listed area list
price bedrooms bath square feet feet and
so on so you can basically create a copy
of the headers here and this is where we
will give our Advanced criteria to
filter the data so the conditions which
need to be met is we need to look at
North County so for example here in area
I can basically go ahead and select one
of the values North County now the
criteria is having two bedrooms only so
let's say bedrooms and let's say the
value should be 2 and then basically I
am saying a single type family so when
you would want the single type family so
here under type I can give the criteria
single type so this is my and condition
so we are saying North County area
having two bedrooms and the type is
single family now this is the criteria
which basically means if I select this
this one tells me that this is m one row
onwards till V
2 so this is what we have and we would
want to filter based on this so let's go
ahead and then go for data filter
advanced
and here in advanced it says filter the
list in place now that's not what we
would want to do so I'll say copy to
another location this basically selects
the list range so which is telling me A1
to J 126 so that's the
columns and rows selected criteria range
is based on M1 V2 which we have given
here and copy to I would say for example
from M7
2
P 7 now this is the area where I would
this is the place where I want the
result let's say okay and now I get my
data which is based on the question
which has been asked that you would want
the agents with a house in North County
area having two bedrooms and single type
family so in this way you can basically
do Advanced filtering get your data and
get it stored in the sheet anywhere at a
different location well I could have
also done filtering in place and that
would have replaced the data
which we have but that's not what we
want we would want the filtered result
in a different place so this is how you
can do some Advanced filtering we can
also use Excel to filter out the data in
one particular column which might be
conditional or say using some numerical
filters now here say for example the
problem statement is that you would want
to display all the houses whose list
price is between
45
000 to 600 000 or say for example we
would want to filter out the data to
something else say for example let's say
I have I would want to filter out the
data between three hundred thousand and
four hundred thousand so we can
basically
update this
say for example I am saying I'm
interested in 300
thousand
two
to four hundred thousand and that's the
criteria to filter out the data now
there are two different ways or there
are two easier ways to do it one is I
would want to
look at the list price so I can select
this I can go ahead and do a filter here
and in the list price now this is where
we would want to do the filtering so
it's pretty easy you can click on this
one and then you can go into number
filters and you can choose between now
that's one easier way of doing it so I
could basically select this I could say
I'm looking for Value which is greater
than or equal to three hundred thousand
and then is less than or equal to 400
000 so if I just do this I have applied
my filter and I have my data which is
filtered based on my criteria right so
that's one easier way of doing it or let
me do a Ctrl Z
now let the filter be there which you
can anyways use but what we can also do
is as we have seen earlier methods so
get a
set or get your column headers here
and then you are giving two columns here
now the only difference between
my
this set of columns which I have one two
three four and then you have seven and
ten columns and if you see here we have
4 5 6 7 8 9 10 11 right so whenever you
want a and condition you will basically
add the columns where I can give add
condition if it is the same column if it
was a different column then it would be
same number of columns but and
conditions will lie in the same line and
or condition would lie in a different
line now here I can give this value so I
am looking for listed price being
between three hundred thousand so I am
saying it should be greater than or
equal to three hundred thousand and then
I can say less than or equal to 400
000 now that's my criteria and then I
need my result here which is in M 7 so
what I can do is once I have given my
criteria which I will be using to filter
I can get into Data I can get into
advanced and then I can say copy to
another location so it is selecting my
A1 to J 126 criteria range is based on
M1 to
W2 and then I would want my result from
this particular column so let's say okay
and now you have your data filtered out
in a different location in the sheet
which has been filtered based on your
and condition so you can filter out the
data in this way or you could just apply
a filter on a column and give the
conditions
now let's solve one more interesting
problem and here we would want to use
Excel where we would have an and and an
or condition so say for example this is
the data given to you and the question
is that you would want to find all the
houses in North County again that's a
spelling mistake but then they are North
County area with a list price greater
than 300 000 and having three or four
bedrooms so the bedroom has conditional
so it has R 3 and 4 and then basically
you have list price which is greater
than three hundred thousand now I could
have obviously selected The Columns and
then basically gone for a filter so I
can just do a filtering here and then
I'm looking for list price being greater
than 300 000 so which we can always give
a number filter and I can say great
greater than and then I can say greater
than or equal so I can say greater than
and then I can give 3030 300 000 and
that's basically the filter which we
would want and here I would want to
select the bedrooms which should be just
either three or four so if for example
here I go in here and I unselect this
and then I say 3 and 4 right so I am
getting my data which is greater than
300 000 and it should have the bettering
values which will be either 3 or 4
selected now that's one way of doing it
let's do a control Z and get it back
2 as we were or you can even just say
clear filter so you get your data back
as it was so what we can do is we can
hear
give the criteria
so for example I have my list price now
this is what I would want as a condition
so let's say greater than
three hundred thousand
300 000 and bedrooms should be three
and then I can say greater than three
hundred thousand
and then I can say four so this one
basically gives me a situation where
your list price has to be greater than
three hundred thousand and bedroom
should be either three or four so we
have given our filtering criteria now to
get the result what we can do is we can
go into Data we can go into advanced and
we can say copy to another location so
our list range is selected which is
columns a to J row number 1 to 126 your
criteria range is given in M1 to V3
where we have specified and we are
saying the result would be in M 7 to V7
so if I do this now I have got the same
data which we were seeing earlier and
here the bedroom values are three or
four and basically the list price is
greater than three hundred thousand so
this is a simpler way in which you can
create your filters and all this
Advanced filtering what we are seeing it
will be saved with your sheet you can
always go back and change this value or
you could do filtering where one person
has to look into the filter to see what
values have been selected
now that we have looked into some
operations which we can perform in Excel
using filter or sorting the data
creating your tables let's also quickly
look at functions and formulas which can
be used for doing some easy calculations
or computations now Excel can be used in
different kind of data analysis so for
example you have different inbuilt
functions which can be used and we can
always check for a particular function
so for example if I had if I wanted to
look at a particular function I could
just type in here something for example
is and then it shows me all the possible
functions and you can always have a look
at the detail of the function for
example you have is even which will
return true if the number is even if we
would say is logical so I could search
for is logical and that tells me whether
a value is logical value true or false
and returns your value true and false
now we can obviously
say subtotal so you can search for any
of these useful functions and that tells
me what this function can be used for so
returns a subtotal in a list or a
database you have many other such
functions such as integer sum average
you may be interested in working on
truncating
some data getting the absolute value
getting the square root basically
getting a count or getting a max value
you can look for any particular
functions within your Excel sheet now
you also have other functions such as
now or time for example let's look at
Now function so I can search for
now
and here it is so this is Returns the
current date and time formatted as a
date and time so this is the function
which we would want to use and if I just
give the function it tells me what is
the current time let's first look at the
description of time here so say for
example I would want time it says
converts hours minutes and seconds given
as numbers to an Excel serial number
formatted with a time format so for
example if I would say two hours and
then 30 minutes and 30 seconds and if I
do this it has basically converted this
into your time format so you can always
use different inbuilt functions for your
work now we will also look at some
Advanced functions like sumf or some ifs
you have countif and countifs and you
can be working on various
functionalities of excel to easily help
you in doing some calculations
computations working with your data
working with your different cell values
so let's look at some example of using
functions like sum or sum if so for that
let's go to this sheet and here we have
some data now I have already applied a
filter which can allow me to filter out
the data so it says find the total units
that were sold in the east region now we
know that in region we have east
and
I have multiple regions I could
basically be saying unselect all and
select only East
and say OK and that basically gives me
the units which were sold and if I place
my cursor here and then if I did a auto
sum so it would basically give me
the function which is being used so
something like subtotal and it is
basically working on your rows which is
E2 to e44
and here we can just do this so that
gives me the total but this is this is
fine you could do that but
it would be good if we know how do we
use a function like sumif to do that so
here I am seeing this is the subtotal
where I am looking at the values and
basically what I have done is I have
filtered out the region and then
basically I am getting a count but this
does not give me clearly how a sum was
calculated from all the values which
were listed what we can also do is let's
do a control Z and let's get it back so
now we have our data and we would want
to get the total units that were sold in
the east region so what I can do is I
can start typing in my formula and for
that I'll use an inbuilt function so for
example I would be interested in going
for sumif now it says sum if adds the
cells specified by a given condition or
criteria when you talk about some ifs
this is when you could give set of
conditions or multiple criterias so
let's look at some if let's do this now
obviously this gives me an error because
the formula
is not right so we have to basically
come in here and let's start with sum if
now when I say sum if it shows me there
is a function with sum if which we would
want to use and here once I open up the
bracket it tells me okay what is the
range of data which you are interested
in so I am interested in all the units
that were sold in the east region so we
are interested in the region
which is here so I can basically be
selecting this and this tells me
you are interested in the data here so
let's not take the header value so let's
say B2 and then we can go all the way to
the end so we can basically select this
way
that's the data we have select this and
hit enter so now here it has selected B2
to B4 but we need to basically now give
the criteria so the criteria is either a
value or you can point it to a cell
which has that particular value so as
per our problem statement we are looking
for the units which are sold in east
region so I can select the value East
here and then my sumif needs the range
on which you want to
calculate a sum so
let's select this and now we are
interested in finding out the sum of
units so that's basically this column e
column so I can basically type in
instead of selecting so I can say e and
I'm interested in e 2 2 e 44 so that
basically selects the area or all the
values and now let's do this so that
basically gives me the sum is 691 right
now this is the criteria where I have
pointed it to a cell and whatever value
that cell has well I could have done
something like this so I could have
selected East giving the value and then
doing it it still does the same thing
and in this way you have more clarity
that you are using some if you are
filtering the data so you have given the
rows
you are given the criteria and then you
have given the range on which you would
want to sum up the values now similarly
if the question was what was the total
revenue generated from binder
now we would want to find out what is
the total revenue generated from binders
that means my filtering criteria will be
binder
and then I want to find out the total
revenue generated so we have the revenue
generated field also here and we have we
don't have any region to be filtered we
are just looking for binder so let's
again start doing the same thing so we
can go for some if we can open the
bracket now it needs the range so we are
interested in revenue generated now
that's the summation we want and we
would want to
get the range of data so here we can
basically select
uh d
2
D 44
so that's the data selected now I would
want to give the filtering criteria so
let's say binder and then we need to
give the range on which the sum needs to
be calculated so that's my Revenue
column so that's G so I can say G to to
G 44 and that basically selects the
column and then you get your sum so it
tells me what is the total revenue
generated from binder now I could be
doing this for other things also so say
for example if you would want to filter
out something else you could basically
just drag and drop here and then I could
come here and change this to say instead
of binder I would be interested in say
pencil
if that's the criteria you are
interested in remember to change this so
that you take all the values
and here we will change
it to select the relevant rows and then
this is the date I am getting so I know
that this is the revenue generated
from pencil this is the revenue
generated from binder now this is a
simple use case where we are using some
if what if we would want to use some IFS
so sum ifs let's have a look at how we
get to some ifs so sumif says where you
would want to work on
doing some calculation but then you
would want
multiple criterias to be met so let's
see how we get this so here what I can
do is let's work on this problem
statement which says what is the total
revenue generated from central region
where the item is a pencil so that's
something which I would want to check
now when we are answering this question
we can also look at the order in which
things have been asked in the question
so it says what is the total revenue
generated from central region so we need
the total revenue generated we know
there is a revenue column we are
interested in getting the total revenue
generated
we are saying the filtering criteria is
central region and we say in that we
would be interested only in the item if
it is pencil how do I do it so I can use
sumifs where you can pass in multiple
criteria so let's start with some IFS
and when I start with some ifs let's
open up the bracket so it says sum range
it says criteria range then it gives one
criteria and you can given any number of
criterias so for example we are
interested in total revenue generated
now that's my G column so let's follow
in the same order so let's say G2
so that's my first value and then I know
there are 44 rows here so I can say G 44
and you can obviously check if that has
selected all the rows now that's my
total revenue generated so I would want
the total revenue generated so I'm
saying setting this sum range
then I need to give the criteria range
so it says from central region so
central region comes into column two
that's B so let's say b 2 to B 44 so
that's my criteria range then you have
to give your criteria but we need to
filter out the region being Central so
let's select this now either I could
point to a value in the cell or I can
just give the exact value here
we can also give a
wild card or matching pattern so that
also works now this one is fine we are
now also interested in finding the total
revenue generated when the region is
Central and the item is pencil how do we
do that so for
item we know the columns the column is
D2 so let's select D two to D 44 so that
basically selects all the rows in the D
column and we need to give the
filtering criteria so let's do a comma
and then just given our value so let's
say pencil and then let's close your
bracket and that basically gives you the
result
so we need to just follow the order of
our question which says
what is the total revenue generated so
we are looking at the revenue column we
are selecting all the rows
then it says from the central region so
we need to select the region column and
give the filtering criteria Central or
point to a cell which contains that
value and then it says we would be
interested only in item being pencil so
then you select the column which has all
the items and provide your filtering
criteria that is pencil so that's your
easy calculation of using some if which
we compare with sum if here so sum if
here was just having
your criteria
so basically you are selecting your rows
giving your filtering criteria and then
your sum range in some ifs we are giving
multiple condition now same thing can be
done here it says how many units were
sold by sales representative Johns or
Jones where the cost of each item was
greater than 4. so how many units were
sold by sales representative
so when we talk about how many units
that's your e column
so let's start with that so let's say
sumifs I would be interested in E column
and let's give the range so it says sum
range so those are the number of units
on which we would want to find
the sum then it says you need to give
the criteria range so we say sales
representative where the name is
Jones so sales representative is in
sales rep column C so let's say C2 to
c44
now
then we need to give our filtering
criteria so let's say Jones
is the sales representative where we are
interested about whom we are interested
in and then we the question says where
the cost of each item
so cost of each item is what we are
interested in you have unit cost
so that's what we are interested in so
that would be f and then say F2 to f44
and then you need to give your cost so
it says where each item is greater than
4 so let's select this
and let's
do this so this tells me 3 0 1. now
similarly let's answer our third
question which says how many units did
Jones sell excluding
pencil item so we would want to find out
what was the total number of units units
that were sold
and that units or that should not
include the pencil item how do we start
doing this so let's start with some IFS
now we know that you start with some ifs
you need to give the sum range so we are
interested in the number of units so
let's basically go in and select our
number of units which were sold so
that's your column e so I can say E2 to
e44 that's where I would want to perform
the sum now I'm saying
how many units that were sold where
we are talking about sales rep being
Jones so let's see let's select the
columns
C and then give
the range
after that we need to give our giving
filtering criteria which is zones and
then we are interested in the items but
excluding pencil so items is in column D
so let's say d to d44 and then we have
to give
our criteria so we can say well that
should exclude pencil
so I can basically say
pencil
and let's close this
and let's check so that's my formula
which says that these are the number of
units which the sales representative
whose name is Jones had sold and that
does not include pencil as an item let's
also look at an example of using countif
or countifs now both of these can be
very useful when you would want to
calculate certain values so for example
if I would want to work on count if
let's try solving this problem now
remember you can answer these questions
using filters and that can be an easier
way but then sometimes you may want to
get the formulas so that you can make
your spreadsheet and your calculation
more
dynamic in nature and that will
basically depend on the values in the
columns or rows so for example if I have
find the total number of times Gill has
made a sale now if I look at my data
here it tells me that for every sales
representative there is some value in
the sale and it says sales has greater
than 3 so for example Jones you have
sales greater than three or you have
Jardine which is sales greater than 3
and so on so what we are interested in
is doing a quick count and finding out
the total number of times Gill has made
a sale how do we do that so we can use
this count if function and if I go into
countif it says counts the number of
cells within a range that meet the given
condition now what's our condition our
condition is Gill and we would want to
find out how many times the name say
Gill appears or kill has made a sale now
I could just say count if and then open
up a bracket I need to give a range so
let's say we are interested in looking
at the range so let's say we will choose
sales rep so I can say C and then I can
say C2 to c44 now that's my range
let's not give that in quotes
so you have to give a range so let's do
a count if that selects the data and
then we need to give the condition so
for example let's here give the name
which is Gill and then close this so
that basically tells me it is five times
kill appears here we can check this so I
can go in here I can add a filter and
then might be I would be interested in
looking at only Gill
and that basically gives me 5 right so
we can always do that and we can be
using formulas like this now what about
this question so which basically says
which sales representative made a sale
more than three times now we it might be
looking little confusing when I say for
example let me clear out this filter
now we have sales greater than 3 and we
would want to find out which sales
representative made a sale more than
three times now I could basically check
for every sales representative here if
they have made a sale more than three
times and what I can do is I can just
say equals I can start with count if
then I need my filtering criteria so
that's your range so first thing is we
will choose for example is choose C2 to
C 44 and then we need to
give and criteria what is the criteria
we need basically a sales representative
so I can choose the value here in C2
and then I can close this and then I can
say the sale has to be greater than 3
and let's check so it tells me the
Boolean value that yes this guy has made
sales more than three times and what we
can do is we can just drag and drop
which basically gives me the value for
other sales rep you can always check the
value is automatically changing to this
value in cell and for example let's go
in here so this is obviously 2 so it
saves me false right and you can
basically get the values for all your
values so that basically tells me which
sales representative has made a sale
more than three times
now like sumifs we also have countifs
where you can give multiple criterias so
for example the question is how many
orders were placed from the east region
after this particular date so we have
a date criteria we also have the region
criteria and we need to basically get
the count of number of orders which were
placed now I can in this case use count
ifs
and this basically says that you can
start within criteria so it says how
many orders were placed from east region
after particular date so date is in my
First Column so for example I could say
a
start with 2 and say for example let's
go a44 that should have selected all the
rows
and then I need to
once I have given the criteria range I
need to give the criteria so we are
saying the date has to be greater than
10th Feb so let's give it
2
10
2019
and then you need to say
how many orders were placed so you need
to give the criteria second criteria
range
so we are looking at the number of
orders which were placed
from the east region
so when I would want to look at the
region that's your column B so I can
basically say B2 to B 44
and here
I would basically give my criteria so
the criteria is East
let's give that
and once you have done this you would
want to find out the total number of
orders so let's select this and if I do
this it tells me 13 now is that right so
we are looking for your date
your region being East and then getting
the total number of orders so here I can
just do account if I'm saying A2
to a44
wherein I have given the date criteria
that it should be greater than
10th Feb because I do not want to count
10 Feb it says after 10 February and
then you are saying the region has to be
East so we would want to find out the
total number of orders so my region is
east and that gives me the result
now similarly you can also find out how
many times Gill
sold pencils so here we will have to
give the range so
let's start with count
ifs
now here I would want that item is
pencils so we can as well select column
D2
d44
then you have to give your criteria so
that's your
pencil
and then we are looking for sales rep
which is Gill so
that is my column C so let's say c 2 2 C
44
and the value should be
just kill
so it tells me
it's twice where Gill has sold pencils
so we can obviously check this by going
in here
choosing my filter and then let's search
for rep being just Gill
okay and now we are interested only in
the item being pencil so I can say well
let's get to pencil only and that tells
me twice so you can obviously re-verify
using filters but using functions or
using formulas it is always good to
calculate and that can be making your
computation and calculation more dynamic
let's look at one more interesting
feature of Excel and that's your
conditional formatting now as you see on
the screen conditional formatting has
different rules which can be applied on
your data and that allows you to
basically differentiate or easily
identify data values which are
based on certain criterias or rules so
when you talk about conditional
formatting you have different options
such as you can highlight cell rules you
can get top and bottom values you can
apply different rules apply different
color scales and you you can easily
manage these rules so conditional
formatting is very useful for people who
would want to work on huge amount of
data and easily perform some data
analysis
it's easy to use as it is shown here and
with your conditional formatting
you can format cells based on a preset
condition you can perform conditional
formatting to identify cells you can
highlight a few significant cells and
you can easily perform conditional
formatting as shown on the left side
now how do we work with conditional
formatting let's have a quick look so
say for example we have our Excel sheet
and if you see here I am highlighting
the sales person who have generated
Revenue greater than 10 000. so we can
be looking at the values where the
revenue generated by a particular sales
person is greater than 10 000 it has a
particular
color and how do we get here so for
example let's select this data
and what I could do is
I could go into conditional formatting
now I could basically highlight cell
rules and we could just say greater than
that's an easier way I could also go
ahead and create a new rule but then I
can use one of this option I can say
greater than and let's give some value
might be we would be interested in
looking at any value greater than 12 000
so let's choose 12.
1000
and here it says what color would you
want to select so for example I would
say something like
yellow filled with dark yellow text
and let's say okay so right now what I'm
doing is I have all the values where the
revenue generated was greater than 10
000 but then I have also selected all
the sales people who have made or who
have generated Revenue greater than 12
000. so I can just wait Ctrl Z to see
the previous result now here I had the
values which were greater than 10 000
and the one which we did just now
basically
highlighted the values which are greater
than twelve thousand so this is one
simple example now we can look at some
other examples say for example you want
to format cells using three color scale
so if you look at the values here I have
a three color scale mainly in green
yellow and red and how do you do this so
for example I can go in here and I can
go to conditional formatting so I would
want to go for color scales and here you
can create different rules so we can set
up a two color scale so we can say
format only values that are above and
below average I can format only cells
that contain something I can get the top
and bottom value so these are different
ways in which I can have a three color
based scale now what I will do is I will
select this and let me show you the rule
which which I have so for example I can
go into manage rules and if you see here
there are certain rules which have been
specified now what does that mean so you
would want to specify a three grade
scale so for example if I would want to
look at my first rule it tells me that I
am choosing three color scale I can
choose lowest value percentile and
highest value and that basically will
select the cells based on their values
so what we could have done is I can
basically
use one of these values I can delete
these rules which I have created so for
example I have all these rules but you
should always carefully remember that
the rules will be applied in the order
shown so for example if I just delete
these rules
and then say apply and say okay my data
is back now it does not have any
highlighting now I can go in here I can
say condition sorry conditional
formatting I could go for color scale so
I could basically go into
new rule so I would want
the
cells to be using three color scale so
let's
choose three color scale now when you
say three color scale it says what will
be the color of lowest value
and we could choose might be any one of
this let's choose red I can say midpoint
is percentile 50 and then the highest
value is green and if that looks good
let's say okay and now if you see the
lowest values have been highlighted as
read you have mid values and then you
have the positive value so this is the
three color scale and that easily helps
me in identifying the data based on the
cell values
now in conditional formatting what you
can also do is you can basically color
the cells based on their value so what
we are seeing here is if the revenue
generated is greater than average then
that shows in green and if the revenue
generated is lesser than average that's
shows in Orange now how do we do that so
we can basically again manage some rules
so I can basically create a new rule now
here I can select one of the options
which says format only values that are
above or below average and that's the
option I would want to select now I can
select this and it says format values
that are above average so in our case we
had it in green so for example I'll say
above average and then here I can go for
a particular color
so you can go for a particular size so
let's go and look into
the formatting so for example let's
choose yellow
say okay now I am saying wherever the
cell values are above average it would
be yellow instead of green
and let's go in here let's go and look
into manage rules so this is basically
the rule which we are applying now we
can also add a new rule
and I need to select the values so for
example I will say
here so we had gone for above now we'll
go for below we'll go for format we will
choose red we'll say okay we'll say now
these are basically
the rules which we have created and here
it says applies to your data so right
now it has not been applied so for
example if I select this and then I
could basically choose my area just hit
on enter and similarly you can go in
here and then select your area hit on
enter and say apply say okay and now if
you see I have really chosen bright
colors but then I have said wherever my
revenue generated is above average it
should be in yellow and
below average should be in Red so we
wanted above
average to be in green and below average
to be in Orange so
that's what we have here right so you
can always
color code your cell values based on
some rules which you are setting up now
similarly you can also find the top 10
and bottom 10 values and that's pretty
easy so you can just select this and
then you can go into conditional
formatting you can go for top and bottom
values top 10 items bottom 10 items or
you can go in for more rules so you can
say format only top or bottom ranked
values so you have top 10 now you can
choose the color and for example I'll go
for blue
and
I'll say okay so now if you see my top
10 values are blue now similarly I can
add one more rule so I can say new rule
and I can say let's go for top or bottom
let's go for bottom let's go for format
let's say orange
say okay say okay and that's it so now
you have your values which are top or
bottom 10 values so you are using
conditional formatting where you are
basically highlighting your cell values
based on different colors and here
easy conditional formatting based on
different rules helps us to do that now
similarly you can also have the values
which is basically
showing you how the values are
increasing so what we can do is we can
select our columns either you could
apply this to all the columns now here I
have applied this only to Jan and April
now I could apply this for June so let's
say June so you can go for gradient fill
you can go for solid fill you can
obviously just select the color and that
takes care of the things you can say for
example select this and now this is
selected what I would want to might be
format this so I can go in here I can go
into manage rules now that will tell me
what rule has been applied
in the order so I can just do a edit
Rule and that basically says this is a
solid fill which is color you have no
border this is basically color as black
now I can go for something like gradient
fill
and I can say okay and now if I say
apply and okay so this basically is Like
Your First Column so you can use
conditional formatting for various use
cases and you can highlight the values
so anyone who would look at the value
would automatically notice which are the
higher values which are lower values
might be here the revenue is getting
generated or was getting generated but
did not grow Beyond a particular value
and so on now similarly you can also go
in for different options say for example
here we would want to
see if the revenue was dropping
or if the revenue was
if the revenue decreased or say for
example if the revenue was going up for
this particular sales person so here we
are looking at Carol so in Jan the
revenue Generation by sales was very
high then in Feb it was falling down
in March it was kind of stable then in
April it went way below so we can
obviously work on this wherein we can
grade our cell values so what we can do
is we can go in for highlighting the
cell values now you can go for color
scales you can go for Icon sets and this
is where you can choose your different
shapes
so you could choose one of these shapes
so for example I would be interested in
looking at the indicators like
directional I could go
using this three arrows I can go in for
this color I can choose directional and
then my values are
automatically using directional now what
we can also do is we can then go into
manage rules and that basically tells me
what rules have been applied so for
example the latest one is the icon set
which I have chosen it shows the
selected columns I can obviously do a
edit Rule and then I can choose so I'm
saying the format style is icon sets I'm
not using a data bar I'm not using color
scale now here I have chosen the style
of icons
and then here you can basically give
some values so you have icon which is
green when the value is greater than or
equal to 67 percentage
when I say hyphen or minus it is less
than 67. it's way below 33 percentage
then you give this value so you can
obviously edit and easily highlight your
cell values based on this icon set so I
can apply this and that's how I use
conditional formatting so conditional
formatting can be very useful if you
would want to use icon set if you want
to use your data bars if you would want
to highlight particular values if you
would want to color code based on some
calculation if you would want to
use a three color or a two color scale
or if you would want to just find out
values based on some simple calculation
so conditional formatting is used
extensively by data analysts or people
who are working in business intelligence
teams or people who would want to use
Excel to easily identify the data
easily identify the cells which contain
particular value or finding out less
significant or more significant cells to
then pull out values and carry out your
computations calculations or analysis
try giving a shot to Simply transport
strategy program in data analytics this
is from her University in collaboration
with IBM and this should be the right
choice the link in the description box
below should navigate you to the home
page where you can find a complete
overview of the program being offered
and so why exactly do we need to do time
series analysis typically we would like
to predict something in the future and
it could be stock prices it could be the
sales or anything that needs to be
predicted into the future that is when
we use time series analysis so it is as
the name suggests it is forecasting and
typically when we say predict it need
not be into the future in machine
learning and data analysis when we'll
talk about predicting we are not
necessarily talking about the future but
in Time series analysis we typically
predict the future so we have some past
data and we want to predict the future
that is when we perform time series
analysis so what are some of the
examples uh it could be daily stock
price the shares as we talk about or it
could be the interest rates weekly
interest rates or sales figures of a
company so these are some of the
examples where we use time series data
we have historical data which is
dependent on time and then based on that
we create a model to predict the future
so what exactly is time series so time
series data has time as one of the
components as the name suggests so in
this example let's say this is the stock
price data and one of the companies so
there are two columns here column B is
uh price and column A is basically the
time information in this case the time
is a day so that primarily the closing
price of a particular stock has been
recorded on a daily basis so this is the
time series data and the time interval
is obviously a date time series or time
intervals can be daily weekly hourly or
even sometimes there is something like a
sensor data it could be every few
milliseconds or microseconds as well so
the size of the time intervals can vary
but they are fixed so if I am saying
that it is daily data then the interval
is fixed as daily if I'm saying this
data is an hourly data then it is the
data is captured every year and so on so
the time intervals are fixed the
interval itself you can decide based on
what kind of data we are capturing so
this is a graphical representation the
previous one here we saw the table
representation and this is how to plot
the data so on the y-axis is let's say
the price or the stock price and x axis
is the time so against time if you plot
it this is a Time series graph would
look so as the name suggests what is
time series data time series data is
basically a sequence of data that is
recorded over a specific intervals of
time and based on the past values so if
we want to do an analysis of Time series
as data we try to forecast a future and
again as the name suggests it is time
series data which means that it is time
dependent so time is one of the
components of this data time series data
consists of primarily four components
one is the trend then we have this
seasonality then cyclicity and then last
but not least irregularity or the random
component sometimes is also referred to
as a random component so let's see what
each of these components are so what is
strength trend is overall change or the
pattern of the data which means that the
data may be let me just uh pull up the
pen and show you so let's say you have a
data set somewhat like this a Time
series data set somewhat like this all
right
so what is the overall trend there is an
overall trend which is upward Trend as
we call it here right so it is not like
it is continuously increasing there are
times when it is dipping then there are
times when it is increasing then it is
decreasing and so on but overall over a
period of time from the time we start
recording to the time we end there is a
trend right there is an upward Trend in
this case so the trend need not always
be upwards there could be a downward
Trend as well so for example here there
is a downward Trend right so this is
basically what is a trend overall
whether the data is increasing or
decreasing all right then we have the
next component which is seasonality what
is seasonality seasonality as the name
suggests once again changes over a
period of time and periodic changes
right so there is a certain pattern
um let's take the sales of programmed
clothes for example so if we plot it
along the months so let's say January
February March April May June July and
then let's say it goes up to December
okay so this is our December a deep I
will just mark it as D and then you
again have Jan Feb March and then you
get another December okay and just for
Simplicity let's mark this as December's
as the end of the year and then one more
December okay so what will happen when
if you're talking about warm clothes
what happens the sales of warm clothes
will increase probably around December
when it is cold and then they will come
down and then again around December
again they will increase and then the
sales will come down and then there will
be again an increase and then they will
come down and then again an increase and
then they will come now let's say this
is the sales pattern so you see here
there is a trend as well there is an
upward Trend right the sales are
increasing over let's say these are
multiple years this is for year one this
is for year two this is for year three
and so on so for multiple years overall
the trend there is an upward Trend the
sales are increasing but it is not a
continuous increase right so there is a
certain pattern so what is happening
what is the pattern every December the
sales are increasing or they are peaking
for that particular year right then
there is a new year again when December
approaches the sales are increasing
again when December approaches the sales
are increasing and so on and so forth so
this is known as seasonality so there is
a certain fluctuation which is uh which
is periodic in nature so this is known
as seasonality then cyclicity what is
cyclicity now cyclicity is somewhat
similar to seasonality but here the
duration between two cycles is much
longer so seasonality typically is
referred to as an annual kind of a
sequence like for example we saw here so
it is pretty much like every year in the
month of December the sales are
increasing however cycling
what happens is first of all the
duration is pretty much not fixed and
the duration or the Gap length of time
between two cycles can be much longer so
recession is an example so we had let's
say recession in 2001 or 2002 perhaps
and then we had one in 2008 and then we
had probably in 2000 2012 and so on and
so forth so it is not like every year
this happens probably so there is
usually when we say recession there is a
slum and then it recovers and then there
is a slump and then it recovers and
probably there is another bigger slum
and so on right so you see here this is
similar to seasonality but first of all
this length is much more than a year
right that is number one and it is not
fixed as well it is not like every four
years or every six years that duration
is not fixed so the duration can vary at
the same time the gas gap between two
cycles is much longer compared to
seasonality all right so then what is
irregularity irregularity is like the
random component of the time series data
so there is like you have part which is
the trend which tells whether the
overall it is increasing or decreasing
then you have cyclicity and seasonality
which is like kind of a specific pattern
right uh then there is a cyclicity which
is again a pattern but at much longer
intervals plus there is a random
component so which is not really which
cannot be accounted for very easily
right so there will be a random
component which can be really random as
the name suggests right so that is the
irregularity component so these are the
various components of Time series data
yes there are conditions where we cannot
use time series analysis right so is it
can we do time series analysis with any
kind of data no not really so what are
the situations where we are we cannot do
time series analysis so there will be
some data which is collected over a
period of time but it's really not
changing so it will not really not make
sense to perform any time series
analysis over it right for example like
this one so if we take X as the time and
Y as the value of whatever the output we
are talking about and if the Y value is
constant there is really no analysis
that you can do leave alone time series
analysis right so that is one another
possibility is yes there is a change but
it is changing as per a very fixed
function like a sine wave or a COS wave
again time series analysis will not make
sense in this kind of a situation
because there is a definite pattern here
there is a definite function that the
data is following so it will not make
sense to do a Time series analysis now
before performing any time series
analysis uh the data has to be
stationary and typically time series
data is not stationary so in which case
you need to make the data stationary
before we apply any models like arima
model or any of these right so what
exactly is stationary data and what is
meant by stationary data let us take a
look first of all what is non-stationary
data time series data if you recall from
one of my earlier slides we said that
type series data has the following four
components the trend seasonality
cyclicity and random random component or
irregularity right so if these
components are present in Time series
data it is non-stationary which means
that typically these components will be
present therefore most of the time A
Time series data that is collected raw
data is non-stationary data so it has to
be changed to stationary Data before we
apply any of these algorithms all right
so a non-stationary Time series data
would look like this which means like
for example here there is an upward
Trend the seasonality component is there
and also the random component and so on
so if the data is not stationary then
the time series forecasting will be
affected so you cannot really perform a
Time series forecasting on a
non-stationary data so how do we
differentiate between a stationary and a
non-stationary Time series data
typically or technically one is of
course you can do it visually in
non-stationary data the the data will be
more flatish the seasonality will of
course be there but the trend will not
be there so the data May if we plot that
it may appear somewhat like this right
it's a horizontal line along the
horizontal line you will see compared to
the original data which was there was an
upward Trend so it was changing somewhat
like this right so this is
non-stationary data and this is our
stationary data would look visually what
does this mean technically this means
that stationarity of the data depends on
a few things but the mean the variance
and the covariance so these are the
three components on which the
stationarity of the data depends so
let's take a look at what each of these
are for stationary data the mean should
not be a function of time which means
that the means pretty much remain
constant over a period of time right so
there is there shouldn't be any change
so this is how the stationary data and
this is a non-stationary data I've shown
in the previous slide as well so here
the mean is increasing that means there
is an upward Trend okay so that is one
part of it and then the variance of the
series should not be also a function of
time so the variance also should be
pretty much common or should be constant
rather so this is a if we visually we
take a look this is our time series
stationary's data would look when the
variance is not changing here the
variance is changing therefore this is
non-stationary and we cannot apply type
series for casting on this kind of data
similarly the covariance which is
basically of the ith term and the I plus
MTH term should not be a function of
time as well so covariance is nothing
but not only the variance at the ith
term but the relation between the
variance at the ith term and I plus M
meth or the I plus NF term so as again
once again visually this is how it would
look if the covariance is also changing
with respect to time so these are the
three all three components should be
pretty much constant and that is when
you have stationary data and in order to
perform time series analysis the data
should be stationary okay so let's take
a look at the concept of moving average
or the method of moving average and
let's see how it works we'll do simple
calculations so let's say this is our
sample data we have the data for three
months a January February March the
sales in hundreds of in thousands rather
not hundreds thousands of dollars is
given here and now we want to find the
moving average so how do we find the
moving average we call it as moving
average three so moving average three is
nothing but you take three of the values
or the readings add them up and divide
by three basically the way we take a
mean or average of the three values so
that is as simple as that so that's the
average first of all so what is moving
average moving averages if you now have
a series of data you keep taking the
three values the next three values and
then you take the average of that and
then the next three values and so on and
so forth so that is how you take the
moving average so let's take a little
more detailed example of car sales so
this is how we have the car sales data
for the entire year let's say so rather
for four years so year one we have for
each quarter what a one two three four
and then year two quarter one to three
four and so on and so forth so this is
how we have sales data of a particular
car let's say or a showroom and we want
to forecast for year five so we have the
data for four years we now want to focus
for the fifth year let's see how it
works first of all if we plot the data
as it is uh taking the raw data this is
how it would look and uh what do you
think it is is it stationary no right
because there is a trend upward Trend so
this is not a stationary data so we um
we need to later we will see how to make
it stationary but to start with just an
example we will not worry about it for
now we will just go ahead and manually
do the forecasting using what is known
as moving average method okay so we are
not applying any algorithm or anything
like that in the next video we will see
how to apply an algorithm how to make it
stationary and so on all right so
um here we see that all the three or
four components that we talked about are
there there is a trend there is a
seasonality and then of course there is
some random component as well cyclicity
may not be it is possible that cyclicity
is not applicable in all the situations
for sales especially there may not be or
unless you're taking a sales for maybe
20 30 years cyclicity may not come into
play so we will just consider uh
primarily the trend seasonality and
irregularity right so Random it is also
known as random irregularity right so we
were are calling the random or
irregularity component so these are the
three main components typically in this
case we will talk about so this is the
trend component and we will see how to
do these calculations so let's take a
look
re-draw the table including the time
code we will add another column which is
the time code and uh this is the column
and we'll just number it like one two
three four up to 16. the rest of the
data Remains the Same okay so we will do
the calculations now now let us do the
moving average calculations
um or ma4 as we call it for each year so
we take all the four quarters and we
take an average of that so if we add up
these four values and divide by 4 you
get the moving average of 3.4 so we
start by putting the value here so that
will be for the third quarter let's say
one two three the third quarter then we
will go on to the next one so we take
the next four values as you see here and
take the average of that which is the
moving average for the next quarter and
so on and so forth now if we just do
average it is not centered so what we do
is we basically add one more column and
we calculate the centered moving average
as shown here so here what we do is we
take the average of two values and then
just adding these values here so for
example the first value for the third
quarter is actually the average of the
third and the fourth quarter so we have
3.5 now it gets centered so similarly
the next value would be 3.6 plus 3.9
divided by 2 so which is 3.7 and so on
and so forth okay so that is the
centered moving average this is done
primarily to smoothen the data so that
there are not too many rough edges so
that is what we do here so if we
visualize this data now this is how it
looks right so if we take the centered
moving average as you can see there is a
gradual increase if this was not the
case if you had not centered it the
changes would have been much sharper so
that is the basically the smoothening
that we are talking about now let's go
and do the forecast for the fifth year
so in order to do the forecast what
we're going to do is we will take the
centered moving average as our Baseline
and then start doing a few more
calculations that are required in order
to come up with the prediction so what
we are going to do is we are going to
use this multiplicity or multiplicative
model in this case and this is how it it
looks so we take the product of
seasonality and the trend and the
irregularity components and we just
multiply that and in order to get that
this product of these two We have
basically the actual value divided by
CMA YT value divided by CMA will give
you the predicted value of y t is equal
to the product of all three components
therefore s t into i t is equal to YT by
CMA so this is like this is equal to y t
right so therefore if we want s t into y
t the product of seasonality and
irregularity it is equal to YT by CMA so
that is how we will work it out I also
have an Excel sheet feet of the actual
data so let me just pull that up all
right so this is how the data looks in
Excel as you can see here here one
quarter one two three four here two
quarter one two three four and so on and
this is the sales data and then this is
the moving average as I mentioned this
is how we calculate and this is the
centered moving average so this is the
primary component that we will start
working with and then we will calculate
since we want the product of SC into i t
that is equal to YT by CMA so if you see
this values are nothing but the YT value
divided by CMA so in this case it is 4
by 3.5 which is 1.14 similarly 4.5 by
3.7 1.22 and so on and so forth so we
take we have the product St into it and
then the next step is to calculate the
average of respective quarters so that
is what we are doing here average of
respective quarters and then we need to
calculate the D seasonalized values so
in order to get decisionalized value we
need to divide YT by St that was
calculated so for example here it is 2.8
by 0.9 so we got the decisionalized
value here and then we get the trend and
then we get the predicted values so in
order to get the predicted value which
is basically we predict the values for
known values as well like for example
here one quarter one we know the value
but now that we have our model we
predict ourselves and see how close it
is so we predict it as 2.89 whereas the
actual value is 2.8 then we have 2.59
the actual value is 2.1 and so on just
to see how our model works and then
continue that into the fifth year
because for fifth year we don't have a
reference value okay and if we plot this
we will come to know how well our
calculations are how well our manual
model in this case we did not really use
a model but we did on our own manually
so it will tell us the trend so for
example the predicted value is this gray
color here and you can see that it is
actually pretty much following the
actual value which is the blue color
right and the gray color is the
predicted value so the ACT wherever we
know the values up to year four we can
see that our predicted value are
following or pretty much very close to
the actual values and then from here
onwards when the year 5 starts the blue
color line is not there because we don't
have the actual values only the
predicted value so we can see that since
it was following the trend pretty much
for the last four years we can safely
assume that it has understood the
pattern and it is predicting correctly
for the next one year the next four
quarters right so that is what we are
doing here so these four quarters we did
not have actual data but we have the
predicted values so let's go back and
see how this is working in this using
the slides so this is we already saw
this part and I think it was easier to
see in the Excel sheet so we calculated
the St i t the product of St and it
using the formula like here y by YT by
CMA we got that and then we got ST which
is basically YT so this is the average
of the first quarters for all the four
years and similarly this is the average
of the second quarter for all the four
years and so on so these values are
repeating these are they are calculated
only once they get repeated that's see
here and then we get the decisionalized
data and that is basically YT by St so
we calculated St here and we have YT so
y t by s t will give you the DCS analyze
data and we have got rid of the seasonal
and The Irregular components so far now
what we are left with is the trend and
before we start the time series
forecasting time series analysis as I
mentioned earlier we need to completely
get rid of the non-stationary components
so we are still left with the trend
component so now let us also remove the
trend component in order to do that we
have to find the or we have to calculate
the intercept and slope of the data
because that is required to calculate
the trend and how are we going to do
that that we will actually use what is
known as regression tool or Analytics
tool that is available in Excel so you
remember we have our data in Excel so
let me take you to the Excel and here we
need to calculate the intercept and the
slope in order to do that we have to use
the regression mechanism and in order to
use the regression mechanism we have to
use the Analytics tool that comes with
Excel so how do you activate this tool
so this is how you would need to
activate the tool from Excel you need to
go to options and in options there will
be add-ins and in add-ins you will have
analysis tool pack and you select this
and you just say go it will open up a
box like this you say analysis tool pack
and you say Okay And now when you come
back in to the regular view of excel in
the data tab you will see data analysis
activated so you need to go to file
options and add-ins and then analysis
tool pack typically since I've already
added it it is coming at the top but it
would come under inactive application
add-ins so when you're doing it for the
first time so don't use VBA you just say
analysis tool pack there are two options
one with VBA like this one and one
without VBA so just use the one without
VBA and then instead of just saying okay
just take care that you click on this go
and not just okay so you say go then it
will give you these options only then
you select just the analysis tool pack
and then you say okay all right so and
then when you come back to the main view
you click on data okay so this is your
normal home view perhaps so you need to
come to data and here is where you will
see data analysis available to you and
then if you click on that there are a
bunch of possibilities what kind of data
analysis you want to do if there are
options are given right now we just want
to do regression because we want to find
the slope and The Intercept so select
regression and you say okay
and you will get these options for input
y range and input X range input y range
is the value YT so you just select this
and you can select up to here and press
enter and input X range you can for now
you start with the baseline or you can
also start with the D season value so
you can just click on these and say okay
I have already calculated it so these
are the intercept and the coefficients
that we are getting for these values and
we will actually use that to calculate
our Trend here right so which is in the
J column so our trend is equal to
intercept plus slope into the time code
so The Intercept is uh out here as we
can see in our slide as well so if you
see here this is our intercept and the
lower value is the slope so we have
calculated here and it's shown in the
slides as well so intercept so the
formula is shown here so our trend is
equal to intercept plus slope into time
code time code is nothing but this one t
column a one two three four okay so
that's how you calculate the trend and
that's how you use the data analysis to
from Excel using these two we calculate
the predicted values and using this
formula which is basically trend is
equal to into set plus slope into time
code and then we can go and plot it see
how it is looking and therefore so we
see here that the predicted values are
pretty close to the actual values and
therefore we can safely assume that our
calculations which are like our manual
module is working and hence if we go
ahead and predict for the fifth year so
till four years we know the actual value
as well so we can compare our model is
performing and for the fifth year we
don't have reference values so we can
use our equations to calculate the
values or predict the values for the
fifth year and we can go ahead and
safely calculate those values and when
we plot for the fifth year as well the
predicted values we see that they are
pretty much they capture the pattern and
we can safely assume that the
predictions are fairly accurate as we
can also see from the graph in the Excel
sheet that we have already see okay so
let's go and plot it so this is how the
plot looks this is the CMA or the
centered moving average the green color
and then the blue color is the actual
data red color is the predicted value
predicted by our handcrafted model okay
so remember we did not use any regular
forecasting model or any tool we have
done this manually and the actual tool
will be used in the next video this is
just to give you an idea about how
behind the scenes are under the hood how
forecasting Works a Time series analysis
how it is performed okay so it looks
like it has captured the trend properly
so up to here is the known difference we
have reference and from here onwards is
purely predicted and as I mentioned
earlier we can safely assume that the
values are accurate and predicted
properly for the fifth year so let's go
ahead and Implement a Time series
forecast in r first of all we will be
using the arima on to do the forecast of
this time series data so let us try to
understand what is arima model so arima
is actually an acronym it stands for
auto regressive integrated moving
average so that is what is arima model
and it is specified by three parameters
which is p d and q p stands for auto
regressive so let me just mark this so
there are three components here Auto
regressive integrated moving average
okay so these three parameters
correspond to those three components so
the P stands for auto regressive D for
integrated and Q for moving average so
let us see what exactly this is so these
three factors are p is the number of
Auto regressive terms or ar we will see
that in a little bit and D is how many
levels of differences that we need to do
or differentiation we need to do and Q
is the number of lagged forecast error
so we'll see what exactly each of these
are so AR is the number of Auto
regressive terms and which is basically
denoted by the p and then we have t
which is for the number of times it has
to be differentiated and then we have q
which is for the moving average so what
exactly AR terms so in terms of the
regression model autoregressive
components refer to the prior values of
the current value what we mean by that
is here when we talk about time series
data focus on the fact that there is
regression so what exactly happens in
regression we try to do something like
if it is simple linear regression we do
have some equation like Y is equal to MX
plus C where there are actually there
are two variables one is the dependent
variable and then there is an
independent variable let me just
complete this equation as well MX plus C
right so this is a normal regression
curve or a simple regression curve now
here we are talking about Auto
regression or or Auto regressive so Auto
regressive as the name suggests is
regression of itself so which means that
here you have only one variable which is
your maybe the cost of the flights or
whatever it is right and the other
variable is basically time dependent and
therefore the value at any a given time
and that we will denote as Y T for
example so there is no X here there is
only one variable and which is y and we
say y t which is basically the predicted
value at a time interval T for example
is dependent on the previous value so
for example there may be A1 and then y t
minus 1 and then there will be like plus
A2 and right plus A2 and Y T minus 2 and
all right and then plus A3 into y t
minus 3 all right so basically here what
we are saying is there's only one
variable here but there is a regression
component so we are doing a regression
on itself so that's how the term Auto
regression comes into plane so only
thing is that it is dependent on the
previous time values so there is a lag
let's say this is the first lag second
leg third lag and so on so the current
value which is y T is dependent on the
previous time lag values so that is what
is auto regression component so this is
what is shown here for example in this
case instead of Y we are calling it as X
so that's the same and this is
represented by some equation of that
sort depending on how many lags we take
so that is the AR component and the term
p is basically determines how many lags
we are considering so that's the P4 now
what is d t is the degree of
differencing so here differencing is
like to for the non-seasonal differences
right so for example if you take the
values like this which are given for
five or six and so on and so forth if
you take the differencing of one after
another like for example 5 minus 4 or 4
minus 5 the next value with the previous
value so 4 minus 5. so this is known as
the first order differencing so the
result is minus one similarly 6 minus 4
is 2 7 minus 6 is 1. so this is first
order differencing and here we call it
as D is equal to 1 okay and same way we
can have second order third order and so
on then the last one is q q is the
actually why we call it moving average
but in reality it is actually the error
of the model so we also sometimes
represent as e t all right so now arima
model works on the assumption that the
data is stationary which means that the
trend and seasonality of the data has
been removed that is correct okay so
this we have discussed in the first part
how what exactly is stationary data and
how do we remove the non-stationary part
of it now in order to test whether the
data is stationary or not there are two
important components that are considered
one is the autocorrelation function and
the other is the partial order
correlation function so this is referred
to as ACF and pacf all right so what is
sort of correlation and what is the
definition or correlation is basically
the similarity between values of a same
variable across observations as the name
suggests now how do we actually find the
article relation function the value
right so this is basically done by
plotting and autocorrelation function
also tells you how correlated points are
with each other based on how many time
steps they are separated by and so on
that is basically the time lag that we
were talking about and it is also used
to determine how past and future data
points are related and the value of the
autocorrelation function can vary from
-1 to 1. so if we plot this is how it
would look autocorrelation function
would look somewhat like this and there
is actually a readily available function
in R so we will see that and you can use
that to plot your nautical relation
function okay so that is ACF and we will
see that in our rstudio in a little bit
and similarly you have partial order
correlation function so partial order
correlation function is the degree of
association between two variables while
adjusting the effect of one or more
additional variables so this again can
be measured and it can also be plotted
at its value once again can go from -1
to 1 and it gives the partial
correlation of Time series with its own
lagged values so lag again we have
discussed in the previous couple of
slides this is a psef plot would look in
our studio we will see that as well and
once we get into the r studio and with
that let's get into our studio and take
a look at our use case before we go into
the code let's just quickly understand
what exactly is the objective of this
use case so we are going to predict some
values of four custom values and we have
the data of the airline ticket sales of
the previous years and now we will try
to find the or predict the or forecast
the values for the future years all
right so we will basically identify the
time series components like Trend
seasonality and random Behavior we will
actually visualize this in our studio
and then we will actually forecast the
values based on the past values or
history data historical data so these
are the steps that we follow we will see
in our studio a little bit just quickly
let's go through what are the steps
reload the data and it is a Time series
data and we try to find out what class
it belongs to the data is actually air
passengers data that is already comes
pre-loaded with our studio so we will be
using that and we can take a look at the
data and then what is the starting point
what is the end point so these are all
functions that are not really available
we'll be using and then what is the
frequency which is basically frequency
is 12 which is like yearly data right so
every month data has been collected so
for each year it is 12 and then we can
check for many missing values if there
are any and then we can take a look at
summary of the data this is what we do
in exploratory data analysis and then we
can plot the data visualize the data how
it is looking and we will see how the
data has some Trends seasonality and so
on and so forth all right then we can
take a look at the cycle of the data
using the cycle function and we can see
that it is every month that's the cycle
end of every 12 months it new cycle
begins so each month of the year is a
the data is available then we can do box
plots to see for each month how the data
is varying over the various 10 or 12
years that we will be looking at this
data and uh from exploratory data
analysis we can identify that there is a
trend there is a seasonality component
and how the seasonality component varies
also we can see from the box plots and
we can decompose the data we can use the
decompose function rather to see the
various components like the seasonality
trend and the irregularity part okay so
we will see all of this in our studio
this is how they will look this is the
Once you decompose and this is how you
will actually you can visualize the data
this is the actual data and this is the
trend as you can see it's going upwards
this is the seasonal component and this
is your random or irregularity right so
we call it irregularity or we can also
call it random as you can see here yes
so the data must have a constant
variance and mean which means that it is
stationary before we start any analysis
time series analysis and without so
basically yeah it is stationary only
then it is easy to model the data
perform time series analysis so we can
then go ahead and fit the model as we
discussed earlier we'll be using arima
model there are some techniques to find
out what should be the parameter so we
will see that when we go into our studio
so the auto arima function basically
tells us what should be the parameters
right so these parameters are the p d
and q that we talked about that's what
is being shown here so if we use Auto
arima it will basically take all
possible values of this PDQ these
parameters and it will find out what is
the best value and then it will
recommend so that is the advantage of
using Auto arima all right so like in
this case it will tell us what if we use
this parameter Trace we set the
parameter Trace is equal to true then it
will basically tell us what is the value
of this AIC which has to be minimum so
the lower the value the better so for
each of these combinations of p d and q
it will give us the value is here and
then it will recommend to us which is
the best model okay because each other
has the lowest value of this AIC it will
recommend that as your best PDQ values
so once we have that we can see that we
will basically we can potentially get a
model or the equation model is nothing
with the equation and based on the
parameters that we get and we can do
some Diagnostics we can do some plotting
to see how whether there is a plot for
the residuals so which shows the
stationarity and then we can also take a
look at the ACF and tscf we can plot the
ACF and pacf and then we can do some
forecasting for the future here so in
this case we have up to 1960 and then we
can see how we can forecast for the next
10 years which is 1970 up to 197. and
once we have done this can we validate
this model yes definitely we can
validate this model and uh to validate
the findings we use uh jump box test and
this is how you just call box.test and
then you pass these parameters and you
will get the values that will be
returned which will tell us whether this
how accurate this model is how accurate
the connections are so the values of P
are quite insignificant in this case we
will see that and that also indicates
that our model is free of
autocorrelation and that will basically
be it so let's go back and into our R
studio and go through these steps in
real time so we have to import this
Library forecast package is not
installed you have to go here and
install the forecast package okay so
that's the easy way to install rather
than so click on this install I will not
do it now because I've already installed
so the first step that's only one time
then after that you just have to load
into memory and then keep going so we
will load this data called air
passengers so by calling this data
method and if you see the the data
passengers is loaded here and if we
check for the class it is a Time series
data TS data so we can check for the
dates you can also view the data in a
little bit and the start date is 1949
and January
and our end date is 1960 December and
the frequency is 12 which is like
collected monthly so that is the
frequency which is uh 12 here and then
we check if there are any missing values
there are no missing values and then we
take a look at the summary of the data
this is all exploratory data analysis
and then if you just display the data
this is how it looks
and then we need to decompose this data
so we will kind of store this in an
object yes data and then use that to
decompose and store the new values let
me just clear this for now and if we
decompose basically as we have seen in
the slides decomposing is breaking it
into the trend seasonality and irregular
or random components then you can go
ahead and plot it so when you plot it
you can see here let me Zoom this this
is our original plot or observed value
as it is known as then we have
decomposed the three parts which is
basically the trend as you can see there
is a upward Trend then the seasonal
component so this is some regularly
occurring pattern and then there is a
random value which is basically you
cannot really give any equation or
function or anything like that so that's
what this plotting has done and then you
can actually plot them individually as
well so these are the individual plots
for the trend for the seasonal component
and the random component all right so
now let's take a look at the original
data and see how the trend is in a way
so if we do this linear regression line
it will show that it is going upwards
and we can also take a look at the cycle
that are there which is nothing but we
have a frequency of 12 right so the
Cycles will display that it is January
February to December and then back to
January February and so on and so forth
and if we do box plots for the monthly
data you will see that for each of the
months right and over the 10 years that
the data that we have we will see that
there is a certain pattern right this is
also in a way to find the seasonality
component so while January February
sales are relatively low around July
August the sales pick up so especially
in July I think the sales are the
highest and this seems to be happening
pretty much every year right so this is
every year in July AI there seems to be
a peak in the sales and then it goes
down and slightly higher in December and
it's all and so on so that is Again part
of our exploratory data analysis and
once again let's just plot the data now
as I said in order to fit into an arima
model we need the values of PD and Q now
one way of doing it is there are
multiple ways actually of doing it at
the earlier method of doing it was you
draw the autocorrelation function plot
and then partial order correlation
function plot and then observe that and
where does this change and then identify
what should be the values of PE and Q
and so on now R really has a very
beautiful method which we can use to
avoid all that manual process that we
used to do earlier so what R will do is
there is a method called Auto arima and
if we just call this Auto arima method
and it will basically go and test the
arima model for all possible values of
this parameters PDQ and then it will
suggest to you what should be the best
model and it will return that best model
with the right values of PD and Q so u v
as data scientists don't have to do any
manual you know trial and error kind of
stuff okay so we got the model now and
this is the model it has PDQ values are
two one one PDQ and this is the seasonal
part of it so we can ignore it for now
and so if we want to actually understand
how this has returned these values to
one one as the best one there is another
functionality or feature where we can
use this Trace function or Trace
parameter so if you pass to Auto arima
the trace parameter what it will do is
it will show you how it is doing this
calculation what is the value of the AIC
basically AIC this is what you know
defines the accuracy of the model the
lower the better okay so for each
combination of PDQ it will show us the
value of AIC so let's run it before
instead of me talking so much let's run
this if we run Auto arima with Trace you
see here there is a red mark here that
means it is performing it's executing
this and here we see the display right
so it starts with certain values of PDQ
and then it finds that value is too high
so it starts with again with some 0 1 1
0 and so on and so forth and ultimately
it tells us okay this is our best model
you see here it says this is our best
model two one one let's go back and see
did we get the same one yes we got the
same one when we ran without Trace as
well right now why is two one one let us
see where is two one one here is our two
one one and if you compare the values
you see that one zero one seven is
pretty much the lowest value and
therefore it is saying this is our best
model all other values are higher so
that's how you kind of get your model
and now that you have your model what
you have to do you need to predict the
values right so before that let us just
do some test of these values so for that
you install T Series again if you are
doing it for the first time you would
rather use this package and install and
say T-Series and install it and then you
just use this Library function to load
it into your memory all right so now
that we got our model using Auto arima
let us go ahead and forecast and also
test the model and also plot the ACF and
psef remember we talked about this but
we did not really use it we don't have
to use that but at least we will
visualize it and for some of the stuff
we may need this T-Series our library so
if you are doing this for the first time
you may have to install it and my
recommendation is don't use it in the
code you go here and install T-Series
and I will not do it now because I have
already installed it but this is a
preferred method and once you install it
you just load it using this libraries
function and then you can plot your
residuals and this is how the residuals
look and you can plot your ACF and psef
okay so this is how your tacf looks and
this is how you ACF logs for now there
is really nothing else we need to do
with ACF and pacf is just to visualize
how that how it looks but as I mentioned
earlier we were actually using these
visualizations or these graphs to
identify the values of p d and q and how
that was done it's uh out of scope of
this video so we will leave it at that
and then we will forecast for the next
10 years how do we focus that so we call
forecast and we pass the model and we
pass what is the level of accuracy that
you need which is 95 percent and for how
many period right so basically we want
for 10 years which is like 10 into 12
time periods so that's what we are doing
here and now we can plot the forecast
value so you see this is the original
value up to I think 62 or whatever and
then it goes up to 72. this blue color
is the predicted value let's go and zoom
it up so that we can see it better so
from here onwards you're forecasting and
you can see that it looks like our model
has kind of learned the pattern and this
pattern looks very similar to what we
see in the actual data now how do we
test our model so we can do what is
known as the Box test and we pass our
model here residuals basically with
different lags and from those values
here the P values here we find that they
are reasonably in low P values which
means our model is fairly accurate by
giving a shot to Simply transport
strategy program in data analytics this
is from Purdue University in
collaboration with IBM and this should
be the right choice the link in the
description box below should navigate
you to the home page where you can find
a complete overview of the program being
offered
applications of data analytics now the
sky's the limit on this in today's world
almost every business Act of Life your
music on your Spotify are driven by data
analytics but some of the big players
when you go in there job hunting are
going to be your fraud analysis if you
want to go make a lot of money and
you're good at it and you like dealing
with numbers go join the banks and track
down the criminals who are stealing
money it's a lot of you know it's a big
thing to protect credit cards predict
sales purchases bad checks any of those
things when you can track them down is
huge
Healthcare exploding there is everything
from trying to find cures for the covet
virus or any of the viruses out there
using your cell phone to diagnose
different ailments that way you don't
have to go and see the doctor you can
actually just go in there and take a
picture of the funky growth on your arm
hopefully it's not too big and then they
send it in there and the data analytics
goes in there looks at it and says oh
this is what this is this is a
professional you need to go see or don't
need to see
and that's just one aspect of healthcare
the database is being generated by
Healthcare and getting the right doctors
and helping the doctors analyze whether
something is benign or malignant if it's
cancerous all those things are now part
of the ongoing Health Care growth in
data analytics
Inventory management
think one of those huge warehouses where
they're shipping out all the goods how
do you inventory that in such a way so
that you maximize the stuff that's being
purchased the most near the entrance and
all the other stuff towards the back or
even pre-ship it so it's huge to be able
to inventory the manager inventory and
pretty soon they'll just have a drone
come in there and start picking up some
of those boxes and move them around also
deliver your Logistics again this goes
from getting from point A to point B you
can combine it with our inventory so you
pre-ship stuff if you know a certain
area is more likely to purchase it how
do you get it the delivery to the most
destinations the quickest in the short
amount of time and then they even
pre-stack the trucks going out and
that's all done with data analytics how
do we stack all that stuff so it comes
out in the right order
targeted marketing huge industry any
kind of marketing whether you're
generating the right content for the
marketing who are you targeting with
that marketing researching the people
what they want so you know what products
to Market out there all those things are
huge
and these are just a few examples you
can probably go Way Beyond this from
tracking forest fires
to astrology and studying the stars all
of this is part of data analytics now
and plays a huge role in all these
different areas
City Planning is another one you know
you can see a nice organized City like
this one where you can get in and out of
the neighborhoods if you're a fire truck
police officers need to be able to get
in and out you want your tourists to be
able to come in you still want the place
to look nice and you have the right
commercial development the right
Industrial Development like enough
residents for people to stay all those
things are part of your City Planning
again huge in data analytics
so sky's the limit on what you use it
for let's take a look at types of data
analytics
and this can be broken up in so many
ways but we're going to start with
looking at the most basic questions that
you're going to be asking in data
analytics and the first one is you want
descriptive analytics what has happened
hindsight how many sales per call ratio
coming out of the call center if we have
500 tourists in a forest and they have a
certain temperature how many fires were
started how many times did the police
have to show up to certain houses all
that's descriptive the next one is
predictive Predictive Analytics is what
will happen next we want to predict this
is great if you have a ice cream store
and you want to predict how many people
to work at the ice cream store in a
certain day based on the temperature
coming up in the time of the year
and then one of the biggest growing and
most important parts of the industry is
now prescriptive analytics and you can
think of that as combining the first two
we have descriptive and we have
predictive then you get
pre-scriptive Analytics
how can we make it happen foresight what
can we change to make this work better
in all the industries we looked at
before we can start asking questions
especially in City development there's a
good one
if we want to have our city generate
more income and we want that income to
be commercial based what kind of
commercial buildings do we need to build
in that area that are going to bring
people over do we need huge Warehouse
cells Costco sales buildings or do we
need little mom pod joints that are
going to bring in people from the
country to come shop there or do you
want an industrial setup what do you
need to bring that into industry in
there is our car industry available in
that area if it's not a car industry
what other Industries are in that area
all those things are prescriptive we're
guessing we're guessing what can we do
to fix it what can we do to fix crime
and area with education what kind of
education are we going to use to help
people understand what's going on so
that we lower the rate of crime and we
help our communities grow better that's
all prescriptive it's all guessing we
went foresight into how can we make it
happen how can we make this better
and we really can't not go into enough
detail on these three because a lot of
people stumble on this when they come in
and are doing analytics whether you're
the manager a shareholder or the data
scientist coming in you really need to
understand the descriptive analytics
where you're studying the total units of
furniture sold and the profit that was
made in the past here we go into
Predictive Analytics predicting the
total units that would sell and the
profit we can expect in the future gear
up for how many employees we need how
much money we're going to make and
prescriptive analytics finding ways to
improve the sales and the profit so we
can sell maybe a different kind of
furniture we're going to guess at what
the area is looking for and how that
marketing is going to change
data analytics process steps so let's
take a look at some of the basic
processing and what that looks like when
you're working with this data
so there's five basic steps the five
steps of processing and this changes and
then there's a lot of things that go on
when they talk about agile programming
the whole concept of agile is you take
some kind of framework like this and
then you build on it depending on what
your business needs
so the first step is data collection
and usually with a large company you
might have somebody who is responsible
for the database management you may have
another one where they're pulling apis
they're pulling data off of maybe the
Census Bureau maybe something very very
specific domain specific so if you're
analyzing cancerous growths and how to
understand them then the data collection
is going to be those measurements they
take from the MRI or it might be even
the MRI images they've used those also
so there's a lot of things with data
collection and how to control that and
make sure it has what you need and is
clean and you don't have misinformation
coming in
once you have the data collected there's
a data preparation
so stage two is we take that data and we
format it into something we can use
probably one of the biggest formats that
you see is when you're processing text
how do you process text well you use
what they call a one hot encoder and
each word is represented by a yes no
kind of setup so it'd be like a long
array of bits that's one way to prepare
it and so you know bit number one is the
bit number two is has or whatever it is
other preparations might be if you're
using neural networks you might be
taking integers or float numbers and
converting them to a value between 0 and
1. that way you don't have one of them
creating a bias in there so there's a
lot of different things that go into
Data preparation that is 80 percent of
data science so when we talk about the
data analytics which is a little bit
more on the math side and they usually
say talk about a data scientist kind of
being the overall
prepare this stuff you're going to spend
80 percent of your data preparation
data exploration that's the fun part
this is where you're exploring things
and it is maybe 10 to 15 percent of what
you do with the data you spend with the
data exploration it is probably the most
important step because this is where you
got to start asking questions if you ask
your questions wrong you're going to get
some wrong information if you're working
with a company and they want to know the
marketing values then you really got to
focus on hey how do we generate money
for this company or fraud how do we
lower the fraud rate while still
generating a profit for data modeling
this is where we start actually getting
into the data code which model to use
that predicts what's going to happen
uh and then result interpretation we
want to be able to interpret those
results usually see that in your matplot
library where you create nice beautiful
images so it shows up on their dashboard
for the marketing manager or for the CEO
so they can take a quick look and say
hey I can see what's going on there you
want to reduce it to something they can
easily read they don't want to hear the
scientific terms they want to see
something they can use and we'll talk
about that a little bit more when we
start looking at some of this in a demo
since this is data analysis with python
we've got to ask the question why python
for data analytics I mean there's C plus
plus there's Java there's dot net from
Microsoft why do people go to python for
it
so the number of reasons one it's easy
to learn with simple syntax
you don't have a very high type set like
you do in Java and other coding so it
allows you to kind of be a little lazy
in your programming that doesn't mean
that it can't be set that way and that
you don't have to be careful it just
makes means you can spin up a code much
quicker in Python the same amount of
code to do something in Python A lot of
times is one two or three or four lines
where when I did the same thing say in
Java I found myself with 10 12 13 20
lines depending on what it was
it's very scalable and flexible so
there's our flexibility because you can
do a lot with it and you can easily
scale it up you can go from something on
your machine to using a pi spark under
the spark environment and spread that
across hundreds if not thousands of
servers across terabytes of data or
petabytes of data so it's very scalable
there's a huge collection of libraries
this one's always interesting because
Java has a huge collection of libraries
C has a huge collection of
libraries.net does and they're always in
competition to get those libraries out
Scala for your spark all those have huge
collection libraries this is always
changing but because Python's open
source you almost always have easy to
access libraries that anybody can use
you don't have to go check your
licensing and have special licensing
like you do in some packages
graphics and visualization they have a
really powerful package for that so it
makes it easy to create nice displays
for people to read
and community support because python is
open source it has a huge community that
supports it you can do a quick Google
and probably find a solution for almost
anything you're working on
python libraries let's bring it together
we have data analytics and we have
python so when we're talking data
analytics we're talking python libraries
for data analytics and the big five
players are numpy pandas matplot Library
scipy which is going to be in the
background so we're not going to talk
too much about the scientific formulas
inside pi and PSY kit
so numpy supports in dimensional arrays
provides numerical Computing tools
useful for linear algebra and Fourier
transform
and you can think of this as just a grid
of numbers and you can even have a grid
inside a grid or data it's not even
numbers because you can also put words
and characters and just about anything
into that array but you can think of a
grid and then you can have a grid inside
a grid and you end up with a nice
three-dimensional array if you want to
talk three-dimensional array you can
think of images you have your three
channels of color four if you have an
alpha and then you have your X Y
coordinates for the image we're looking
at so you can go x y and then what are
the three channels to generate that
color
and numpy isn't restricted to three
dimensions you could imagine uh watching
a movie well now you have your movie
clips and they each have their X number
of frames and each of those frames have
X number of X Y coordinates for the
pictures in each frame and then you have
your three dimensions for the colors so
numpy is just a great way to work within
dimensional arrays
now closely with numpy is pandas useful
for handling missing data perform
mathematical operations provides
functions to manipulate data
pandas is becoming huge because it is
basically a data frame and if you're
working with big data and you're working
in spark or any of the other major
packages out there you realize that the
data frame is very Central to a lot of
that and you can look at it as a Excel
spreadsheet you have your columns you
have your rows or indexes and you can do
all kinds of different manipulations of
the data within including filling in
missing data which is a big thing when
you're dealing with large pools or lakes
of data where they might be collected
differently from different locations
and matplot Library
we did kick over the sci Pi which is a
lot of mathematical computations which
usually runs in the background of the
for of numpy and pandas although you do
use them they're useful for a lot of
other things in there but the matplot
library that's the final part that's
what you want to show people and this is
your plotting library in Python several
toolkits extend matplot Library
functionality there's like a hundred
different toolkits to extend matplot
Library which range from how to properly
display star constellations from
astronomy there's a very specific one
built just for that all the way to some
very generic ones we'll actually add
Seaborn in when we do the labs in a
minute several tool kits extend matplot
Library functionality and it creates
interactive visualization so there's all
kinds of cool things you can do as far
as just displaying graphs and there's
even some that you can create
interactive graphs we won't do the
interactive graphs but you'll see you'll
get a pretty good grasp of some of the
different things you can do in matplot
library
let's jump over to the demo which is my
favorite roll up our sleeves get our
hands in on what we're doing now there's
a lot of options when we're dealing with
python you can use pie charm as a really
popular one
uh and you'll see this all over the
place so it's one of the main ones
that's out there and there's a lot of
other ones I used to use netbeans which
is kind of lost favor I don't even have
it installed on my new computer
but the most popular one right now for
data science Now pycharm is really
popular for python General development
for data science we usually go to
Jupiter notebook or anaconda and we're
going to jump into Anaconda because
that's my favorite one to go to because
it has a lot of external tools for us
we're not going to dig into those but we
will pop in there so you can see what it
looks like so with Anaconda we have our
Jupiter lab we have our notebook these
are identical Jupiter lab is an upgrade
to the notebooks with multiple tabs
that's all it is and we'll be using the
notebook and you can see that pycharm is
so popular with python that we even have
it highlighted here in Anaconda as part
of the setup
Jupiter notebook can also be a
standalone so we're actually going to be
running Jupiter notebook and then you
have your different environments I have
we're going to be under main Pi 36
there's a root one and I usually label
it Pi 36
the reason is is currently as of writing
this tensorflow only works in 3 6 and
not in three seven or three eight for
doing neural networks but you can
actually have multiple environments
which is nice they're they separate the
kernel so it helps protect your computer
when you're doing development and this
is just a great way to do a display or a
demo especially if you're looking for
that job pull up your laptop open it up
or if you're doing a meeting get a
broadcast up to the big screen so that
the CEO can see what you're looking at
and when we launched the notebook it
actually opens up a file browser in
whatever web browser you have this
happens to be Chrome and then you can
just go under new there's a lot of
different options depending what you
have installed python3 and this just
creates an Untitled version of this and
you can see here I'm actually in a
simply learned folder for other work
I've done for simply learn
and that's where I save all my stuff and
I can browse through other folders
making it really easy to jump from one
project to another
and under here we'll go ahead and change
the name of this and we'll go ahead and
rename it
data analytics data analytics
just so I can remember what I was doing
which is probably about 50 of the
folders in here right or files in here
right now uh so let's go ahead and jump
in there and take a look at some of
these different uh tools that we were
looking at
and as we go through the demo let's
start with the numpy uh the least
visually exciting and I'm going to zoom
in here so you can see what we're doing
and the first thing we want to do is
import numpy
and we'll import it as NP that is the
most common numpy terminology
and let's go ahead and change the view
so we also have the line numbers I don't
know why we probably won't need them but
I'm looking for easy reference and then
we'll create a one dimensional array we
just call this array one
and it equals NP dot array and you put
your array information in here in this
case we'll spell it out you can actually
do like a range and other ways there's
lots of ways to generate these arrays
but we'll just do one two three so three
integers
and if we print
our array one
we can go ahead and run this
and you can see right here it prints one
two three you can see why this is a
really nice interface to show other
people what you're doing with the
Jupiter notebook
uh so this is the basic we've created an
array this is a one-dimensional array
and then an array is one two three one
of the nice things about the Jupiter
notebook is whatever ran in this first
setup
is still running it's still in the
kernel so it still has the numpy
imported as in p and it still has our
variable
arr1 for array one equal to NP array of
one two three
so we go to the next cell
we can check the type of the array we're
just going to print
we say hey what's what what is this
setup in here and we want type
and then we want what is the type of
array one let's go ahead and run that
and it says class numpy ND array so it's
its own class that's all we're doing is
checking to see what that class is
and if you look at the array class
probably the biggest thing you do I
don't know how many times I find myself
doing this because I forget what I'm
working on and I forget I'm working with
a three-dimensional or four-dimensional
array and I have to reformat somehow so
it works with whatever other things I
have and so we do the array shape the
array shape is just three because it has
three members and it's a one-dimensional
array that's all that is
and with the numpy array we can easily
access stick with the print statement
if you actually put a variable in
Jupiter notebook and it's the last one
in the cell
it will be the same as a print statement
so if I do this where array one of two
it's the same as doing print
array of two that's those are identical
statements in our Jupiter notebook
we'll go and stick with the print on
this one
and it's three so there's our print
Space 2 and we have 0 1 2
2 equals three we can easily change that
so we have array one of place two
equals five
and then if we print our array one
you can see right down here when it
comes out it's one two and five
and there I left the print statement off
because it's the last variable in the
list it'll always print the variable if
you just put it in like that
that's a Jupiter notebook thing don't do
that in pie charm I've forgotten before
doing a demo
and we talked about multiple Dimensions
so we'll do an array two-dimensional
array
and this is again a numpy array
and in the numpy array
we need our first Dimension we'll do one
two three
and our
second dimension uh three four five and
you can see right here that when we hit
the uh we'll do this we'll just do array
two
and we can run that and there's our
array two one two three three four five
we can also do array two
of 1
and then we can do let's do a zero it
doesn't really matter which one actually
let's do two there we go and if I run
this it'll print out five because here
we are this is zero zero one two three
is under zero Row three four five is on
our one row now we start with zero and
then the two zero one two goes to the
five
and then maybe we forgot what we were
working with so we'll go do array two
dot shape
and if we do array two of shape
uh we'll go and run that we'll see we
have two rows and each row has three
elements a two-dimensional array two
three if you looked up here when we did
it before I just had three comma nothing
when you have a single entity it always
saves it as a tuple with a blank space
but you can see right here we have two
comma three
and if you remember from up here we just
did this array 2 of oh let's go what is
it one
comma two
we run that we get the five you can also
count backwards this is kind of fun
and you'll see I just kind of Switched
something on you because you can also do
one comma two to get to the same spot
now two is the last
one zero one two it's the last one in
there we can count backwards and do
minus one and if we run this we get the
same answer whether we count it as uh
let's go back up here whether we count
this as 0 1 2
or we count backwards as minus one minus
two minus three and you can see that if
I change this minus one to a minus two
and run that
I get 4 which is going backwards minus 1
minus two so there's a lot of different
ways to reference what we're working on
inside the numpy array
it's really a cool tool it's got a lot
of things you can do with it
and we talked about the fact that it can
also hold things that are not values and
we'll call this array s for Strings
equals NP dot array
and put our setup in there brackets and
let's go
China
um
India
USA
Mexico doesn't matter we can make
whatever we want on here and if we print
that out
we run this you can see that we get our
numpy array China India USA Mexico it
even gives us our d-type of a U6
a lot of times when you're messing with
data we'll call this array R for range
just to kind of keep it uniform in P dot
a range
so this is a command inside numpy to
create a range of numbers
and if you're testing data Maybe you
want maybe you have equal time
increments that are spaced a certain
point apart but in this case we're just
going to do integers
and we're going to do a setup from 0 20
skipping every other one
and we'll print it out and see what that
looks like
and you can see here we have 0 2 4 6 8
10 12 14 16 18 like you expected it
skips every one
and just a quick note
there's no 20 on here uh why well this
starts at zero and counts up to 20. so
if you're used to another language where
explicitly says less than or less than
equal to 20 like for x equals zero X
plus plus X is less than 20. that's what
this is it just assumes X is less than
20 on here
if we want to create a very uniform uh
set you know zero two four six what
happens if I want to create numbers uh
from 0 to 10 but I need 20 increments in
there we can do that with line space so
we can create
um an R we'll call this l
equals I don't think we'll actually use
any of this again so I don't know why
I'm creating unique identifiers for it
but we'll do NP
Lin space
and we're going to do zero
to 10 or 0 to 9. remember it doesn't it
goes up to 10. and then we want to let's
say we have 20
different
um
increments in there so we're creating a
we have a data set and we know it's over
a certain time period and we need to
divide that time period by 20 and it
happens to just have 10 pieces in it and
here we go you can see right here we
have 20 or has 20 pieces in it but it's
over 10 years we got to divide it in the
middle and you can see it does it goes
0.52 remember yeah there's our 10 on the
end so it goes up to 10.
and then we can also do random there's
NP dot random if you're doing neural
networks
usually you start it by seeding it with
random numbers
and we'll just do NP dot random and
we'll just call this array
we'll stop giving it unique numbers
we'll print that one out and run it
and you can see we have random numbers
they are zero to one so you'll see that
all these numbers are under one and you
can easily alter that by multiplying
them out or something like that if you
want to do like zero to a hundred you
can also round them up if it's integer 0
to 100 there's all kinds of things you
can do but generates a random float
between 0 and 1. and you have a couple
options you could reshape that or you
can just generate them in whatever shape
you want and so we can see here we did
three and four and so you can see three
rows by four variables
same thing as doing a reshape of 12
variables to three and four
and if you're going to do that you might
need an empty data set I have had this
come up many times where I need to start
off with zero and I don't know you know
because I'm going to be adding stuff in
there or it might be 0 and 1 or 1 is if
you're removing the background of an
image you might want the background is
zero and then you figure out where the
image is and you set all those boxes to
one and you create a mask so creating
masks over images is really big and
doing that with a numpy array of zero
and we can also uh
give it a space
and we'll just do this all in one shot
this time
and we'll do the same thing like we did
before
zeros and in this case we'll do a two
comma three
and so when we run this
forgot the asterisks around it I knew I
was forgetting something there we go
so when we run this you can see here we
have our 10 zeros in a row and maybe
this is a mask for an image and so it
has uh two rows of three digits in it so
it's a very small image a little tiny
pixel
and maybe you're looking to do something
the opposite way instead of creating a
mask of zeros and filling in with ones
maybe you want to create a mask of ones
and fill them in with zeros and we'll
just do just like we did before with the
three comma four and when we run this
you'll see it's all ones and we could
even do this even we'll do it this way
let's do
10.
10 by 10 icon and then you have your
three colors and you can so creates
quite a large array there for doing
pictures and stuff like that when you
add that third dimension in
if we take that off it's a little bit
easier to see
we'll do 10 again
and you can easily see how we have 10
rows of 10 ones
you can also do something like create an
array
and we'll do 0 1 2.
and then in this array we actually print
it right out we want a repeat and so you
can actually do a repeat of the array
and maybe you need this array
um let's repeat it three times
so there's our repeat of an array repeat
three times
and if we run this
you'll see we have zero zero zero one
one two two two
and whenever I think of a repeat I don't
really think of repeating being the
first digit three times the second digit
I really always think of it as zero one
two zero one two zero one two it catches
me every time but the actual code for
that one is going to be tile and again
if we do a range three
and we run this you can see how you can
generate one zero one two zero one two
zero one two
and if you're dealing with an identity
Matrix we can do that also if you're big
on you're doing your matrixes and we'll
just dive into t
I guess we'll go ahead and spell it out
today any tricks
and the command we're looking for is um
i e y e and we'll do three and then
we'll just go ahead and print this out
there we go there's our identity Matrix
and it comes out by a three by three
array because there's our Matrix
and then it puts the ones down the
middle and for doing your different
Matrix math
and we can manipulate that a little bit
too
we talk about
Matrix is
we might not want ones across the middle
in which case we now have the diagonal
so we can do an NP dot diagonal
and we do a diagonal let's put in the
diagonal
one two three four five and when we run
this
again this generates a value and by just
putting that value in there's the same
as putting print around it or putting
array equals and then print array and
you can see it generates a diagonal one
two three four five and there's your
your beginning of your Matrix array for
working with matrixes
and we can actually go in reverse let's
create an array equals remember our
random
random.random and we'll do a five by
five array oops there we go
five five
and just so you can see what that looks
like
helps if I don't mistype the numbers
which in this case I just need to take
out the brackets and there you go you
have your your five by five array set up
in there and we can know because we're
working with Matrix is we might want to
do this in reverse and extract the
diagonals which would be the 0.79 the
0.678 and so on
and we simply type in NP dot diagonal
and we put our array in there
and this will of course print it out
because it returns it as a variable and
you can see here here's our diagonal
going across from our Matrix
and we did talk about shape earlier if
you remember you can do print the shape
out you can also do the dimensions so in
Dimensions very similar to shape it
comes out and just has two Dimensions we
can also look at the size so if we do
size on here we can run that and you can
see as a size of 25 two dimensions and
of course five by five and that was from
the shape from earlier that we looked at
there's our 5x5 shape
and if you remember earlier we did
random well you can also do uh random I
talked a little bit about manipulating 0
to 1 and how you can get different
answers you can also do straight for the
integer part and we'll do minus 10.
210
4
and so we're going to Generate random
integers between minus 10 to 10 we're
going to generate four of those that's
when we run that we have 7 minus 3 minus
six minus three they're all be between
minus 10 and 10 and there's four of them
and now we jump into some of the
functionality of arrays which is really
great because this is where they come in
here's the array and you can add 10 to
it and if I run this there takes my
original array from up here
with the integers and adds 10 to all of
those values so now we have oh this is
the decimal that's right this is a
random decimal I had stored in Array
but this takes a random decimal the
random numbers I had from 0 to 1 and
adds 10 to them and we can just as
easily do uh
minus 10
we could even do
times 2
and we could do divide by two
and it would it'll take that random
number we generated and cut in half so
now all these numbers are under 0.5
another way you can
change the numbers to what you need on
there
and as you dig deeper into numpy we can
also do exponential so as an exponential
function which would generate some
interesting numbers off of the random
so we're taking them to the power I
don't even remember what the original
numbers in the array were because we did
the random numbers up there here's our
original numbers and if you build an
exponential on there this is where you
get e to the X on this and just like you
can do e to the X you can also do the
log so if you're doing logarithmic
functions
that reinforce learning you might be
doing some kind of log setup on there
and you can see the logarithmic of these
different array numbers
and if you're working with a log base 2
you can do you can just change it in
there in P log 2. you have to look it up
because this is not log one two three
four five it is Log and log 2. so just a
quick note that's not a variable going
in that is an actual command there's a
number of them in there and you'll have
to go look and see what the
documentation is but you can also do log
10. so here's log value 10.
some other really cool functions you can
do with this is your sign
so we can take a sine value of all of
our different values in there
and if you have sine you of course have
cosine
we can run that
so here's the cosine of those and if
you're doing activations in your numpy
array and you're doing a tangent
activation there's your tangent for that
and the tangent activation is actually
from neural networks that's one of the
ways you can activate it because it
forms a nice curve between from whether
you're generating 1 to negative one with
some discrepancy in the middle
just jumping a little bit in there into
neural networks
and then we get into it we just put the
array back out there so we can see it
when we're doing this as we're getting
into this you can also sum the values so
we have NP sum
and you can do a summation of all the
values in this array and you'll see that
if you added all these together they'd
equal 12.519 and so on I don't know what
the whole setup is in there
but you can see right here the the
summation of this one of the things you
can also do is by axes so we could do
axes equals zero
and if we run the summation of the axes
equals zero
and you can think of that in numpy as
the rows so that would be or
you can think of that in numpy as being
the columns we're summing these columns
going across
and you can also change this to one
and now we're summing the rows
and so that is the summation of this row
and so forth and so forth going down
and maybe you don't need to um
know the summation maybe what you're
looking for is the minimum
so here's our minimal you're looking for
and this comes up a lot because you have
like your errors we want to find the
minimal error inside of this array and
just like the other one we can do axes
equals zero
and you can see here
0.0645 is the smallest number in this
First Column is 0.0645 and so on
and if you have a minimum well you might
also want to know the max maybe we're
looking for the maximum profit and here
we go you can see maximum 0.79 is the
maximum on this first column and just
like we did before you can change this
to a one on axes you can take the axes
out of here and just find the max value
for the whole array and the max value in
here was 0.8344 so on so on
and since we're talking data analytics
we want to go ahead and look at the mean
pretty much the same as the average this
is the mean across the whole thing and
just like we did before we could also do
axes equals zero
and then you'll see this is the mean of
this axis and so on
and we have mean we might want to know
the median
and there's our median our most common
numbers if we have median we might want
to know the standard deviation
or if we have the average a lot of times
you do the means in the standard
deviation
we can run that and there's our standard
deviations along the axes we can also do
it across the whole array
if we're going to do standard deviations
there's also variance
which is your VAR
and there's our variance across the
different levels
and so if we looked at that we looked at
variance we looked at standard deviation
the median and the means there's more
but those are the most common ones used
with data analytics and then going
through your data and figuring out what
you're going to present to the
shareholders
and some other things we can do is we
can actually take slices you'll hear
that terminology
and a slice might be like we have a five
by five array but maybe we don't want
the whole array maybe we want from one
on we don't want the zero in there so we
got up to four and maybe on the second
part we just want
two to row three and see this notation
right here says 1 to the end and if we
run this you can see how that generates
a single row to the end and then row two
and three now remember it doesn't
include three that's why we only get the
one column so if we wanted two and three
you would need to go ahead and go two to
four so it goes up to four
we could also do this in reverse
just like we learned earlier we can go
minus one whoops
and when we go to minus one
it's the same thing because we have 0 1
2 3 4 this is the same thing as two to
four it goes two to the last one
also very common with arrays is you're
going to want to sort them so we still
have our array up here that we randomly
generated and we might want to um
sort it and we'll go and throw an axis
back in there
axis equals one if we run this
you can see from the axis that it sorts
it
the point two being the lowest value to
the highest value by the row we can also
change this of course to axis zero if
you're sorting it by columns so maybe
your values are based on columns
and then of course you can do the whole
array
and we can sort that I don't usually do
that but you know I guess sometimes you
might come up
and so you can see right here we have a
nice sorted array
something now let's just go ahead and
reprint our array so we can look at it
again starting to get too many boxes up
there something else you can do with an
array
is we can take and transpose it
this comes up more than you would think
when you transpose it you'll see that
the rows in the column are transposed so
where
0.79.57 0.064 is a column now we've
switched it and we have
0.79.42 as the index
you can see this really more dramatic if
we take a slice
we'll just do a slice of the first
couple
and then we'll just do all the other the
full rows and if we run this you can see
how it comes up a little bit different
and we'll just do the same slice up here
so you can see how those two look next
to each other
there we go there's our slice run
and so you can see the slice comes up
and it has a one two three four five
columns now we have one two three four
five rows and three columns versus three
rows
and the original version when they first
started putting this together was a
function so the original version was
transpose and this still works you can
still see it generates the same value as
just a capital T
so many times we flip this data because
we'll have an X Y value or we'll have an
image or something like that and it's
being read one way into the next process
and the next one needs it the opposite
so this actually happens a lot you need
to know how to transpose the data really
quick
and we can go ahead oh let's just take
here's our transpose we'll just stick
with the transpose on here
and instead of doing it this way we
might need to do something called
flattening why would you flatten your
data if this is an array going into a
neural network
you might want to send it in as one set
of values instead of two rows and you
can see here is all the values as a
single array it just flattens it down
into one array
so we covered our scientific means
transpose median some different
variations on here some of the other
things we want to do is what happens if
we want to append to our array so let's
create a new array
I'm getting tired of looking at the same
set of random numbers we generated
earlier so we'll go ahead and create a
new array here something a little
simpler so it's easier to see what we're
doing
and four five six seven eight uh that's
good enough we'll just do four five six
seven eight
and if we print this array
there it is four five six seven eight
and we might want to append something to
the array so we have our array we need
to extend it you got to be very careful
about appending things to your array and
there's a number of reasons for that
one is runtime because of the way the
numpy ray is set up a lot of times you
build your data and then push it into
the numpy array instead of continually
adding on to the array
and then it also usually it
automatically generates a copy for
protecting your data so there's a lot of
reasons to be careful about appending
this way but you can certainly do it and
we can just take our array we're going
to create a new array array one
and if we print array one and we append
8 to it you'll see four five six seven
and then there's our 8 appended on to
the end
and if you want to append something to
an array you'd probably also want to
whoops
array one let's try that again there we
go now we have the a dependent on to the
end
so you can see four five six seven eight
and then we pinned it another eight on
there
and if you're going to append something
you might want to um
go ahead and insert instead of appending
it might be you need to keep a certain
order and we can do the same thing we do
our array
and we're going to pin or insert at the
beginning and let's go ahead and insert
uh one two three one two three and we go
ahead and print our array two we run it
and you can see one two three a pin is
inserted at the beginning
inserts a lot more powerful and that you
can put it anywhere in the array we can
move it to the one spot and there we go
one two three we can do a minus one
just for fun and you'll see it comes up
one two three and we're counting
backwards by one
I'm imagining do minus zero
and run this and it turns out that minus
zero puts it back at the beginning
because that's why it registers a zero
just takes a minus sign off
and just like we add numbers on we might
want to delete numbers and so let's do
an NP dot delete well let's let's keep
it a little bit make it a little easy
here to watch we'll go ahead and create
an array three
then we'll do NP delete and we're just
working with array 2
and we want to do is delete zero space
so if we look at this here's our array 2
our Raid 2 starts with one and when we
delete the space on here
and print that out we deleted the one
right out of there
and we can also do something like this
where we can do it as a slice and we can
do let's do one comma three
and if we run one comma three you'll see
we've deleted the one space
and the three space out which deleted
our two and four
now keep in mind when you're messing
with adding lines and deleting lines
you have to be really careful because
there's a time element involved as far
as where the data is coming from and
it's really easy to delete the wrong
data and corrupt what you're working on
or to insert stuff where you don't want
it so there's always a warning when we
talk about manipulating numpy arrays
and just like anything else we're doing
we'll create an array C which equals
we'll just do our our numpy array that
we just created our Olympia array 3 and
we can do copy so you can make a copy of
it maybe you want to protect your
original data or maybe you're making a
mask and so you copy the array and then
the new array make all these alterations
and change it from values to zero to one
to mask over the first one and of course
we if we do array C since it equals a
copy of array three it's the same thing
one three five six seven eight
and now we're getting into combine and
split arrays
I end up doing a lot of this
and I don't know how many times I end up
fiddling with this and having a mess
so but but you do it a lot you know you
combine the arrays you split them you
might need one set of data for one thing
another set of data for the other
so let's go ahead and create two arrays
array one array two
and I want you to note
in the terminology we're going to look
for is concatenate what that means is
we're going to take
um
we'll call this a raycat I like a ray
cat there we go our array cat our
concatenated array
we're taking array one and two
and it's very important to really pay
attention to your axes and your accounts
I can't merge two arrays that have like
if their axes are messed up and I'm
merging on axis zero it's going to give
me an error and I'll have to reshape
them so you got to make sure that
whatever you're concatenating together
works
and what that means
as you can see here we have one two
three four one two three four and then
five six seven eight five six seven
eight along the zero axis
these each are four values
um so it's a two by four value and if we
go ahead and switch this to one you can
see how that's that flips it a little
bit so now we have one two three four
five six seven eight
it's interesting that we chose that one
if I did something like this
where this is now
there we go and we concatenate it we run
this and it gives me an answer okay
because I have two by two and I'm using
axes one but if I switch this to axis
zero where now it's got three and five
it gives me an error so you got to be
really careful on that to make sure that
your whatever axes you are putting
together that they match so like I said
this one oops X is one axis one has two
entities and since we're going on axis
one or by row you can see that it lets
it merge it right onto the end there
and you could imagine this if this was a
x y plot of value or the x value going
in and the predicted y value coming out
and then you have another prediction and
you want to combine them this works
really easy for that
and we'll go back and let's just put
this back to where we had it
oops I forgot how many changes I made
there we go I'll just put it Oops I
messed up in my concatenation order here
[Music]
we go
okay so you can see that we went through
the different concatenation axes is
really important when you're doing your
concatenation values on here
we'll switch us back to one just because
I like the looks of that better there we
go two rows
now there are other commands in here so
we can do cap V
equals npv v stack
this is nothing more than your
concatenation
but instead we don't have to put the
axes in there because it's v stands for
vertical and so if we print out
cat
V and we run this
you can see we get the one two three
four one two three four and that would
be the same as making this axis zero for
vertical stack and if you're going to
have a vertical stack you can also have
an H stack
so if we change this to from v-stack to
oops here we go H stack and we'll just
change this from cat to cat
and I run this
it's the same as doing axis zero the
process is identical in the background
this is like a legacy setup your v-stack
and your H stack most people just use
concatenate and then put the axes in
there because it's much has a lot more
clarity and is more more commonly used
nowadays
the last section in numpy we're going to
cover is
underst is kind of data exploration and
that'll make a little bit more sense in
just a moment sometimes they call them
set operations but let's say we have an
array one two three four five six three
whatever it is I think so we generate a
little array here and what I want to go
ahead and do is find the unique values
in that array
uh so maybe I'm generating what they
call a one hot encoder and so these
values then all become I need to know
how long my bit Ray is going to be so
each word how many how many each word is
represented by a number and then I want
to know just how many of those words are
in there if we're doing word count very
popular thing to do
and you can see here when we do unique
uh we have one two three four five six
those are our unique values
uh some of the things we can do with the
unique values is we can also instead of
doing just unique we can do uniques
our unique values and counts of each
unique value
and this is very similar to what we just
did up here where we we're doing NP
unique but we're going to add a little
bit more into there
and it's just part of the arguments in
this and we want to do return
counts equals
true so instead of just returning the
unique values we want to know how many
of those unique values are in each one
and we'll go ahead and print
our uniques
and print
our counts
let me run that you can see here we have
our unique value one two three four five
six just like we had before and then
there's two of the first of two ones two
twos two threes two fours one five two
sixes and so on and you can go through
and actually look at that if you want to
count them but a quick way to find out
your distribution of different values so
you might want to know how often the
word the is used versus the word and if
each word is represented as a unique
number
and along the set variables we might
want to know let me just put a note up
here
we're going to start looking at
uh intersection
and we might want to also know
differentiation
and neither
so when we're whoops neighbor
neither
so what we're looking at now is we want
to know hey where do these two arrays
intersect and we have one two three four
five three four five six seven we might
want to know what is common between the
two arrays
and so when we do that we have NP
intersect
and it's a 1D array one dimensional
array
and then we need to go ahead and put
array 1 array two
and if we run this
we can see they intersect at three four
five that's what they have common
and because we're going to go ahead and
go through these and look at a couple
different options let's change this from
intersect 1D
and we'll do the same thing we'll go
ahead and print this
so we might want to know the
intersection where they have
commonalities
another unique word is Union of 1D so
instead of intersect
we want to know all the values that are
in both of them so here's our Union of
1D when we run that you can see we have
one two three four five six seven so
it's all the different values in there
and the last one of the last words we
have two more to go so we want to know
what the set difference is
uh and so that's where the you'll see
that if you remember set we talked about
that being the what they call these
things so the set difference
have a 1D array we run that you can see
that one is only in one array and two is
only in one array
and if we want to know uh what's in
Array 1 but not in Array two we might
want to know what is in Array one but
not two and what's in two but not one
and this would be the set X or 1D on
here so we have the four different
options here where we can do an
intersection what do they both have in
common we can do a union what are all
the unique values in both arrays
we can see the difference what's in
Array one but not array two so set diff
1D and then set X or what is not in one
but is in two and what is in not in two
but in one
so we dug a lot in numpy because we're
talking there's a lot of different
little mathematical things going on in
numpy a lot of this can also be done in
pandas although usually the heavy
lifting is left for numpy because that's
what it's designed for let's go ahead
and open up another python 3.
setup in here
and so we want to explore what happens
when you want to display this this is
where it starts getting in my opinion a
little fun because you're actually
playing with it and you have something
to show people and we'll go ahead and
rename this we're going to call this uh
pandas and Pi plot
so pandas pipelot just so we can
remember for next time and we want to go
ahead and import the necessary libraries
we're going to import pandas as PD now
remember this is a data frame so we're
talking rows and columns and you'll see
how pandas work so nicely when you're
actually showing data to people and then
we're going to have numpy in the
background numpy works with pandas so a
lot of times you just import them by
default
Seaborn sits on top of the matplot
library so sometimes we use the Seaborn
because it kind of extends it's one of
the 100 packages it extends the matplot
library probably the most common used
because it has a lot of built-in
functionality almost by default I
usually just put Seaborn in there in
case I need it and of course we have
matplot Library as pipelot as PLT and
note we have as PD as NP as SNS as PLT
those are pretty standard so when you're
doing your Imports I would probably keep
those just so other people can read your
code and it makes sense to them that's
pretty much a standard nowadays
and then we have the strange line here
it says Amber sign matplot Library
inline
that is for Jupiter notebook only so if
you're running this in a different
package you'll have a pop-up when it
goes to display the matplot library you
can with the most current version of
Jupiter usually leave that out and it
will still display it right on the page
as we go and we'll see what that looks
like
and then we're going to go ahead and
just do the Seaborn the sns.set and
we're going to set the color codes
equals true let them just keep the
default one so we don't have to think
about it too much
and we of course have to run this the
reason we run this is because these
values are all set if we don't run this
and I access one of these afterward
it'll crash
the cool thing about Jupiter notebooks
is if you forgot to import one of these
you forgot to install it because you do
have to install this under your anaconda
setup or whatever setup you're in you
can flip over to Anaconda and run your
install for these and then just come
back and run it you don't have to close
anything out
and we'll go ahead and paste this one in
here real quick where we have car equals
PD dot read underscore CSV and then we
have the actual path
this path of course will vary depending
on what you are working with so it's
wherever you saved the file at and you
can see here I have um like my OneDrive
documents simply Learn Python data
analytics using python slash car CSV
it's quite a long file
when we open that up what we get is we
get a CSV file and we have the make the
model the year the engine fuel type
engine horsepower cylinders and so on
and this is just a comma separated file
so each row is like a row of data think
of it as a spreadsheet
and then each one is a column of data on
here and as you can see right here it
has the make model so it has columns for
a header on here
now your pandas just does an excellent
job of automatically pulling a lot of
this in so when you start seeing the
pandas on here you realize that you are
already like halfway done with getting
your data in I just love pandas for that
reason numpy also has it you can load a
CSV directly into numpy but we're
working with pandas and this is where it
really gets cool is I can come down here
and I can print remember our print
statement we can actually get rid of it
and we're just going to do car head
because it's going to print that out the
head is going to print the top values of
that data file we just ran in
and so you can see right here it does a
nice printout it's all nice and in line
because we're in Jupiter notebook I can
scroll back and forth and look at the
different data and just like we expected
we have our column and it brought the
header right in
one thing to note is the index it
automatically created an index 0 1 2 3 4
and so on and we're just looking at the
head so we've got zero one two three
four
you can change this you might want to
just look at the top two
we can run that there's our top two BMWs
another thing we can do is instead of
head we can do tail
and look at the last three values that
are in that data file and you can see
right here numbered them all the way up
to
11913. oh my goodness they put a lot of
data in this file I didn't even look to
see how big the file was so you can
really easily get through and view the
different data in here when you're
talking about Big Data
you almost never just print out car in
fact let's see what happens when we do
if we run this and we just run the car
it's huge in fact it's so big that the
pandas automatically truncates it and
just does head plus tail so you can see
the two so we really don't want to look
at the whole thing I'm going to go back
to let's stick with the head
displaying our data there we go so
there's a head of our data it gives us a
quick look to see what's actually in
there I can zoom out if we want so you
can actually get a better View
although we'll keep it zoomed in so you
can see the code I'm working on
and then from the data standpoint we of
course want to look at
data types what's going on with our data
what does it look like now this you know
you show your when you're talking to
your shareholders they like to see these
nice easy to read charts they look like
a spreadsheet so it's a nice way of
displaying pieces of the chart
we talk about the data types now we're
getting into the data science side of it
what are we working with well we have
make model we have an integer 64 for the
year engine fuel type is an object if we
go up here you can see that there most
of them are
like you know it's a set manual rear
wheel drive so they might be very
limited number of types in there
and so forth and it's either going to be
a float 64 an integer or an object it's
the way it's going to read it on here
and the next thing you're going to know
is like your columns
and since it loaded the columns
automatically we have here the make the
model the year the engine the size all
the way up to the MSRP
and um just out of something you'll see
come up a lot is whenever you're in
pandas and you type in dot values it
converts it from a pandas list to a
numpy array
and that's true of any of these so then
you end up in an empty array so you'll
see a little switch in there in the way
that the data is actually stored and
that's true of any of these in this case
we want car dot columns
they have a total list of your car
columns
and like any good data scientist we want
to start looking at analytical summary
of the data set what's going on with our
data so we can start trying to piecemeal
it together so we can do car
describe
and then we'll do is we'll do include
equals
all
so a nice Panda command is to describe
your data if you're working with r this
should start looking familiar
and we come down here and you can see
count there's a make the model of the
year how many of each one how many
unique values of each one the top value
of each one what's most common the
frequency the mean clearly on some of
these it's an object so really can't
tell you what the average is it'd just
be the top ones the average I guess
the year what's the average year on
there all this stuff comes down here
your standard deviation your minimum
value your maximum value what's in the
lower quarter fifty percent Mark where's
that line at and what's in the upper 75
percent the top 25 percent going into
the max
now this next part is just cool this is
what we always wanted computers be back
like in the 90s instead of 5 000 lines
of code to do this maybe not five
thousand all right I built my own plot
Library back in 95 and the amount of
code for doing a simple plot was um
I don't know probably about 100 lines of
code
this is being done in one line of code
we have our car which is our pandas we
generated that it's our data frame and
we have dot hist for histogram that is
the power of Seaborn now it's still
going to generate a numpy graph but
Seaborn sits on top and then we can do
the figure size this is just um so it
fits nicely on the paper on here and we
do something simple like this and you
can see here where it comes up and does
say matplot library and does subplots
and everything
but we're looking at a histogram of all
the different pieces in our database and
we have our engine cylinders that's
always a good one because you can see
like they have some that are they had a
null on there so they came out as zero
maybe a couple maybe one of them had a
two cylinder engine away back when four
is a common uh six a little less common
and then you see the eight cylinder 12
cylinder engines whether it's got to be
a Speedster or something
uh but you can see right here just
breaks it down so now you have how many
cars with how many whatever it is
cylinders horsepower and so on and it
does a nice job displaying it
you can see if you're working with your
uh
you're going into your demo it's really
nice just to be able to type that in and
boom there it is they can see it all the
way across
and we might want to zero in and use
like a box plot and this time we'll go
ahead and call the Seaborn SNS box plot
and we're going to go ahead and do
vehicle size in versus engine horsepower
XY plot and the data comes from the car
so if we run this we end up with a nice
box plot
you see our mid-size Compact and large
you can see the variation there's our
outlier showing up there on the compact
that must be a high-end sports car a
large car might have a couple engines
and again we have all these outliers and
then your deviation on them
very powerful and quick way to zero in
on one small piece of data and display
it for people who need to have it
reduced to something they can see and
look at and understand and that's our
Seabourn box plot or sns.box plot
and then if we're going to back out and
we want a quick look at what they call
pair plotting we can run that and you
can see with the Seaborn it just does
all the work for you it takes just a
moment for it to pull the data in and
compile it
and once it does it creates a nice grit
in this grid if you look at this one
space here which is you might not be
able to see the small number it says
engine horsepower this is engine
horsepower uh to the year was built and
it's just flipped so everything to the
right of the middle diagonal is just the
rotation of what's on the left and as
you expect the engine horsepower gets
bigger and bigger and bigger as time
goes on so the the year it was built the
further up in the year the more likely
you are to have a heavy horsepower
engine
and you can quickly look at trends
with our pair plot coming up and look
how fast that was that was it took a
couple a moment to process but right
away I get a nice view of all these
different information which I can look
at visually in and kind of see how
things group and look
now if I was doing a meeting I probably
wouldn't show all the data
one of the things I've learned over the
years is people myself included love to
show all our work you know we were
taught in school show all your work
prove what you know the CEO doesn't want
to see a huge grid of graphs I guarantee
it so we want to do is we want to go
ahead and drop
um the stuff that might not be
interested in and we're gonna I'm not
really a car person a guy in the back is
obviously so you have your engine fuel
type we're gonna drop that we're gonna
drop Market category vehicle style
popularity number of doors vehicle size
and we have the axes in here if you
remember from numpy we have to include
that axis to make it clear what we're
working on that's also true with pandas
and then we'll look at just the what it
looks like um from the head and you can
see that we dropped out those categories
and now we have the make model year and
so forth and we took out the engine fuel
type Market category Etc
and this should look familiar to you now
when you start working with pandas I
just love pandas for this reason look
how easy it is it just displays it as a
nice spreadsheet for you you can just
look at it and view it very easily it's
also the same kind of view you're going
to get if you're working in spark or Pi
spark which is python for spark across
Big Data this is the kind of thing that
they they come up with this is why
pandas is so powerful
and we may look at this and decide we
don't like these columns and so you can
go in here and we can actually rename
the columns
simple command car equals car rename
columns equals engine horsepower equals
horsepower this is just your standard
python dictionary
so it just Maps them out and you know
instead of having like a lengthy if it
here we had engine horsepower we just
won horsepower we don't need to know
it's the engine horsepower
engine cylinders we don't need to know
that it's for the engine because there's
only one thing we're describing if we're
talking about cars and that cylinders
and we'll go ahead and just run this and
again here's our car head and you can
see how that changed we have model year
and horsepower versus model year engine
horsepower engine cylinders and just
cylinders
again we want to keep reducing this so
it's more and more readable the more
readable you get it the better and of
course we can also adjust the size a
little bit
so that when it prints out instead of
splitting it on two lines we get like a
single line we can do that also that's
just your control Mouse app or plus sign
you use in Chrome that's a chrome
command
and if you remember from numpy we had
shape well pandas works the same way we
can look at the shape of the data so we
now have 11 914 rows and 10 columns so
you'll see some similarities because
pandas is built on numpy
and questions that come up just like you
did in numpy we might want to know
duplicate rows and so we can do car and
look at this switch here
um we're doing a selection this is a
panda selection with the brackets
but we want to select it based on
car.duplicated so how many duplicates on
there
so it's starting to look a little bit
different as far as how we access some
of the data in here this can be a
logical statement and we get the number
of duplicate rows we have 989 rows by 10
columns again
and this is one of those troubleshooting
things that we end up doing a lot more
than we really feel like we should we
might go ahead and do like a car count
just to see how many rows we're dealing
with and then right after that we might
want to go ahead and say hey let's drop
duplicates so remember we did all the
duplicates on there so car equals car
dot drop duplicates and then we can
print the head again we'll just do car
head here and you can see the data on
there looks the same as before
and just note that we did car equals car
draw duplicates there are commands in
here where you can do where it changes
the actual value and it works on some of
them and not on others depending on what
you're doing but by default it always
returns a copy so when we do this we're
reassigning it to car
and you can see it's the same header but
we want to go ahead and do count and see
how the count changes let's go ahead and
run this and you can see here instead of
11 914 we have 10
925. so we've removed about a hundred
cars and we're duplicated just slightly
under 100 there
and then as we're prepping our data we
might want to know
um car is null so it's going to count
the values of null and then we want to
sum that up and when we do that we do
the car is null function dot sum we end
up with HP the horsepower 69 have null
values and 30 have cylinders have no
values now if you don't put the sum at
the end it's just going to return a mask
with the true false of is it null or is
it not by zero and one so you're summing
up the ones underneath each column
and this of course then you have to
decide what you're going to do with the
null values there's a lot of different
options it might be that you need to put
in the average or means
maybe you want to put in the median
value there's a lot of different ways to
fill it usually when you first start out
with the data a lot of let me just drop
your null values and you can see here
car dot drop in a
which is equal to all and then we're
going to go ahead and count it and you
can see that we've dropped almost
another 100 values so from 10 1925 to 10
8 27.
and maybe 75 or so values
so we cleaned that this is really a big
part of cleaning data you need to know
how to get rid of your null values or at
least count them and what to do with
them
and of course if we go back to um
counting our null values we should now
have
no no values there we go and you'll see
there's zero null values
I don't know how many times I've been
running a model that doesn't take null
values and it crashes and I just sit
there and look at it trying to get why
did that crash it should have worked
it's because I forgot to remove the null
values
so we've been jumping around a lot we're
going to go back to finding outliers and
let's go ahead and bring that back into
our Seaborn and if you remember we did a
box plot earlier this time we're going
to do a box plot just on the price and
you can see here our price value
and we have the deviation with the two
thinner bars on each side of the main
value and then as we get up here we have
all these outliers in fact we have one
way out here that's probably a really
expensive high-end car is what we're
looking at
if you were doing fraud analysis you
would be jumping on all over these
outliers why are these deviation from
the standard what are these people doing
again this is probably like I said a
really high-end expensive car out here
that's what we're looking at and we can
also look at the box plot for the
horsepower
and we'll put that in down here
and run that
and you can see again here's our
horsepower and it just jumps and there's
these really odd huge muscle cars out
here that are outliers
and we're going to jump into making this
a little bit more as you started
displaying your data or your information
to your shareholders we're going to look
at plotting a histogram for the number
of cars per brand
and the first thing we want to go ahead
and do is we have with our car go back
over here here we go we have our make
value counts largest plot and we want to
do a kind equals bar
fig size 10-5
and right off the bat we jump up here
and we see Chevrolet it's going against
what was it it's um figure resolution
the value counts and we want the largest
value so here's our value counts and
compared to what the different cars are
Chevrolet puts out a lot of different
kinds of cars I didn't realize that they
made that many cars or different types
and then for readability let's go ahead
and add a title number of cars by make
number of cars and make if you looked at
this the first time you would have been
like well what the heck am I looking at
well we're looking at the number of cars
by make and then you can see here now
we're talking about the type of cars and
the different ones are put out Lotus I
guess only had a few different kinds of
cars over there very high-end cars
and then as uh doing data analytics and
as a data scientist one of the things I
am most interested in is the
relationship between the variables
so this is always a place to start we
want to know what's going on with our
variables and how they connect with each
other
so the first thing we're going to do is
we're going to go ahead and set a figure
size because we want to make sure it
fits our graph we'll just go ahead and
set this one plot Figure Set to figure
size 2010. if you never use the matplot
library which is sitting behind Seaborn
whatever is in the PLT this is what's
loaded it's like a canvas you're
painting on so the second you load that
Pi plot as PLT anything you do to that
is affecting everything on it
and then we want to go ahead since we're
using Seaborn
we'll go ahead and create a variable C
for relationships or correspondence
and car dot c-o-r-r that's a correlation
in Seabourn on top of pandas again one
line and you get the whole correlation
on there
and because we're working with Seabourn
let's put it into a nice heat map if
you're not familiar with heat maps that
means we're just using color as part of
our
setup so we have a nice visual
and we can see here that the Seaborn
connected to the pandas prints out a
nice chart
we'll talk a little bit about the color
here in a second it prints out a nice
chart this is a chart I look at as a
data scientist these are the numbers I
want to look at and we'll just highlight
one of them here's cylinders versus
horsepower the closer to one the higher
the correlation so 0.788 pretty high
correlation between the number of
cylinders and how heavy the horsepower
is
I'm betting if you looked at the year
versus horsepower we just look at that
one here's year and horsepower 0.314 not
as so much but if you combine them you
don't actually add them but if you
combine them you'll start to see an
increase in Horsepower per year and
cylinders you could probably get a
correlation there and just like 0.78 is
a positive correlation you might notice
if we look at cylinders
and or let's look at horsepower and
mileage so if we go here to horsepower
to mileage you get a nice negative we'll
do cylinders that's a bigger number
with cylinders to the miles per gallon
it's a minus 0.6 so it's a negative
correlation the closer to -1 the more
the negative correlation is
and then the chart you would actually
show people is a nice heat map this is
all our colors and it's just those
numbers put into a heat map the darker
the color the higher the correlation you
can see straight down the middle
obviously the year correlates strictly
with the year horsepower with horsepower
and so on that's why it's a one the
closer to the one the higher the
correlation between the two pieces of
data
now this is a good introduction pandas
goes Way Beyond this most the
functionality and numpy since pandas
sits on it is also in pandas and then it
even has additional features in it and
we use Seaborn pretty extensively
sitting on top over our pie plot so keep
in mind that our PI plot has a ton of
other features in it that we didn't even
touch on in here we couldn't even if you
had a sole course in it there's just so
many things hidden in there depending on
what your domain you're working on but
you can see here here's our Seaborn and
here's our matplot library that's all
our Graphics that we did and then the
Seaborn works really nicely with the
pandas we really like that
try giving a shot to Simply Dance plus
strategy program in data analytics this
is from her University in collaboration
with IBM and this should be the right
choice the link in the description box
below should navigate you to the home
page where you can find a complete
overview of the program being offered
so let's start with what is number numpy
is the core library for scientific and
numerical Computing in Python it
provides high performance
multi-dimensional array object and tools
for working with arrays and I'll go a
step further and say there are so many
other modules in Python built on numpy
so the fundamentals of numpy are so
important to latch onto for the python
so you can understand the other modules
and what they're doing numbers main
object is a multi-dimensional array it's
a table of elements usually numbers all
of the same type indexed by a tuple of
position integers in numpy dimensions
are called axes take a one-dimensional
array or we have remember dimensions are
also called axes you could say this is
the first axis 0 1 2 3 4 5 and you can
see down here it has a shape of six why
because there's six different elements
in it in the one dimension array and
they usually denote that as six comma
with an empty node on there and then we
have a two dimension array where you can
see 0 1 2 3 4 5 6 7 and in here we have
two axes or two dimensions and the shape
is two four so if you were looking at
this as a matrix or another mathematical
functions you can see there's all kinds
of importance on shape we're not going
to cover shape today but we will cover
that in part two did you know that
numpy's array class is called ND array
for numpy data array now we're going to
take a detour here because we're working
in Python and two of my favorite Tools
in Python is the Jupiter notebook and
then I like to use that sitting on top
of anaconda and if you flip over to
jupiter.org that's j u p y t e r dot org
you can go in here you can install it
off of here if you don't want to use the
Anaconda notebook but this is the
Jupiter setup the documentation on the
Jupiter Jupiter opens up in your web
browser that's what makes it so nice is
this portable the files are saved on
your computer they do run an IPython or
iron pi Python and you can create all
kinds of different environments in there
which I'll show you in just a minute I
myself like to use Anaconda that's
www.anaconda.com if you install Anaconda
it will install the jupyter notebook
with the Anaconda separate and you can
install Jupiter notebook and it'll run
completely separate from anaconda's
Jupiter notebook and you can see here
I've now opened up my anaconda Navigator
what I like about the Navigator this is
a fresh install on a new computer which
is always nice I can launch my Jupiter
notebook from in here I can bring other
tools so the Anaconda does a lot more
and under environments I only have the
one environment and I can open up the
terminal specific to this environment
this one happens to have python 37 in it
the most current version as of this
tutorial and then you open a terminal if
you're going to do your pip installs and
stuff like that for different modules
you can also create different
environments in here so maybe you need a
python36 python35 you can see we're
having a nice framework like Anaconda
really helps so you don't have to track
that on your own in the Jupiter notebook
and your different Jupiter notebook
setups we'll go ahead and launch this
jupyter notebook and then I've set my
browser window for a deep fault of
chrome so it's going to open up in
Chrome and you can see here this opens
up a folder on my computer we have a
couple different options on here
remember I set the environment up as
python 3.7 you would install any
additional modules that aren't already
installed in your python on this and it
keeps them separate so you do have to
for each environment install the
separate module so they match the
environment on there and in here we have
a couple things we can look up what's
running you have your different clusters
again this is I just installed this on a
new machine so I just have the one a
couple things in here that were run on
here recently and when we go on here is
we then have on the upper right new and
from the pull down menu you'll see
python3 and this will open up a new
window
and now we're in Jupiter python so this
is a python window and we'll just do a
print
and this of course is also hello world
and we'll run that and it prints out
hello world in the command line there's
a couple special things you have to know
we're not going to do today which is on
Graphics if you've never seen this
one of the things you can do is you can
also do a equals hello world and if you
just put the a in there now if you do a
bunch of these we have a equals hello
world b equals goodbye world and you put
a b a then return B it'll only run the
last one but you can see here if you put
the variable down here it will show you
what's in that variable
and that has to do with the Jupiter
notebook inline coding so that's not
basic python that's just Jupiter
notebook shorthand which you'll see in a
little bit so back to our numpy numpy
array versus python list python list
being the basic list in your python why
should we use numpy array when we have
python list well first it's fast the
numpy array has been optimized over
years and years by multiple programmers
and it's usually very quick compared to
the basic python list setup it's
convenient so it has a lot of
functionality in there that's not in the
basic python list and it also uses less
memory so it's optimized both for Speed
and memory use and let's go ahead and
jump into our Jupiter notebook since
we're coding best way to learn coding is
to code just like the best way to learn
how to write is right and the best way
to learn how to cook is cook so let's do
some coding here today and just like any
modules we have to import numpy we
almost always import it as in p that is
such a standard so you'll see that very
commonly we can just run that and now we
have access to our numpy module inside
our Python and then the most common
thing of course is to go and create a
numpy array and in here we can send it a
regular list
and so we'll go ahead and send this a
regular array let's go one two three to
make it simple and then I'm just going
to type in a and we'll run this
and so you can see down here the output
is an array of one two three and we
could also do print just a reminder that
this is an inline command so that
wouldn't work if you're using a
different editor you can see that it's
an array one two three but we'll go and
leave it as a
kind of a nice feature so you can see
what you're doing really quick in the
jupyter notebook and just like all your
other standard arrays I can go a of zero
which is going to be a value of one of
course we do a of one you go all the way
through this
I have one has a value of 2 in it so
whether using the numpy array or the
basic python list that's going to be the
same that should all look pretty
familiar and be pretty straightforward
remember the first value is always zero
and when we set on there so let's take a
look why we're using numpy because we
went over the slide a little bit but
let's just take a look and see what that
actually looks like and what we want to
look at is the fact that it's fast
convenient and uses less memory so let's
take a glance at that in code and see
what that actually looks like when we're
writing it in Python and what the
differences are
and to do this I'm going to go ahead and
import a couple other modules we're
going to import the time module so we
can time it and we're going to import
the system module so that we can take a
look at how much memory it uses and
we'll go and just run those so those are
imported so we'll do b equals a range of
one yeah 1000 is fine
and so that's going to create a list of
one thousand zero to 999 remember it
starts at zero and it stops right at the
1000 without actually going to the 1000.
and let's go ahead and print
and we want system dot get size of
and we'll pick any integer because we
have you know zero to a thousand we'll
just throw one in there five it doesn't
matter because it's gonna whatever
integer we put in there is going to
generate the same value because we're
looking the size of how how much memory
it stores an integer in and then we want
to have the link of the B that's how
many integers are in there
and if we go ahead and execute this and
run this at a line we'll see Oops I did
that wrong comma
if we multiply them together
we'll see it generates 28 000. so that's
the size we're looking at is 28 000 I
believe that's bytes that sounds about
right
so let's go ahead and create this in
numpy and we'll go with C equals NP and
this is a range
so that's the numpy command to do the
same thing that we were just doing in a
list
and we'll also use the same value on
there the 1000
and then once we've created the C value
of C for NP dot a range let's go ahead
and print and we can do that by doing C
dot size
times C dot item size
when it's very similar we did before we
did get the size of so the C size is the
size of the array and each item size
just reversed so that's the size of an
integer five item size is going to be
the integers and C size now let's just
take a look and see what that generates
and wow okay we got 4 000 versus 28 000.
that's a significant difference in
memory how much memory we're using with
the array and then let's go ahead and
take a look at uh speed let's do um oh
let's do size we can trade this with
lower values and it would happen so fast
that the npra kept coming up with zero
because I just rounded it off so size
and let's create an L1
Bulls range of size
and we'll do an L2
I'll just set up to the same thing it's
also range
of size on there there we go
and then we can do on A1 equals
NP dot a range size
and then let's do an A2 equals NP dot a
range we'll keep it the same size
and what we're going to do is we're
going to take these two different arrays
and we're going to perform some basic
functions on them but let's go ahead
actually just load these up now we'll go
ahead and run this so those are all set
in memory
except for the typo here
quickly fix that
there we go so these are now all loaded
in here and let's do a start equals
time dot time
so it's just going to look at my clock
time and see what time it is and it will
do result equals and let's do oh let's
say we got an array and we're going to
say
let's do some addition here X Plus y
for X comma y
in and we'll zip it up here
two different arrays so here's our two
different arrays we're going to multiply
each of the individual things on here L1
L2
there we go so that should add up each
value so L1 plus L2 each value in each
array
then we want to go ahead and print
and let's say python list took
and then we'll do
time
dot time
we'll just subtract the start out of
there so time whoops I messed up on some
of the quotation marks on there
okay there we go
time minus the start
and we'll convert that to seconds so
we'll go uh just in milliseconds or
times one thousand
and let's hit the run on there this is
kind of fun because you also get a view
while we're doing this of some ways to
manipulate the script
as you can see also my bad typing there
we go okay so we'll go ahead and run
this
and we can see here that the python list
took 34
actually I have to go back and look at
the conversion on there but you can see
it takes roughly 0.34 of a second and we
go ahead and print the result in here
too
let's do that
and we'll run that just so you can see
what the what kind of data we're looking
at
and we have the zero two four six eight
so it's just adding them together it
looks pretty straightforward on there
and if we scroll down to the bottom of
the answer again we see python list took
46 a little different time on there
depending on what um core because I have
this is on an eight core computer so it
just depends on what course running on
what else is pulling on the computer at
the time and let's go back up here and
do our start time paste that into here
and this time we're going to do a result
equals and this is really cool notice
how elegant this is It's So
straightforward this is a lot of reason
people started using numpy is because I
can add the two arrays together by
simply going A1 Plus A2 makes a lot of
sense both looking at it and it's just
very
convenient remember that slide we're
looking at fast convenient and less
memory so look how convenient that is
really easy to read real easy to see and
I don't know if we don't need to print
the result again so let's just go ahead
and print the time on here and we'll
borrow this from the top part
because I really am a lazy typer
and this isn't the python list this is
the numpy list or numpy array
and let's go ahead and see how that
comes out and we get 2.99 so let's take
a look at these two numbers 46 versus
2.99 so we'll just round this up to
three that's a huge difference that's
that's like more than 10 times faster
that's like 15 times roughly at a quick
glance I'd have to go do the math to
look at it and it's going to vary a
little bit depending on what's running
in the background the computer obviously
so we've looked at this and if we go
back here we found out it's much faster
yes there's different going to be
different speeds depending on what
you're doing with the array very
convenient easy to read and it uses less
memory so that's the core of the numpy
that's why a lot of people base so many
other modules on numpy and why it's so
widely used
so we did glance at a couple operations
when we were looking at speed and size
let's dive into a little bit more into
the basic operations
when these are always nice to see I mean
certainly you want to go get a cheat
sheet if you're using it for the first
time you know look things up Google is
your friend we did this with the most
basic numpy dot array or np.array and
we'll go ahead and create an array let's
do pairs one comma two
and then let's do a three comma four and
if we're going to do that let's do five
comma six
here we go and if we go ahead and take
this and run this I go ahead and do our
a down here so it's in line and I'll
print that out you can see it makes a
nice array for us so we have a and if
you look at that we have three different
objects each with two values in them and
hopefully you're starting to think well
how many dimensions or indexes is that
and you'll see three by two so let's go
ahead and take a look and let's go how
about a dot in Dimensions speaking of
which we'll run that and we have two
dimensions for each object
then we can do the item size so a DOT we
saw this earlier we looked up how many
items it was up here where we wanted to
multiply item size times the actual size
of the object so the memory is being
used versus the item size
we should see 4 there
memory is compressed down that's always
a good thing
and then the shape the shape is so
important when you're working with data
science and you're moving it from one
format to another
so we have our shape we just talked
about that we have three by two three
rows by two objects in each one
generally I don't look too much at the
size but the dimensions I'm always
looking up this is nice you can automate
it so you may be converting something
you might need to know how many
dimensions are going into the next
machine learning package so that you can
automatically just have it send that
information over
so we looked at a shape let's go and
create a slightly different array in P
dot array let's go ahead and just do as
our original
setup here
and one of the features we can do which
is really important is we can do D type
equals in this case let's do NP
float 64. and so what we've done is
converting all of these into a float and
we type in a
and now instead of having one two three
four five six you see they're all float
values 1.0 there's no actual zero in
there just there's a one dot or the one
period two three period four period five
period six period
and this again data science I don't know
how many times I've had to convert
something from an integer to a float so
it's going to work correctly in the
model I'm using so very common features
to be aware of and to be able to get
around and use
and we'll also do let's just curiosity
item size
we'll go and run that
and we see that it doubled in size so
it's not a huge increase well doubling
is always a big increase in computers
but it's not a huge increase compared to
what it would be if you're running this
in the python list format
and then we did the shape earlier
without having it set to the float 64.
let's go ahead and do a shape with it
set to 64. and it should be the same
three comma two so it all matches so
we've gone through and remember if you
really if this is all brand new to you
according to the Cambridge study at the
Cambridge University if you're learning
a brand new word in a foreign language
the average person has to repeat it 163
times before it's memorized
so a lot of this you build off of it so
hopefully you don't have to repeat it
163 times but we did manage to repeat it
at least twice here if not a little bit
more and let's go ahead and take this
we're going to go look at one more setup
on here and let me just take this last
statement here on the converting our
properties of our data and instead of
float 64
let's do complex let's just see what
that looks like and let's go ahead and
print that out and
and so we now have a complex data set up
and you'll see it's denoted by the one
dot plus 0 dot j
and if we flip over here and do a basic
search for numpy data types better to go
to the original web page but pull up a
bunch of these you can see there's a
whole list of different numpy data types
shorthand complex we have complex
complex 64 complex 128 complex number
represented by 264-bit floats real and
imaginary components
one option on there float 16 float32
float shorthand for float 64 most
commonly used
and of course all the different ones
that you can possibly put into your
numpy array so we covered a basic
Edition up there we're comparing how
fast it runs but some very basic
components how to set up a numpy array
how many dimensions it has item size
data type item again we went to item
size and there's also the
shape probably one of the more used I
used a shape all the time very commonly
used
and then down here you can see where we
actually created a numpy complex data
type
so let's look at some other features in
numpy one of them is you could do numpy
Dot
zeros
and we're going to do three comma four
there we go and we'll go ahead and run
this and you can see if I do NP dot
zeros I create an array of zeros
this is really important I was building
my own neural network and I needed to
create an array where I initialized the
weights and I want them all to be the
same weight in this case I want them to
start off with zero for the particular
project I was working on and there's
other options like you can do p1s
and we'll do the same thing three comma
four we'll run that
and you can see I've created a an array
of numpy ones in this case it comes out
as a float array
and this is an interesting to note
because we have let's go back to our
Python and do L Range Five
and we'll print the L so there's our
list
and if I run that
it doesn't create the range until after
the fact until you actually execute it
that's an upgrade in python python 27
actually created the array 0 1 2 3 4.
this one actually creates the script and
then once it's used it then actually
generates the array and if we do that in
numpy a Range remember that from before
and if we do a numpy a range 5
and let's do uh
l or we can just leave it as numpy
that's fine there we go just run that
you can see there we actually get an
array 0 1 2 3 4 for the value of the
numpy arrange a range five generates the
actual array
and for part one we're going to do just
one more section on basic setup
and we're going to concatenation
concatenation example
there we go we're gonna do strings let's
take a look at strings what's going on
with there and let's do
oh let's see print
Let's do an NP character something new
here and we're going to add and then
here's our brackets for what we're going
to add
oh and let's say
um let's do
hello
comma hi
and in the brackets on there let's
create another one
and this one's going to be
ABC and we'll do
x y z so we're just creating something
randomly making sum up on here and then
we'll go ahead and just print this
if we run that and come down here and of
course make sure all your brackets are
open and closed correctly and then you
can see in here when we concatenate the
example in numpy it takes the two
different arrays that we set up in there
and it combines the hello with the ABC
and the high with x y z
and if we can also do something like
print oh let's do NP character dot
multiply
so there's a lot of different functions
in here again you can look these up it's
probably good to look them all up and
see what they are but it's good to also
just see them in action let's do hello
space comma three
and we'll run this one
and run that without the error and
you'll see it does hello hello hello so
we multiplied it by three and we can
also let's just take this whole thing
here instead of retyping it
and we can do character Center so
instead of multiply this to Center
and over here keep our hello going
to space out of there and let's do
Center at 20.
and fill character
equals we'll fill it with dashes
so if we run this
you can see it prints out the hello with
dashes on each side and we keep going
with that we can also in addition to
doing the fill function we can play with
capitalize we can title we can do
lowercase we can do uppercase we can
split split line strip join these are
all the most common ones and let's go
ahead and just look at those and see
what those look like each one of them
so we're going to do the hello world
all-time favorite of mine I always like
to say Hello Universe and you can see
here we do Capital H with the world but
so we want to capitalize so capitalize
is the first one in the array so we get
Hello World on there and we can also
take this and instead of capitalizing
another feature in here is title and
let's just change this to how are we
doing
how are you doing so are we able to do
you and let's run that
and you can see here because we created
it as a title that capitalizes the first
letter in each word
and in this one we're going to do
character lower
two different examples here we have an
array we have Hello World all
capitalized and we have just hello and
you can see that one is an array and one
is just a string if we run that you get
a an array with Hello World lowercase
and hello lowercase and if we're going
to do it that way we can also do it the
opposite way there's also upper
and let's paste those in there and you
can see here we have character Dot Upper
opposite there
python.data and we'll do python is easy
hopefully you're starting to get the
picture that most of the Python and the
scripting is very simple it's when you
put the bigger picture together and
starts building these puzzles and
somebody asks you hey I need the first
letter capitalized unless it's the title
and then we have you start realizing
that this can get really complicated so
numpy just makes it simple and we like
that and so in this case we did python
data it's all uppercase python is easy
like shouting in your messenger python
is easy and then if you're ever
processing text and tokenizing it a lot
of times the first thing you do is we
just split the text and we're just going
to run this in P dot character.split are
you coming to the party if we do that it
returns an array of each of the
individual words are you coming to the
party splitting it by the spaces
and then if you're going to split it by
spaces we also need to know how to split
it by lines
and just like we have the basic split
command we also have split lines hello
and you'll see here the scoop in for our
new line and when we run that if you're
following the split part with the words
you should see hello how are you doing
the two different lines are now split
apart
and let's just review three more before
we wrap this up commonly used string
variable manipulations we have strip and
in this case we have Nina admin Anita
and we're going to strip a off of there
let's see what that looks like and then
you end up with nin dimin Nate it
basically takes up all leading and
trailing letters in this case we're
looking for a more common would be a
space in there but it might also be
punctuation or anything like that that
you need to remove from your letters and
words and if we're going to strip and
clean data we also need to be able to
reformat it or join it together so you
see here we have a character joined
we'll go ahead and run this
and it has on the first one it splits e
so letters up by the colon and the
second one by the dash and you can see
how this is really useful if you're
processing in this case a date we have
day month year year month date very
common things to have to always switch
around and manipulate depending on what
they're going into and what you're
working with
and finally let's look at one last
character string we're going to do
replace if you're doing misinformation
this is good pulling news articles
replacing is and what in this case we're
just doing here is a good dancer and
we're going to replace is with was
and you can see here he was a good
dancer hopefully that's not because he
had a bad fall he just was from like you
know 1920s and it's gotten old
so there we go we covered a lot of the
basics in numpy as far as creating an
array very important stuff here when
you're feeding it in how do we know the
shape of it the size of it what happens
we convert it from a regular integer
into a float value as far as how much
space it takes we saw that that doubled
it item size you have your in dimensions
and probably the most used is shape and
we'll cover more on shape in part two so
make sure you join us on part two
because there's a lot of important
things on shaping in there and setting
them up we also saw that you can create
a zeros based array you can create one
with ones if we do a range you can see
how it is a lot easier to use to create
its own range or a range as it is in
numpy you saw how easy it was to add two
arrays we saw that earlier just plus
sign then we got into doing strings and
working with strings and how to
concatenate so if you have two different
arrays of strings you can bring them
together we also saw how you can fill so
you can add a nice headline dash dash
dash we saw about capitalize the first
letter we saw about turning it into a
title so all the first letters are
capitalized doing lowercase on all the
letters upper for all the letters just
lower and upper nice abbreviation we
also covered how to split the character
set how to strip it so if you want to
strip all the A's out from leading A's
and ending A's or spaces you can do that
very easily also how to join the data
sets so here's a character join option
for your strings and finally we did the
character replace now let's go ahead and
dive in there since we're going right
into part two which is getting some
coding going under our belt and here in
our Jupiter notebook we can go under new
and create a new folder python3 I think
I forgot to do this last time but we
could just do the control plus plus
which in any browser enlarges a page
makes it a lot easier to see always a
nice feature another beautiful benefit
of using jupyter notebooks and let me go
ahead and show you a neat thing we can
do in Jupiter this is nice if you're
working with people and you're doing
this as a demo on a large screen I'm
going to do the hashtag or pound symbol
array manipulation kind of a title that
we're working on and then I'm going to
call this cell cell type markdown as
opposed to code and you'll see it
highlights it here and then if I run it
it just turns it into array manipulation
and then we're specifically going to be
working on array manipulation changing
shape to start with and we'll go ahead
and Mark this cell also a markdown so
has a nice little look there and then it
comes up and you can see it just like I
said it just highlights it it makes it
very bold print just making it easier to
read not a python thing but a Jupiter
thing that's good to know about
especially if you're working with the
shareholders since they're investing
money in you of course the first thing
you want to do is import we're going to
import numpy
as in p and that should be standard by
now by now you you start a Python
program you're doing some data science
numpy is just something you bring in
there and let's go ahead and create our
array and we're going to do that as the
NP dot a range remember that's uh zero
well we're going to do 0 to 9.
and uh we'll print
a little title on the original array
we'll just print that array a remember
from the first lesson so we have our
array which is zero one two three four
five six seven eight and let's add a
print space in between
let's create a second array B but we
want this to reshape array a and what
does that mean
and the command is simply reshape and
then we have nine items in here and this
is so important right now so be very
aware if I did some weird numbers in
here it's not going to work
and we want multiples of nine we know
that three times three is nine so we're
going to reshape our a array by 3 by 3
and then we're going to print let's give
it a title oops I have too many brackets
in there modified array and then let's
go ahead and print
RB and let's see what that looks like
and as we come down here you can see
we've taken this and it's gone from 0 1
2 3 4 5 6 7 8 to an array of arrays and
we have 0 1 2 3 4 5 6 7 8. and so we
split this into three by three and you
can guess that if I tried to reshape
this let's just do a five by three which
is 15. that's going to give me an error
so it's not going to work you're not
going to reshape something unless the
shape all the the data in there matches
correctly
so we can take this nine this flat 9 and
we call it a flex it's just a single
array and we can reshape it into a three
by three array and first you might think
matrixes which this is used for that
definitely I use it a lot in graphing
because it'll come in that I have an
array that's X Y comma X Y 1 y 1 comma
X2 Y2 and so the shape of it might be
two by the length of the number of
points and I need to separate that into
X flat array and a y flat array and you
can see this can be very easy to reshape
the array doing that and we can of
course go back we can do B
um print
and we'll do B dot Latin remember I said
it's called a flatten array and if we
run that you'll see this goes back to
the original one it takes this zero one
two three four five six seven eight and
flattens it back to a single array
and then one other feature to be aware
of is if we flatten it one of the
commands we can put in there is order
let me just go ahead and do that order
equals
F strangely enough F stands for Fortran
they hold Fortran days I remember
actually studying Fortran programming
language in this case you'll see that it
uses the first like zero three six is
the order so instead of flattening it
like we had before zero one two three
four five six seven eight it now does
zero three six one four seven two five
eight and if you go to the numpy array
page you can see here that they have the
flatten and you just open up the numpy
and D array flatten setup to look it up
and they have three different options
they have c f and a and it's whether to
flatten in C which was based on how the
C code works for flattening originally
worked which is row major Fortran which
is column major or preserve the column
Fortran ordering from a so whatever it
was in the default is the C version so
the default that you saw you could put
orders equals c and it'd have the same
effect as we saw there before you could
even do order equals a that would also
have the same effect because that's the
default so really the only other thing
you need to change on here is to change
it to C if you need it and you can see
right here or F I mean not C the only
thing you really want to change it to is
to your f for the Fortran order which
then does it by column versus by row and
let's look at
here we go reshape so let's create a
range of 12
and let's reshape it
I will do 4 comma 3 for this one and
remember this is numpy I forgot the NP
there
NP dot arrange
and we can type in just a for print or
you can do full print a and of course a
Jupiter notebook even have a little
extra print at the beginning if we run
this we'll see we create a nice array of
0 1 2 it's reshaped it so we have four
rows and three columns or you could call
that three columns and four rows zero
one two three four five six seven eight
nine ten eleven
but this one is so important we'll do NP
transpose
a and let's go ahead and run that
and it helps if I get all the s's in
there and don't leave an S out and
you'll see here we've taken our array if
you remember correctly we had 0 1 2 3 4
5 6 7 8 9 10 11 and we've swapped it so
we've gone from a three by four or a
four by three to a three by four
and this really helps if you're looking
at like a huge number of rows and the
data all comes in like let's say this is
your features in row one your features
in row two and this is x y z well when
you go to plot it you send it all of X
in one array all of one another one
array and all Z in another array
it's really important that we can
transpose this rather quickly
this is kind of a fun thing I can
highlight it and do brackets around it
if you remember correctly
because we're in Jupiter it doesn't
matter where we do the print or not
it'll automatically print it for us and
you see if I hit the Run button it comes
up at the same exact thing
okay let's play with the reshape and you
know let's Zoom this up a little bit
here
make that even bigger so you can really
see what's going on and let's play with
the reshape just a little bit more we'll
do b equals
NP dot a range let's do eight
reshape we'll do two comma four
let's go ahead and print B
and then run that
and you'll see we have now the two rows
this is a bit more like so we have four
maybe two rows of four things so this
might be all of our X components and our
y components so we can switch it back
and forth real easy important to note
here whether we do two comma four or in
the case of four comma three this has 12
elements and so however you split it up
it's got to equal 12. so 4 times 3
equals 12 that's pretty straightforward
same thing down here two times four
equals eight if I change this and let's
say I do 2 comma three let's just run
that in and you'll find we get an error
because you can't split 8 up into two
rows by three
we have to pick something that it can
split up and arrange it in so let's go
ahead and run that and just for fun
let's go um
reshape our B again if I can type
reshape our B again and what else goes
into eight well we could do two by two
by two
so we can take this out to three
different dimensions
and then of course if we um because this
is going to come out you as a variable
we can just go ahead and run it
and it'll print it we can also do a
print statement on there just like we
did before and you'll see we have two
different groups of two variables of two
different dimensions so two by two by
two and let's go ahead and assign this
to a variable C equals B reshape and
let's do something a little different
let's roll the axes roll
axes
and we'll take our C and do two comma
one
and if we go ahead and run this it's
going to print that out whoops
hit a wrong button there let's do that
one again and you roll the axis and you
can see that we now have a set of zero
one two three four five six seven we now
have the zero two one three four six
five seven
so what's going on here we're taking and
we're rolling the numbers around and
let's just simplify this we'll just do
it with C comma 1 and run that and so if
we roll a single axis you get 0 1
and then it rolled the four five up and
then we have two three six seven and if
we do two let me see what happens there
and this is one of those things you
really have to play with and start
filling what it's doing
we've now taken 0 2 4 6 1 3 5 7. so you
can see we've now rolled by two digits
instead of rolling the one set up we now
rolled two digits up there and so if we
go back and we do the one
so we've rolled it up 0 1 4 5 and then
we're going to take the 2 in there and
we've rolled the 0 1 2 3 4 5 and 6 7. so
we start rolling these things around on
here there's a lot of different things
you can do on this
what's another way to manipulate the
numbers on your uh numpy
and finally let's go ahead and
swap
axes we'll do c
now let's just go ahead and run that
let's
error on there
that's because it requires a multiple
arguments lift out the arguments so now
we can swap and we get the zero two one
three four six five seven so you can see
everything's been swapped around
so next thing we want to go over is we
want to go over numpy arithmetic
operations
how can we take these and use these let
me just go ahead and put this cell as a
markdown there we go we'll run that so
it has a nice thing all right nice title
on there that's always helpful
and let's start by creating two arrays
we'll do a as an EP NP
range a range nine and let's reshape
this
three by three so by now you should be
saying this reshape stuff and this
should all look pretty familiar we have
our zero one two three four five six
seven eight on there and let's create a
second one B okay this time instead of
doing a range let's do NP array we'll
just create a straight up array
and we'll do an array of three objects
so it's going to be three by one and if
we go ahead and print a b out let me run
that this is actually pretty common to
have something like this where you have
a three by whatever it is in a three by
by three array when you're doing your
math you kind of have that kind of setup
on there
and what we can do is we can go
P dot add a b don't forget we can always
put a print statement on there so if we
add it you'll see that it just comes in
there and it goes okay we're adding 10
to everything and we could actually do
something more oh make it more
interesting 11 10 11 12. so it's change
B's now 10 11 12 and let's run that
and you can see that we have 10 and then
you had 1 plus 11 is 12. 2 plus 12 is 14
13 so 10 plus 3 is 13 11 plus 4 is 15
and 12 plus 5 is 17 and so on we'll put
this back since that's how the original
setup was let's do 10 by 10 by 10 and
run that and run that and get the
original answer and if you're going to
add them together we need to go ahead
and subtract
a b
and we run that
we get minus 10 minus 9 minus 8 just
like you would expect so we have our
subtraction 0 minus 10 is minus 10 and
so on and if you're going to add and
subtract you can guess what the next one
is we're going to multiply
and we'll multiply
a b
and this should be pretty
straightforward you should expect this
if we multiply 10 times 0 we got zero 10
times 10 is 10 and so on
and finally if you're going to multiply
let's
divide
what happens when you do divide a by B
and we run this
we're going to get 0 and this is 0
divided by 10 is 0 1 divided by 10 is
0.1 2 divided by 10 is 0.2 and so on and
so on so that math is pretty
straightforward it just makes it very
easy to do the whole setup and again if
we went this and let's say well let's
change this up up here instead of 10 we
do a hundred
and make this a thousand there we go if
we run that and then we do the add you
can see we got 10 plus 100 plus a
thousand same thing with the subtract
same thing with the multiply
and then you can also see the same thing
here with the Divide so a lot of control
there with your array and your math
again let's set this back to 10 oops
it's right up here wrong section
there we go 10. we'll just go ahead and
run these and get back to where we were
and this brings us to our next section
which is slicing and let's put in our
we'll just make this a
of course it gives us a nice looking
slicing there and the slicing means
we're just going to take sections of the
array so let's create an array in p a
range
let's just do 20.
and if you remember if we do a we have a
0 to 19.
and then we can do a and remember we can
always print these this can always be
put in a print but because I'm in
Jupiter if you're doing a demo in
Jupiter that is it's just so great that
you have all these controls on here so
we could slice 4 on and this should look
familiar because this is the same as a
python and a lot of other different
scripting languages if we do four we go
0 1 2 3 it's the first four in the thing
and the skip summon starts with this one
the first four are skipped then from
there on you can also do the opposite
and go till the fourth one if we run
that we get 0 1 2 3 quite the opposite
on there we can do a single item so we
can pick object number five on the list
run that and five happens to be five
because that's the order they're in and
then this one's interesting because I
can do s equals slice
and let's create a slice here and let's
do two comma 9 comma
yeah let's leave a 2 on there so we'll
create an S Slice on here and then if we
take our array and we do array of s
we're taking our slice in there and
let's go ahead and run that and let's
take a look and see what it generated
here first off we started with two so we
have two at the beginning we're going to
end at 9 which happens to be eight so it
stops before the nine remember when
we're doing arrays in Python and then we
step two so two four six eight we could
do this as three let me run that and you
can see how that changes two five eight
and we could do this as let's leave this
at three and if we change this to 10
oops let's make it 12. there we go
and we run that we have 2 5 8 11. so
that's pretty straightforward it's a
very nice feature to have on here where
you can slice it and take different
parts of the series right out of the
middle so now that we've accessed the
different pieces of our array let's get
into iterating iteration and this is
interesting because my sister who runs a
college data science division
first question she asks is how do you go
through data and she's asking can you do
you know how to iterate through data do
you know how to do a basic for Loop do
you know how to go through each piece of
the data and in numpy they have some
cool controls for that it just sends a
mark down there we go and run it
and it's called the nditter
I'm sure what the ND stands for but ND
it or for iterator
so before we do that though let's create
an array or something we can actually
iterate through we'll call it a equals
NP a range let's do something a little
funny here or funky
and we'll do 0 45 5. I'm not sure why
the guys in the back pick this
particular one was kind of a fun one and
if I run that we do this you can see we
get 0 5 10 15 20 25 30 35 40. that's
what this array looks like and this is
from our slice you could this is just a
slice that's all that is is we create a
slice of 0 45 0 to 45 step five and so
we can do with this we can also do a
equals V shape let's go ahead and take
and reshape this and since there's nine
variables in there we'll do a reshape at
three by three so if we run that oops
missed something there that is the a
that really helps so if we do the a
reshape and we'll go ahead and print
that out we get 0 5 10 15 20 25 30 35
40. and then we simply do 4X in
our numpy ND enter
of a
colon and we'll just go ahead and print
X
and let's see what happens here when we
run through this and we print each one
of those
it goes all the way through the whole
array so it's the same thing we just saw
before we got 0 5 10 15 20 25 30 35 40.
so it prints out each object in the
array so you can go through and view
each one of these
and certainly if you remember you could
also flatten the array and just do for a
in that also and get the same result
there's a lot of ways to do this but
this is the proper way with the ND
iterator because it'll minimize the
amount of resources needed to go through
each of the different objects in the
numpy array
and hopefully you asked this question
when I just did that
and the question is how can I change
this instead of doing each object so
first of all let's go ahead and take my
cell type and Mark that down run it and
so we're going to work on iteration
order C style and F style remember C
because it came from the C programming
and F because it came from the old
Fortran programming so let's give us a
reminder I will do a print a and we'll
do 4 x n n p iterate a but we also want
to do this in a specific order and you
know what I'm a really lazy typer so
let's go back up here
let's see ND iterator missing the ND
part of a and let's do order equals
C we'll print X on there and let's do
that again and this time order
equals
f
iring an order equals F and let's go
ahead and run this
and see what happens here and the first
thing you're going to notice our
original array 0 5 10 15 20 25 30 35 40.
when we do order C that's the default 0
5 10 15 20 and so on and then when you
come down here
you'll see F order f is 0 15 30. so it
takes the first digit of each of the sub
arrays or the second dimension and then
it goes into the second one 520 35 10 25
40. so slightly different order for
iterating through it if you need to do
that so we've covered reshaping we
covered math we've covered iteration
we've covered a number of things the
next section we want to go ahead and go
over
I was going to be joining arrays so we
need to bring them together let me go
ahead and take the cell and make it a
markdown
cell type markdown there we go and run
that so let's work on joining arrays so
we can bring them together and what
different options we have
and let's do we'll do an NP array one
two comma three four
we'll go ahead and print
do oops
first
these Rays aren't that big so let's just
go ahead and keep it all on one line a
so if we run this first array one two
three four whoops I forgot that it
automatically wraps it when you do it
this way so we'll go ahead and keep it
separate
print a there we go and let's go ahead
and do a b
and we'll do five six seven eight and
notice I'm keeping the same shape on
these two arrays depending on what
you're doing those shapes have to match
let's go ahead and print
second array
print
B and run that up
I've missed something up there let me
fix that real quick
and I was reformatting it to go in
separate lines I messed that up there we
go run all right so we have first array
one two three four secondary five six
seven eight
and we'll put a carried return on there
and the keyword we use is concatenate
and if you're familiar with Linux it
usually means you're adding it to the
end on there and we're going to do what
they call a long axis zero so we have
concatenate a b along axis zero let's go
ahead and run that and see what that
looks like
so we have one two three four five six
seven eight so now we have an array that
is four by two has a nice shape of four
by two one here
and if we're going to do it along the
axis 0
you should guess what the next one is
we're going to do it along the one axes
and let's see how those differ from each
other let's just go ahead and run that
and again all we're doing is adding in
the axes equals one so we have our
concatenate we have a b and then axis
one remember a couple things one these
are the same shape so we have a two by
two same dimensions going in there
you're going to get an error if you're
concatenating and they're not if you
have something that instead of one two
is one two three four five six with a
five six seven eight they'll give you an
error on there in fact let's take a look
and see what happens when we do that let
me just take this
one two three three four five and let's
run that and if we come down here oh we
got there it says it says all the input
array Dimensions except for the
concatenation axes M must match exactly
so it'll let you know if you mess up
that's always a good thing let's go
ahead and take this back here and let's
go ahead and run that
and so we have our zero axes which is
one two three four five six seven eight
and bring them together and you'll see a
very different setup here when we do it
along the axis one we end up with
instead of uh four by two we end up with
a two by four one two five six three
four seven eight and this is changing
which axes we're going to go ahead and
concatenate on what I find is when
you're talking about the concatenate or
the joining arrays you really got to
play with these for a while to make sure
you understand what you mean by the axes
it looks very intuitive when you're
looking at it actually 0 1 2 3 4 5 6 7 8
axes one is then splitting in a
different way one two five six three
four seven eight
when you're actually using real data you
start to really get a feel for what this
means and what this does
so if we're going to do that let's go
ahead and look at splitting the array
and I'll do that in a markdown and run
it there we go so you have a nice little
title there
and we'll go ahead and create an array
of nine
let's do NP split
we'll do a and we're going to split it
by three
let's just see what that looks like so
if we split it we get an array 0 1 2 3 4
5 we get three separate arrays on here
now remember we're looking at let me
just print a up here
so we're looking at zero one two three
four five six seven eight and then we
can split it into three separate arrays
and let's take this we're going to do
this right down here let's move the a
split down here instead of the three
let's do four comma five put that in
Brackets
and so we do it this way we have zero
one two three four five six seven eight
and that's kind of interesting I wasn't
sure what to expect on that but we get
when you split an a by four comma five
you get a totally different setup on
here as far as the way to split the
array
and to understand how this works I'm
going to change the five to a 7. and
this will visually make this a little
bit more clear
so we had four and five I went zero one
two three four five six seven eight and
you see the markers four and five when
we do four and seven I get zero one two
three four five six seven eight and so
what you're looking at here is the first
marker is this is going to go to four
so there's our first split at the four
the marker of four and then the second
split is going to be at position seven
and this is the same thing here a four
position five that's why we're splitting
it in those two sections we could also
do it seven let's just see what that
looks like run and you can see I now
have zero one two three four five six
seven eight
so we could split in all kinds of
different ways and create a different
set of multiple arrays on here and split
it all kinds of different ways and
before we get into the graphs and other
miscellaneous stuff
let's go ahead and look at resizing the
array I'm going to take the cell and set
the cells a markdown and run it
give us a nice title there and we'll do
an array an in Peru of one two three and
four five six here I'm just going to
just print
let's go print a DOT shape then we'll go
ahead and run that oops hit a wrong
button there
hit the comma instead of the dot so we
have a shape of two comma three here
this is important to note because when
we start resizing it it's going to mess
with different aspects of the shape
and so we'll go ahead and do a print
scoop in for a blank line there we go
let's do b equals
NP dot resize
we're going to resize a
and let's resize it with three by two
and then we'll just go ahead and
print
B
and print the period shape not a comma
I'll run that
oops forgot the quotation marks around
the end I'll go ahead and run that and
let's just see what that looks like so
we have one two three four five six
original array with the shape of two
three
and then we want to go ahead and resize
it by three two and we end up with one
two three four five six and we end up
with the shape of three two that
shouldn't be too much of a surprise you
we got six elements in there we can
resize it by two three was the original
one and then we're actually just
reshaping is how that kind of comes out
as when you resize it like that
but what happens if we do something a
little different
and let's go ahead and just take this
whole thing and copy it down here so we
can see what that looks like
and instead of doing three two remember
last time I did the um to reshape it I
messed with the numbers and it gave me
an error well when you resize it you
don't have to match the numbers they
don't have to be the same dimensions so
we can instead of going from a 2 3 to a
3 2 we can resize it to a three three so
let's take a look and see how it handles
that and we come down here to 3 3 we end
up with one two three four five six and
it repeats one two three
so it actually takes the data and just
adds a whole other Block in there based
on the
and repeating it
all right now at this point you know
we've been looking at tons of numbers
and moving stuff around we want to go
ahead and do is get a little visual here
because it um
certainly you can picture all the
different numbers on there but let's
look at the histogram let's put this
into a histogram let me go ahead and run
that
and to do that we're going to use the
Met plot Library so from matplot library
we're going to import pipelot as PLT
that's usually the notation you see for
pi plot so if you ever see PLT in a code
it's probably Pi plot in the matplot
library
and then the guys in the back did a nice
job and gals too guys and gals back
there our team over at simply learn put
together a nice array for me 2087 4 40
53 with a bunch of numbers that way we
had something to play with and what we
want to do is we want to do plot the
histogram I remember a histogram says
how many times different numbers come in
and then we're going to put them in bins
and we have been 0 to 20 to 40 to 60 to
80 to 100.
you might in here with the matplot
library they call them bins you might
hear the term buckets or they put them
in buckets as a really common term then
we want to give it a title so the way it
works is you do your plt.hist for
histogram your PLT title and your PLT
show and we're doing just a single array
in here in the numpy array of a and
let's go ahead and run this piece of
code
take it a moment to come out there's his
finger size so it's generating the graph
and you can see we have and let's just
take a look at this and go down a size
there we go okay so now we can see we're
taking a look at here so between 0 and
20 we have three values so we have a 20
here we have a four and a 11 and a 15.
zero one two three it's actually Four
values but they start at zeros remember
we always count from zero up and from 20
to 40 we got 20 this is 142.
2 3 4
5 6.
and so you can see in the histogram it
shows that the most common numbers
coming up is going to be between the 40
and 60 range least common between the 80
and 100 this looks like a age
demographics is what this looks like to
me and you can see where they would have
put it in the buckets of different age
groups which would be a nice way of
looking at this histograms are so
important and so powerful when you're
doing demos and explaining your data so
being able to quickly put a histogram up
that shows what's common and how it's
trending is really important and using
that with a nubby is really easy and you
know what let's take the same data and I
want to show you why we do bins or why
we have buckets of data I'm used to
calling it buckets why we have bins
let's do it instead of by 20 let's do it
by tens and see what happens
and what happens when you do it by tens
is you miss out on the you can see a
nice curve here on the first one and on
the second one it looks like a ladder
going up and a plummet a letter going up
and a plummet and a ladder going down
so the first would be more indicative of
an age group and the second one would be
what she would get if you divide it
incorrectly you wouldn't see the natural
trend
I don't know what this would be maybe
how much food they eat hopefully not
because I'm in 50s I'm right in the
middle there which means I eat a ton of
food compared to everybody else but it's
some kind of democrat maybe it's mental
maybe it's knowledge because we we hit a
certain point then we start losing our
marbles start leaking out or something
so you start off knowing something and
then as you get older you grow more but
you can see here we lose that you lose
that continuity in the thing if you
split the histogram into too many bins
or too many buckets
and if you actually plotted this by the
individual numbers it would just be a
bunch of dots on
it wouldn't mean a whole lot
and reflected graphs there turns out
there are a ton of useful functions in
numpy
I'm sure there's even new ones that are
aren't going to be in here but let's
just cover some important ones you
really need to know about if you're
using the numpy framework
one of them is Lane space function this
is generating data so you have a line
space we have one three ten and when we
do that we end up with 10 numbers so if
you count them there's 10 numbers
they're between 1 and 3 and they're
evenly spaced we get 1 1.222 but these
are all there's a total of 10 here and
it's right between the one and three
range
that could be there's a lot of uses for
that but they're probably more obscure
than a lot of the other common numpy
arrays set up
a real common one is to do summation so
we'll do summation where you do in this
case we create a lumpy array of one of
two different arrays One Two Three or
two different dimensions one two three
three four five and we're going to sum
them up under axes zero which is your
columns and if you remember correctly
columns is the one plus three two plus
four three plus five so we have three
columns and if we change this we'll just
flip this to one
we get two numbers so we get one two
three all added together which equals
six and three plus four plus five which
equals twelve we'll set this back this
back to zero
there we go since it says we're looking
at zero
and these probably could have been some
of these could be used in our math
section square root and standard
deviation two very important tools we
use throughout the machine learning
process and data science and simply we
take the NP array we have again the one
two three four five six three four five
I don't know I need to keep recreating
it I probably could have just kept it
but we can take the square root of a so
it goes through and it takes a square
root of all the different terms in a and
we can also take the standard deviation
how much they deviate in value on there
and there's a gravel function we can run
that
and NP array is X we're going to do x
equals a we changed it from a to x x
equals Ravel and this sets it up as
columns so we have one two three four
five this is all columns on here very
similar to the flattened function
so they kind of look almost identical
but we also have the option of doing a
ravel by column and then another one is
log so you can do mathematical log on
your array in this case we have one two
three and we'll find the log base 10 for
each of those three numbers there's a
couple of them they don't you can't just
do any number here after log but there
is also log base 2. log base 10 is
pretty commonly used on here I'm going
to run that there we go
before we go let's have a little fun
let's do a little practice session here
on some more challenging questions so
you start to think how this stuff fits
together right now we just looked at all
the basics and all the basic tools you
have so let's do some numpy practice
examples and let's start by figuring out
how do you plot say a sine wave in numpy
what would that look like and so in this
project we wouldn't have to do this
because I've already run these but we'd
want to go ahead and import our numpy as
NP and import our matplot library
pipelot as PLT so we get our tools going
here and then we'll break it into two
sections because we need our x Y
coordinates in here so first off let's
create our x coordinates and our x
coordinates we're going to set to an a
range
and we want this error a range since
we're doing sine and cosine it's going
to be between 0 and 0.1
and then we use our NP and we actually
can look up numpy stores Pi so you have
the option of just pulling Pi in there
directly from numpy it has a few other
variables that it stores in there that
you can pull from there but we have
numpy pi and we generate a nice range
here and let's go ahead and run this and
just out of curiosity let's see what x
looks like I always like to do that so
we have
0.1.2.3.4 so we're going zero to in this
case 9.4 3 times numpy Pi Pi is like
three point something something
something that makes sense it should be
about nine and we're doing intervals of
0.1 so we create a nice range of data
and then we need to create our y
variable and so Y is going to Simply
equal NP or numpy dot sine of X and then
once we have our X and Y and if we print
let's go and just print y see all that
we'll do this let's do this so it looks
print
X print y
so we basically have two arrays of data
so we have like our X axes and our
y-axis going on there
and this is simply a
plt.plot because we're going to plot the
points and we'll do X comma Y and then
we want to actually see the graph so
we'll do plot dot show and we'll go
ahead and run that and you see we get a
nice sine wave and here's our number 0
through 9 and here's our sine value
which oscillates between minus one and
one like we'd expect it to then for the
next challenge let's create a six by six
two dimensional array and let one and
zero be placed alternatively across the
diagonals
oh that's a little confusing so let's
think about that we're going to create a
six by six two dimensional so the shape
is six by six two dimensional array and
let one and zero be placed alternatively
across the diagonals
now if you remember from lesson one we
can fill a whole numpy array with zeros
or ones or whatever so we're going to do
NP I'll create a numpy zeros and we're
going to do a six by six and we'll go
ahead and make sure it knows it's an
integer even though it's usually the
default and just real quick let's take a
look and see what that looks like so if
I run this you can see I get six by six
grid so six by six zero zero zero zero
zero
now if I understand this correctly when
they say ones and zero placed
alternatively across the diagonals they
want the center diagonal maybe that's
going to stay zero all the way down
and then the next diagonal would be ones
all the way across diagonally and then
the next one zeros the next one ones and
the next one zeros and so on hopefully
you can see my mouse lit up there and
highlighting it so let's take a little
piece of code here
and we'll do Z One colon colon two comma
colon colon two equals one and wow
that's a mouthful right there so let's
go ahead and run this and see what
that's doing and so what we're doing is
we're saying hey let's look at in this
case Row one there's one and then we're
going to go every other row two so we're
going to skip a row so skip here skip
here skip here so we're going down
this way and we're going every other row
going this way it's hard to highlight
columns so you can see right here where
the that we're not touching each row is
like this row right here is not being
touched okay so we're going to start
with Row one and then we're going to
skip a row and another one and so we're
going every two rows and then in every
two rows we're looking at every two
starting with the beginning that's what
this thing blank means so we're going to
start with the beginning and we're going
to look at all of them but we're going
to skip every two so starting with Row
one
we look at all the rows but we do we do
it by two steps so we go one Skip One
you know one Skip One One skip one one
if you lift this out and do every one
this would just be once in fact let's
see what this looks like if I go like
this
and run it you can see that I guess get
once
so
this notation allows us to go down
each row row by row and we're going to
do every other row set up on there and
so if we're going to start with Row one
we also
control Z
try that there we go we'll start with
row zero again we're going to go each
row step two so we'll start with row
zero and we'll go every other row and
this time we'll start with one column
one and again we go every other one
going down
step that's what that step two is
skipping every other one we're going to
set that equal to one so let's see what
that looks like and you can see here we
get our answer we get zero one zero zero
but it has the ones going in diagonals
on every other diagonal and zero on
every other one
a little bit of a brain teaser that one
trying to get that one to work out so
you can see how you can arrange your
rows and here's your step in your
different axis on there
and then the next one is find the total
number and locations of missing values
in the array the first challenge is to
create some missing numbers so let's
create our array Z we're going to do a
numpy DOT random dot Rand 10 comma 10.
and before we do the second part let me
just take the second part out
and let's just see what that looks like
so let's run that and there we go so we
have a 10 by 10 random array it randomly
is picking out numbers
and next we want to go ahead and take
our random integer size equals five and
then we're going to do a random random
10 size equals 5. so in the Z we're
going to select a number of random
spaces here and set them equal to null
value
and let's go ahead and run that so you
can see what that looks like and if we
look at the array
we've created one two
three four this should be a fifth one in
here my eyes may be filling me so we've
created a series of oh because zero zero
to five zero one two three four so we
got five there are different null values
on here and
this is kind of a neat notation to
notice that we can generate
random integers size equals five so this
generates five by five miniature grid
inside of this to tell it where to put
the nands at so that's kind of a cool
little thing you can do
and we want to look up and see how many
null values are in there
and this is simply just NP is Nan of Z
simple so if it is nand then we want to
sum it up so we're going to sum up all
of the different null values on there
now let's do one one more feature in
here which is really cool
let's go ahead and print the indexes so
NP argue where NP is
Nan of Z so we're going to create our
own another NP array and let's run this
and we'll see here there comes up with
the four indexes so we didn't count four
of them up there
it tells you where they are one nine two
zero four six five four
and then let's go ahead and run this
again run run there we go this time I
got five let's get for random numbers
another fun one that I always like to do
it's very similar because we have NP is
nand z dot sum so we're summing the
number of nands and we can get the
indexes and you can reshape the indexes
but you can also just do we'll do an
inds where NP is Nan of Z
and let's just print let's print that
print inds let's see what that looks
like
and it's very similar we have we have
zero one three zero six three eight six
nine three if I have split it into two
different arrays
so we have our X and our y kind of
coordinates going there and what I can
now do is I can now do Z inds
equals and at this point you can also
instead of getting the sum you can get
the means or the well the numbers and
that kind of thing you have or the
average as it is so be one thing you
could do and you can pick up the average
that's very common in data science to
get the average and just use that for a
value
but we'll go ahead and just set it to
zero and then let's go ahead and print
our Z and run that and you can see we
come down here we have wherever there
was a null value it is now zero and you
can set this to whatever you want this
is another way to replace data or help
clean data depending on what it is
you're doing
so wow we covered a lot of stuff so a
quick rehash going over everything we
went into there we looked at array
manipulation changing the shape
how to switch that around we even had
the flatten down there which remember we
have another command lower that's
similar we could change the order by F
remember F stands for Fortran very
strange connotation but there's C and F
C as the standard and F switches it to a
different order
to be honest I usually have to look it
up because I almost never use f but when
you need it you're like oh my gosh it
was the other order just do a quick
Google so we talked about reshape making
sure that the dimensions are the same
you don't want to have like something
that has 12 objects in it and reshape it
to C 11 and 5 because it doesn't work it
doesn't divide into 12. we can transpose
so we can switch them so we can go from
a four by three to a three by four Oops
I did that the other way around
three by four to four by three
we covered reshaping the array we did
the roll the axes you can do some weird
things with swapping and rolling axes
and transposing the numbers
we dug a little bit into the arithmetic
so we talked about adding we talked
about subtracting multiplying dividing
and you know at this point it's so
important we just look up the numpy
mathematics and you can see here they
have just about everything your
trigonometry
uh your hyperbolic functions rounding
sums products differences there are so
many all these different miscellaneous
mathematical connotations so you know
Google it go to the main numpy page and
look at the different setups you can do
on there so we covered that and we did
slicing how to break it apart we did
iterating over the array we covered
joining arrays and how to concatenate
remember concatenate just means add on
to it so in this case how are you adding
B onto a is how you'd read that from
Linux you should catch the concatenate
because that's used regularly there
splitting the array we talked about how
to split the array in different ways so
you can split it in Array of arrays all
kinds of different ways to split the
array up how to resize it and remember
resize does not have to have the same
shape but if you resize it it will take
the data and begin at the beginning and
add new rows on if the size is bigger if
it's smaller it truncates it it just
cuts the end off
we looked at how to do a histogram and
how to plot that
we mentioned buying buckets or bins as
they call them in pi plot and then we
covered a lot of other useful functions
in numpy we talked about the line space
setup for doing
um
numbers in a series how to sum the axes
up again that's part of the mathematical
formulas there that we looked at there's
a sum there's also means and median all
of those you can compute in numpy and
you can also do the square root and
standard deviation
the Ravel function very similar to the
flat
to be honest I almost always just use
the flat but you know the Revel has its
own kind of functionality that it does
and then we went into some numpy
practice examples we challenge you to
create a sine wave in numpy and how to
do that we're kind of looking for that a
range remember how we do the a range and
you can
have your beginning value your n value
which they did is three times pot number
pi and we're going to do intervals of
0.1 and then y just equals the numpy
sine of x there's our math from the math
page we were just looking at remember
that's right at the top
and finally we went down here we had
this kind of a little brain teaser how
to do diagonal zeros and Ones playing
with the different connotations of Z of
the numpy array
and then we did a random size and we
played a little bit with how to with the
null values playing with null values
if you're doing any data science you
know null values are like a headache
what do you do with them big sets of
data you get rid of them small sets of
data you have to factor something in
there like figure out the average or the
median there and then replace it with
that
pandas really is a core python module
you need for doing data science and data
processing there's so many other modules
that come off of it there's actually
sits kind of a numpy so if you already
had our numpy array hopefully you've
already gone through the numpy tutorial
one and two So today we're going to
cover what is pandas we'll discuss
series we'll discuss basic operations on
series and then we'll get into a data
frame itself basic operations on the
data frame file related operations on a
data frame visualization and then some
practice examples roll up our sleeves
and get some coding underneath there and
let's start with just some real general
what is pandas pandas is a tool for data
processing which helps in data analysis
and provides functions and methods to
officially manipulate large data sets
now this is a step down from say using
spark or Hadoop in Big Data so we're not
talking about Big Data here but we are
talking about pandas I mean there is
some connections there's a like an
interface going on with that so there is
availability but you really should know
your pandas because if you're working in
Big Data you'll know there's data frames
well pandas is a data frame primarily it
has a couple different pieces we'll look
at here and if you've never worked with
data frames before a data frame is
basically like an Excel spreadsheet you
have rows and columns you can access
your data either by the row or the
column when you have an index and
different that kind of setup and we'll
dig more into that as we get deeper into
pandas but think of it as like a giant
Excel spreadsheet that's optimized to
run on larger data on your computer and
then I said it that it's a data frame so
the data structures in pandas are series
one-dimensional arrays and then we have
data frame two-dimensional array and it
really centers around the data frame the
series just happens to be part of that
data frame and here's a closer look at a
pandas series series is a
one-dimensional array with labels it can
contain any data type including integer
strings floats python objects and more
so it's very diverse if you remember
from numpy we studied they had to be all
uniform not in pandas and pandas we can
do a lot more and pandas actually kind
of sits on numpy so you really need to
know both of those if you haven't done
the numpy tutorials and you can see here
we have our index one two three four
five and then our data a b c d and e
very straightforward it's just two
columns and we have a nice index label
and a column label for the data and then
a data frame is a two-dimensional data
structure with labels we can use labels
to locate data and you can see here we
had if we go back one we had our index
one two three four five so in each one
of these series they would share the
same index over there the row index so
you have your row index DF dot index and
then you have a column index df.columbs
and this would look like I said this
would be really familiar if you've done
any work with spreadsheets Excel so it
kind of resembles that this does make it
a lot easier to manipulate data and add
columns delete columns move them around
same thing with the rows so you have a
lot of control over all of this now
we're of course going to do this in our
Jupiter notebook you can use any of your
python editors but I highly suggest if
you haven't installed Jupiter and
haven't worked with it it is probably
one of the best ways for easily
displaying a project you're working on I
skip between a lot of different user
interfaces or Ides for editing my Python
and it's just simply jupiter.org
j-u-p-y-t-e-r dot org and then I always
let mine sit on anaconda anaconda.com
and just real quick we'll open that up
for you oops offline mode don't show me
that again but you can see here that I
have different tools that I can actually
install in my anaconda including the
Jupiter notebook which comes by default
and then I have access to the
environments and again that's
anaconda.com named after the very large
one of the largest world's largest
snakes and then Jupiter notebook in this
case jupiter.org and when we're in in
our I'm going to go in here to our
Jupiter notebook and we're going to go
ahead and just do new and a Python 3 and
this will open up a Python 3 Untitled
folder so diving right in let's go ahead
and give this a title pandas tutorial
and we'll go up to cell and we'll change
the cell type to mark down so it doesn't
execute it as actual code one of those
wonderful tools when you have jupyter
notebooks you can do demos with this and
let's go ahead and import pandas and
usually people just call it PD that has
become such a standard in the industry
so we'll go ahead and run that now we
have our pandas has been imported into
our jupyter notebook
and then oh we can go ahead and let me
do the Control Plus since it's Internet
Explorer I can enlarge it very easily so
you have a nice pretty view oops too big
there we go and whenever you're working
with the new modules good to check your
version of the module in pandas you just
use the in this case PD dot underscore
underscore version underscore underscore
that's actually pretty common in most of
our python modules there's different
ways to look up the version but that's
one of the more common ones and we'll go
ahead and run that we get
0.23.4 and if we go to the pandas site
we see
0.23.4 is the latest release and of
course a reminder that if you're going
to an environment you need to install it
so you'll need to do pip install pandas
if you're using the PIP installer we're
going to close out of that
and the first thing we want to do is
we're going to work with series a lot of
the stuff you do in series you can then
do on the whole data set we need to do
what create one we need to manipulate it
take pieces of it so query it query it
delete so you can delete different parts
of it so we want to do all those things
with the series and we'll start with the
series and then almost all the code in
fact all the code does transfer right
into
the actual data table so we go from a
series of a single list of one column
and then we'll take that and we'll
transfer that over to the whole table
and we'll start by creating let's put
them there we go creating a series from
list
and let's just call this ARR equals and
we'll do 0 1 2 3 4. if you remember from
our last one we could easily do R equals
range of five which would be zero to
four what we'll do R equals zero to four
and we'll call this S1 and we'll go PD
and series is capitalized this one
always throws me is which letters do you
capitalize on these modules they're
getting more and more uniform but you
got to watch that with python and we're
just going to go ahead and do ARR so
we're just going to take this python
list and we're going to turn it into a
series and then because we're in Jupiter
we don't have to put the print statement
we can just put S1 and it'll print out
this series for us now let's go ahead
and run that and take a look
and you'll see we have two rows of
numbers so the first one is the index
now it automatically creates the index
starting with zero unless you tell it to
do differently so we get 0 index row 0
is 0 1 1 2 2 3 3 4 4. and because it's a
series it doesn't need a title for the
column there's only one column so why
title it
and this also lets you know that it's a
data type of integer 64. so we print
this out this is our series our basic
series we've just created now let's do a
second series
PD and we'll use the same
data list and let's go ahead and do
order we'll give it an order equals oh
let's do it this way
let's go index equals order
and it helps if we actually give it an
order so we'll do order equals and let's
do one two three four five so instead of
starting with zero we're going to give
it an order starting with one we're
going to run that and we'll go ahead and
print it out down here S2
and we'll see that we now have an index
of one two three four five and that
represents zero one two three four in
the series and we're still data type
integer 64. and very common is you're
missing with numpy arrays is we can
import our number as NP remember that
from our numpy tutorials we can go ahead
and create a numpy at a random with the
random numbers of five and let's just
see what that end looks like so we can
see what our numpy looks like so we have
some nice random float values here 2.33
so on and this from our last tutorial
the numpy tutorial one and two and
instead of calling it order let's call
it index and we're going to set our
index equal to a b c d and e I want to
show you that the index doesn't have to
be an integer so it can be something
very different here and then let's go
ahead and create our we'll just use S2
again and here's our NP for numpy
series capital s
and N is our NP for number
PD for pandas there we go switching my
anachronisms so we have pd.series of N
and we want to do our index equals our
index we just created
and then let's go ahead and see what
that looks like S2 is a printed and
let's run that and we can see here we
have a nice Series going on a b c d and
e for our indexes so instead of it being
0 1 2 3 or 4 we can make this index
whatever we want and you can see the
numbers here going down that we randomly
generated from the numpy array so we
used numpy to create our Panda Series
right here
and so continuing on with creating our
Series this one I use so often we create
a series from a dictionary so we have
our dictionary in this case we went
ahead and did a of one B is 2 C of three
D4 EF5 so each one of those is a key and
then a value and then we're going to use
oh let's use S3 equals PD for pandas
series and then we want to go ahead and
just do D in here
print out S3 here and let's go ahead and
run this and you can see we got a is one
B is 2 C is three D is four e is 5. and
it's still of integer 64 Because the
actual data is one two three four five
and it's all integers 64 type 64. and
the last thing we want to do in the
creating section of our series is to go
ahead and modify the index because we're
going to start modifying all this data
so let's start with modifying the index
of the series and if you remember let's
do a print this time S1
I'll go ahead and run this and the
reason I did print is because it only
prints out the last variable so if I put
S1 up here and we're going to do another
variable back down lower it won't print
the first one just the last one and
we're going to go ahead and take S1
the index and we're just going to set it
equal to a new index and obviously the
number of objects in our index has to
equal the number of objects in our data
and then because it's a last variable we
can go ahead and just do an S1 and let's
run that and you can see how we went
from 0 to 0 0 1 2 3 4 as our index we've
now altered it to a b c d and e so this
would be much more readable or might be
representational of a larger database
you're working with
So Cool Tools we've covered creating
database based on our basic array python
array we've showed you how to reset the
index
that we showed you how to use a numpy
array so you can put a numpy array in
there it's all the same you know
pd.series numpy array and then we can
set the index on there and the same
thing with the dictionary so it's very
versatile how it pulls in data and you
can pull in data from different sources
and different setups and create a new
series very easily in the pandas and
then we looked on changing your index so
now we have a new index on here
and then we want to go ahead and do some
selection let's do some basic
slicing most common thing you'll
probably do on here and we'll just do S1
this notation should start to look
really familiar again this is going to
put an output so I'd usually it doesn't
change S1 this just selects it so we
might do a equals S1 and then print a
and you'll see that it just looks at the
first three zero one two and we can do
the same thing by not having the a in
there I'll go ahead and take that out
but it's just a reminder that it's not
actually changing S1 it's just viewing
S1 so simple slicing on here and we can
likewise do an append oops before we do
a pin let's just do a quick kind of fun
one we'll do two minus one and you'll
see it covers everything but the E of
course you can do minus two on this side
so one another way to select it is to go
how far from the end and likewise we can
do a 2 here c d e to the end so it
starts at the second one and another way
we can do this is we can do a minus 2
over here
and that looks at just the last two in
the slice so you can see how easy it is
to slice the data and of course there's
no reason to do this but you could
select all of them if you wanted to view
all of them on there oops 32 there's not
32 so it's just going to show the first
three there we go and then we can also
append so I can take and oh let's create
another series and append one to it and
if you remember we had S3 there's our S3
and we have our S1 I'm going to do S1
and let's go ahead and do oh let's call
it S4
equals S1
a pin S3
so we're just going to combine those two
into S4
and if we go ahead and print S4 on here
you'll now see that we have a b c d e a
b c d e zero one two three four one two
three four five because we started the
data at one
so very easy to compend one series to
the next and if we're going to append
one series to the next we need to go
ahead and drop or delete one and drop is
a keyword for that and let's just do e
our index e and so if I run this
you'll see that it'll print it out and
ABCD there's no e and remember all these
changes if I type in S4 again you'll see
that S4 still has e in it so this change
does not affect the series unless you
tell it to so I'd have to do like x S4
equals s4.drop e and there's another way
to do that which we'll show you later on
let me just cut this one out
there we go all right so we've covered
all kinds of Cool Tools here we have
appending we have slicing we did all the
creating stuff earlier as you can see
here on the setup how easy it is to
manipulate the series
so next what we want to get into is we
want to get into
operations that happen on the series let
me go ahead and change this cell to
mark down there we go and
so series operations what can we do with
the series and let's start by creating a
couple arrays we'll call it array one
and we'll do 0 through 7 and array two
six through six seven eight nine five I
don't know let me threw the five on the
end but let's go ahead and run those so
those load up into Jupiter and we'll do
this a little backwards we're going to
do S5 equals a panda series of array two
so I'm doing this in reverse and then
when we do S5 you'll see that we have
zero to four it automatically assign the
index
67895 for our series and let's go ahead
and do the same and we'll call this S6
and we'll set this equal to PD series
for our first array and if we do an S6
down here to print it out
we'll see something similar I got zero
through six zero one two three four five
seven for the data so those are two
series we just created series six five
and six
and one of the first things we can do is
we can add one series to the next so I
can do S5 dot add S6 and let's see what
that generates and just a quick thing if
you've never used pandas what do you
think is going to happen with the fact
that this only has five different values
in it and this one has seven values
so let's see what that does and we end
up with 6 8 10 12 9 and it goes oh I
can't add this there's nothing there so
it gives us a null return very different
than the numpy that would have given you
an error this instead tells you there's
no value here because we couldn't
generate one so we can easily add S5 dot
add S6 and likewise we can do S5 Dot
sub for subtract S6 and we'll run that
and on the add the subtract and you
guessed it we're going to do multiply
and divide next again you can see
there's the null values where it can't
subtract the two because there's no
values there to subtract we can also do
S5 multiply mul they're all three
letters on these that's one of the ways
to remember how they figured out the
code for this so remember these are all
three letters mole we'll go ahead and
run this and you can again you can see
how they're multiplied together and then
we can also do the S5 div three letters
again S6 and run that
and you'll see here this goes to
Infinity because we have 0 in the wrong
position so it actually gives you a
whole different answer here that's
important to notice and then in the null
values because there's no data and it
can't actually produce an answer off of
null off of missing data and since we're
in data science let's do S6
median so let's look at the median data
which is simply median sorry for those
who are following the three letters
because median is not three letters and
you can see an S6 is 3.0 and let's do a
print here and we'll do median
or average S6
and let's print Max
S6 and just like median there's max
value and if we're going to have a max
value we should also have a minimum
value so let's pop in minimum
we'll go ahead and run this and you're
starting to see something that would be
generated like say an R where you're
starting to get your different
statistics we have a medium value of
three max value of 7 and a minimum value
of zero and what it does when it hits
these null values if there is no values
in there because we could still do that
we could actually you know what let's go
up here and do
let's pick this one where we multiplied
let's go s seven equals
I'm going to print the S7 just so I keep
it nice and uniform so I still have my
S7 down there and run it and then I want
to take the S7
because S7 now has null values and an
Infinity value and let's see what
happens
this is going to be interesting because
I want to see what it does with infinity
and we end up with a median of six
maximum of 27 and minimum of zero which
is correct it drops those values so when
it gets to there and it doesn't know
what to do with them it just drops those
values and then it computes it on the
remaining data on there so that's
important to know when you're making
these computations you're looking at Min
and Max and median
you're not going to know that there's no
values unless you double check your data
for the null values that's a very
important thing to note on there so just
a real quick
review on there we've done our created
our PD series and we've gone ahead and
done addition subtraction multiplication
division all those are three letters so
sub Min div add and then we looked at
median maximum and minimum so we're
going to go ahead and jump into the next
big topic which is to create a data
frame so now we're going to go from
series and we're going to create a
number of series and bundle them
together to make a data frame
there we go cell type markdown and
so we have a nice title on there it's
always good to have a good title all
right so our first data frame we'll jump
in with some stuff that looks a little
complicated we'll break it down first
I'm going to create some dates and you
know what let's just go ahead and do
this I want you to see what that looks
like what I'm creating here I've created
a series of dates PD date range and
we're going to use these for the index
okay so when you look at this you'll see
that it's just basically it comes out
kind of like a basic python list or
numpy array however you want to look at
it with our different dates going down
and we've generated six of them and it's
going to have whatever time it is right
now on your on the thing for the date
for the time that's that time stamp
right there and then you'll see we have
11 19 2008 11 20 11 19 and looking into
the future there so that's all this is
is generating a series of dates that
we're going to use as our index and this
is a pandas command so we have a date
range which is nice that's one of the
tools hidden in there in the handles
that you can use and next we're going to
use numpy to go ahead and generate some
random numbers in this case we'll do the
np.random.random and six comma four you
can look at this as rows and columns as
we move it into the pandas and of course
you could reshape this if you had those
backwards on your data but we want the
six to match the rows and we have six
periods so our indexes should match
along with the rows on there and then
you know before we do the next one let's
go ahead and just print out our numpy
arrays and see what that looks like here
we have it one two three four by one two
three four five six four by six so it's
a nice little setup on there and since
working with data frames can be very
visual let's give our columns we have
four columns and we're going to give
them names A B C and D so now we have
columns on there also and then let's put
this all together in a data frame and we
can actually you know what let's do this
since I did it with everything else
let's go ahead and do columns and you
can see there's our columns on there
and we'll go ahead and do df1 equals
pandas Dot data frame and note that the
D and the f are capitalized series it
was just the S and I always highlight
this because you don't know how many
times these things get retyped when you
forget what's capitalized on there it's
a minor thing you'll pick it up right
away if you do a lot of it and the first
thing we want to do is we want to go
ahead and take our numpy array because
we're going to create our data frame off
of is the numpy array and then we want
our index equal to our dates so there's
our index in there and then we also have
columns equals columns and then finally
let's see what that looks like now
remember we had all the different data
that just looked like a jumble of data
we have our column names and everything
else our numpy array kind of just a
jumble array over there four by six you
can sort of read it but look how nice
this looks I mean this is you come into
a board meeting you're working with your
shareholders this is pretty readable
this is you know this is our date this
is our a b c d whatever it is maybe it's
one of these dates has your leads
closures lost leads total dollar made
you know whatever it is if it's in a
business maybe it's measurements on some
scientific equipment whether searching
material you know where this is like
high of the temperature low of the day
humidity of the day whatever it is so
you can see that we can really create a
nice clear chart and it looks just like
a spreadsheet you know we have our rows
and we have our columns and we have our
data in there now this one I use all the
time if we're going to create we can
create it like you saw here with our
numpy array very easy to do that and
reshape it you can also create it with a
dictionary array so here we have some
data and let me just go down a notch so
you can see all the data on there we
have an animal in this case cat cat
snake dog dog cat snake cat dog we have
the age so we have an array of Ages we
have the number of visits and the
priority was it a high priority yes no
and then we're going to take that we're
going to create some labels we have a b
c d e f g h i and what I want you to
notice on this is we have a title animal
and then we have basically a python list
and these lists they don't necessarily
have to be equal because we can have
non-data you know in p.net numpy Array
null value but we want to go ahead and
create labels that are equal to the
number in the list so a the first cat B
the second cat C the snake D the dog and
so on so we'll go ahead and create our
labels which we're going to use as an
index and we'll call this DF let's do it
this way we'll call this df2
equals PD for pandas data frame and then
we have our data just like we did before
and then we have our index equals
labels
and if we're going to go from there
let's go ahead and print it out so we
can see what that looks like df2 so
let's go ahead and run that and another
again you have a nice very clean chart
to look at we've gone from this mess of
data here to what looks like a very
organized spreadsheet very Visual and
easy to read animal age visits priority
and then a through J cats and all the
different animals so on and so on and
then when you do programming a lot of
times it's important to know what the
data types are so we can simply do df2
D types
and if we run that we can see that our
animal
is an object because it's just a string
but it comes in as an object age is a
float 64 integer 64 and then priority
again is just an object
and exploring this this one's very
popular let's go df2 head and if we
print that out the df2 head Returns the
first five and we can change this you
don't have to do five you might want to
just look at the top two maybe you want
to look at let's see now let's do six so
maybe we'll look at just the top six in
the database in your data frame and you
can actually this creates another data
frame so I could have a DF 3 equal to
df2 and this now takes the df2 and just
the first six values so if we do df3
run get the same answer
and if we do it the head of the data we
can also do the tell it's the same thing
DFT you can look at the last we'll just
do the tell which by default does five
the last five and of course you can just
look at the last three of those real
quick just to see what's at the end of
the data and this is the tell I love
doing the tell of one because I'll have
like the index or something like that
and it will just show me the last
whatever the last entry was looking at
stock values and I might want to look at
just the last five days of the stock
values I can do that with the data frame
tail
and some other key things to look up are
the index so we can do df2 dot index
and I want you to notice that this isn't
a call function so if I put the brackets
on the end it'll give me an error
because index is not callable it's just
an object in there so we do df2 dot
index there's also columns
so we can go ahead and let's do uh let's
print this remember the first one is not
going to show unless I print it and then
df2 columns so now we can see we have
our indexes and we have our columns
listed here df2. columns animal age
visits priority that tells you what kind
of object it is or what kind of data
type it is and they're both object and
then finally df2 dot values and again
there's no brackets on the end of
df2.values because this is an actual
object it's not a callable function so
we'll go ahead and run that and it
creates this displays a nice array a
very easy way to convert this back to a
numpy array basically so before I go
into the next section let's just take a
quick look at what we covered so far
with the data frame we came up here we
created our data frame we did it from a
numpy array first setting the columns
and the index the index is setting it up
is the same as when we set up the series
so that should look very familiar so is
the whole format the numpy array the
index dates and the columns columns and
remember in our numpy array we're
looking at row comma column so six rows
four columns is how that reads in the
data frame
and we went ahead and also did that from
a dictionary in this case animal was the
column name with all the date data
underneath that column and then age with
that data visits that data priority that
data and then of course we added our
labels in there for our index so there's
no difference in there but it
automatically pulled the column names
important to know when you're dealing
with the data frame and importing a data
frame this way
and then we did looking up D type we
looked at head and tail looking at your
data really quick we also did index and
columns and values and note these don't
have the brackets on the end
so the next thing we want to do is go
ahead since we're dealing with data
science is we want to go ahead and
describe the data so we have def2 dot
describe to do that and we're going to
manipulate it in just a minute but let's
just see what this generates
and you can see right here we have age
and visits so looking at our data from
up above let me just go all the way up
here animal age business priority
and it does a nice job generating your
age versus visits which has all the data
you have your account your means your
standard deviation your minimum value 25
or in this group 50 75 and your maximum
value so this will look familiar as a
data science setup with your describe
for a quick look at your data Frame data
so let's start manipulating this data
frame moving stuff around and we'll
start with transposing and it is simply
capital T for transpose and when we run
that it flips The Columns and the
indexes so now the indexes are all
column names and the columns are all
indexes animal age visits priority so if
we had come in here with our data shaped
wrong up above where we had a four by
six we can quickly just swap it if we
had it backwards not a big deal and we
can also sort our data so something that
you can't deal which is more difficult
to do with a lot of other packages and
the data frame is really easy to do take
our data frame df2 and we're going to
sort underscore values buy equals age
and so when we run this you'll see the
default is ascending so we have 0.5 to
2.53 and everything else is organized so
if you look at your indexes they've been
moved around because each index it moves
a whole row not just the one piece of
data is not being sorted so a very quick
way to sort by age are different data in
the data frame and in addition to
sorting it we can also slice the data
frame so I could do df2 and this should
look familiar from earlier we'll just do
one to three so we're going to pull out
oops it does help if I use the DF
instead of just D and we're going to
pull up just between one and three so we
have not zero which is a we have B which
is 2 or B which is one and C which is
two so one two and then it does not
include 3 which is the standard in
Python and we can even do something like
this we can combine them which is always
fun because remember this returns a data
frame so if I take df2 dot sort foreign
values and we'll do by equals age this
is just kind of fun and then I'm going
to slice it there we go double check my
typing and run it and now you should see
fa because F A are now one and two on
there
I'm seeing very quickly create a whole
string on here which narrows it you know
that you can sort it then slice it and
do all kinds of fun things with your
data frame we'll just go back to the
original one run there we go and if we
can slice it by row we can also query
the data frame so we can do df2 and this
is a little different because I'm going
to create an array within an array and
in this case we're going to look at oh
let's do age comma
visits so look at the different format
in here we have one to three so we've
done this by slicing by an integer value
and then on here I've done df2 age comma
visits in an array and when I run this
you can see that we get just these two
columns on here we get age and visits so
it's a quick way to select just two
columns or select number of columns
you're working with
and if you stop there we did the slicing
almost identical to slice is I location
which uses the integer location one
comma three there's a push in pandas to
move to this particular setup instead of
doing just a regular slice and that's
because this can be confusing when we
slice one to three and then we select
agent visits so there is a push to go
ahead and move to an eye location which
does the same thing you can see here BC
it's the same as up above there's also a
copy command so we can do df3 equals df2
copy we're just going to create a
straight copy of it and if we do df3
that'll be the same as the df2 on there
so df3 equals df2. copy and then let's
do df3 dot is null so we're looking for
null values
and this will return a nice map and
you'll see that everything is false
except when you go up here under the cat
or H they had a null there and so if we
go to have a couple up here also
underneath of let's see the dog okay
there's a bunch of nulls in here there's
D up here so let's look at D down here
and you'll see false true there it is
there's our null value so we can create
a quick chart of null values you can use
this to do other things we can leverage
that null value to maybe take an average
or something and fill those null spaces
with data and we can also modify the
location so here's our df3 location
and notice this is location not I
location I location has I for integer
location uses the in this case the
variables on the left and what we can do
on here and we're going to set this
equal to 1 5 and some I'll pick a spot
let's go back up here where we had let's
do F A just let's see what were they
looking at oh here we go let's do F and
H and up here f is set to age of 2.0 and
we find out that that's incorrect data
so we go ahead and switch to df3 equal
and then we go and print out our df3
and if we go to F and H it is now 1.5 so
we're just changing the value in the D
of three and this is changing the actual
data frame remember a lot of our stuff
we do a slice and like it returns
another data frame this changes the
actual data frame and that value in the
data frame
so we've covered location and I location
is null making a copy here's our I
location which is equivalent of a slice
and also selecting columns so now we
want to dive just take a little detour
here and let's look at tdf3 means
and this is kind of nice because you can
do this you can either do this by as you
can select a single column here by the
way you can just add the column
selection right here like we did before
so we could have age
look up the mean that just creates a
series if I run that there's our age
but if I take that out instead of
selecting it we can do the whole setup
and it has age and visits so why doesn't
it have priority or animal well those
are not integers so it's really hard
they're non-numerical values so what is
the average I guess you could do a
histogram which probably will look at
that later on but the only two things we
can really look at is age and visits and
we have the average or the mean on the
age is 3.375 and the mean on visits is
1.9 and let's do df3
visits we'll go ahead and steal the
visits again
and remember all those different
functions we looked at for a series well
we can do those here we can do the sum
so if we run that we'll see that these
sum up to 19. we could also look up
minimum if you remember that from before
the minimum is 1 Max so all that
functionality is here I'll just go back
to summing it up and adding it all
together so real quick we've shown you
how to take the series operations and
put them into the data frame and then we
can actually this is interesting one we
can just do df3 sum run and you'll see
the different summations on there it
just combines them I like the way it
just combines the strings on there for
priority and animal we've looked at is
null we've also looked at copying along
with the different slices which we
talked about earlier so let's talk about
strings let's dive into the string setup
on there and let's go ahead and create a
string series string equals PD series
and we just put it right in there we
have a c d a a b a c a popped in a null
value cow and Al I don't know why they
picked Cal and Al in the background
someone must like those animals and of
course we can just do string if we run
that you'll see leave the r out we'll
get an error but if we put it in there
you'll see that we have a simple series
0 a 1C 2D and it automatically indexes
it zero to eight and then we can go
string dot lower so when we're talking
about our data frame in this case or our
data series string in this case we use
the string function Str and we're going
to make it lower and we go ahead and put
the brackets on there and you'll see
that we've gone from capital a Capital C
so on to ABC and Baka CBA Cal Al they
were all lowercase already and of course
if you want to go lower you can also do
upper and we'll go ahead and run that
and you can see we now have ACD AAA
everything's capitalized except for the
null value which is still null all right
so we looked at a few basic string you
can see that string functions upper and
lower we're going to jump into a very
important topic I'm even going to give
it its own header on here because it's
such an important topic what do you do
with missing values Panda has some great
tools for that so we'll dive into those
we'll call we'll work with df4 and if
you remember the DF copy from above
we're just going to make a copy of df3
and let's just take a quick look at the
data we're working with oops df3 forgot
the three on there there we go so here
we have our cat snakes and dogs
hopefully not all in the same container
because that would be just probably mean
to all of them so we made a copy we're
going to be working with df4 and the
reason we made a copy is we want to go
ahead and fill the data and we just
simply do fill in a and then we're going
to give it the value we want to put in
there we'll go at the value 4. so I can
run in here and you'll see now that df4
now has where the N A was is filled with
the value of four same thing down here
a lot of times we'll compute the mean
first so I might do a mean page equals
df4 and then we want to go ahead and do
age
dot mean
and then I'll do something like this df4
I only want to select the age and I want
to fill that
with the mean h i run in there and
you'll see that our df4h now has the
means in there just a quick way of
showing you how you can combine these
let me go back to our original one there
we go and run that and keeping with good
practices df5 equals df3 dot copy
a little printer df5 which should be the
original one
and then on the df5 we can now drop our
missing data
kind of Simply drop in a and we're going
to use how equals any so I'm going to
drop any row that has missing data in it
and you'll see we had D here with
missing data and H and then let's go
ahead and see what df5 looks like when
we do that
there we go and there it is D is gone
and so is H so we create a new data
frame off of this missing those values
now if you have a lot of data dropping
values is a good way to take care of it
because you don't miss some data if you
have not a whole lot of data you're
working with like the iris data set or
something like that or something small
you want to start trying to find a way
to fill that data in so you don't lose
your computational power of the data you
got so just a quick look at processing
null values
or missing values you can fill them
usually with the means some people use
medium or the mode there's different
ways you can fill it one way is means
and we can also just drop those rows
those are the two main things we do with
missing data
here we go uh we're going to cover next
this is I so love data frames for this
file operations it saved me so much time
because they have so many different
tools for bringing data in and saving
data so we're looking at the data frame
file operations it's really streamlined
I don't know how many times I'll go on
to different data downloads and they'll
have Panda download standard on there
just because it's so widely used so
let's start with the most common file is
a CSV so we have df3 to CSV or animal
and let me just show you the folders
going into right now I have some
Untitled a few things in here but
nothing labeled animal so we go ahead
and run this and this is now saved the
animal to my hard drive and you can now
see the animal folder up here and if I
let's do edit with a notepad oh let's
open up with just a regular notepad
there we go or wordpad if I open that up
you can see it's comma separated our
titles they don't have an index on the
categories on the top and the index
comma then all the different data is
separated by commas
standard CSP file on there and if we're
going to send it to CSV and notice the
format is dot 2 underscore CSV and it's
just the name of the file we're sending
it to you can also put the complete path
by default it's going to go whatever the
active directory this program is running
on that's why those other folders are in
there so we have our df3 to CSV and then
if we're going to put it in there we
want to also get it back out and we'll
call this one DF underscore animal
equals PD read underscore CSV I always
have to remember is two underscore CSV
and read underscore CSV I always want to
do like a capital in there and not the
underscore we're going in here again
it's the active directory so if I now do
print out my DF animal
and let's just do the head we only want
to look at the first three lines so if I
go ahead and run this we'll see the
first three lines and they should match
up here what we saved to our CSV so very
easy to save and import from our CSV
files on here
and it turns out DF
3 also has a two Excel they actually
have a lot of different formats but you
know old school Excel is real popular
for so long still is we can go ahead and
save it as animal dot xlsx we're going
to call the sheet named sheet1 and then
I can also do DF we'll call it animal 2.
two and this one's going to come from
and the same format on here there we go
so we still have our animal xlsx
the sheet 1 that's where it's coming
from index columns equals none so we're
not going to we're going to suppress the
indexing on the columns n a values and
it'll just assign that 0 1 up on your
indexes so if it says index columns
equals none that's what it does and then
we've added null values because there's
no values in here and we want to just
make sure that they're marked as n a and
we'll go ahead and just print out the
animal animal to there we go and let's
run that let's make this let's just do
the whole thing so we'll go ahead and
run that and it probably doesn't help
that I completely forgot the read so
animal 2 equals PD dot read
Excel there we go Excel so now we go
ahead and run it and what we expect is
happening here we have the same data
frame on here and if I flick back to my
folder you can now see that we have the
animal one of these is in Excel and one
of these is a CSV on here and so there's
our two file Types on there and they
have other formats these are just the
two most common ones used and I don't
know how many times I've had stuff from
Excel I need to pull out if you've ever
played with Excel it's a nightmare on
the back end because of the way they do
the indexing
so this just makes it quick and easy to
pull in an Excel spreadsheet so we
looked at two different ways to bring
data in and save it to files we've
looked at all kinds of different ways of
manipulating our data set and slicing it
and creating it for our data frame let's
get in there put your visualization
always the big thing at the end because
one it lets you check to see what you
did make sure it looks right and then
also if you're going to show somebody
else it makes it very clear what's going
on if they see something visual so this
is where a really important part of data
science is so let's go ahead and bring
in our tools we're going to do import
numpy as NP we want to make sure we have
our Amber sign matplot library in line
this just lets Jupiter know that we're
going to print it on this page if you're
using a different IDE you don't really
necessarily need that but this does help
it displays correctly in Jupiter
notebook and if you remember for earlier
we could create a we're going to call it
TS we're going to create a pandas which
are cute cuddly creatures versus a
pandem short for pandemonium no so we
have TS equals PD series and we're just
going to create a random setup of 50.
we'll do an index we'll set it equal to
the pandas date range today periods
equals 50 so the 50s should match and I
want you to notice something here I did
not import the matplot library why
because it's already in there pandas
already has its built-in connection and
interface with matplot Library so you
don't have to import it and we'll go
ahead and do TS equals TS dot cumulative
sum we're going to do the cumulative sum
it's a little reformatting there and
we'll go ahead and plot it and let's
take a look at what that looks like so
we have a nice graph here we have the
dates on the bottom we've set this up so
we have a nice range between in this
case minus four to looks like about two
maybe or one minus four and one so what
we've done here we've plotted a basic
series just a single row of data and
we've set indexes on there but we can
also do the whole data frame on there
and let's see what that looks like so
first let's go ahead and create the data
frame we have here random number so
we're going to do 50 by 4 and then we'll
go ahead and create columns a b X and Y
just because we can index is a ts.index
on there so we're going to use the same
index as before just to keep it nice and
uniform we've already generated the
dates to go with it and then we can do
just like we did with the series we can
also do with the data frame
DF equals DF cumulative sum so we're
going to sum the whole data frame and
then we'll do simply DF plot and this
puts that in and let's go ahead and run
this and look how easy and quick that
was to generate a nice graph with all
the different data on there so we have
our shared index we have the shared
columns and then we have the different
data from each one that we can easily
look at and compare so very quick way of
displaying data you can imagine if you
were working in oh I think I mentioned
stock earlier because I've been doing
some analysis of stock lately so you'd
have your date down here and then you
would have stock a Stock B stock X Y
whatever it is and you can put them all
on one chart and see how they what they
look like next to each other and this
isn't too far off from what some of
those graphs looks like and this is just
randomly generated so stock has a lot of
Randomness in it which is one of the
reasons I actually play with it for
doing some of my models on for testing
them out now there are a lot of features
in pandas so we're going to show you one
more thing on here there's some of the
things like I didn't go too deep we
looked at the top two for importing data
from a CSV and from an Excel spreadsheet
showed you how to quickly plot the data
there's more settings in there you can
do we're going to do one more thing down
here and this is kind of a fun one
changes to a markdown and run that so
how would you remove repeated data using
pandas
and this is where you have a data set
that comes in and maybe it's feeding
from one location and instead of noting
that it's repeated the date like oh
let's go back to stocks that's a good
visual we have the stocks from the 23rd
and it adds another row and it's the
same row it's importing the 23rd again
and again so now you have that data
repeated three times and you need to go
back and figure out how to get rid of it
how do you track that down so let's
start by creating a quick database our
data frame not a database I keep saying
databases the data frame and we'll just
make this data frame has using our
dictionary going in this data frame only
has one data Series in it which is fine
so if we do DF to print it out you'll
see
a12224567 and so on and so how would you
remove that well there is a neat feature
in data frames called shift
along with another feature that lets us
select just certain information and
we'll go with the location function put
that in Brackets remember that from
above location and then in the location
let me just spread this out a little bit
so it's really easy to read in fact I'm
going to go upscale on that since we're
doing some a little bit more complicated
here
what you can see on this on the location
is I have DFA dot shift so this is going
to shift up one by default you can
actually change this to two or three you
could even do a minus one and it shifts
the other way but it's going to shift up
by one by default that's going to say if
that does not equal DF of a then we want
that if you look down here we had one
two two two two when we run this Logic
on here and we do the shift it now gets
rid of all the duplicates so we went
from one two two two two four four five
whatever it was here it is one two two
two four four four five five five six
six six two one two four five six seven
eight and you'll see on the index it
just deletes them out of there so the
index stays the same obviously you don't
want the dates to change if you're
working with an index dated setup so it
just deletes those duplicates out of
there this is just a quick way to
introduce you to one the fact that you
can add logic gates into here and two
the I location allows you to use shift
so there's the shift function and then
the I location selects that based on
true or false
wow so we've actually covered a lot
today in pandas we've really covered
into the basics of selecting your
different series out of your column out
of your data frame how to index rows how
to slice how to plot hopefully you'll
take this beyond that and start
combining these different things and you
can create long strings and really
explore your data generate some nice
graphs if you're in jupyter Notebook
it's a great demo to show others and I
didn't know this about Jupiter notebook
you can do this in Jupiter notebook and
then you can download and I always I
never really look too closely at all the
downloads but you download as an HTML
and post it to your blog so it's got a
neat feature in there but any of this is
really powerful tool all of this is
really powerful tools for doing your
data science try giving a shot to Simply
Dance plus strategy program in data
analytics this is from Purdue University
in collaboration with IBM and this
should be the right choice the link in
the description box below should
navigate you to the home page where you
can find a complete overview of the
program being offered
so now let us understand what are those
life cycles of data analytics
we will begin with the discovery phase
this is the first phase
now that Mark has understood the
business problem
he will also start focusing on
identifying the resources that is the
data resources
some may be internal data resources that
is available within the firm could be
some transactional data
and some external data sources may be
via web scrapping identifying and
capturing some competitor price on the
products
after Gathering all of these right data
Mark will focus on data preparation
which is the next phase
now Mark either individually or along
with the team will start focusing on the
data preparation which includes data
wrangling which means cleansing the data
imputing the records
if there are any missing values or you
know he may also go ahead by removing
those records if they are not required
and also doing some exploratory data
analysis which can include some
statistical analysis like looking at the
data distributions understanding the
summary of this data distribution at
individual variable level doing some
bivariate analysis and also trying to
you know figure out which are the
important variables that might be
required for the model building phase
after performing all of these Eda
activities
which also include some visualization
Mark will now sit with his team and try
to identify the suitable models
the suitable models could be simple
statistical techniques or it can also be
some machine learning models so let's
say that Mark and and his team has
identified some five models
that can provide the required result
and out of these five models they will
filter down and they will prioritize
only three models
now there are only three models that
Mark and his team have finalized
after this they start focusing on model
building activity
now for model building activity
they have a data set already in place
so this data set will be split into
training data set and test data set
it's not only training and test we can
also do some validation in between
training and test but here let's focus
on training and test data set
he will separate 75 percent of the data
as training and 25 percent of the data
as test
now if your question is that why perform
this activity of splitting the training
data set and test can't we just go with
the one single data set
what happens if you just utilize one
single data set
let us say that you have used the
original data set
and also executed this data in one of
the selected model
and you will also observe the accuracy
let's say the accuracy return is about
98 percent
98 percent is a very good accuracy
percentage
and you may also be overconfident
because of this that may be a case due
to overfitting
now what will happen when you add some
new records into this data set and you
re-run it
the executed model may not return you
the same accuracy what you had seen
the accuracy might be 72 percentage now
that's not fair right
to avoid these overfitting issues
we ensure that some new records are
tested separately
so hence we locate 25 percent of the
data to the test data set
and then we predict those records the
unknown records which is located in the
test data set we predict them and then
we test the accuracy of training data
set and the test data set and we make a
comparison
now let us say the accuracy result of
training is 98 percent and the test is
97 percent
in this case we can say that the model
is performing really well
however while executing the model there
are certain things that has to be
considered for example inclusion of the
parameters
tuning the parameter which also will
execute the optimal results so this is
very important
now let's say after performing this
model building
the time comes to analyze the results
that is the next phase
now the team will sit and analyze the
result
and they will notice that out of the
three filtered models only two models
are returning excellent accuracies
they will sit with the business team and
they will also explain them the result
and what are the activities that they
have performed to obtain this result
some of the stakeholders may be
technically Savvy some of them may be
non-technical people so it has to be
very important that you also communicate
these results accordingly
all right now that you have the results
you will also gauge them based on the
business objective
which was developed in phase one
looking at the results of the two models
now the business might select one of the
model and say that okay this particular
model seems to be returning some right
information and it also appears valid to
us let's go ahead with this one model
so finally the result and the model
needs to be operationalized
and that is where the team will start
documenting the business problems the
steps that were taken for executing the
models and they will include all the
codes and the findings and finally they
will implement this model so that the
business can view the results and also
utilize them for strategic decision
making at their form
so I'll be good so far with the
understanding of the life cycle
all right great now let us focus on the
types of analytics what are the types of
analytics can somebody tell me which are
the types of analytics you are aware of
okay Predictive Analytics great
descriptive analytics good
all right good enough
now let us focus on this example of
Google Maps
so as we look at this particular Google
Map we understand that the blue color
root is nothing but the root direction
from Sacramento to fluorine and also we
see a display of the
duration estimated as well the distance
to travel from Sacramento to fluorine as
well as we see another route here that
is gray colored
this is a substitute root or the
connecting root just to avoid the
traffic which is in Orange within the
blue color root so the Gray colored root
as well shows us the estimated duration
to travel as well the distance
now let us understand what is
descriptive Analytics and why do we
focus on this particular map example
as we understand the root map the
estimated duration as well the distance
to travel
wire the blue color root as well the
gray color root this is one way of
understanding descriptive analytics as
in what is happening
but there is another way of
understanding descriptive analytics that
is by focusing on summarized past data
and this is a descriptive Analytics
we see that what had happened
in the previous year now let us focus on
Predictive Analytics
what is Predictive Analytics
this type of analytics looks into the
historical and present data to make
predictions of the future
what does this mean
so Google has already suggested the best
route
which is the blue color root to travel
from Sacramento to Florin and the
duration is 18 minutes
and distance is 9.7 miles
let's assume that Google map has already
collected historical data of this
particular route
and based on the available data their
model has predicted the best route
and also the duration that will be taken
to travel from Sacramento to Florin
now let us focus on prescriptive
Analytics
prescriptive analytics describes the
solution to a particular problem
what was the problem in this case on the
predicted best route
some predictions on the traffic
congestions and that's when Google Map
recommends The Substitute Roots correct
now these substitute roots are also
prescribed by Google Map as a
recommendation
so prescriptive analytics is nothing but
a solution and a recommendation provided
for a problem
so in this case we have the best route
and we also have other substitute troops
so let's quickly summarize
descriptive analytics is about
summarizing the past data or to see what
is happening for example in the Google
Maps scenario
Predictive Analytics is about what would
happen
and prescriptive analytics is about
prescribing the solution the best
solution and the recommended Solutions
now let us refer back to Mark's earlier
scenario
Mark along with this team identify the
best model they also did some testing
and they got identified the best results
and they also finalized on one
particular model
based on that particular model now the
results have to be provided in such a
way that they are the best results and
also the recommendations correct
so it may not be just one single
solution it may be a solution with
couple of other recommendations as well
so that is exactly what happens in the
entire process of data analytics
I hope this has been clear so far
yes okay
and now let us focus on benefits of
using R why do companies extensively use
R for data analysis and why is it chosen
firstly R is an open source programming
language which means that there is no
license required to work with r
and R does not require you to have a
coding experience which means that a
non-technical person in your team can
also learn R very easily and start
coding or building models in few lines
of codes
R can also be used with other
programming languages such as Java C
plus and pythons and integration of r
with other programming tools or bi tools
is very simple and easy
and various statistical models are
readily available in R which also means
that there are plenty of in-bit
libraries and packages already available
and Reporting the results of an analysis
becomes easier by using this inbuilt
packages and for creation of these
models in just simple few lines code
with this understanding of the benefits
of fusing are let us quickly hop on to
our studio and start performing the
Hands-On exercise for data analysis
for this exercise we will use a data set
named as demographics which is in a DOT
CSV file tab firstly let us load the
data set to our studio and we will
locate this in an variable named as demo
we also refer to this as a data frame
and now you will notice that a variable
is created in an environment section
which is in the bottom right hand side
of the rstudio window
and this particular variable comprises
of 510 observations or records with 8
variables
let us simply expand this particular
data frame and have a quick check on the
data structure and understand the data
types
this particular data frame includes
variables such as h
marathu income the unit of income is
dollar per day
education levels the car price car
category with several levels gender and
retired status
now let us view the top six records of
this particular data set
for this let's simply type head of demo
and the result is now visible in the
console section
if you are interested to view all the
records then simply type view of demo
and this new window will show you every
single record that is being loaded to
our studio
you may also simply apply the filters
and the filters section here on
individual categorical variables
now that we have loaded the data set and
also viewed individual records let us
focus on creating subsets of Records by
applying filters on individual variables
or multiple variables
so firstly let us apply filter on gender
we will only retrieve the records which
gender is equal to female and we will
locate these records in a variable named
as demo 2.
as you notice now in the environment
section the second variable is also
created that is demo and this now is
comprising of 250 observation which
means the records are filtered down to
only gender female
next let us see how to apply a filter on
income variable let us only retrieve the
records where income is greater than
100.
let's view the result
as we say here all the records include
income greater than 100.
now let us modify this query
and we will ensure that the retrieve
records includes income greater than 100
and also specific variables are returned
let's say we only want to have
the first variable third variable and
the seventh variable returned as the
result
let's have a quick check
so we only have the first third and the
seventh variable returned
how about we only exclude
the variable 6 to 8
for this we include a prefix of minus
sign
and now let us see what is the result we
have the variables from first to the
fifth variable however we don't have 6 7
and 8 variable
I hope it's clear so far
yes all right
now let us see how can we apply
condition by including both the
variables that is gender and income and
then we will filter the record and
create a subset of data
now let's view the result
income is greater than 100
and the gender is only female
this is one way of creating subsets
however now let us see how to use the
subset command and create the subset
let's create a subset of Records by
applying filter on marital status and
age
we'll only retrieve records where
marital status is equal to married
and age is greater than 35.
okay
let's now view the result
so here we have the age greater than 35
and marital status is married
let's use the same code
and this time we will retrieve selected
variables let's say variables ranging
from 1 to 3.
let's have a quick check so there are
three variables age is greater than 35
and marital status is married
now let us see how to structure the data
by sorting the data frame in ascending
and in descending order
we will apply this order function on the
variable income
firstly let us see how to order income
variable in ascending order
let's do a quick check
and here we have income in ascending
order
now let's see how to modify the same
code
and view the records
with income in descending order
so we have now the income in descending
order
how about include two variables and sort
the variables accordingly
firstly we will sort the records
by ordering income and age in ascending
order
let's quickly view the result we have
income in ascending and age as well in
ascending
let's now modify this code
this time we will order income and
descending
and age in ascending
let us view the result so the income
isn't descending and age is in ascending
order
so hope this is clear on how to solve
the data frame by ascending and
descending order per variable or by
using multiple variables with this we
will focus on learning statistical
analysis
how to perform statistical analysis on
individual variable or multiple variable
let's start by understanding the data
distribution of variable income
so that we identify what is the minimum
value of income what is a maximum what
is the range what is median what is the
mean and we will also focus on the
quantile distribution
which is also analyzed in a box plot
what is the minimum value
in the variable income
it's nine
and what is the maximum
so we have the maximum value
now let us see what's the range
so the range shows you the result with
minimum and the maximum value
how about the difference of Maximum and
the minimum now let us focus on other
summaries of data distribution for this
variable income let's identify what is
the mean value of income
the mean is 78.
let's also understand what is the
standard deviation
all right so the standard deviation is
one one two dollar
let us see what is the variance the
variance should be larger than the
standard deviation
now let's say what is the median
absolute deviation
as you notice here the median absolute
deviation value is lower than standard
deviation why do we make this comparison
from this it is evident that median
absolute deviation is robust outliers
and standard deviation is
sensitive to outliers and also to the
change in the mean value
now let us understand the quantile
distribution this is the same analysis
that is visualized in a box plot ranging
from zero percent to hundred percent
identifying the individual data points
and we can also refer and compare this
to the Min the Max and the median values
let us quickly see what is the median
value of income
as you notice here the median is 45 as
well the 50th percent of quantile is 45
which means zero percent is minimum and
hundred percent is the maximum value
now if your question is what is 25 and
75 percent this is again used for
identifying the range of interquartile
the interquartile range is nothing but
the difference of 75 percent minus the
25 percent
let's quickly see what is the IQR of
income
the iqrf income is 58. let us do a quick
check
75 percent of quantile is 86 and 25 of
quantile is 28.
and the value is 58 which is equal to
the IQR result
now that we have focused on the
statistical analysis of the individual
variables data distribution let us focus
on the data visualization
in this we will have a pictorial
representation of analysis to identify
the outliers to see what is the minimum
and where do we see the data densely
populated and how is it scattered Etc
we will begin with creating a histogram
now histogram can be used for univariate
analysis which means in this scenario we
will consider income variable and we
will see how the count of income ranges
gets distributed in a histogram
for this we will have to install a
package called as ggplot2 and also call
this Library ggplot
let us install the package
and now let us call the library
all right and we are ready now to begin
with visualization
for this we will use the geometric
object histogram
on the data demo data frame
let me expand this window so that the
code is visible
and also use an aesthetic mapping
for variable income this will be helpful
for filling colors or filtrations Etc
and only include 30 bins
with individual bin size width
of 100 which means there will be 100
incomes in individual bins
let's quickly look at the distribution
of this histogram
as you notice there are couple of
outliers
the counts of these income range are
very limited
however we see the densely populated
income ranges with higher counts
between 0 to 200. dollars per day
this is also a way to identify and
segment the customers based on their
income ranges
now let us see how to change the color
of this histogram and also the border of
the histogram
for this we will include some additional
options
such as fill
Ed with blue color
and the Border color
is black
now as you notice here the executed code
provides us the histogram with blue
color bars and black color Border Lines
now we will focus on creating a faceted
grid facing grid is also an aesthetic
mapping object
we will see how to enable the multiple
histograms across the marital status and
the genders so that we identify how the
income is distributed for individual
maratha status as well the genders
let's Zoom this View and have a look at
it
as you notice here there are some
interesting outliers here in the data
distribution
female unmarried drawing higher income
and male unmarried and married also
drawing higher income as compared to the
females
whereas if you notice that the female
unmarried is drawing much higher income
than the male
this may also be very much related to
the age
now let us see how to create a stacked
histogram when I say a stacked histogram
I mean instead of feeling the color we
will fill the gender
so that there is a stack within the
histogram
so as you notice here I have made couple
of changes I have included fill equal to
gender within the aesthetic mapping now
let us look at this histogram as you see
here the gender is filled in the
histogram hence we have stacked
distribution of female and the male
now let us focus on creating a bar chart
with education versus income where we
can identify the education levels and
the income ranges for these education
levels
foreign
as you notice here we are going to
create a visualization where we have the
aggregation in form of mean and the
geometric object used here is bar plot
now let's Zoom this View and understand
which education level
have higher average income
so as we see here the blue color bar is
the post undergraduate degree which
means this education level draws higher
average income as compared to other
education levels
now let's create a histogram where we
will see car price and the number of
cars for individual category
let's look at this visualization
this visualization provides with some
interesting Insight just by looking at
the distribution of the car prices and
the counts of the cars at the car
category economy and even the luxury
luxury car category or car prize is
pretty much distributed whereas Economy
Car category is densed which means that
we could also look back into the income
and age variables and try to figure out
further more insights and then segment
the customers for further targeting of
these customers
now what happens if we simply change
this bin width to 30.
As You observe here changing the bin
width or increasing the bin width will
also reduce the number of bins
now we only have four bins here
and the car category is filled that is
what we have enabled within the
aesthetic mapping
and we see some more interesting insight
as you look at the standard and the
luxury car category
the car prices are pretty much
overlapping
for the car category luxury and standard
this could be the starting car price of
the luxury brands
now let us create
a clustered bar chart
let's look at this visualization
in this visualization As You observe
though we have enabled fill equal to
gender in the aesthetic mapping we do
not have the view in stack form but we
have the bars one besides the other it
is also because we have enabled a
position called as position equal to
Dodge in the code
now what is the inside that we can draw
from this visualization as you see post
graduate degree with female gender is
drawing higher average income as
compared to any other education level
now let us see how to create a box plot
for variable income across the genders
so the box plot can be enabled if there
is a bivariate analysis to be performed
on a continuous variable and a
categorical variable or multiple
categorical variables with a continuous
variable
foreign
what does this say
we have data distribution of income
for individual genders that is for
female and the male and we also notice
outliers here
anything about this whisker is
considered to be outliers
it might make more sense if we also
include some coloring for these outliers
maybe also enable shape
yeah
foreign
the outliers and it's colored Orange
let's see if we can also enable the
shapes
and now we have here the outlier color
as well the shape enabled
now let us see how to enable a violin
plot
what is the utility of violin plot with
the box plot we understand the analysis
and the distribution of the data points
is to identify the outliers to know what
is the minimum value what is the max
what is the median and what are the
outliers
but what is the purpose of a violin plot
let us have a quick check
foreign
As You observe there is some
concentration of data points in the
bottom of every car category
however the concentration is higher for
standard car category as compared to
economy and the luxury
now this is an interesting Insight that
you wouldn't have come across in box
plot the box plot is a very good
representation for identifying outliers
however violin plot will help you focus
on the nuances which is not captured by
the box plot
we can also simply combine the box plot
and the violin plot together
simply include this jaw object
let's Zoom this now you have a
representation of box plot and the
violin plot both combined in a single
visualization
interestingly you notice the outliers as
well the concentration in the bottom of
this violin plot so this could be some
interesting insights that you draw and
focus on these data points and
understand what exactly is happening
there
now let's focus on the density plot that
is density estimate of the histograms
rather than just viewing the frequencies
now we see the frequency in the y-axis
across the income distributions
how about enabling the probability as
true
so that we enable the density instead of
the frequency
so now we have the density in the y-axis
and in the x-axis we still have the
income distribution
foreign
this is the way of also adding a line
plot which is a density plot on the
histogram now as You observe here the
density plot is not in the same level as
the bar so let us adjust this line
for this we will include
adjust
let's say equal to 3 and now let us see
how the visualization appears now the
density plot is on the same level as the
bar
now let us see how to create a cross
table
for car category
and gender
for this let us call the library d e s c
r
now let us create the visualization
enabling cross table for car category
and
gender
let's look at the result in console as
you see here now we see the counts of
the gender for Individual Car category
the values over here represents that
there are 67 females
falling within the car category economy
and 80 males within the car category
economy
and for luxury we see that the count of
female is higher than the male as well
the proportions now how do you
understand what proportions are
presented here
we may simply turn off some of the
proportions like the t-test the
chi-square Etc let us see how to enable
that
now let's look at the result this looks
better
now that we have the counts the female
counts and the male counts across
Individual Car category we also see the
percentages rather than just looking at
the absolute value so there are
45.6 percentage of female within the car
category economy and 54.4 percentage of
mail within the car category economy
similarly across rest of the car
categories this kind of cross table or a
contingency table is also helpful when
you want to analyze the different
categorical variables and identify the
counts or the proportions now let us see
how to use a scatter plot of age versus
income
scatter plot is a visualization used for
bivariate analysis when you want to
perform some analysis between two
continuous variable at a data point
level rather than performing the
analysis at an aggregated level such as
sum or mean
and now we have a scatter plot of age
versus the income age in the x-axis and
income in the y-axis
though we did not see any kind of a
positive correlation or a negative
correlation but we still see some
interesting insights over here
some of the data points are pretty much
scattered and much away from densely
populated data points
I hope the learning has been informative
and interesting so far we have covered
the concepts of data analytics as well
we have performed some Hands-On doing
some statistical analysis and also
creating interesting visualization
try giving a shot to Simply Dance plus
strategy program in data analytics this
is from her University in collaboration
with IBM and this should be the right
choice the link in the description box
below should navigate you to the home
page where you can find a complete
overview of the program being offered
So currently I am on my MySQL workbench
let me connect to the local instance
so I'll give my password
I'll click on OK
all right so this is my MySQL workbench
query editor so first we are going to
learn sub queries let me give a comment
and write sub queries
all right
so first of all let's understand what a
sub query is so a sub query is a query
within another SQL query that is
embedded within the where Clause from
clause or having clause
so we'll explore a few scenarios where
we can use sub queries so for that I'll
be using my
database that is SQL underscore intro so
I'll write my command use SQL underscore
intro
now this database has a lot of tables
I'll be using the employee stable that
is present inside SQL underscore intro
Let me just expand this and you can see
here we have an employee stable
so let me first show you
the contents within this table I'll
write select star from employees
let me execute it
okay you can see here we have the
employee ID employee name age gender
there's date of join Department City and
salary and we have information for 20
employees if I scroll down you can see
there are 20 employees present in our
table
so let's see you want to find the
employees whose salary is greater than
the average salary
in such a scenario you can use a sub
query so let me show you how to write a
sub query
I'll write the select statement
in the select statement I'll pass
by column names that I want to display
so the column names I want are the
employee name
then I want the department of the
employee and the salary of the employee
from
my table name that is employees
next I'll use a where condition where
my salary should be greater than the
average salary of all the employees so
I'll write salary greater than
after this I am going to write my sub
query
so I'll give select
average of salary
from
my table name that is employees
and I'll close the bracket and give a
semicolon
so what it does is
first it is going to find the average
salary of all the employees that are
present in our table
once you get the average salary number
we'll use this where condition where
salary is greater than the average
salary number
so
the inside sub query let me run it first
if I run this
this gives you the average salary of all
the employees which is 75 350 dollars
now I want to display all the employees
who have salary greater than 75
350 dollars so let's run our sub query
there you go so there are eight
employees in our table who have a salary
greater than the average salary of all
the employees
all right
next
let's see another example
suppose this time you want to find the
employees whose salary is greater than
John's salary
so we have one employee whose name is
John
let me
run the table once again
okay if I scroll down
you see we have an employee
as John you see this our employee ID 116
is John and his salary is 67
000 I want to display all the employees
whose salary is greater than John's
salary so basically all the employees
who are earning more than 65 000 I want
to print them
so let's see how to do it
I'll write
select
I want the employee name comma the
gender of the employee
I also want the department and salary
from my table name that is employees
I'll write where
salary is greater than
I'll start my
opening bracket inside the bracket I am
going to give my inner query that is
Select
salary
from employees
where
the employee name is John
So within single quotations I'll give
John as my employee
and end with a semicolon
so let me first run my inner query
so this will give us the salary that
John has which is sixty seven thousand
dollars now I want the employees who are
earning more than sixty seven thousand
dollars so let's run our
sub query
okay so you can see
12 rows returned which means there are
12 employees in our table who are
earning more than sixty seven thousand
dollars you see here all these employees
have a salary greater than sixty seven
thousand dollars
okay
now
you can also use sub queries
with two different tables so suppose you
want to display some information that
are present in two different tables you
can use sub queries to do that
so
for this example we'll use
a database that is called classic models
you can see the first database
so let me use this database called
classic models I'll write use classic
models
now this database was actually
downloaded from the internet there's a
very nice website I'll just show you the
website so this is the website that is
MySQL tutorial.org
you can see here they have very nice
articles blogs from where you can learn
MySQL in detail so we have
downloaded the database that is classic
models from this website you see here
they have a MySQL sample database if you
click on this
it will take you to the link where you
can download the database so they have
this download link which says download
MySQL sample database and the name of
the database is classic models all right
so we are going to use this classic
models database throughout our demo
session if I expand the tables
section you can see there are a lot of
tables that are present inside this
classic models database we have qriket
customers as employees
office this orders order lines and many
more
so for our sub query we'll be using two
tables that is order details and
products table first let me show you the
content that is present inside
the products table first
if I run this
you see here it says 110 rows return
which means there are 110
different products that are present in
our table which has the product code the
product name
product line we have the product vendor
description quantity in stock Buy price
MSRP
the other table we are going to use is
order details which has the details of
all the orders
let me show you
the records
or the details tables has okay so there
are thousand records present in this
table you have the order number the
product code quantity ordered price of
each item you have the order line number
as well
okay
now
we want to know the product code the
product name and the MSRP of the
products whose price of each product is
less than hundred dollars for this
scenario we are going to use two
different tables and we are going to
write a sub query
Okay so
if you see here in the order details
table we have a column called price each
I want to display the product code the
product name and the MSRP of the
products which have a price of each
product less than hundred dollars
so the way I'm going to do is
I'll write select
product code comma
product name
now one thing to remember
that this product name is actually
present inside our products table
and product code is present in both the
tables that is production order details
here you can see this is the product
code column
comma MSRP which is present inside the
products table again
from my table that is
products
where
I'll write
product
code
I'm going to use the in operator
next I'll write my inner query that is
Select
product code
from my table
order details
where
my price of each product
is less than 100 dollars
let me run this
okay so you can see there are total 83
products in our table which have
a price less than hundred dollars you
can see the
price here
okay
now we learn another Advanced Concept in
SQL which is known as stored procedures
I'll just give a comment saying
stored procedure
okay
so first let's understand what is a
stored procedure
a stored procedure is an SQL code that
you can save so that the code can be
reused over and over again
so if you want to write a query over and
over again save it as a stored procedure
and then call it to execute it
so in this example I want to create a
stored procedure that will return the
list of players who have scored more
than six goals in a tournament
so I have a database called SQL
underscore IQ
these are a few databases that I've
already created so this database has a
table called players if I expand the
tables
option you see we have a table called
players and you can see the columns
player ID the name of the player the
country to which the player belongs to
and the number of goals each player has
scored in a particular tournament
so I'll write a stored procedure that
will return the list of top players who
have scored more than six goals in a
tournament
so first of all let me Begin by
using my SQL underscore IQ database
will run it
so now we are inside the SQL underscore
IQ database
let me
select
star from players to show the
values that we have in the players table
you can see there are six players in our
table
we have the player ID
the names of the players the country to
which these players belong to and the
goals they have scored
so I'll write a stored procedure
thank you
so the stored procedure syntax is
something like this it should start with
a delimiter
okay
in the delimiter I'll write
Ampersand ampersand
next I'll write
create
procedure
followed by
the procedure name
let's say amount to
name my procedure as top underscore
players
next statement is begin
after begin I'll write my select
statement
I want to select the name of the player
the country
and the goals
each player has scored
from my table that is
players
where
I'll write goals is greater than
6.
will give us semicolon
then
I'll end my
procedure with a delimiter that was
double ambison
next
I'll write delimiter
and give a semicolon
now the semicolon suggests
this is a default delimiter
and there should be a space
okay
now let's run our
stored procedure
there you go so you have successfully
created our store procedure
now the way to run a stored procedure is
you need to use the
call method and give the procedure name
that is top underscore place in our case
with brackets and a semicolon
let's execute it
okay that is some problem here
so we made a mistake while creating a
procedure
the name of the column is goals are not
goal
let me create that procedure again
okay it says the procedure top
underscore player already exists let's
just
edit the procedure name instead of top
player we'll write it as top players and
similarly we'll edit here as well
now let's create it again
okay now to call my procedure I'll write
call space followed by the procedure
name which is top underscore players
if I run this you can see
we have two players in our table who
have scored more than six goals so we
consider them as the top layers in a
particular tournament
all right
now there are other methods that you can
use while creating a stored procedure
one of the methods is by using an in
parameter
so when you define an in parameter
inside a stored procedure the calling
program has to pass an argument to the
stored procedure
so I'll give a comment
stored procedure using in parameter
all right
so for this example I'll create a
procedure that will fetch or display the
top records of employees based on their
salaries
so if you have a table
in our SQL underscore IQ database which
is called employee details
I am going to use this table you can see
we have the name of the employee the age
sex then we have the date of joint City
and salary
using this table I'll create a procedure
that will fetch or display the top
records of employees based on their
salaries
and we'll use the in parameter so let me
show you how to do it I'll write
delimiter
this time I am going to use
forward slash
I'll write
create
procedure
followed by the procedure name let's say
SP for stored procedure
sort by
salary is the name of my procedure
and inside this procedure I'll give my
parameter
in
I'll create a variable bar and assign a
data type integer
then I'll write begin
followed by my select statement where
I'll select the name
each
salary
from
my table name that is
EMP details or employee details
I am going to order this by
salary
descending
and
I want to display
limited number of
records so I'm using this
limit keyword and my variable VAR which
I
created here
I'll end my select statement
I'll end my stored procedure with
forward slash
and I'll go back to my default delimiter
that is semicolon
all right
so let me
run this
there should be a space here all right
so let's run this
okay you can see we have successfully
created our
second stored procedure which is sp
underscore sort by salary
now you can also check whether the
stored procedure was created or not here
you have an option to see the stored
procedures let me just refresh this
and you can see we have
three stored procedures that we have
created so far one is sp underscore sort
by salary
the other two were top underscore player
and top underscore players
okay
now let's call our stored procedure I'll
write call
space followed by the stored procedure
name which is sp underscore sort
by salary
and inside this I'll give my parameter
which was actually VAR and this VAR
we have used in limit
let's say I want to display only the top
three records of the employees who have
the top three highest salaries
okay so let me run it
there you go so Amy Sarah and Jimmy but
the top three employees who have the
highest salary
so you saw how you could use the in
parameter in a stored procedure
we created a variable and that variable
we used in our select statement and we
called our stored procedure and passed
in that
variable
okay
now
instead of a select statement inside
stored procedure you can also use other
statements let's say update
so I'll create a stored procedure to
update the salary of a particular
employee
so in this procedure instead of Select
statement we'll use the update command
in this example we'll use the in
operator twice
so let me show you how to do it
I'll write my delimiter first which is
going to be forward slash
then I'll write create
procedure
my name of the procedure is going to be
update salary
and inside the
update salary name
I'll write in
and then
temp
underscore name which will be a
temporary name variable and the type
I'll assign is worker 20
I'll again use my in parameter I'll
write in
next
my other variable would be new
underscore salary
and the data type would be float
I'll write begin
and write my update
command or update statement I'll write
update
table name that is employee details
set
salary
equal to
new underscore salary
where
name is equal to
my temporary variable that is temp
underscore name
so this is my
update command and I'll
end the delimiter
all right
so let's run this
okay we have successfully created our
stored procedure if I refresh this
you can see I have my stored procedure
update underscore salary
okay
now let's say
first of all I'll
display
my
records that are present inside
employee underscore details table okay
so we have six rows of information let's
say you want to update the salary of
employee Jimmy or let's say
Mary
from seventy thousand to let's say
seventy two thousand
or let's say eighty thousand
so I'll
call my stored procedure that is update
underscore salary
and this time I'm going to pass in two
parameters the first parameter will be
the employee name and next with a comma
I'll give my new salary that I want to
so my employee name let's say is Mary
and the salary I want
to be updated is let's say eighty
thousand dollars
I'll give a semicolon
and I'll run it
you can see it says one reflected now
let's check our table once again
there you go if you see this record
for Mary we have successfully updated
the salary to eighty thousand dollars
moving ahead
we learn to create a stored procedure
using the out parameter so I'll give a
comment
stored procedure using out parameter
okay
so suppose we want to get the count of
total female employees
we will create total employees as an
output parameter and the data type would
be an integer
the count of the female employees is
assigned to the output variable which is
total underscore emps using the into
keyword
let me show you how to write a stored
procedure using the out parameter
so first I'll declare my delimiter
to forward slash
I'll write
create
procedure
followed by the procedure name
it is going to be SP underscore
count
employees
and inside this I am going to give my
out parameter and the variable name that
is total underscore
emps which is total employees and the
data type will be integer
next I am going to write begin
followed by my select statement that is
Select I want the
count of total employees
and the output I am going to put into
my new variable that is total underscore
emps
from my table that is EMP underscore
details
where
sex is equal to
F which means female
I'll give a semicolon
next I'll end it
and I'm going to change the delimiter to
a default delimiter that is colon
so let me tell you what I am doing here
I am creating a new stored procedure
that is sp underscore count employees
using this stored procedure I am going
to count the total number of female
employers that are present in our table
EMP underscore details so I've used my
out parameter and I'm creating a new
variable called total underscore emps
the data type is integer here in the
select statement I'm counting the names
of the employees and the result I am
storing it in total underscore emps
I have used my wear condition where the
gender of the sex is female
so let's run this
okay so we have created our stored
procedure let's refresh this
okay you can see we have a new stored
procedure SP underscore count employees
now
to call it I'll write call
the name of the procedure that is Count
underscore SP underscore count
employees
within brackets I'll pass in the
parameter as
at the rate
F underscore EMP
will give a semicolon then I'll write
select
at the rate F underscore
EMP as female employees
okay
so as is an alias name
let's run this one by one first I'll
call my procedure and then we'll display
the total number of female employees you
can see in our table we have three
female employees
all right
now with this understanding
let's move on to our next Topic in this
tutorial on Advanced SQL now we are
going to learn about triggers in SQL
so I'll give a comment here
reverse in SQL
so first let's understand what is a
trigger
so a trigger is a special type of stored
procedure that runs automatically when
an event occurs in the database server
there are mainly three types of triggers
in SQL we have the data manipulation
trigger we have the data definition
trigger and login triggers
in this example we will learn how to use
a before insert trigger
so we will create a simple students
table that will have the students roll
number the age the name and the students
marks
so before inserting the records to our
table we'll check if the marks are less
than zero
so in case the marks are less than 0 a
trigger will automatically set the marks
to a random value let's say 50.
so let's go ahead and create our
table that is
students
all right
so I'll write
create
table
student
now this table will have the student
rule number
the data type is integer
it will have the age of the students
again the data type is integer we have
the names of the students so the third
column would be
name
the data type would be variable
or varying character
size I am giving it as 30 finally we
have the marks as floating type
so let's create this table which is
student
so we have created our table
now
I'll write my
trigger command
so trigger command will start with
delimiter like how our usual stored
procedures have
next
this time I'll write create trigger
then you need to give the
okay
name of the trigger that is Mark
underscore
let's say verify
I am going to use a before insert
trigger so I'll write before insert
on my table name that is student
next I'll write for each row
if
new DOT
marks
is less than zero
then
we'll set
new DOT
marks equal to 50.
so this is my
condition first we'll check
before inserting if any student has
marks less than 0 we'll assign a value
50 to that student because usually the
marks are not less than 0 in any exam
I'll write end if
semicolon and I'll close the delimiter
so this is my
trigger command I'll run it
it says trigger already exists so in
this case we need to update the trigger
name let's say
I'll write marks underscore verify
underscore
student for SD
let's run it again
okay there is an error here because
in our table the column name is Mark and
not marks so here we need to change it
as Mark instead of marks
all right
let's run it
okay so we have created our trigger
now
let me insert
a few
records to the student table
so I'll write insert into student
I'll write
values
and give the values as
501 which is the student roll number the
age is let's say 10
the name is it's a root
and the marks is let's say 75 point
0
if a comma we'll insert our second
student record
student rule number is 502
ages 12
the name is let's say Mike
and this time I'm purposely giving a
value of minus 20.5
give another comma
we'll insert the Third
record
for student rule number 503
age is 13
the name is Dave
and
let's say the marks obtained by div is
90
.
now we'll insert our final record for
student number
504
these is 10
name I'll enter as Jacobs
and this time again I'm purposely giving
the marks in negative
12 point let's say 5.
close the bracket
and give a semicolon
and I'll run my insert statement okay so
we have inserted four rows of
information to our student table
now
let me run the select query I'll write
select star from student
if I run this you see the difference
there you go
so originally we had inserted
for 502 the marks was minus
20.5 and for 504 for Jacobs the marks
was minus 12.5
our trigger automatically converted the
negative marks to 50 because
when we created our trigger we had set
our marks to 50 in case the marks were
less than zero
so this is how a trigger works
now you can also
drop a trigger or delete a trigger you
can just write drop trigger followed by
the trigger name
in this case our trigger name is
Max underscore big Phi underscore St
I'll just paste this here
and if you run this it will
automatically delete your trigger
you give this as a comment okay
now moving on
now we are going to learn about another
crucial Concept in SQL which is very
widely used
this is known as views
so views are actually virtual tables
that do not store any data of their own
but display data stored in other tables
views are created by joining one or more
tables
I'll give a comment as views in SQL
okay
now to learn views I am going to use my
table which is
present inside classic models database
now this database as I mentioned we had
downloaded
we had downloaded it from the internet
so first of all let me write
use
classic models so I'll switch my
database first
all right now we are inside classic
models
so here
let me show you one of the tables which
is called customers so I'll write select
star from customers
okay I missed s here
let's run it again so this is my
customer table which is present inside
classic models database it has the
contact last name the contact first name
the customer name customer number we
have the address State country and other
information
now I'll write a basic view command
using this customer table the way to
write is I'll write create
View
followed by The View name which is cast
underscore details
then you write as
select
I am going to select a few column names
from my original customer table which is
this one so I need the customer name
let's say I need the phone number
and the city so you have this
information here you have the phone
number
and the City
all right
I'll write from my table that is
customers
if I run this
my view the discussed details will be
created
let's run it there's some error here
because the name of the table is
customers and not customer
I'll give an S and I'll run it again
all right so you can see we have created
our view
and
to display the contents that are present
inside our view I can write select star
from followed by The View name that is
custom details
please
let's run it
there you go so we have the customer
name the phone number and the City of
the different customers that we have in
our table
all right
now let's learn how you can create views
using joins so we'll join two different
tables and create a view
so for that I am going to use my
products table and the products lines
table I'm talking about the products
table and the product lines table
present inside classic models database
so before I start let me display the
records that are present inside the
products table
let's run it so these are the different
products you can see here
now let's see what we have in product
lines table
so we have the product line the text
description and there's some HTML
description and image
so
I'll create a view by joining these two
tables and will fetch specific records
that are present in both the tables
so let me first start by writing create
View
followed by The View name that is
product underscore
description
as I'll write
select
product
name
comma
then I'll write
quantity
in stock
I also want the MSRP
now these three columns are present
inside the products table and next from
the
product line stable I want the text
description
of the products
so I'll write from
product stable
I'll give an alias as p
followed by Inner join my other table
that is
product lines as let's say PL
on
the common column that is product line
so P Dot
product line
is equal to
I'll give a space
PL Dot
product line
okay
so here we have used an inner join to
fetch specific columns from both the
tables
and our view name is product underscore
description let us run it all right so
we have our view ready
now let me
View
or display what is present inside our
product underscore description
View
I'll add select start from
product underscore description
let's run it
there you go so we have the product name
the quantity in stock msrpn textual
descriptions of the different products
in the table
okay
now there are a few other operations
that you can perform let's say you want
to rename a view instead of product
underscore description you want to give
some other name
so I'll just give a comment rename
description
so to rename a description you can use
the
rename statement I'll write rename table
product
underscore description Which is my
old name
I want to change this name to let's say
I'll give vehicle description
since
all our products are related to some of
the other vehicle so I'll write vehicle
description
okay let us run it
all right so here you can see I have
renewed my
View
so here if I just refresh it and I'll
expand this you can see we have the cash
details view and we have the vehicle
underscore description View
okay
now either you can view all the views
from this panel
or you can use a command let's say
I'll write display views
is the comment
now to show all the views you can use so
full tables
where
table
underscore
type is equal to within single quote
I'll write View
so this is the command that will display
all the views that are present inside a
database
there is some
error here let's debug the error this
should be
okay so instead of table types it should
be table type equal to view
let's run it
you can see the two different views that
we have one is customer details another
is vehicle underscore description
okay
now you can also go ahead and delete a
view
for that you can use the drop command
so I'll write drop
view followed by The View name let's say
I want to delete
customer underscore details or cast
underscore details view
I'll write draw View cast underscore
details
let's run it
you can see here we don't have the cast
underscore details view anymore
all right
now moving to our final section in this
demo
here we will learn
about Windows functions
the windows functions were Incorporated
in MySQL in the 8.0 version
so Windows function in MySQL are useful
applications in solving analytical
problems so using the employees table
present inside my SQL underscore intro
database
so we'll find the total combined salary
of the employees for each department
so first let me switch my database to
SQL underscore
into database
I'll run it okay
I'll display my table
that is employee
so here we have 20 employees in our
table
using this table
we are going to find the combined salary
of the employees for each department so
we'll partition our table by department
and print the total salary and this we
are going to do using
some windows functions in MySQL
so I'll write
select
I want the employee name
the age of the employee
and the department of the employee
comma
next I'll write
the sum of salary
over
I want to partition it by
Department
so I'll write Partition by Department
which is Dept
and I'll give an alias as
total salary
so that it will create a new column with
the name total salary
from my table that is employees
the output will be a little different
this time
let's execute it and see the result
there you go so here we have created
another column in our result that is
total salary and for each of the
employees and the respective departments
we have the highest salary so in finance
the highest salary of one of the
employees was
155 000 dollars
similarly if I come down we have the
highest salary from HR if I scroll
further we have the highest salary from
it marketing product sales and the tech
team
all right
now we'll explore a function which is
called row number
now the row number function gives a
sequential integer to every row within
its partition
so let me show you how to
use the row number function I'll write
select
row underscore number function
over
my
column would be
salary so I'll write order by salary
I'll give the
Alias as row num
we give a comma
and I want to display the employee name
and the salary of the employee
from my table that is employees and I'll
order by
salary
so let's see how our row number function
will create
sequential integers okay you can see
here we have a ronum column and we have
successfully given row numbers to each
of the records you can see it starts
from 1 and goes up till 20.
okay
now this row number function can be used
to find duplicate values in a table
to show that first I'll create a table
I'll write create table
let's say I'll give a random name that
is demo
and let's see we have in this table the
student ID which is of type integer and
we have the student
name
which is of type worker
the size is 20. I'll create the small
table with a few records
let's create this table first
now we are going to insert a few records
to our demo table so I'll write insert
into
demo
values
I'll give 101
the name is
Shane
give a comma
I'll insert the second student name 102
the name is
Bradley
you give a comma
this time
for 1 0 3 we have
two records
let's say the name of the student is
hirath
you give a comma I'll copy this
and we'll paste it again so we have
duplicated one zero three
next we have one zero four
the name of the student let's say is
Nathan
then again let's say for
the fifth student which is Kevin we have
two records
I'll copy this
and I'll paste it here
let me give a semicolon and we'll insert
these records to our table demo
all right
now
let me just
run this table for you I'll write select
star from demo
if you see this we have a few
information that are duplicated in our
table
that is for student id103 and student ID
105.
now I am going to use my
row number function to find the
duplicate
records present in my table I'll write
select student underscore ID comma
student underscore name
I'll give another comma and write
row underscore number
over
within brackets
I'll write partition
by
St underscore ID
comma
St underscore name
okay
then I'll write
order by St underscore ID
close the bracket I'll give an alias as
runum
from my table that is demo
let's just run it
you can see here
okay
let me just
delete n from here and do it again
all right if you see here there is just
one student in the name Shane we have
one student in the name Bradley but here
if you see for herath
the second record it says to which means
there are two records for hirath and if
I scroll down there is one record for
Nathan and there are two records for
Kevin which means Kevin is also repeated
okay
now we are going to see another Windows
function that is called rank function
in MySQL so the rank function assigns a
rank to a particular column
now there are gaps in the sequence of
rank values when two or more rows have
the same rank so first of all let me
create a table
and the name of the table would be a
random name will give it as let's say
demo one
and it will have only one
column let's say variable a of type
integer
we'll create this table first okay
now let's go ahead and insert a few
records to our
table which is demo one so I'll write
value
one zero one
zero two
let's say one zero three is repeated
I'm doing this purposely so that in the
output you can
clearly distinguish what the rank
function does
we have one zero four
one zero five
we have one zero six
and let's say 106 is also repeated
finally we have
one zero seven
okay
let me insert these values to my table
that is demo one okay this is done
now
if I write
select
VAR underscore a
and use my rank function I'll write rank
over
then I'll
order by my variable that is VAR
underscore e
as
an alias name let's say test rank
from
my table that is demo one
let me execute this and show you how the
rank function works
now if I run this there you go
so here if you mark
so for variable a101
the test rank is one for one zero two
the test tank is 2 but
for this value which is 1 0 3 the test
rank is repeated because there was a
repetition for one zero three
so we have skipped the rank 4 here for
104 the rank is 5 now for one zero five
the rank is 6 now
106 again since the record was repeated
twice we have skipped the eighth Rank
and
our rank function assigned the same
value which is seven for one zero six
and for the last value 107 the rank is
9.
all right
now moving ahead we'll see
our final Windows function which is
called first value
so first value is another important
function in MySQL so this function
Returns the value of the specified
expression with respect to the first row
in the window frame
all right
so what I'm going to do is
I am going to select
the employee name
the age and salary
and I'll write
first underscore
value which is my function and pass in
my employee name
and then I'll write over
order by
my
column that is salary descending
I'll give an alias as highest underscore
salary
from my table that is employees
so let me run this and see how the
first
underscore value function works all
right so in our table
Joseph was the employee who had the
highest salary which was hundred and
fifteen thousand dollars so
the first value function populated the
same
employee name throughout the table you
can see it here
now you can also use the first
underscore value function
over the partition
so let's say you want to display the
employee name who has the highest salary
in each department so for that you can
use the partition
I'll write select EMP underscore name
comma
I want the department
and the salary
comma
I'll use my function that is first
underscore
value
followed by
the name of the employee inside my first
value parameter
I'll write over
here I am going to use partition
I am going to partition it by
department since I want to know the
employee name who has the highest salary
in each department
and I am going to order by
salary
descending
and I'll give my Alias again as highest
salary
from
my table that is employees
so let's run this and see the difference
in the output okay
so as you can see here
we have
the employee who had the highest salary
from each department so for freelance
Jack had the highest salary from HR it
was Marcus similarly in it it was
William
if I scroll down for marketing it was
John a product it was Alice who had the
highest salary similarly in sales we had
Joseph
and in Tech we had Angela
so this is how you can use the first
underscore value function using
partition
all right
so that brings us to the end of this
demo session on our tutorial
so let me just scroll through and show
you what we did from the beginning first
we learned about sub queries in SQL so
we initially wrote a simple sub query
and then we used our classic models
database which was downloaded from the
internet and also shown you the link
from where you can download this
database here we used two different
tables and we performed a sub query
operation
we learned how to create stored
procedures
so we learned how you can use the in
operator or the in parameter as well as
the out parameter in stored procedure
after stored procedure we learned
another crucial Concept in SQL which is
called triggers now triggers are also
special kind of stored procedures so we
saw how to write a before insert trigger
you can see it here
next
we learned how to delete a trigger we
also saw how to work with views in SQL
so views are basically virtual tables
that you can create from existing tables
we also saw how you can use views using
two different tables and an inner join
and
we learned how to display views how to
rename
view names
how to delete a view and finally we
explored a few Windows function
by giving a shot to Simply Dance plus
strategy program in data analytics this
is from her University in collaboration
with IBM and this should be the right
choice the link in the description box
below should navigate you to the home
page where you can find a complete
overview of the program being offered so
let's discuss what's in it for us today
now we will first talk about why power
bi you know why it's a popular tool and
what problem it solves what is power bi
uh and what are the primary features of
power VI uh which you can use in your
day-to-day data analytics visualizations
creating fancy reports creating
meaningful intelligent reports
for your organization for your personal
use for crunching numbers for generating
reports real time Etc
now the most popular tool for power bi
is the power bi desktop I'll show you
certain uh aspects of power bi desktop
and then I'll show you the steps how to
install power bi desktop on your machine
and then definitely power bi desktop is
a free tool provided by Microsoft but
then you can also subscribe for an
Enterprise version which is primarily
used by Enterprises for publishing their
data so we'll see the difference and
then overview of our dashboards which
can be created uh you know what kind of
dashboards can be created in power pi
so this is the agenda for us today
now why power bi so generally uh you
know visualization tools reporting tools
are required in order to create and
prepare and analyze meaningful data it
could be a data for an organization it
could be a social media platform data it
could be a data from iot devices but
something which needs to be analyzed and
some intelligent inferences and data
mining has to be done on top of it now
imagine there is today we are in a world
where terabytes of data and information
is getting generated on an instantaneous
basis on minute-by-minute basis so it
becomes very essential to churn out
something meaningful something
intelligent out of it in the market
there are a lot of other tools which are
available like clicks uh all tricks
Tableau and power bi so power bi is a
Microsoft product which is one of the
most popular products and it comes as a
free to download product Microsoft power
bi desktop which is available and I'll
show you a couple of ways how you can
install it on your machine but why power
bi is uh popular is because it provides
a lot of out of the box features drag
and drop features which we will talk
about in our subsequent sessions and you
know classes but today's session is
primarily focused on giving you guys an
introduction on what is the purpose of
power bi and what all problems it solve
uh in the in the real world so power bi
allows you to view analyze and visualize
huge quantities of data and the data
could be in any format Excel CSV text or
it could be a direct connection to a
database like SQL MySQL Azure Oracle
anyone IBM db2 so it supports n number
of uh you know data types or data sets
and it's very powerful in terms of data
connectivity
so it uses powerful compression
algorithms to import and Cachet the data
within the dot pbix file so it's as
convenient as a simple software if
suppose you import a data and then you
prepare a report and then you can easily
share the reports with your peers or
someone who's co-developing with you
either through Power bi cloud services
or even you can share the pbix file in
an email
or through any other means and you can
share the data set with the concern and
they can then work on the report
independently so there are different
ways there is no uh kind of a limitation
for you know working on power bi there
are multiple ways and it is very fast it
is the most fast uh tool to work with
Excel because definitely Excel is also a
Microsoft Technology so it works very
fast on Excel based data and gives you
numbers and Reporting at a very high
speed
so now once you have imported the data
power bi allows you to model the data
allows you to work intelligently on the
data it allows you to model data in a
way that if you are importing data from
multiple Excel sheets importing data
from multiple tables you can easily
create a relationship between those
tables or data sets in power bi and then
create visually appealing reports
meaningful reports as I've been
emphasizing and make sense out of that
data no data in silos is of any use data
in Silo means a single worksheet or a
single data set will not churn out any
meaningful information until unless you
basically join it clubbit merge it Union
it append it with some other data sets
because a single data set will never be
able to hold that much amount of
information which is generally required
for a you know important report
so it has easy drag and drop
functionality with features that allow
you to copy all formatting across
similar visualizations so just like in
Microsoft Excel we use format painter to
copy the format of one cell to another
similar feature is very very similar to
excel products or Microsoft product they
have provided that if you have applied a
a particular theme on a report you can
easily replicate that on an any other
report the font uh the header size the
background color you don't need to do it
again and again so there's a lot of
reusable features which are also
available
okay now
as I said Excel is a Microsoft product
power bi is a Microsoft product so they
have intercompatibility you can publish
data from Excel to power bi now with the
latest developments and enhancements
as of today pixel has also plugged in a
new feature called Power pivot which
I'll uh show you later down the line but
that also allows you to do a quick
analysis no you can't create of course
complex reports or uh fancy reports like
power bi but Power pivot allows you to
create you know quick measures quick
functions quick calculations on your
data quickly only in Excel so it's an
Excel plugin but whereas you know you
can power bi is also compatible with
Excel so when you create a report in
power bi it gives you inbuilt feature to
export your power bi report into Excel
format directly you don't need to do any
programming for it also
you can easily publish when you publish
your power bi reports it allows you to
give some inbuilt intelligence of
analyzing your reports in Excel and it
gives you all those of all those
features of exporting your power bi
reports into Excel which is not
available in any other tool or otherwise
those tools have to create plugins
create add-ins and probably they might
charge for it but Power bi comes with
lot of out of the box features which are
very very helpful for analyzing data in
Excel and vice versa
Azure Cloud now Azure itself again is a
Microsoft cloud Tech stack so using
power bi with Azure allows you to
analyze and share large volumes of data
so Azure basically Azure database or
Azure Cloud servers are meant to hold
huge amount of data and power bi allows
you to have seamless connection you can
easily connect to Azure data Lake
you can reduce the time it takes to get
insights and increase collaboration
between business analysts data engineers
and data scientists so azure
data Lake becomes the central focal
point where all your analysts Engineers
can keep working on on the centralized
piece of data and churn out their
reports data scientists primarily job is
to keep the data in a structured way
optimized way optimize the input output
operations disk operations and memory
utilization so that the reports also get
churned out in a faster manner so every
uh you know every person has their own
role in order to give a quick
refreshable report a quick rendered
report any report which is taking huge
amount of time to get rendered will not
eventually be used by the business users
because then it does not solve the
purpose the report should be fast
reports should have uh you know
appropriate filters slices and dices you
should be able to you know create the
reports or dynamically or you should be
able to analyze visualize the data
dynamically so all those visualization
features are available in power bi and
it works seamlessly either it is small
data or huge data it allows that you to
work on those kind of data in a seamless
fashion
right so this is just a very uh quick uh
example of how our typical dashboard
looks like dashboard is nothing but you
know you have clubbed a couple of
multiple reports on a single page and
you know if you change a filter uh uh a
single filter on the page all the
reports will honor that filter and the
numbers will change accordingly so if
you see the in this example there is a
filter or a drop down or product ID
product name employee name or supervisor
or a date range so whatever date range
or filter you will apply all the reports
on this dashboard will get changed based
on the filter you have selected so power
bi allows you to get insights from data
and turn insights into action to take
data driven business decision and that
is the ultimate goal of any
visualization tool that is the purpose
for which visualization tools are bought
and purchased by the organizations and
data is fed into them
Now power bi fetches data from Factory
sensors social media sources to get
access to real-time Analytics so that
you are always ready to make timely
business decisions so so basically there
is a a feature of live connection or cut
off data connection so either you can
work on data which is deciding on a
machine and you can just work on the
cutoff data like for example it's
there's a data which is available for
sales 2017 2018 and you're just working
on a historical data it's a cut off data
or it could be possible that you want to
be connected live uh to a real-time iot
based sensor based data or social media
data like Twitter Facebook feeds or you
know you're connected to live Google
worksheets that is also possible you
just need to publish your Google sheet
for a public domain embed that you are
into Power bi and then whoever updates
that Google sheet automatically the
power bi report will also start honoring
and consuming the new data which is
added in the uh Google sheet so all
those kind of real-time streaming
analytics is also possible and that is
one big feature and very important
feature of power bi which is widely used
and has a very huge uh you know Market
acceptance and Market utilization
now what is power bi
power bi is a business analytics service
provided by Microsoft that lets you
visualize your data and share insights
right so earlier
you know uh
Microsoft used to have a technology
called ssas now they have replaced
actually ssas and SSRS with power bi so
basically you can use power bi on the
data which is there in your Excel or any
other data source and the power bi
service or power bi desktop basically
creates a connection to those data sets
and uh imported cache it and give you a
handle to it in order to work with it so
you can create these fancy meaningful
visualizations like for example there is
a geographical map if you are importing
data for a country or a continent or a
region power bi will automatically
detect that it's a geographical
information and give you a map with
latitude longitude information and you
just need to plot
your uh your numbers on the map either
you can use bubbles or or either you can
use triangles or whatever data structure
you you want to use but all the mapping
will be available geographically then
you can create pie charts which is shown
in this visualization you can create
tree maps you can create cards where you
know you can highlight the most
important numbers like sales total sales
of your company across all the regions
or the growth chart or the month on
month uh you know sales of your
organization or number of total number
of products or units sold so whatever is
important and to be highlighted for the
management to take any meaningful
decision or any insights you want to
share power bi visualization tool the
power bi visualization uh
uh chart allows you to drag and drop and
create wonderful reports okay
so what are the features of power bi so
power bi desktop is something a
standalone tool which you need to
install on your machine it allows you to
build reports by accessing data easily
you do not need Advanced report
designing or query skills to build a
report though yes it is beneficial that
if you know some SQL programming
analytical programming or you are aware
of advanced features of any analytical
tool that might help you but that's not
a showstopper you can easily build
reports in quick turnaround time without
needing any technical background you
just need to have some analytical uh
mindset and you can create uh Savvy
visualization and you know analytical
reports
stream analytics as I mentioned you can
create a live connection uh with any
kind of data it could be iot it could be
media social media it could be Google
Docs it could be uh you know any other
kind of uh you know live connection it
could be a live database connection
itself so any insertions or updations or
deletions happening will automatically
reflect in your report
yes multiple data sources and that has
to be the primary criteria for any tool
to be popular if any visualization tool
is limited to certain data sets then you
know it will not be highly acceptable in
the market
and custom visualizations right so as I
showed you certain uh examples in the
past in the previous presentation uh
that feature is very important because
someone might want to look the kpis look
at the kpis from a different perspective
some management might want to look at
the kpis from a different perspective so
you need to you need to have that
capability to create different
visualization from the same data set now
let's take a look at how
to install power bi desktop on your
machine
so basically what you need to do is
you need to go to this URL
power bi dot microsoft.com en US desktop
okay
and you need to just
enter this
now you can download it for free so just
click over here
and
it will open Microsoft store so
basically now what Microsoft have done
in the latest operating systems is that
when you are trying to download you can
actually directly go to the Microsoft
store
and search for Power bi
so let's wait for a couple of seconds
right here so power bi desktop in
Microsoft store for me it's already
installed so it's asking me to open it
I'll open it in a while but for you for
anyone who is not installed he will see
the button of install over here and it
will automatically install in your
machine and then you can easily go and
open power bi desktop now if I click
open over here
now this is the
uh UI of the power bi desktop
I'm not going to go right now in
creating reports right away we will talk
about that in a subsequent session with
sample data sets and we will cover the
features of power bi desktop one by one
but this is what it is this is the whole
tool of power bi which is having the
visualization pane all the different
visualizations are you know can be
created from this pane
then this is the pane which allows you
to select the data data fields then
there is a report view data View and
relationship view the data model view
where multiple relationships you can
create you can view the data in the grid
of the tables which you will create and
the reports so you can create multiple
reports on multiple Pages you can keep
adding pages either you can drag create
multiple reports on a single page and it
will become a dashboard or you can
create separate independent reports on
the single page
and these are the menu options which we
will talk about how you can change color
scheming you can do data modeling you
can create new reports and you can also
transform data which is the biggest
feature extract transform and load the
data apply different logic changing data
types massaging the information creating
new joins appending the data you know
adding new columns etc etc uh we that is
what you can do in transform data so
this itself is a whole different world
it's a dedicated topic so we will talk
about that in our subsequent sessions so
what's in it for us today we will be
learning how to connect to data
different data types data files like
Excel PDF then what are the different
data importing modes and then I will
also show you practically different sets
how to import them in power bi and use
it for your visualization purpose now
what are the steps to connect to data so
now we will go directly into Power bi
and try to import one by one few most
commonly and popularly used data sets
which are most commonly used in a
day-to-day activity rest of course there
are power bi supports n number of data
sources but we will do something
practical on the most popular ones so
let's let's open our power bi
now this is my power bi and first I want
to show you that how can I import data
directly from a web page and import the
data now it is asking for a URL in order
to import data so what I have done is I
have created a Google Excel sheet with
simple data with rows and columns and
what I have done is I have shared this
sheet as published to web
okay so you just need to say publish to
the web the link as web page and
say done it's it's automatically
published and say link so copy the link
which you have published on the web copy
this link
and then go back to your tableau
paste that link over here and click ok
Now power bi will try to establish a
connection with this Google doc sheet
because it's published on the web
you need to wait for a while while it is
reading
okay now it has read one of the HTML
tables I'll select this one now you can
see it has it is showing me a preview of
the table which is there on my
Google sheet right it has 11 rows so it
has all showed all the 11 rows so now I
can go and transform this data because I
can see my headers are there starting
from the second row so there is an
opportunity for me to transform the data
so I'll go and transform it so that it
looks clean
okay so first is I need to remove the
first row which is the
null row remove the top rows
okay and then I need to use the first
row now as a header so you just click
this option use first row as headers
that's it so now if you see my row ID
order ID order date ship date all my
data is now ready so I can say close and
apply
click apply changes
now this is an example of web data
import you can go and preview your data
right now the biggest advantage of this
data connection is that it's a live data
so for example I insert another row
let me change the order ID
some some I've sub change some basic
stuff and I
it's Auto saved Ctrl s now I'll go to my
tableau and I'll refresh
now you can see as I refreshed my
power query editor I click refresh all
and I got my new row which is there in
the live data I got that fetched from my
okay I got that row the row number ID
number 12 so I have to say close and
apply
now you can see the new row the row
number 12 is now available in my new
data set in the data set because it's a
live connection it's a live connection
with the web based Google sheet okay so
this is one
important way in which you can import
data
now let's try to import data from a text
file
now I've already prepared our text file
called
sub categories Dot txt
now let me just open it in a notepad now
it's a very plain simple file tab
separated file in which you have product
subcategory ID subcategory name and
product category key so basically to
which product category this particular
sub product belongs to right so what I'm
going to do is I'm going to go back to
my get data option
and I am going to select text slash CSV
option
and I'm gonna select option mod product
subcategories Dot txt
okay so now power bi has identified that
it's a tab delimited file it has
recognized the headers
Etc right and I can now directly load
this file
okay
so now once the data is imported in
power bi it is like
irrelevant to me it's a composite data
in import I am doing right so in my
presentation when I am talking about
importing data there are different
importing modes right import our data
import can happen through different ways
okay
one is direct query mode in which I
create a live connection to the database
which I'll also show you uh using MySQL
and Ms SQL server and also you can do a
composite mode in which you can have
data imported from Excel plus you can
have direct query modes so you can have
multiple uh modes to connect and create
a composite data model and that's what
we are doing right now in our practical
so what we are doing over here is one we
have imported data from the web second
we have imported data from a text table
now after
after doing text now our next task is to
import from CSV let's try another one so
now I have imported product subcategory
now I'll import
a CSV file so again I'll choose the
option Text slash CSV and now in this
CSV file let me open this CSV file and
show you what it is it
so this is a list of all my products
product key product sub category Key
Products uh stock keeping unit Etc a
simple CSV file and I'm gonna import
that
okay so now it is identified the
delimiter is comma rather than a tab and
it has already recognized the headers
correctly so I load it
okay so now my products are there
product subcategories are there for
product categories now what I have done
is I have created as Excel mode now so
now Excel I am using to import my
product category
so now I have to click on the option of
import data from Excel and I'll say
product categories
select the sheet load
and now so my products product
categories products are categories do
with different uh data storage types but
still now the data is imported into
Power bi it is a composite data model
now another very important data type
which you can import is the PDF also
right so what I have done is I have
created a PDF called customers my
customers data is lying in a PDF so what
I've done is I've created a PDF
which has data for some columns are
there like you know customer key prefix
first name last name birthdate marital
status gender email address annual
income total children etc etc so this is
the data set
which I have created in PDF so what I'm
going to do is
I'm going to select PDF now
and import customers.pdf
and see it has recognized my table on
page one which I am going to load
okay
you can rename this as PDF
table
okay so this basically these are the
different type of
data
types we have imported PDF Excel text
CSV and web page
now let's take a look at another
interesting data set which you want to
import is the my SQL Server data set
so what I have done is I've already
installed MySQL server on my local
instance and there's already a schema of
SQL live tutorial over there and I have
certain tables already
prepared over there like Department
employee Etc so my goal is now to import
this data or create a live connection
with this data set
now in order to import
my SQL database Connection in power bi
you need to First
download a
connector MySQL power bi connector so
you need to go to this link
and then click on download and install
the MySQL connector based on the
operating system you have you click on
download and install it
after you have done this go back to
Power bi
and then give
the IP address of the database in my
case it's there in this local machine
and the schema which I want to import is
SQL live tutorial so I'll give the name
click connect
okay now it's connected so now it is
asking me which particular tables you
want to create a connection with I am
choosing department and employee
and I'm just loading them
okay so now this is the exact data which
is there in the employee and Department
in MySQL okay so this is one example of
how to create connectivity between power
bi and MySQL
now I want to do the same thing using
SQL Server Microsoft SQL Server so I
have also installed Microsoft SQL server
on my machine
and I have used the SQL Express so this
is the name of my server so which I'll
copy the server name
and
go to get data
select SQL Server
and for now database is optional I can
say direct query click ok
now it is showing me what all tables I
can import so in my SQL Server tutorial
in my SQL Server I have
I have these three tables customers
employee attrition Olympic events so I
can use probably the customers one which
is
now you can see this is the data the
customer's data which is lying in my SQL
Server okay so I can preview it and load
it
so now you can you can preview the data
in uh Power bi that is this is the data
so I can rename his customers from
mssql
and this is from
MySQL
and
okay so now
this is not the only data
sets you can import now if you take a
look at the options which power bi gave
what different type and variations of
data it can it has compatibility to
import from
okay so we can just take a look at the
categorization on the left hand side
first there are five ways like Excel
text XML Json is also possible you can
evenly directly import an entire folder
and within the folder whatever uh data
types of files are there it will detect
it PDF power key or even SharePoint
folder which is itself Microsoft uh
technology then different kind of
databases SQL server and MySQL we just
saw but it's not only limited to this
you can connect to Microsoft Access
ssas Oracle database IBM db2 postgres
SCI base teradata and then sap
databases Amazon redshift Impala vertica
Snowflake and N number of databases
which are there in the market today
Amazon
Etc
then it also allows you to connect with
its own power platforms power bi
platforms data mods power bi data flows
dataverse Etc
Azure there are different kind of
storage
mechanisms in Azure and Azure itself is
a Microsoft Technology so it has a
compatibility of a lot of azure uh based
data storages like Azure SQL database
blob storage uh Azure data breaks right
Azure HD insights path so if you have
those kind of services running on your
observed cloud services you can even
import them over here
now online services like you know you
have erps running uh or some data which
is shared on the internet if you want to
import it uh that is also possible
through certain products uh Dynamics 365
Microsoft Exchange online Salesforce
Google analytics Adobe analytics GitHub
LinkedIn sales if you want to do some
analysis of some social networking uh
you know feeds that also you can import
and then other miscellaneous are also
there web-based Hive R script python
script if there's something to import
get data from uh Google Sheets like we
store one example in our video right now
so there are multiple options available
now once you have imported the data
which is relevant to you uh in our
subsequent sessions we will see how to
create relationships but just giving you
a glimpse that whatever data you are
importing power bi Auto detect certain
relationships and it will create for you
but then you can go and manually also
change so this is the composite data
model which is getting created in the
back end while you are importing the
data you can easily go and manage these
relationships either keep them as is you
can delete and create new ones manually
so there is no limitation in that
so this is what we have witnessed we
have imported data from different files
types data types and then you know we
have tried once it is imported into uh
Power bi then there is no limitation of
how you use it you can create
visualization across different data sets
and then create your
standard reports
so this is the example of importing data
from web importing data from a database
from a PDF
and then once you have data you can
shape and combine data you can basically
do what whatever transformation you want
to do you want to uh make joins merge
the data so for example if we go back to
our power bi and if I go back to my
transform data section
now as I have now different data sets
available with me I have I can do any
kind of you know operation
transformation on the data right uh
so like I showed you I uh upgraded the
header row because one of the imported
data was not showing the header
correctly uh or this columns like this
exact one column is extra I can remove
the column
right all those Transformations whatever
I do in the back end gets captured in
the applied steps section right
this is the customer data you can create
uh you can merge it you can append it uh
you know with other data sets right
let's for example I want to create a
merge data set of my categories and sub
categories so I can say more selectors
two data sets and say merge queries as
new
and
I can select product categories and
product subcategories select
product category key on both the sides
and then they do a left Auto Zone so
whatever product categories are there
I'll get the subcategories associated
with it and I'll create a new table
which will have
now I have the table which has the
category and the subcategory and
subcategory in one table itself so I can
rename it now to as
category
sub category
table it's a it's a merge basically it's
a join between category and subcategory
now I have a common table right and I
can close and apply
so imagine I have created a
new table which is imported created from
one data set is which is Excel page
another data set which is text based
see this category subcategory table so
now I can use it
the way I want
in my visualization reports so that's
what the presentation says right that
once you have uh the imported data you
can shape you can combine you can adjust
you can do whatever transformation you
want to do and create your visualization
okay so what topics we are going to
cover today we are going to talk about
different types of data modeling and the
most important part
and aspect of data modeling is the
cardinality the cardinality which you
basically decide after reviewing the
nature of data and after you've imported
it what kind of cardinality you have to
basically highlight right and there are
different type of cardinalities which
you might have heard earlier also if you
are from PL SQL background like one is
to one one is too many Etc we will we
will talk about that
now what are the different types of data
modeling now dimensional data modeling
is one of the most popular and most uh
you know widely used uh modeling in
dimensional data modeling you have
Master data uh like for example customer
data date store data product data so
these are like you know uh less
frequently changing data sets so there
is an organization right and you have
set of customers their email ID phone
numbers
Etc that will change less frequently as
compared to the sales transactions
because transactions are happening every
day every minute so sales is a more fast
changing data set in dimensional
modeling which is in the terminology of
data uh is also called as a fact and
customer store product which are like
more of static data and less changeable
data is sometimes called a diamond
ancient so this is a typical dimensional
data model which is typically used uh
sometimes right and then there is
another model which is relational model
this is a typical model which we have
been using in database design like you
know primary key foreign key
relationships so for example you have a
customer who has purchased a product so
probably he might have the customer
might have the details of the product
which is processed and you will make a
join between customer and product table
and even you can make a join between
product or product type or customer or
product types customer table will also
have a key to the product type so this
is less conducive for reporting but it
is more of a transactional relational
model but of course this is also
feasible but from the power bi
perspective when we talk about reporting
and visualization this is the most
extensively used dimensional data model
and this is what we are going to see in
our example now so what I'm going to do
is I'm gonna show you certain data sets
first we will prepare and create certain
offer data sets and then we will Import
in our sample power bi file and then
slowly slowly we will create the
relationships
now one important thing which you need
to understand that in power bi if you go
to Power bi there is an option that that
power bi Auto detect new relationships
after data is loaded and import
relations from the data source on first
load so for example if you are importing
the data from a database where you have
already defined the primary keys and the
foreign key relationships so uh that is
the first option which power bi will
Auto detect and secondly if suppose you
are importing two different kind of data
sets one is Excel one is csb and if
power bi detects a common column key
columns it will auto detect a
relationship which you can go and later
change modify manage in your
relationship
menu manage relationship menu in power
bi which I'm gonna show you okay so if I
open a power bi and this is where the
option lies go to file
go to options and setting options
data load
and these are the two options which are
by default check you can uncheck it and
auto and manually prepare relationships
there's no limitation to that but if you
uh keep it checked then power bi will do
its job to detect the relationships okay
now coming to the next important factor
cardinality
now before I start playing around with
my data and start showing you certain
relationships it's very important to
understand these four types of
cardinalities one is many to one right
so basically
many to one means that many orders
contain data of One customer so per
order one customer is there so from
customer to order or product or delivery
address it's a one-to-many relationship
and from the other side from order to
customer perspective it's a many-to-one
relationship
okay
second other cardinality is one is to
one one is to one relationship is only
applicable when you are saying it's an
extension of the current table so for
example in one table you have employee
details and you are extending the
details of the employee in another table
like employee address employee ID so
that is like one is to one there is no
multiple records of a single employee in
the address table only one employee ID
exists right
now one is too many as I said is the
reverse side of many is to one so in
customer table only one customer record
exists per customer and one customer can
place many orders for multiple products
and can also have multiple delivery
addresses so that way this is a typical
one is to many relationship we will be
seeing this example also in our sample
data set
and last is the many-to-many
relationship now many to many is a very
typical example so which I'm going to
show you practice practically and in our
case we will see that like for example
you have placed an order for a
particular product uh you know but there
are multiple fulfillments which has
happened so suppose you made order for
10 products but at the back end when the
company is fulfilling it is first
fulfilling the first two products then
the rest three so basically you the
Fulfillment is happening in batches so
one order ID might have a multiple
fulfillments for the same order ID so
there will be a multi many to many
relationship which I'll show you
practically so now with this background
let's
start importing our data now the first
important thing which we need to import
is the Master data so first I'll import
all my master Dimensions which is uh
which I'm going to you know use in my
example so first is the customers table
customers data
so this is the customer details the
customer key prefix first name last name
birth date marital status and gender
some redundant columns are also present
but we'll remove it
so my customer data is loaded now
today's session is all about this
section of modeling so we will keep our
Focus over here
okay now some columns probably some
blank columns are there I can select
them and say delete from model
yes
okay so now this is my customers data
with the relevant columns and the key
per customer customer key
now there's no relationship in this
model right now right because only
single table is there and the associate
data is only imported now let me also
import my another important master table
is the products
select the products data product key
product sub category product SKU product
name model name product description
color size so just see all the relevant
information only specific to the product
is available so I report it
okay now see there's no relationship
between product and customers directly
because until unless a customer makes an
order places an order for a particular
product there is no join right so now
between these two tables the most
important now another table which will
now make sense is the sales order table
sales table
now I I'm assuming that power bi have
Auto detected the relationship now you
can see that because I've already uh
take that check box now let's see what
power bi what relations power bi has
Auto detected let's first say check the
relation between customer and sales I'll
double click this
join now what it has done is it has
created a join of many to one
between sales and customer so what does
that mean is that one customer has can
place many orders right and that is that
it is detected by the quality of the
data and the data sampling which power
bi has done you can also reverse this
relationship here I can select customers
and I can select sales now it has become
one too many so that you can also do
manually so that is what I said whatever
power bi is detected it is up to the
description of power bi internal uh
configuration and algorithm but you can
go and change it so this is now you can
this is by default active so we want to
keep it active One customer many sales
orders cross filter Direction means that
only from customers to sales is the
filter applicable not reverse I'll come
to this with my another example but
first let's
change the relationship so one is too
many means from One customer and many
sales orders
similarly let's see what has happened at
the product site
of the relationship
similarly power bi many sales orders for
one product you can for simplicity's
sake you can say products
sales
product key is the join now just focus
one more thing please also see the the
column on which the join is is the grade
column grade out column product key is
also here and product key is also there
and it is what we wanted so one is too
many relationship from product to sales
table and act
now looks fine this is something which
is looking logical and probably now we
can
proceed further to create a report
now let me explain the cross filtering
with an example
now for example I want to check in a
report
that what is the count of products which
uh which a particular customer has
ordered
okay so what I'll do is I'll select the
product count of product name
now if you see
and for each customer in front of each
customer name the count is coming as 293
293 it is getting repetitive because
because there is a one-way filter
Direction filter between customers and
sales and sales and products right so
this join
is single sided it means that from
customer to product you can't find a
relationship because it's a single side
cross filter right what does this if I
change it to both it means that it is
equal to a join between product and
sales and every product detail now is
appended to the sales table so if I want
to make you visualize this you need to
go here
I'll first open my sales table
we can also open it here let's make
click it says okay now if I click OK you
can see the single arrow is changed to
Double Arrow it means it's a it's a both
site filter
So when you say a both side filter it
means that implicitly within power bi
you can imagine that all the product
columns now will get appended because of
both ways filter you have applied and if
you go to your report now see the change
of the numbers now 40 20 so the total
count of products across all my
customers come out to be 293. now the
report looks uh correct if I change the
relationship from back to single between
product and sales then you can't make a
join between customers and products
basically you can't derive the product
count from the product table see this
if you have to live with it then you
would have to go to the sales table get
the product key and get the value of
count of product key but that is not
correct okay
so if you want a report in which you
want the count of product name
and even if you want a count of distinct
product name so this will not come
correctly you would have to go and
change the direction of the filter which
is from single to both so this is a
typical example they where you want to
use a
two directional filter
now let's proceed further and import
other data set in order to show you
another example
now I want to show you an example of
one is to
1.
so I have another table which is called
customer details so the key in this
table is again customer key but only
email address annual income total
children education level Etc other
details of the customer is there
so I'm loading the customer details now
you see it has Auto detected a one is to
one relationship
but what is the meaning of one is to one
means One customer key only has one
entry in customer details there is no
multiple entry so if you click this
button
it's a one is to one and the cross
filter can be both or single doesn't
matter because one customer will have
only one value you can make this as
active okay and if you go to the
customer report table you can now easily
associate a
email address with the first name you
will get one is to one
record
so now you can see that with one is to
one relationship
with the first name I have Associated
the email ID and for each email ID
there is a Associated first name method
so this is an example of
one is to one relationship so in this
example what we have explained is that
for each customer there is an Associated
customer detail right uh so you have the
first name email address education level
homeowner occupation and total children
count
so in this report what we have done is
uh if you click over here so the first
name and the email address okay so
there's a one is to one relationship
and then
and if you drag
the customer key
report takes time to render and even if
you can
so this is the reporting output you have
the customer key first name associated
email address and the count of product
names uh which the customer has ordered
now this is an example of 1 is to 1.
now I want to show you an example of
many too many now for that I'll import
my
fulfillment data set
foreign
data set there is a column for
order number so basically what I'll do
is I'll drag order number from here to
here
okay so now what has
um
uh Power bi detected
I'll do one thing I'll select sales over
here
fulfillment over here and order number
to odd number
okay so it's a many-to-many relationship
so it means that per order I have
created multiple batches to fulfill that
particular order now
many to many relationship is a
definitely a candidate for
both ways cross filter detection uh a
Direction but you can you can check that
but definitely uh Power bi shows a
warning that this relationship has
cardinality to many to many and this
should only be used if it is expected
that neither column contains unique
values okay so we know that fact that's
why we are accepting this relationship
as many to many because we know there
are multiple order numbers over here in
the sales table which are mapped to the
multiple order numbers in the
Fulfillment table we'll click ok
now you want to keep the uh
direction as both ways or One Direction
that is up to you the way you want to
map the report so I can double click
over here
and you can even click so now you can
select from which way single filter you
want from fulfillment to sales or sales
to fulfillment I'll prefer sales to
fulfillment and click ok
okay now we have our all our different
kind of relationships over here uh which
we have tried to shortlist one to many
many to one one to one which is uh this
example and many too many
now if I show you further relationships
which you can keep on adding like for
example I have a uh the example of
territories
now in which particular territory the
sales was done
map it over here
okay so now it's a typical one is too
many relationship because territory is
my master table uh where I have a static
list of continent country region and it
is mapped to the uh territories which
are for in which my orders have been
placed so it's a typical one is too many
so that way you know you can keep on
adding data then you have
uh details of returns
now this is another transactional table
which is about the orders which have
been returned rather than being you know
written by the customers so you have a
product key
uh so automatically power bi has
detected a relationship between the
product key and the product uh table
right and even if you can join the
territory key in which territory the
return has happened
right so mostly the most common
relationship which you will observe is
the one is too many because
as I told earlier the most common
relational model is the dimensional
model uh the static data the slow
changing Dimensions the scds are the
master tables and the most frequent
changing are the fact tables so if I
talk about a typical dimensional model
the Fulfillment table sales table and
the territory stable sorry uh the
Fulfillment table sales table and my
returns table are the fact tables of my
data model now so far what we have done
as per our last session is that we did
data modeling on the different data sets
which we had imported in power bi like
products sales data returns fulfillment
customer details uh and customer Master
data calendar details Etc so in the last
session we prepared a data model and
established the relationships between
these different data sets like one is to
one one is too many many to one one is
to one Etc and we saw the examples now
once our relational model is prepared
our data model is prepared now our next
activity is to create certain additional
columns which we want to derive basis
the data which we have imported
so for example I'll start with my
product data set now in my product data
set I want to introduce a column which
basically
categorizes that if any product which
has a color uh you know red black or
gray I am going to tag it as a colored
product rest I'm gonna say not a colored
product right so all these are like
example of byte type product skus so for
that now in order to introduce a new
column you just need to do what you need
to select the table in the data grid go
to the table tools and say new column
okay so column will get appended to the
rightmost part and you will start seeing
a uh formula section typical to like you
get in your Excel now I'm gonna say that
the name of my column is going to be
byte type color okay and I'm just
creating a if condition if
product
color
is equal to
black
okay
or
or if it is equal to red
or if it is equal to
Gray
then
say yes it's a colored product I'll say
no
okay so now you can see this is the
product color red and black they are
saying byte type color Yes blue is no
multi is no etc etc so this is a classic
example of an if and else condition
based conditional column
okay so you can create such columns now
second column custom column which I want
to create is I'm going to call as
discount
now this is the pricing of my products I
want to associate certain discounts
which I am ready to give to my customers
this is the product category like what
is the pricing of the category again I'm
going to use make use of if else but in
a nested way so if I am saying if my
product
price is less than 100
then I will give
zero percent of
uh zero percentage of discount so 0 into
product price just to keep it consistent
now I'm seeing
else
if less than 100 and 0 else I'll check
again that
if
the price is
less than
500 then I am ready ready to give one
percent discount
on the
product price
else
I'll move further so like this I have
created a formula
so what I'm saying is if product price
is less than 100 give zero percent if it
is less than 500 then give one percent
less than two thousand then one point
five percent less than three thousand
then two percent and otherwise else less
than three thousand if a two percent
else three percent right now after this
column is created
now you can check right so this C the
product price for this particular
product it is less than 100 so that's
why there is no discount it is
uh between 100 to 200 then this has been
given a one percent discount so like
this all the discount column is now
calculated now this column is available
just like a regular column in my product
table
now after this I'll go to my sales table
now in sales table I want to
identify a
create a column called as cost
there is no
product cost column over here so that
will be derived
so let's create a column
called as cost
and it is derived by order quantity
into now the cost of the product
is in the product table and I know I
have already created a relationship
between product and sales table
so I just need to select the product
cost column now only keyword which I
have to use in power bi is the related
keyword so this will pick up the
relation
and now for this particular sale order
the cost has already been derived so
this order number this is the cost for
which uh the product has is the costing
of the product for this particular order
thank you
okay
now I'm gonna create another conditional
column over here called as
order status
I'm saying if
any order whose order quantity is
greater than 2
then for my organization it's an urgent
order
else it is a
normal order
oh sorry
mustard
so this is my
order status column and I have my order
quantity
urgent or normal
so any order which has order quantity
one is normal any order which is having
order quantity as greater than
2 is
urgent you can see this
so there's a this whole power bi
uh tabs and sheets allow you to also
review the data what you're doing so
it's very convenient
now
I have my sales data now what I want to
bring within the sales is my discount
column so here also I want to bring the
discount which I have created
so I'll say discount
[Music]
will be order quantity
into
related
product discount right so the discount
calculated column which I had created
under products I'll bring over here
now I am creating the order level
discount so if you see for this
particular order there's a 25 uh
uh you know for 25 rupee discount at the
cost is 100 000 rupees okay
and what is the order price now so I
have taken the order cost the discount
now I have to create a column call as
price order price so that will be
again order quantity
into related
price which is per product
price and
okay so now I have the cost the discount
and the price right available with me
now I want to
calculate the total
uh
total revenue total profit and loss
right
per order how much
so first I'll calculate per order how
much revenue I am generating so now I
have to generate a column called as
Revenue
revenue is price
minus discount
so 1700 minus 25
1700 minus 25 2071 minus 42 and if I
want to calculate the profit per order
then it is
Revenue
Minus cost
okay so now you can see you know typical
custom columns calculated columns which
you have created are all playing around
with the number numeric values numeric
data primarily and trying to give
inferences into per order cost discount
per order price revenue and profit so
typical calculation columns which I have
prepared in front of you
now let's take a look at other different
variations of custom columns
I'll create certain columns for text
based custom columns calculated columns
using Text data
so I am now moving towards my customer
table in which I have customer key
prefix first name last name birth date
marital status and gender
now I want to create a new column in
which I want to derive the age of each
customer as of today right so I'll use
another function a date function called
as
date div
now date div so I want the difference
between the birth date of the customer
and as of today
in years
okay so this customer as of today 68
year old one is 74 68 57 Etc so this is
one derivation of a calculated column of
age
let's take another example now this is a
text based column where I want to
derive the full name of the customer
now here I'll say first
lowercase in lower case I'll concatenate
the prefix
then ampersand
space
ampersand
first name
Ampersand space
ampersand
last name
and closing brackets
Etc
full name
okay so this is an example of full name
in low cases
now another
calculated column conditional column at
the customer level
I want to identify a flag which says who
is my Target customer base is the
demographics shared over here Target
customer
so I'll say
if
the marital status is equal to
m
and
total children
hurry
and total children annual income
so okay so let me change the logic a bit
so marital status is M and
age
is
less than
50
these customers are my Target customers
okay so I would say
yes
else no
say this
he is a marital status is married Logan
Diaz and age is less than 50 else
everyone so if I try to filter
so these are the my Target customers
69 out of 1178 so this is just a
conditional column but a logical
condition an example which I am trying
to highlight over here
okay now
let's look at certain calendar
date oriented columns calculated columns
very typical like now I have a simple
date column now I'll keep adding certain
columns which are you know which help
you which will help you understand how
we can uh
you know do some calculations on the
dates so like for example I want a date
which is 12 days after the current date
the date in the column so just simple 12
Days After
select the date and add 12.
now if you see the date format you can
go and change the format at the top
and if whatever you feel like like this
now see 12 days after first Jan 2015 is
13 Jan 2005. you can go and change the
format and other details
let me also show you if I go to my cost
and other columns I can go and change
the format this is like a currency cost
is currency so I can go and
select the
change the currency type and you can
even show the dollar value or whatever
currency type it is
so for numbers you can do currency or
text or dates you can select the format
so this is available at the column tools
level
now in customers
like we had our
column of full name so now what all
things are available format as text okay
data type text so very minimal options
are there with date you have options of
the date format
now next
I want a column which defines the expiry
date okay so eight months prior
to the
expiry date within which is coming up in
eight months so I'll create a column
called as eight months expiry
and then there is a e date function I'll
use that I'll use my
date in the data set
comma I'll say eight so now this date
column will append eight months to my
actual date and again I can go and
change the format
correct
now another important column like I want
to know the date name
so I'll use a function called as
format
and I'll select my calendar date column
and I'll say give me the DDD
format of it so it will give me the day
name the day the day name of on that
particular date
next
years in between
so I want the years in between the
today's date and the date of my calendar
so equal to
D Def
the calendar date
comma today
comma year
end
seven years 2015 to 2022
then last date of the month
so if I want what is the last date of
this particular calendar month
I will use a function EO month which is
there and label in the
power bi so I'll say
last date of the month
equal to
e o month
then
just select the calendar CSV date
comma
0 in months and enter
change the format
then similarly start of the month
so I'll use a function called start of
month
so for for all January dates end of the
month is 31st Jan and start of the month
will remain 1st of Jan
change the format
foreign
next I want to know what is the weak
number of that particular date so now
there is an inbuilt function called
week number week num and just pass the
date and you will get
the weak number first week of the year
second week of the year Etc
now another very
good example is whether the weak
day is a weekday or a weekend right so
what is it's a weak type okay
so I'll check I'll put a if condition
and there is a function called weekday
and I'll pass the calendar
if it is less than 6
it means it's a weekday as it's a
weekend
all Saturday
and Sunday D names will come as weekends
or else everything else will come as
weekly
so these are very different variations
of different column types calculated
columns which is a very important
utility and uh and any proper bi project
you will definitely be ending up
creating n number of calculated columns
to derive your numbers to prepare your
reports but it's important to understand
what all things we can do yes there are
n number of functions available in power
bi uh but uh what I've tried to Showcase
over here is some important functions
but Basin is your utility basis your
problem statement you can look up for a
relevant function in the power bi
dictionary
now with this
introduction to calculated column this
is the base for us to now get into our
next session where we'll be talk we will
be talking about creating dacs measures
and Dax functions we will be using power
bi Dax functions which is much more
powerful than simple uh calculated
columns where you can do more uh complex
calculations uh you can calculate totals
and then use them in the reports so for
that we will look up into our next
session
try giving a shot to Simply Dance plus
strategy program in data analytics this
is from third University in
collaboration with IBM and this should
be the right choice the link in the
description box below should navigate
you to the home page where you can find
a complete overview of the program being
offered in this session we will start
with certain exercises which we will
perform in Tableau in order to
understand some basic concepts
now in order to learn Tableau the basic
first step is to import a sample data so
in our case what we have done is we have
imported a sample Superstore which is in
Excel format a sample superstore.xl
which has three worksheets in it orders
people and returns so by importing this
data into Tableau first of all we will
create relationships between these
sheets in order to identify who all have
placed orders and how many people have
returned the orders we will do some
analysis on the orders placed by certain
set of people and Order returned by a
certain set of people
now as we have imported uh the sheet we
will make certain joins so the first
step is to drag the orders
table the order sheet on the
relationship canvas here okay and you
can see the data sample data the first
100 rows over here
okay then now we need to create an inner
join with people's table between order
and people okay so if you see
it has automatically detected the field
names on which the inner join has to be
created so on the order side you have
region and on the right hand side which
is the people data you have also a
region okay so both these columns are
common and that's how we have made a
join between orders and people data
so if I close this box and go and check
the people's data
yeah
open
so see the region and the person these
two columns from the people table have
now been
joined with the orders table right so it
means that these are the orders in a
particular region which has been placed
by
in this region
let me show you the sample super stored
Excel file now this is the structure of
the file you have a sample list of
transactions basically the orders which
are placed by customers
across multiple regions
south west of USA South Region west
region then you have a list of order IDs
which have been returned so basically
the order ID in the returns sheet
matches with those orders in the order
stable
and then you have the people
sheet in which you have region
and a person associated with that region
the sales person associated with that
region okay so basically when we are
combining joining orders with people we
are joining that
which orders
belongs to which region and who's the
sales person associated with it so what
we have done over here is we have made a
inner join means all the orders should
belong to particular region and that
region is in the people's sheet
and then in the second step
now we will make a
left join between returns and orders
not inner join we'll make a left join
between returns and orders and we will
make a join using the order ID
okay so just edit this
click on left
and
select order ID as the join column now
what does left join means left join mean
is that consider all the orders from the
orders table and only consider the
orders from the returns table which have
data means which are returned otherwise
show null for the order IDs which are
not returned so if you see this is the
these are the two columns from the
return statement and these are null
because this is relevant to the order
IDs which are not returned okay which
has been accepted by the customer but
these are the orders for which you see
data in the returned and Order ID column
it means that these have been returned
now with these joins in place please
save your book and now we have our
relations created in the
um
in the Tableau now we are ready to
create certain reports and extract
certain kpis using this relationship
model
now we'll move to sheet1
okay
and first we will place
state
and person on the rose
sheet okay
then
I'll go to my
numbers and
put the profit or D so per state per
person how much profit I am making as a
company okay this is my goal to check
now
sort by highest to lowest
so California is giving me the maximum
profit of 76 381 then New York then
Washington so this is the sorted order
in which I have
listed my profit in descending order
now I can also check what are the number
of orders
placed
and check the distinct count
what happens
so out of 120 out of the total orders of
127 okay so this is the number of total
number of orders which have been
returned for California is 127.
it's 16 29 so the sorted order is as per
the revenue as per the profit and this
is the details of the orders which have
been returned per state
so if you see for connected for a
cancers there are zero returns
so you can also extract data
table to refresh his orders and identify
new rows using order date so as and when
new data is being added you can
refresh it now say extract
and now you can save this information
profit
by state
and click save so this is the extraction
of this particular report which is
possible in tableau
so this is the first exercise which we
have completed for
reviewing and analyzing the profit per
state highest to lowest and within that
per state what are the number of orders
which have been returned by all the
customers the distinct count of order
IDs which have been returned
now let's start our second exercise on
creating calculated fields in tableau
now in this exercise we will be doing
certain
activities like we will be creating a
set to show the states which have more
than 100 customers then we will be
creating a calculated field to show an
average sales per customer okay then we
will create a calculated feed to show
the sales goals and then show emerging
and developing stage so these are the
four kpis which we have to derive
now the first thing we have our sample
Superstore data already imported and the
relationships created in a join with
people and left join with returns
now we have our sheet 2 in which we will
create the states a list of states which
has more than 100 customers
so what we have to do is we have to
click right click on the customer name
and click create set
okay
now we have to give the name as states
with 100 plus customers
and then go to the condition tab
select by field
and then apply condition as
count of customer name
greater than equal to
100
and click
ok
now we have this set created states with
100 plus customers
now to determine average sales by
customer we have to now create a
calculated field
so go to the analysis and click on
create calculate field
okay
now name it as
average
sales per customer
and now we will say average
we will use a
foreign
that per customer we are using a level
of definition function include which
means that per customer what is my
average sales right we've already used a
function aggregated function called
average so we are saying per customer
give me the total and then give me the
average per customer so we're going to
click ok
now create another calculated field you
can also create from here
and name is as name it as sales goal
now in this we are going to type the
formula if
minimum
States
with 100 plus customers equal to true
then
sum of
sales
into 1.3
else
average sales per customer into
100 so me we are saying that
if the customer belongs to the set of
states with 100 plus customers then the
sales Target should be
1.3 times the actual sales as of today
else it should be 100 of the average
sales per customer
now let's create another calculated
field which we call as
emerging
or developing state
if distinct count
of customer name
is greater than equal to 100
then
the state is tagged as developing state
yeah
else it is called as
emerging state
okay so we have now
three calculated Fields average sales
per customer emerging or developing
State and sales goals
now we will use this in our reporting
so we will drag sales goal under the
columns
and then I'll drop my state
so now this is the statewise sales goal
depending whether the state has 100 plus
customers or not
then add your customer name
make the measure as count distinct
and make it as discrete
okay so if you see this
we have the count of customers per state
and the
the sales goal
for that particular state
and now
I'll put my sum of sales the total sales
which I want
which is there per state
now go to show me and select
this particular chart
bullet graph
now to bring sales goals to column right
click on the sales access and select
swap reference line fields
now from your left hand panel drag and
drop emerging or developing straight on
the color panel
okay so a merging state is the orange
one and the developing state is the blue
one
and save the sheet as
developing and the emerging States
so if you see this it's an emerging
State because its count is less than the
customer count is less than 100
its sales goal is
57384 but the actual sales is 19511 okay
so now this is a developing State its
count is greater than equal to 100 and
its sales goal and its sales is exactly
the same it matches so that's why you
are saying the bar and the blue bar is
ending exactly where the vertical bar is
foreign
so what we are trying to depict is that
whether the state is going Beyond its
Target sales goal or it's behind it
and you can see that using this
particular vertical bar like for example
Michigan its sales goal is 71 952 but
its actual sales is seven six seven two
seven zero average sales so that's why
it is be above its Target
and it's a developing state
because it has more than 100 customers
so you can even sort
by the count of
the uh customers higher to lower so all
your developing state will group from at
the top and the emerging States Will
Group at the bottom
or you can sort by
the sales goal
so the orange bar is the sales goal or
the blue bar so depending what sales
goal
has been
derived for each state
try giving a shot to Simply Dance plus
strategy program in data analytics this
is from third University in
collaboration with IBM and this should
be the right choice the link in the
description box below should navigate
you to the home page where you can find
a complete overview of the program being
offered in today's session we are going
to take example of the Netflix database
which we have and we will prepare
certain reports in order to identify
what kind of reports we can generate
from such a data set
So currently on my screen what you can
see is the Tableau the Netflix data
sheets the data sources where you have
the Netflix shows movies their
Associated duration uh what kind of
shows or movies are there the release
here are the associated rating
description and a key which is the show
ID now this is the unique identifier for
each show in this Netflix title sheet
and with this show ID all other sheets
are related like you know who are the
directors of the show in which country
the show was released
what is the cost of each show all that
information is in this sheet and to
which particular category basically the
listed in category is being uh listed in
this particular sheets so what we are
going to do is first we are going to
import this sheet into tableau
okay and then create relationship
between them using the Tableau
relationship canvas
so first our the primary transaction
table the sheet in which all the
information related to movies is there
or shows is there is in Netflix titles
and then we will drag
sheet like Netflix cast titles cast now
Tableau has automatically identified the
relationship between the show ID of
titles and show ID of titles cast and it
has made a join
so if you double click this
you can see this relationship
cardinality and the related fields
similarly I'll drag release titles
category
countries and directors
Now by virtue of this all the sheets
have now been joined with Netflix titles
and with this relationship ready we can
start preparing our reports
now let's create a basic report where we
can just glance the data like you know
whatever we were seeing in the Excel how
it looks in w
so you have all the types of movies then
for example I drag the release Here
now first I do not want to consider it
as a
Dimension so I'll just
release here or category per type there
is a release Here and then I'll drop the
listed in
so now these are the categories so per
year the details of the categories right
and then you can
drag the title and the associated rating
so this is just a view of your data
we can name it as shows listing report
now let's try to create some report for
some
measures some numbers
Etc
so now let's check in which country how
many movies or titles were released you
know what is the count
so first let's drag the
country
so as soon as we drag the country
uh
field it is identified that it has the
geographical names and identified the
latitude and longitude details so
Tableau internally does that
automatically and it has identified the
spots across the globe of the relevant
country
now let's drag the listed in on the
color section
now what it is doing is it is showing in
which country what different kind of
uh category wise movies are or shows
have been released like in Sri Lanka
documentaries have been released in
India action and adventure United States
action and adventure like this
now let's
put
a count of
the titles right so if you see 247
action and adventure movies or shows
have been released in United States
okay so this is one inference by this
particular report you can
identify
so let's see this report as
listed
in
by country
okay now let's create another report
we will call it as the per year
statistical report in this first I'll
put the release ears in the column
now
count of Netflix titles so this is the
count of Netflix titles per year 2017
2016 14. and then count of Netflix
titles in the countries
this is the second bar chart okay now
what I'll do is I'll combine it into one
so one report
we are
moving in the bar creating the bar and
one in the line now I just have to
so we will click on the count of Netflix
titles by country right click Market
dual access so now as soon as you click
click this both the uh charts have been
combined and right click over here and
say synchronize access
okay
okay so now if you can see see right in
2017 you had uh 2 303 Netflix releases
across all categories and 1159
titles right one zero six three titles
in 2017 so this is a descending
representation of the count of titles
released per year across category and
across titles
so let's call this as
per year
starts
now let's create another report
number of shows
per title okay
so or or sorry per rating so we will
drag the rating column
and
we will
count of titles
and
unique count of titles and unique count
of show IDs
okay
one is bar and one is line
okay
so this is a report which shows rating
wise titles and the show IDs
so in the tooltip you can see the
listing count of Titus and show ID per
rating
okay so let's call it
shows per rating
now let's create another sheet
call it as
shows
by caste
so in this
what we're going to do is first drag the
cast column on the rows section you have
name all of all the cast and then we
drag the
show ID on the column section and put a
count d
and sort it okay
you can remove the null cost and this is
your
sorted order and on the labels section
you can say show Mark labels and you
will see the count return open care has
done the maximum Shahrukh Khan
Shah so this is the details on this
sorted order that who has done how many
shows okay
then next is
shows by director similar to shows by
cast
drag
director into rows
or differently if you want to prepare
into columns and then count off
show ID
okay sort it
and you can actually filter out the null
director value
and this is the account
put the label
so you can see Jan's Twitter has the
maximum shows
okay
then create another report
shows by category
now similarly drag the listed in
in the rows and show ID count
remove the null category
and sort
labels show Mark labels so International
movies are the highest category dramas
the next comedy International TV shows
this way
you have your category
now let's create a dashboard in which we
want to bring combine all these reports
and
take a common view so first let's drag
listed in by country
then
shows by director
also please let's set the size as
automatic
then let's drag shows by category
and shows by cast
now these four reports are on a single
dashboard we will link them to each
other so go to worksheet actions
add action
filter
select dashboard one
only select shows by category here
and in Target dashboard one
except shows by category keep everything
else
and in the source sheet just select
click select
click ok
okay now whatever category you will
select over here
that related category data will
automatically be shown in other reports
like see International movies across the
globe across countries directors of only
International movies who has done the
maximum
Johnny Tow and shows by cast right that
who has done the maximum International
movies or dramas
or comedies
travel has a maximum number of comedies
and then if you see the comedy movies uh
you know these are the count of comedy
movies released across the countries and
the who is the director
another a very interesting report which
you can prepare is that for example you
want to check in which country maximum
duration of
uh your
maximum duration of
movies have been released so
first let's create a measure
remove this
okay
so if you see
maximum duration of the movies or the
entire Netflix content is maximum United
States then in India these many minutes
next is United Kingdom and you can also
change the colors
whatever you feel is as per your
standards or as per the convention you
can change the color combination
so there are multiple ways you can
generate reports and uh hope you have
understood how you can leverage such a
data to create your reports
start with our first question
it's real important to note that data
mining is a process of finding relevant
information which has not been found
before it is a way in which raw data is
turned into valuable information you can
think of this as anything from the cells
stats and from their SQL Server all the
way to web scraping and Census Bureau
information where the heck do you mine
it from where do you get all this data
and information
then we look at data profiling is
usually done to assess a data set for
its uniqueness consistency and logic it
cannot identify incorrect or inaccurate
data values so if somebody has a
statistical analysis on one side and
they're doing their you might in the
wrong data to then program your data
setup so you got to be aware that when
you're talking about data mining you
need to look at the Integrity of what
you're bringing in where it's coming
from data profiling is looking at it and
saying hey how is this going to work
what's the logic what's the consistency
is it related to what I'm working with
find the term data wrangling and data
analytics data wrangling is a process of
cleaning structuring and enriching the
raw data into a desired usable format
for better decision making
and you can see a nice chart here with
our Discover it we just structure the
data how we want it we clean it up get
rid of all those null values we enrich
it so we might take and reformat some of
the settings instead of having five
different terms for height of somebody
you know in American English or whatever
clean some of that up and we might do a
calculation and bring some of them
together
and validate I was just talking about
that in the last one need to validate
your data make sure you have a solid
data source and then of course it goes
into the analysis very important to
notice here in data wrangling 80 percent
of data analytics is usually in this
whole part of wrangling the data getting
it to fit correctly and don't confuse
that with data cooking which is actually
when you're going into neural networks
cooking the data so it's all between
zero and one values
what are common problems that data
analysts encounter during analysis
handling duplicate and missing values
collecting the meaningful write data the
right time making data secure and
dealing with compliance issues handling
data purging and storage problems again
we're talking about data wrangling here
eighty percent of most jobs are in
wrangling that data and getting it in
the right format and making sure it's
good data to use
number four what are the various steps
involved in any analytics project
understand the problem we may spend 80
percent doing wrangling but you better
be ready to understand the problem
because if you can't you're going to
spend all your time in the wrong
direction this is probably uh the most
important part of the process everything
after it falls in and then you can come
back to it
two data collection data cleaning number
three four data exploration analysis and
five interpret the results
number five is a close second for being
the most important if you can't
interpret what you bring to the table to
your clients you're in trouble
so when this question comes up you
probably want to focus on those two
noting that the rest of it does eighty
percent of the work is in two three and
four well one and five are the most
important parts
which technical tools have you used for
analysis and presentation purposes
being a data analyst you are expected to
have knowledge of the below tools for
analysis and presentation purposes
there's a wide variety out there SQL
Server MySQL you have your Excel your
SPSS which is the IBM platform tab blue
python you have all these different
Tools in here now certainly a lot of
jobs are going to be narrowed in on just
a few of these tools like you're not
going to have a Microsoft SQL Server
MySQL server but you better understand
how to do basic SQL polls and also
understanding Excel and how the
different formats from column and how to
get those set up
number six what are the best practices
for data cleaning this is really
important to remember to go through this
in detail these always come up because
80 percent of most data analysis is in
cleaning the data make a data cleaning
plan by understanding where the common
errors take place and keep
Communications open identify remove
duplicates before working with the data
this will lead to an effective data
analysis process focus on the accuracy
of the data maintain the value types of
data provide mandatory constraints and
set Cross Field validation
standardize the data at the point of
entry so that it is less chaotic and you
will be able to ensure that all the
information is standardized leading to
fewer errors on Entry
number seven how can you handle missing
values in a data set list wise deletion
in listwise deletion method entire
record is excluded from analysis if any
single value is missing sometimes we're
talking about records remember this
could be a single line in a database so
if you have your SQL comes back and you
have 15 different columns every one of
those has a missing value you might just
drop it just to make it easy because you
already have enough data to do the
processing average imputation use the
average value of the responses from the
other participants to fill in the
missing value this is really useful and
they'll ask you why these are useful I
guarantee it if you have a whole group
of data that's collected and it doesn't
have that information in it at that
point you might average it in there
regression substitution you can use
multiple regression analysis to estimate
a missing value that kind of goes with
the average imputation input regression
model means you're just going to get
you're going to actually generate
generate a prediction as to what you
think that value should be for those
people based on the ones you do have
multiple imputation so we talked about
multiple inputs it creates plausible
values based on the correlations for the
missing data and then average the
simulated data sets by incorporating
random errors in your predictions
what do you understand by the term
normal distribution and the second you
hear the word normal distribution should
be you think in a bell curve like we see
here normal distribution is a type of
continuous probability distribution that
is symmetric about the mean and in the
graph normal distribution will appear as
a bell curve the mean median and mode
are equal that's a quick way to know if
you have normal distribution is you can
compute mean median and mode all of them
are located at the center of the
distribution 68 of the data lies within
one standard deviation of the mean 95 of
the data Falls within two standard
deviations of the mean
99.7 percent of the data lies within
three standard deviations of the mean
what is time series analysis
time series analysis is a statistical
method that deals with ordered sequence
of values of a variable of equally
spaced time intervals time series data
on a covid-19 cases and you can see
we're looking at by day so our space is
of days and each day goes by if we take
a graph it you can see the time series
graph always looks really nice if you
have like two different in this case we
have what the United States going over
there I'd have to look at the other
setup in there but they picked a couple
different countries and it is it's time
sensitive you know the next result is
based on what the last one was Cove is
an excellent example of this anytime you
do any word analytics where you're
figuring out what someone's saying what
they said before makes a huge difference
is what they're going to say next
another form of Time series analysis
10. how is joining different from
blending in tableau so now we're going
to jump into the Tableau package data
joining data joining can only be done
when the data comes from the same Source
combining two tables from the same
database or two or more worksheets from
the same Excel file all the combined
tables or sheets contains common set of
dimensions and measures
data blending data blending is used when
the data is from two or more different
sources combining the Oracle table with
the SQL server or two sheets from Excel
or combining Excel sheet and Oracle
table in data blending each data source
contains its own set of dimensions and
measures
how is overfitting different from
underfitting
how he's a good one overfitting probably
the biggest danger in data analytics
today is overfitting model trains from
the data too well using the training set
the performance drops significantly over
the test set happens when the model
learns the noise and random fluctuations
in the training data set in detail
and again the performance drops way
below what the test set has
the model neither trains a data well nor
can generalize to new data performs
poorly both on train and the test set
happens when there is less data to build
and an accurate model and also when we
try to build a linear model with a
non-linear data
in Microsoft Excel a numeric value can
be treated as a text value if it
proceeds with an apostrophe definitely
not an exclamation if you're used to
programming in Python you'll look for
that hash code and not an Amber sign
and we can see here if you enter the
value 10 into a fill but you put the
apostrophe in front of it it will read
that as a text not as a number
what is the difference between count
count a count blank and count if in
Excel
we can see here when we run in just
count D1 through D 23 we get 19 and
you'll notice that there is 19 numbers
coming down here
so it doesn't count the cost of each
which is a top bracket it doesn't count
the blank spaces either with the
straight count
when you do a count a you'll get the
answer is 20. so now when you do count a
it counts all of them even the title
cost of each
when you do count blank we'll get three
why there's three blank fields
and finally the count if if we do count
F of e 1 to e23 is greater than 10
there's 11 values in there basic
counting of whatever is in your column
pretty solid on the table there
explain how vlookup Works in Excel
vlookup is used when you need to find
things in a table or a range by row
the syntax has four different parts to
it we have our lookup value that's a
value you want to look up we have our
table array
the range where the lookup value is
located
column index number the column number
and range that contains the return value
and the range lookup specify true if you
want an approximate match or false if
you want an exact match of the return
value
so here we see vlookup F3 A2 to C8 2
comma zero for prints now they don't
show the F3 F3 is the actual cell that
prints is in that's what we're looking
at is F3 so there's your prints he pulls
in from F3 A2 to C8 is the the data
we're looking into and then number two
is a column in that data so in this case
we're looking for uh age and we count
name as one age is two keep in mind this
is Excel versus a lot of your Python and
programming languages where you start at
zero in Excel we always look at the
cells as one two three so two represents
the age
0 is false for having an exact match up
versus one we don't actually need to
worry about that too much in this 0 or 1
would work with this example and you can
see with the Angela lookup again her
name would be in the F column number
four that's what the F4 stands for is
where they pulled Angela from and then
you have A1 to C8 and then we're looking
at number three so number three is
height name being one H2 and then height
three and you'll see here pulls in her
height 5.8
so we're going to run jump over to SQL
how do you subset or filter data in SQL
to subset or filter data in SQL we use
where and having clause and you can see
we have a nice table on the left where
we have the title the director the year
the duration we want to filter the table
for movies that were directed by Brad
Bird why just because we want to know
who what Brad Bird did so we're going to
do select star you should know that the
star refers to all in this case we're
what are we going to return we're going
to return all title directory year and
duration that's what you mean by all
from movies movies being our table where
director equals Brad Bird and you can
see he comes back and he did the
incredible and Ratatouille
to subsetter filter data SQL we can also
use the where and having Clause so we're
going to take a closer look at the
different ways we can filter here filter
the table for directors whose movies
have an average duration greater than
115 minutes so there's a lot of really
cool things into this SQL query and
these SQL queries can get pretty crazy
select director sum duration as total
duration average duration as average
duration from movies Group by director
having average duration greater than
115.
uh so again what are we going to return
we're going to return whenever we put in
our select which in this case is
director we're going to have total
duration and that's going to be the sum
of the duration we're going to have the
average duration average underscore
duration which is going to be the
average duration on there and then we of
course go ahead and group by director
and we want to make sure we group them
by anyone that has an having an average
duration greater than 115. these SQL
queries are so important
I don't know how many times you the SQL
comes up and there's so many different
other languages not just MySQL and not
Microsoft SQL but in addition to that
where the SQL language comes in
especially with Hadoop in other areas so
you really should know your basic SQL
doesn't hurt to get that little cheat
sheet and glance over it and double
check some of the different features in
SQL
what is the difference between where and
having clause in SQL where where Clause
works on row data in where Clause a
filter occurs before any groupings are
made aggregate conscience cannot be used
so the syntax is select your columns
from table where what the condition is
having Clause works on aggregated data
having is used to filter values from a
group aggregate functions can be used in
the syntaxes select column names from
table where the condition is grouped by
having a condition ordered by column
names
what is the correct Syntax for reshape
function in numpy so we're going to jump
to the numpy array program
and what you come up with is you have in
this case it'd be numpy dot reshape a
lot of times you do an import numpy as
NP
reshape and then your array and the new
shape
and you can see here as we as the actual
example comes in the reshape is a and
we're going to reshape it in two comma
five setups and you can see the printout
in there that prints in two rows with
five values in each one
what are the different ways to create a
data frame in pandas
well we can do it by initializing a list
so you can Port your pandas as PD very
common data equals Tom 30 jerry20 Angela
35 we'll go ahead and create the data
frame and we'll say
pd.dataframe is the data columns equals
name and age so you can designate your
columns you can also do it as a index in
there you should always remember that
the index in this case maybe you want
the index instead of one two to be the
date they signed up or who knows you
know whatever and you can see right
there it just generates a nice pandas
data frame with Tom Jerry and Angela
another way you can initialize a data
frame is from dictionary you can see
here we have a dictionary where the date
equals name Tom Jerry Angela Mary ages
20 21 1918 and if we do a DF PD dot data
frame on the data you'll get a nice the
same kind of setup you get your name age
Tom Jerry Angela and Mary
write the python code to create an
employee's data frame from the
emp.csv file and display the head and
summary of it
to create a data frame in Python you
need to import the pandas library and
use the read CSV function to load the
CSV file
and here you can see we have import
pandas is PD employees or the data frame
employees equals pd.read CSV and then
you have your path to that CSV file
there's a number of settings in the read
CSV where you can tell it how many rows
are the top index you can set the
columns in there
you can have skip rows there's all kinds
of things you can also go in there and
double check with your read CSV but the
most basic one is just to read a basic
CSV
how will you select the department and
age columns from an employee's data
frame
so we have import pandas is PD you can
see we have created our data we will go
ahead and create our employees PD data
frame on the left
and then on the right to select
department and age from the data frame
we just do employees you put the
brackets around it now if you're just
doing one column you could do just
department but if you're doing multiple
columns you've got to have those in a
second set of brackets it's got to be a
reference with a list within the
reference
what is the criteria to say whether a
developed data model is good or not a
good model should be intuitive
insightful and self-explanatory follow
the old saying kiss keep it simple
the model develops should be able to
easily consumed by the clients for
actionable and profitable results
so if they can't read it what good is it
a good model should easily adapt to
changes according to business
requirements we live in quite a dynamic
world nowadays so it's pretty
self-evident and if the data gets
updated the model should be able to
scale accordingly to the new data so you
have a nice data pipeline going where
when something when you get new data
coming in you don't have to go and
rewrite the whole code
what is the significance of exploratory
data analysis
exploratory data analysis is an
important step in any data analysis
process exploratory data analysis Eda
helps to understand the data better it
helps you obtain confidence in your data
to a point where you're ready to engage
a machine learning algorithm it allows
you to refine your selection of feature
variables that will be used later for
model building you can discover hidden
Trends and insights from the data
how do you treat outliers in a data set
an outlier is a data point that is
distant from other similar points they
may be due to variability in the
measurement or may indicate experimental
errors
uh one you can drop the outlier records
pretty straightforward you can cap your
outliers data so it doesn't go past a
certain value you can assign it a new
value you can also try a new
transformation to see if those outliers
come in if you transform it slightly
differently
explain descriptive predictive and
prescriptive analytics descriptive
provides insights into the past to
answer what has happened uses data
aggregation and data mining techniques
examples an ice cream company can
analyze how much ice cream was sold
which flavors were sold and whether more
or less ice cream was sold than before
predictive understands the future to the
answer what could happen use the
statistical models and forecasting
techniques
the example predicts a sale of ice
creams during the summer spring and
rainy days so this is always interesting
because you have your descriptive which
comes in and your businesses are always
looking to know what happened hey did we
have good sales last uh quarter what are
we expecting next quarter in sales and
we have a huge jump when we do uh
prescriptive suggest various courses of
action to answer what should you do uses
optimization and simulation algorithms
to advise possible outcomes example
lower prices to increase sell of ice
creams produce more or less quantities
of certain flavor of ice cream and we
can certainly uh today's world with the
covid virus because we had that on our
earlier graph you could see that as a
descriptive what's happened how many
people have been infected how many
people have died in an area predictive
where do we predict that to go
um do we see it going to get worse is it
going to get better what do we predict
that we're going to need in hospital
beds and prescriptive what can we change
in our setup to have a better outcome
maybe if we did more social distancing
if we tracked the virus
how do these different things directly
affect the end and can we create a
better ending by changing some
underlying criteria
what are the different types of sampling
techniques used by data analysis
sampling is a statistical method to
select a subset of data from an entire
data set population to estimate the
characteristics of the whole population
one we can do a simple random sampling
so we can just pick out 500 random
people in the United States to sample
them they call it a population in
regular data we also call that a
population just because that's where it
came from was mainly from doing census
systematic sampling
cluster sampling
stratified sampling
and judgment or purposive sampling
then we have our systematic sampling
that's where you're doing like using one
five ten fifteen twenty use a very
systematic approach for pulling samples
from the setup cluster sampling that's
where we look at it and we say hey some
of these things just naturally group
together if you were talking about
population which is the really a nice
way of looking at this cluster sampling
would be maybe by a zip code we're going
to do everybody's zip code and just
naturally cluster it that way
stratified sampling would be more
looking for shared things the group has
like income so if you're studying
something on poverty you might look at
their naturally group People based on
income to begin with and then study
those individuals in the income to find
out what kind of traits they have
and then judgmental that is where the
researcher very carefully selects each
member of their own group
so it's very much based on their
personal knowledge and with that we have
reached the end of this session on
Advanced data analytics if you have any
questions please feel free to comment in
the comment section below and we will
have it answered as soon as possible in
case if you find any difficulties in the
code represented or if you require the
resources used in the session like PPD
datasets code documentations anything
please do let us know in the comment
section below and our team of experts
will have it shared with you at the
earliest until next time thank you for
watching stay safe keep learning and get
ahead
staying ahead in your career requires
continuous learning and upskilling
whether you're a student aiming to learn
today's top skills or a working
professional looking to advance your
career we've got you covered explore our
impressive catalog of certification
programs in Cutting Edge domains
including data science cloud computing
cyber security AI machine learning or
digital marketing designed in
collaboration with leading universities
and top corporations and delivered by
industry experts choose any of our
programs and set yourself on the path to
Career Success click the link in the
description to know more
foreign
hi there if you like this video
subscribe to the simply learned YouTube
channel and click here to watch similar
videos to nerd up and get certified
click here