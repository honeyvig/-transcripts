hello everyone welcome to this fantastic
AIML complete full course by simply
learn but before we begin if you enjoy
watching these videos and find them
interesting subscribe to our Channel as
we bring you the best videos daily also
hit the Bell icon to never miss any
updates from Simply Le so let's get
started with this course we will brief
you with an artificial intelligence and
machine learning introduction after that
we will see the skills required for AIML
Engineers moving forward we will cover
the E IML engineer salary e IML projects
for resume and The Core Concepts like
mlops Edge Computing and what is Lang
chain we will walk you through some
fantastic Concepts like AI versus ml
versus DL proceeding with this course we
will teach you different machine
learning algorithms logis IC regression
and many more with Hands-On demo
explanation in Python we will walk you
through the unique projects like fake
news detection system and object
detection and many others these projects
will serve you as the finest portfolios
for your future interviews speaking of
interviews we have covered you along
with the most frequently Asked machine
learning interview questions to help you
crack the most challenging interviews
but before we begin to understand what
machine learning is if you want to
become an AI expert and gain handsome
salary packages look at the wide range
of eiml courses by simply learn and
collaboration with top universities
across the globe by enrolling in these
certification programs you will gain
expertise in skills like generative AI
prompt engineering chat GPT explainable
AI machine learning algorithms
supervised and unsupervised learning
model training and optimization and
there's much more on the list with
hands-on experience in the tools like
Char GPT d python open CV and tens oflow
you will catch the eyes of top
Recruiters in the industry so what are
you waiting for hurry up and enroll now
an year of experience is preferred to
enroll in this course find the course
Link in the description box with that in
mind over to our training experts
picture this a machine that could
organize your cupboard just as you like
it or serve every member of the House a
customized cup of coffee makes your day
easier doesn't it the are the products
of artificial intelligence but why use
the term artificial intelligence well
these machines are artificially
Incorporated with humanlike intelligence
to perform tasks as we do this
intelligence is built using complex
algorithms and mathematical functions
but AI may not be as obvious as in the
previous examples in fact AI is used in
smartphones cars social media feeds
video games banking survey
and many other aspects of our daily life
the real question is what does an AI do
at its core here is a robot we built in
our lab which is now dropped onto a
field in spite of a variation in
lighting landscape and dimensions of the
field the AI robot must perform as
expected this ability to react
appropriately to a new situation is
called generalized learning the robot is
now at a crossroad one that is paved and
the other Rocky the robot must determine
which path to take based on the
circumstances this portrays the robot's
reasoning ability after a short stroll
the robot now encounters a stream that
it cannot swim across using the plank
provided as an input the robot is able
to cross this stream so our robot uses
the given input and finds the solution
for a problem this is problem solving
these three capabili ities make the
robot artificially intelligent in short
AI provides machines with the capability
to adapt reason and provide
Solutions well now that we know what AI
is let's have a look at the two broad
categories and AI classified into weak
AI also called narrow AI focuses solely
on one task for example alphago is a
maestro of the game go but you can't
expect it to be even remotely good at
Jess this makes alphao a weak AI you
might say Alexa is definitely not a weak
AI since it can perform multiple tasks
well that's not really true when you ask
Alexa to play despacito it picks up the
key words play and despacito and runs a
program it is trained to Alexa cannot
respond to a question it isn't trained
to answer for instance try asking Alexa
the status of traffic from work to home
Alexa cannot provide you this
information as she is not trained to and
that brings us to our second category of
AI strong AI now this is much like the
robots that only exist in fiction as of
now Ultron from Avengers is an ideal
example of a strong AI That's because
it's self-aware and eventually even
develops emotions this makes the AI
response
unpredictable You Must Be Wondering well
how is artificial intelligence different
from machine learning and deep learning
we saw what AI is machine learning is a
technique to achieve Ai and deep
learning in turn is a subset of machine
learning machine learning provides a
machine with the capability to learn
from data and experience through
algorithms deep learning does this
learning through ways inspired by the
human brain
this means through deep learning data
and patterns can be better perceived Ray
kheel a well-known futurist predicts
that by the year 2045 we would have
robots as smart as humans this is called
the point of Singularity well that's not
all in fact Elon Musk predicts that the
human mind and body will be enhanced by
AI implants which would make us partly
cyborgs artificial intelligence is
reshipping mines and transforming the
way we live and work it encompasses a
wide range of Technologies including
machine learning deep learning natural
language processing computer vision and
many more with its ability to analyze
vast amount of data and make Intelligent
Decisions AI has become a game changer
across various domains hi everyone
welcome to Simply learns YouTube channel
this AI road map guides you in
navigating the part towards building a
successful career in artificial
intelligence the ultimate goal of
artificial intelligence is to create
create intelligent missions that can
perform complex task exhibit humanik
intelligence and contribute positivity
to society Yi presents exciting career
opportunity in various Industries and
sectors roles like AI engineer data
scientist NLP Engineers computer vision
Engineers AI research scientists robotic
engineers and many more offer exciting
prospects for working with cutting it
Technologies and making an impact
through artificial intelligence
Innovation according to glass door the
average reported salary of an AI
engineer in the United State is around
$15,000 per year however in India it is
10 lakh perom leading companies
worldwide are fully aware of the immense
value of AI and are actively pursuing
skilled AI Engineers to contribute to
their research development and
implementations of AI technology among
this top companies are Google Microsoft
Amazon Goldman apple and JP Morgan Chase
therefore top companies hiring AI
engineer provide excellent opportunity
for aspiring professionals on that note
elevate your career with our Ai and ml
course developed in partnership with P
University and IBM gain expertise in
high demand skills such as machine
learning deep learning NLP computer
vision reinforcement learning CH GPD
generative Ai explainable Ai and more
Unleash Your Potential and unlock
exciting opportunity in the world of AI
and ml this course cover tools like
python tensor flow K CH GPT open AI
matplot LA and many more so hurry up and
find the course Link in the description
box for more details our Learners have
experienced huge success in their
careers listen to their experience find
the simply learn Course Review Link in
the description box without any further
delay let's dive into the topic now if
you desire to become an AI engineer here
are the steps you can follow to achieve
your goal firstly obtain a strong
foundation in mathematics and
programming gain a strong foundation in
creating mathematic Concepts like linear
algebra calculus and probability theory
in addition to that it is crucial to
Acure Mastery of a programming languages
like python commonly used in Ai and
become proficient in coding then earn a
degree in a relevant field that means
earn a bachelor's or master degree in
computer science data science AI or a
related field to gain a comprehensive
understanding of AI principles and
techniques next gain knowledge in
Mission learning and deep learning to
become a AI engineer develop familarity
with ML algorithms neural network works
and deep learning Frameworks example
tensor Flow by torch to train and
optimize models using real world data
set followed by that work on practical
projects gain hands-on experience and
showcase your skills by working on AI
projects moreover build a portfolio of
projects demonstrating your ability to
solve eii problems impressing potential
employers then stay updated with the
latest advancements stay updated with
the latest trends and research in AI a
rapidly involving field and expanding
your knowledge by reading research
papers participating in online courses
and workshops and joining AI communities
next collaborate and network engage with
AI communities attend conference
participate in online forums to connect
with Professionals in this field
collaborating with others can enhance
your learning experience and open new
opportunities later seek internships or
entry-level positions gain practical
experience through AI internships or
entry-level roles in industry or
research institutions this provides
valuable exposures and help develop your
skills finally continuously learn and
adapt in the rapidly involving field of
AI stay updated on new developments
explore specialized areas and embrace
emerging Technologies and tools to
pursue a career as an AI engineer now
that we have covered the essential steps
to become an AI engineer let's explore
the necessary programming languages and
algorithms for aspiring EI Engineers so
mastering programming languages like
python r Java and C++ is vital for
acquiring Proficiency in AI this
languages enables you to construct and
deploy models effectively additionally
it would help if you gain a thorough
understanding of machine learning
algorithms such as linear regressions K
nearest neighbors na base and support
Vector missions this languages and
algorithms are fundamental tools in the
field of AI they will enable you to
develop and Implement effective
artificial intelligence models hello
everyone and Welcome to our video on a
skills squ for an AI engineer with the
Advent of artificial intelligence and
its Superior abilities to improve human
life the need and demand for expert AI
professional is at an all-time high this
expanding demand has led to a lot of
people applying for AI jobs and
upskilling them themselves in the field
of AI but doing this is just like
playing a game but not knowing its rules
for someone to become an AI professional
and landar a job like an AI engineer
they first have to understand the demand
from this role and what skills are
expected of an AI engineer so if you're
someone who wishes to become an AI
engineer or if you wish to upgrade your
skills in Ai and get promoted as an AI
engineer then make sure you watch this
amazing video till the very end in this
video we will be breaking down in
complete detail each and every skill
that you would need in order to crack an
AI engineer job interview but why should
you consider a career in ai ai is not
just a passing Trend it's a sesmic shi
that is reshaping our world and creating
new avenues for Innovation and Discovery
by ring a career in AI you become a part
of dynamic field that thrives on solving
complex problems pushing boundaries and
making a profound impact or Society the
demand for AI professional is scating
across industries from healthare and
finance to entertainment and
transportation organization are actively
seeking talented individuals who can
harness the power of AI to drive their
business forward but what skills does it
take to become an AI engineer how can
you embark on the thrilling Journey we
have answer to all of your questions
also accelerate your career in AI ml
with our comprehensive post-graduate
program in Ai and machine learning gain
expertise in machine learning deep
learning NLP computer vision and
reinforcement learning you will receive
a prestigious certificate exclusive
alumin membership and ask me anything
session by ABIA with three Capstone
project and 25 plus industry projects
using real data set from Twitter Uber
and many more you will gain practical
experience master classes by ctech
faculty and IBM expert ensur topnotch
education simply learn job assist help
you to get notice by Leading companies
this program covers statistic python
supervised and unsupervised learning NLP
neural network computer vision G caras
tensor flow and many more skills so
enroll now and unlock exciting AI ml
opportunities the link is in the
description box below so without any
further delay let's get started some
steps are crucial to master in the field
of AI and becoming an AI engineer let's
go through them real quick first
establish a strong foundation in
mathematics and programming start by
gaining a solid understanding of
critical Concepts like linear algebra
calculus and probability Theory
additionally it is crucial to become
proficient in programming language like
python which is commonly used in Ai and
developed coding skills and the next one
is pursue a degree in relevant field
earn a bachelor or master degree in
computer science data science or AI or
related discipline to acquire
comprehensive understanding of AI
principles and techniques then acquire
knowledge in machine learning and deep
learning familiarize yourself with ML
algorithms neural networks and deep
learning Frameworks that is tensa flow
and pytorch to train and optimize models
using real world data sets afterward
engage in Practical projects gain
hands-on experience and demonstrate your
skills by working on AI projects
building a portfolio of projects that
showcase your ability to solve AI
problems can make a strong impression on
potential employers and the fifth one is
collaborate and network engage with
communities attend conferences and
participate in the online forums to
connect with the Professionals in the
field collaborating with others can
enhance your learning experience and
open up new opportunities and the sixth
one is seek internship or entry level
positions gain practical experience
through AI internship or entry-level
roles in Industries or research
institution this will provide valuable
exposure and help you further develop
your skill and at the last continuously
learning and adapt in the fast-paced
world of eii it is crucial to stay
updated on new developments explore
specialized areas and embrace emerging
Technologies and tools continuous
learning and adaptability are essential
for pursuing a successful career as a AI
engineer so now that you are familiar
with the steps involved in the Journey
of AI engineer let's have a look at the
salary of an AI engineer
so according to glass door the average
reported salary of an AI engineer in the
United States is $115,000 per year
however in India it is 10 lakh perom so
these figures are way better than the
average selling figures of any job road
so let's discuss the skills you need to
become an AI engineer you should have a
strong background in data science and
machine learning here is a breakdown of
your skills number one strong
programming ability the typically refers
to expertise in one or more programming
languages commonly used in data science
and machine learning such as Python and
R Proficiency in programming allows you
to write efficient and scalable code for
data analysis modeling and algorithm
implen and the second one is knowledge
of machine learning algorithms this
involves understanding and familiarity
with the wide range of machine learning
algorithms including both supervised and
unsupervised learning you should be able
to select and apply appropriate
algorithm for specific problem as well
as evaluate and optimize their
performance and the third one is
Proficiency in statistic and Mathematics
sound knowledge of statistics and
Mathematics is fundamental for data
analysis and machine learning you should
be comfortable with statical Concept
hypothesis testing regression analysis
probability Theory L algebra and
calculus and the fourth one
is familiarity with deep learning
Frameworks deep learning has gained
significant popularity in recent years
and familiarity with deep learning
Frameworks like tensor flow P torch or
kasas is valuable these Frameworks
provide tools and libraries for building
training and deploying deep neural
networks for task such as image
recognition natural language processing
and time series analysis and the fifth
one is experience with big data
Technologies dealing with large SC scale
data set requires knowledge of Big Data
Technologies such as apachi Hadoop or
Apache spark or distributed computing
Frameworks understanding how to process
store and analyze data efficiently in
distributed environment and the sixth
one is excellent problem solving and
analytical skills these skills enable
you to break down complex problem
identify key factors and develop
effective Solutions you should be adapt
at critical thinking troubleshooting and
debugging to handle real world
challenges in the data science and
machine learning remember to stay
updated with the latest advancement in
the FI and continuing learning to stay
at the Forefront of data science and
machine learning hello everyone and
Welcome to our video on skills required
for an AI engineer with the Advent of
artificial intelligence and its Superior
abilities to improve human life the need
and demand for expert AI professional is
at an all-time high this expanding
demand has led to a lot of people
applying for AI jobs and upskilling them
themselves in the field of AI but doing
this is just like playing a game but not
knowing its rules for someone to become
an AI professional and landar a job like
an AI engineer they must have to
understand the demand from this role and
what skills are expected of an AI
engineer so if you someone who wishes to
become an AI engineer or if you wish to
upgrade your skills in Ai and get
promoted as an AI engineer then make
sure you watch this amazing video till
the very end in this video we will be
breaking down in complete detail each
and every skill that you would need in
order to crack an AI engineer job
interview but why should you consider a
career in ai ai is not just a passing
trend it's a sesmic shift that is
reshaping our world and creating new
avenues for Innovation and Discovery by
embracing a career in AI you become a
part of dynamic field that thrives on
solving complex problems pushing
boundaries and making a profound impact
on society the demand for AI
professional is scating across
industries from healthare and finance to
entertainment and transportation
organization are actively seeking
talented individuals who can harness the
power of AI to drive their business
forward forward but what skills does it
take to become an AI engineer how can
you embark on the thrilling Journey we
have answered to all of your questions
also accelerate your career in AI ml
with our comprehensive post-graduate
program in Ai and machine learning gain
expertise in machine learning deep
learning NLP computer vision and
reinforcement learning you will receive
a prestigious certificate exclusive
aluminium membership and ask me anything
session by IIA with three Capstone
project and 25 plus industry projects
using real data set from Twitter Uber
and many more you will gain practical
experience master classes by ctech
faculty and IBM expert ensure topnotch
education simply learn job assist help
you to get notied by Leading companies
this program covers statistic python
supervised and unsupervised learning NLP
neural network computer vision G caras
tensorflow and many more skills so
enroll now and unlock exciting AI ml
opportunities the link is in the
description box below
so without any further delay let's get
started some steps are crucial to master
in the field of AI and becoming an AI
engineer let's go through them real
quick first establish a strong
foundation in mathematics and
programming start by gaining a solid
understanding of critical Concepts like
linear algebra calculus and probability
Theory additionally it is crucial to
become proficient in programming
language like python which is commonly
used in Ai and developed coding skills
and the next one is pursue a degree in
relevant field earn a bachelor or master
degree in computer science data science
or AI or related discipline to acquire
comprehensive understanding of AI
principles and techniques then acquire
knowledge in machine learning and deep
learning familiarize yourself with ML
algorithms neural networks and deep
learning Frameworks that is tensorflow
and pytorch to train and optimize models
using real world data sets afterward
engage in Practical projects projects
gain hands-on experience and demonstrate
your skills by working on AI projects
building a portfolio of projects that
showcase your ability to solve AI
problems can make a strong impression on
potential employers and the fifth one is
collaborate and network engage with
communities attend conferences and
participate in the online forums to
connect with the Professionals in the
field collaborating with others can
enhance your learning experience and
open up new opportunities and the sixth
one is seek internship or entry-level
positions gain practical experience
through AI internship or entry-level
roles in Industries or research
institution this will provide valuable
exposure and help you further develop
your skill and at the last continuously
learning and adapt in the fastpac world
of AI it is crucial to stay updated on
new developments explore specialized
areas and embrace emerging Technologies
and tools continuous learning and
adaptability are essential for pursuing
a successful career as a AI engineer so
now that you are familiar with the steps
involved in the Journey of AI engineer
let's have a look at the salary of an AI
engineer so according to glass door the
average reported salary of an AI
engineer in the United States is
$115,000 per year however in India it is
10 lakh perom so these figures are way
better than the average selling figures
of any job road so let's discuss the
skills you need to become an AI engineer
you should have a strong background in
data science and machine learning here
is a breakdown of your skills number one
strong programming abilities they
typically refers to expertise in one or
more programming languages commonly used
in data science and machine learning
such as Python and R Proficiency in
programming allows you to write
efficient and scalable code for data
analysis modeling and algorithm implen
and the second one is knowledge of
machine learning algorithms this
involves understanding and familiarity
with the wide range of machine learning
algor algorithm including both
supervised and unsupervised learning you
should be able to select and apply
appropriate algorithm for specific
problem as well as evaluate and optimize
their performance and the third one is
Proficiency in statistic and Mathematics
sound knowledge of statistics and
Mathematics is fundamental for data
analysis and machine learning you should
be comfortable with statical Concept
hypothesis testing regression analysis
probability Theory L algebra and
calculus and the fourth one
is familiarity with deep learning
Frameworks deep learning has gained
significant popularity in recent years
and familiarity with deep learning
Frameworks like tensorflow py torch or
kasas is valuable these Frameworks
provide tools and libraries for building
training and deploying deep neural
networks for task such as image
recognition natural language processing
and time series analysis and the fifth
one is experience with big data dat
Technologies dealing with large scale
data set requires knowledge of Big Data
Technologies such as Apache Hado or
Apache spark or distributed computing
Frameworks understanding how to process
store and analyze data efficiently in
distributed environment and the sixth
one is excellent problem solving and
analytical skills these skills enable
you to break down complex problem
identify key factors and develop
effective Solutions you should be adapt
at critical thinking troubleshooting and
debugging to handle real world
challenges in the data science and
machine learning remember to stay
updated with the latest advancement in
the field and continuing learning to
stay at the Forefront of data science
and machine
learning machine learning is
transforming Industries and driving
Innovation by enabling systems to learn
from data and make Intelligent Decisions
this has resulted in a skyrocketing
demand for machine learning Engineers
these professionals possess the skills
to develop Advanced algorithms build
predictive models and extract valuable
insights from vast amounts of data the
ARA of Big Data necessitates
professionals who can derive insights
from vast information and machine
learning Engineers possess the skills to
analyze complex data sets extract
valuable knowledge and build predictive
models for accurate forecast process
Automation and optimized decision making
Industries such as Healthcare Finance
eCommerce and autonomous vehicles
heavily rely on machine learning to
enhance processes and drive goal as
businesses increasingly recognize the
power of data driven insights the demand
for skilled machine learning Engineers
continues to search they're instrumental
in Raging the potential of machine
learning to optimize operations improve
efficiency and gain a Competitive Edge
with the market value of machine
learning expected to grow exponentially
now is the time to enter into the field
of machine learning and become an ml
engineer who are at the Forefront of
technological advancement and Bing
lucrative career opportunities in this
Dynamic field that opens doors to a
world of endless possibilities and
Promises is a fulfilling and impactful
career on that note hello everyone
welcome to Simply learn today we will
explore a profession that has witnessed
a huge surge in demand in recent years
which is going to be the next big thing
in the future can you guess which
profession we talking about well you got
it right that's machine learning
engineer having said that if you want to
Embark your career as an ml engineer
then our postgraduate program in& ml can
be the right option for you this ml
postgraduate program by simply learn in
collaboration with pu University covers
a wide range of topics and provides
hands-on experience in developing Ai and
machine learning solutions for
professionals who want to upgrade their
skills and PR to a career in the field
of AI and machine learning this course
features master classes by Purdue
faculty and IBM experts exclusive
hackathons to help you master various
skills like statistics python supervised
learning neural networks NLP and much
more by covering tools and techniques
like numpy pandas python scipi along
with amazing industry projects so what
is stopping you from from building your
future in& ml enroll now course link is
added in the description box below make
sure to check that out without any
further Ado let's get started meet John
who has been working as a software
developer in the IT industry for over 5
years now John has been hearing a lot
about machine learning lately and how it
has been revolutionizing the tech
industry he wonders whether learning
machine learning would be worth it and
that's when his colleague Harry helps
him out hey harry I've been reading a
lot about machine learning engineering
lately what are your thoughts absolutely
machine learning is booming right now
really quite intriguing but how can I
become an ml engineer well with the
right skill set you can my friend
possibilities are endless wow that's
impressive looks like machine learning
can be a rewarding career with great
opportunities and lucrative salaries
absolutely John so let me tell you more
about the working aspects of machine
learning engineer in detail we'll start
this tutorial with a quick introduction
to who ml engineer is and next we'll
understand what exactly does a machine
learning engineer do next we'll dive
into the roles and responsibilities of a
machine machine learning engineer after
that we'll discuss some of the most
important skills that an ml engineer
should possess and finally we'll talk
about the salary and job prospects of a
machine learning engineer so firstly who
is a machine learning engineer a machine
learning engineer is a skilled
professional who combines expertise in
programming mathematics and machine
learning to develop and deploy
intelligent systems that can learn from
data and make accurate predictions or
decisions the role of a machine learning
engineer involves working with large
data sets pre-processing and cleaning
the data selecting appropriate machine
learning algorithms and training models
using this data they often collaborate
with data scientist and domain experts
like software Engineers to integrate
machine learning Solutions into
production environments for optimizing
and fine-tuning models for accuracy and
efficiency as well as evaluating their
performance and finally they perform a
thorough effective data exploration and
visualization techniques in order to
contribute for better understanding of
the data and identifying patterns making
informed decisions through throughout
the machine learning pipelines so what
are the roles and responsibilities of a
machine learning engineer firstly
problem understanding and solution
design machine learning Engineers work
closely with stakeholders domain experts
to understand the business problem or
objective that machine learning can
address they analyze requirements
identify suitable data sources and
propose machine learning based Solutions
this involves translating business
problems into technical specifications
defining measurable goals and designing
the overall architecture of the machine
Learning System data preparation and
Analysis machine learning Engineers need
to have a deep understanding of the data
they're working with this involves
cleaning pre-processing and transforming
raw data into a format suitable for
training and evaluating ml models
exploratory data analysis techniques are
often used to gain insights and
understand the underlying patterns and
relationships within the data model
development and evaluation ml Engineers
are responsible for developing training
and fine-tuning machine learning models
this involves selecting the appropriate
algorithms architectures and techniques
based on the problems at hand you will
need to implement an experiment with
different models such as decision trees
neural networks support Vector machines
and much more and finally monitoring
maintenance and iterative Improvement
after deployment ml Engineers monitor
the performance of deployed models and
address any sort of issues that arise
they design and Implement monitoring
systems to track model performance
detect any kind of anomalies and
identify data drift or concept drift
regular model maintenance is necessary
to retrain and refine models using new
data to maintain accuracy and
adaptability so these are some of the
day-to-day roles and responsibilities of
an ml engineer well let us now talk
about the skillet that a ml engineer
should possess firstly they should have
a strong knowledge on programming tools
now strong programming skills are
essential for machine learning Engineers
programming languages like R which is a
free software environment for statis
iCal Computing and Graphics can surely
help but mainly they use python python
is widely used in the machine Learning
Community due to its Simplicity
extensive libraries and strong ecosystem
so you need to familiarize yourself with
python fundamentals data manipulation
libraries like numai and pandas and
machine learning Frameworks such as
tensorflow pyo and psychic learn also
familiarize yourself with programming
Concepts data structures and algorithms
and additionally knowledge of other
languages like art and Java can be
beneficial secondly machine learning and
deep learning Concepts ml Engineers
should have a deep understanding of
various machine learning algorithms
including supervised learning
unsupervised learning and reinforcement
learning familiarize yourself with
Concepts like KNN linear regression
decision trees support Vector missions
neural networks Ensemble methods and
much more not just that furthermore ml
Engineers need a solid foundation in
mathematics and Statics statistics this
includes knowledge of linear algebra
calculus probability Theory and
statistical analysis understanding these
Concepts is crucial for Designing and
implementing machine learning algorithms
now in addition to these technical
skills machine learning Engineers should
also possess strong analytical and soft
skills to excel in their roles they need
to think critically and out of the box
to understand problems break them down
into manageable components and develop
appropriate machine learning solutions
they should be able to analyze data
identify patterns and make informed
decisions based on the results and
finally communication skills now
effective communication is crucial for
machine learning Engineers they need to
explain complex technical Concepts to
non-technical stakeholders as well while
collaborating with cross functional
teams and present their findings and
recommendations strong written and
verbal communication skills will surely
help convey information clearly and
concisely which not only enhance their
professional growth but also contribute
to their overall effect Effectiveness as
a machine learning engineer with all
that having said let us now discuss some
of the salary and job prospects of a
machine learning engineer now the salary
for machine learning engineer can vary
significantly based on V various factors
in United States of America machine
learning Engineers are in high demand
particularly in technology hubs like
Silicon Valley in California and Tech
hubs like New York according to a report
by glass door the average salary of a
professional ml engineer is around
$19,000 per year now salaries in the US
tend to be higher due to the cost of
living and fierce competition for talent
and entry-level machine learning
Engineers can expect salaries ranging
from $80,000 to $120,000 per year while
mid-level professionals with a few years
of experience can around $120,000 to
$180,000 per year and finally a senior
level or an experienced machine learning
engineer can command salaries exceeding
$200,000 per year as well especially in
Top tire companies or leadership
positions on the other hand in India the
salary range for machine learning
Engineers is relatively lower compared
to the US but it's important to consider
the lower cost of living as well now the
average salary of a machine learning
engineer in India is around 8.5 lakhs
while entry-level machine learning
engineers in India can expect salaries
ranging from 4 lakhs to 8 lakhs per year
and mid-level professionals can ear on
on somewhere between 8 lakhs to 15 lakhs
per year and finally senior level
machine learning Engineers with
extensive experience and expertise can
earn salaries ranging from 15 lakhs to
30 lakhs per year now additionally other
benefits and compensation components
such as bonuses stock options Healthcare
benefits and retirement plan should also
be considered but let's talk about
reality it's crucial to note that these
figures are approximate and can vary
based on various such factors so here
are some of the salary deciding factors
of an machine learning engineer firstly
company and location now the location of
the employing company can impact a
machine learning engineer salary large
technology companies or organizations
with a strong strong Focus on& ml tend
to offer competitive compensation
packages to attract top talent
Additionally the cost of living in
different Geographic areas can affect
salary ranges with salaries often being
higher intake hubs or cities with a
higher cost of living work experience
experience plays a significant role in
determining the salary of an ml engineer
generally Engineers with more years of
experience tend to command higher
salaries this is because experience
indicates a deeper understanding of
machine learning Concepts techniques and
real world applications which makes
experience Engineers more valuable to
the employers out there skill set and
expertise now specific skills and
expertise possessed by a machine
learning engineer can impact their
salaries as well ml Engineers proficient
in popular programming languages like
python R Java and Frameworks like
tensorflow and py toch and librar such
as psychic learn are often in high
demand and can negotiate higher salaries
additionally specialized knowledge in
areas like deep learning natural
language processing computer vision can
also be valuable and potentially result
in higher compensation and finally the
industry that you're working on and the
current demand prints now the industry
in which an machine learning engineer
Works can influence their salary as well
industries that heavily rely on machine
learning and AI applications such as
Finance Healthcare e-commerce and
autonomous vehicles offer higher salary
packages to machine learning Engineers
due to the high demand for their
expertise also it's important to note
that these relative importance of these
factors may vary depending on the
specific circumstances and job market
conditions salaries can also be
influenced by economic factors market
demand and individual negotiation skills
overall machine learning engineers in
both us India and the rest of the world
can earn competitive salaries and it's
essential to assess the overall
compensation package growth
opportunities and Company culture when
evaluating job
opportunities artificial intelligence is
a vast topic and it is something which
will have to master bit by bit however
there are many basic projects that you
can take up as a beginner in AI there
are plenty of resources online but
sometimes as a beginner in AI doesn't
know where to start this video will help
you to get interesting ideas on projects
in AI hey everyone welcome to Simply
learn before moving on please make sure
you subscribe to Simply learns YouTube
channel and press the Bell icon to never
miss any updates let us Define
artificial int elligence First
artificial intelligence is a technique
of turning a computer based robot to
work and act like humans now let me list
out important AI projects for you face
detection system it is a form of
biometric recognition a method for
identifying or confirming someone's
identity by glancing at their face is
called facial recognition people can be
identified by securing a match on facial
ID using this technique real time
visuals videos and photos can be the
sources to run face detection this
technology is mostly employed in
security and law enforcement open CV is
the best technology to create it a
python package called open CV is made
specifically to address computer vision
jobs computer vision is a process used
in the processing of images by computers
it is concerned with the in-depth
comprehension of digital photos or films
it is necessary to automate operations
that can be performed by human visual
systems therefore a computer should be
able to identify items like a statue or
a lamp poost or even the face of a human
being second one is chatbot it is the
best idea if you have chosen chatbot as
your project topic this will make your
resume more attractive in case you're
looking for a job an oracle survey
suggests that 80% of the businesses uses
chatbot you can use Python Java Ruby C++
or PHP as a programming language to
develop a chatbot designing and building
NLP chatbots that accept speech and Text
data is made easier by dialog flow you
can go through many projects to get the
rough idea there are many platforms
which will help you to build a chatbot
regardless of how excellent your chatbot
is there is always room for development
the finest chatboard developers
constantly enhance their bards over the
time using Ai and machine learning
social media recommendation system the
rise of web services like Netflix Amazon
and YouTube has increased the use of
recommended systems in our daily lives
they are algorithms that insist users in
finding information that is pertinent to
them recommender systems are important
in some firms since they can generate a
lot of income or allow you to set
yourself apart from rivals in order to
provide recommendations it evaluates the
relationship between the user and the
object as well as the parallels between
users and positions coming to predicting
stock this application is widely
applicable everywhere as AI career
aspirants one will love to develop stock
prediction applications as it is full of
data this project would be ideal for
students who want to work in the finance
industry because it it can provide I
repeat because it can provide them a
better understanding of various aspects
of the field coming to medical diagnosis
this project is advantagous from a
medical standpoint AI projects can be
created to detect heart-based diseases
and also detect cancer it is intended to
offer patients with heart illness online
medical advice and guidance after
processing the data this system will
search the database for any illnesses
that might be connected to the given
details using data mining techniques
this intelligent system determines the
disease that the patient's information
most closely
resembles based on the system diagnosis
users can then speak with qualified
medical professionals users of the
system can also view information about
various doctors coming to an important
project that is search engine search
engines are utilized by all we look for
information on the greatest product to
buy a nice area to hang out or solutions
to any questions we have MLP is quite
significant in modern search engines
because a lot of language processing
takes place there python is the widely
used language to develop any search
engine search engine is mainly confined
with lots and lots of data it is helpful
for any AI career expirence next is
virtual assistants here the challenge is
to build a virtual assistant to assist
user why do you need virtual assistant
in your devices when you are building
your own it is also interesting for a ml
developer to build a virtual assistant
it involves NLP and data mining
voice-based virtual assistants are
popular today because they make life
easier for users NLP is utilized to
comprehend human language in order to
construct this system when a voice
command is received the system will
translate it into machine language and
store the commands in its database hate
speech detection in social media
automated hate speech detection is a
crucial weapon in the fight against hate
speech propagation especially on social
media for the job many techniques have
been developed including a recent
explosion of deep learning based system
for the objective of detect ing hate
speech a number of techniques have been
investigated including conventional
classifiers classifiers based on deep
learning and combination of both of them
on the other hand a number of data set
benchmarks including Twitter sentiment
analysis have been introduced and made
available for the evolution of the
performance of these algorithms and the
last one is predicting house price you
will need to estimate the sale price of
a brand new home in any place for this
assignment the data set for this project
includes information on the cost of
homes in various City neighborhoods the
UCI machine learning repository is the
place where you may find the data set
needed for This research you will also
receive other data set with information
on the age of the population the city's
crime rate and the location of non-
retail Enterprises in addition to the
pricing of various residences it's an
excellent project to test your knowledge
if you are a up finding a suitable job
in the field of machine learning is
becoming increasingly difficult the
ideal way to display your machine
learning skill is in the form of
portfolio of data science and machine
learning projects a solid portfolio of
projects will illustrate that you can
utilize those machine learning skills in
your profile as well projects like movie
decementation system fake news detection
and many more are the best way to
improve your early programming skills
you may have the knowledge but putting
it to use what is keep you competitive
here are 10 machine learning project
that can increase your portfolio and
enable you to acquire a job as a machine
learning engineer at number 10 we have
loan approval prediction system in this
machine learning project we will analyze
and make prediction about the loan
approval process of any person this is a
classification problem in which we must
determine whether or not the loan will
be approved a classification problem is
a predictive modeling problem that
predict a class label for a given
example of input data some
classification problem include spam
email cancer detection sentiment
analysis and many more you can check the
project link from the the description
box below to understand classification
problem and how to build a loan
prediction system at number nine we have
fake news detection system do you
believe in everything you read in social
media isn't it true that not all news is
true but how will you recognize fake
news ml is the answer you will able to
tell the difference between real and
fake news by practicing this project of
detecting fake news this ml project for
detecting fake news is concerned with
the fake news and the true news on our
data set we create a tfid vectorizer
with escalan the model is then using a
passive aggressive classifier that has
been initialized finally the accuracy
score and the confusion matrics indicate
how well our model performs the link for
the project is in the description box
below at number eight we have
personality prediction system the idea
is based on determining an individual
personality using machine learning
techniques a person personality
influences both his personal and
professional life nowadays many company
are shortlisting applicant based on
their personality which increases job
efficiency because the person is working
on what he is good at rather than what
is compelled to do in our study we
attempted to combine personality
prediction system using machine learning
techniques such as SVD na Bas and
logistic regression to predict a person
personality and talent prediction using
phrase frequency method this model or
method allows users to recognize their
personality and Technical abilities
easily to learn about mod this project
check the link in the description box
below at number seven we have Parkinson
disease system Parkinson disease is a
progressive central nervous system
element that affects movement and cause
Tremors and stiffness it comprises five
stages and affects more than 1 million
worldwide each other in this machine
learning project we will develop an svm
model using python modules psyched learn
numpy and pandas and svm we will import
the data extract the features and label
and scale the features split the data
set design an N model and calculate the
model accuracy and at the end we will
check the Parkinson disease for the
individual to learn about more this
project check the link in the
description box below add number six we
have text to speech converter
application the machine learning domain
of audio is undoubtly cutting as right
now the majority of the application
available today are the commercial the
community is building several audio
specific open source framework and
algorithm other text to speech apis are
available for this project we will
utilize pyttsx3 pyttsx3 is a python text
to speech conversion Library it operates
offline unlike other libraries and is
compatible with python 2 and Python 3
before API various pre models were
accessible in Python but changing the
voice of volume was often difficult it
also needed additional computational
power to learn more about this project
check the link in the description box
below at number five we have speech
recognition system speech recognition
often known as spech to text is the
capacity of a machine or program to
recognize and transfer word spoken
allowed into readable text ml spe
recognition uses algorithm that model
speech in terms of both language and
sound to extract the more important
parts of the speech such as words
sentences and acostic modeling is used
to identify the phenones and the
phonetics on the speech for this project
we will utilize pyttsx3 pyttsx3 is a
python text to speech conversion Library
it operates offline unlike other
libraries and is compatible with python
2 and Python 3 to learn more about this
project check the link in the
description box below at number four we
have sentiment analysis sentiment
analysis also known as opinion mining is
a straightforward process of determining
the author's feeling about a text what
was the user intention when he or she
wrote something to determine what could
be personal information we employ a
variety of natural language processing
and text analysis technology we must
detect extract and quantify such
information from the text to enable
classification and data manipulation in
this project we will use the Amazon
customer review data set for the
sentiment analysis check the link in the
description box below at number three we
have image classification using CNN deep
learning is a booving field currently
most projects and problem statement used
deep learning is an any sort of work
many of you like myself would choose a
conventional neural network as a deep
learning technique for answering any
computer vision problem statement in
this project we will use CNN to develop
an image processing project and learn
about its capabilities and why it has
become so popular we will go over each
stage of creating our CNN model and our
first spectacular project we will use
the CFI 10 data set for image
classification in this project to learn
more about this project check the link
in the description box below at number
two we have face recognition system
currently Tech technology absolutely
amazes people with Incredible invention
that makes life easier and more
comfortable face recognition has shown
to be the least intrusive and fastest
form of the biometric verification over
time this project will use open CV and
pH recognition libraries to create a
phase detection system open CV provides
a realtime computer vision tool library
and Hardware we can create amazing
realtime projects using opencv to learn
how to create face recognition system
for you check the link in the
description box below and last but not
the least we have movie recommendation
system almost everyone today use
technology to stream movies and
television show while figuring out what
to stream next can be disheartening
recommendation are often made based on a
viewer history and preferences this is
done through a machine learning and can
be a fun and the easy project for the
beginners new programmers can practice
by coding in either python or R and with
the data from the movie lens dat set
generated by more than 6,000 users to
learn how to create movie recommendation
system for yourself or for your loved
ones check the project and the
description box below hello everyone
welcome to this video on what is mlops
by simply learn and let me tell you guys
that we have regular updates on multiple
Technologies if you a tech geek in a
continuous ear for the latest
technological Trends then consider
getting subscribed to our YouTube
channel and press that Bell icon to
never miss any update from Simply learn
in this video first we will see what
mlops is after that we will cover the
use of mlops after that we will see some
real world application of mlops after
that we will see components of mlops at
at the end we will see difference
between mlops and devops by the end of
this video I can assure you that all
your questions and doubts related to
what is mlops will have been clear on
that note if you are an aspiring AIML
engineer then there is no better time to
train yourself in the exciting field of
machine learning if you're are looking
for course that covers everything from
the fundamental to Advanced Techniques
like machine learning algorithm
development and unsupervised learning
look no further than our ctech program
in partnership with IBM this AIML
program boot camp is collaboration with
keltech will help you advance your
career as an AIML specialist the AIML
boot cam includes live classes delivered
by industry expert handson LS industry
relevant projects and master classes by
celtech professors key feature of this
amazing course includes earn up to 22
cus from CCH ctme online convocation by
celtech ctm program director simply
learn Career Services help you get
noticed by top hiring companies earn a
boot camp celtech certificate industry
relevant Capstone project in three
domains CCH ctm Circle membership 25
plus Hands-On project across Industries
verticals with integrated Labs so why
wait join now seats are feeling fast
find the course link from the
description box below so moving forward
let's see what is mlops mlops short for
machine learning operation is a set of
practices tools and principles that aim
to streamline and enhance the deployment
management and monitoring of machine
learning models in production of
environments it draws inspiration from
devops development and operation
practices and applies them specifically
to the challenges of deploying and
maintaining ml model effectively moving
forward let's see what is the use of
mlops mlop serves crucial purposes in
the context of deploying and managing ml
models in production environment the
first one is efficient development mlops
helps automate the deployment process of
machine learning models making it faster
repeatable and less error prone this
enables the organization to quickly
transition from development to
production reducing the time it takes to
make model available to end users the
second one is collaboration mlops
encourages collaboration between
different teams such as data scientist
machine learning engineer operation and
business stakeholders by fostering
better communication and Alignment mlops
helps deliver solution that meets
Technical and business requirement the
third one is monitoring and maintenance
mlops provides tools for monitoring the
performance of deploy models in real
world condition this allows teams to
detect anomalies track key performance
metrics and address issue prompty
ensuring that the models continue to
deliver accurate result over time the
fourth one is cost efficiency by
optimizing resource utilization and
automatic process melops can lead to
cost saving efficiently manage
deployment reduce operational overhead
and resource wastage making machine
learning project more economically
viable the fifth one is Security in many
Industries compliance and security are
Paramount concern mlops provides
practices for managing access controls
data privacy and regular environment
ensuring deployed model ader to
necessary standard so in summary mlops
plays a critical role in maximizing the
value of machine learning by ensuring
that models are efficiently deployed
monitored maintained and improved in
production environments it enables
organization to drive consistent
reliable and scalable benefits from
their machine learning investment moving
forward let's see some real world
application of mlops so mlops find
application in various areas where
machine learning models are deployed in
real world production environment some
typical application of mlops include the
first one is Predictive Analytics mlops
helps deploy Predictive Analytics that
analyze historical data to predict
future events these models are used in
finance market sales and other
Industries to forecast Trends and
optimize decision making the second one
is image video analysis mlops is used to
application like image classification
object detection and facial recognition
these Technologies are employed in
security system autonomous vus and
Medical Imaging the third one is
Healthcare and medical diagnostic mlops
AIDS in development and deploying models
for medical imaging analysis disease
diagnosis drug Discovery and patient
risk assessment the fourth one is
autonomous system mlops is crucial for
autonomous systems like wle drones and
Robotics ensuring that AI model function
reliable and adapt to changing condition
so these are some just a few example of
mlop and the application of mlops are
continuously expanding as organization
explore new ways to leverage machine
learning in their operation so
accelerate your career in AIM with
Comprehensive post-graduate program in
AIML machine learning boost your career
with this AIML course delivered in
collaboration with part University and
IBM learn in demand skills such as
machine learning deep learning NLP
computer vision reinforcement learning
gener
prompt engineering chat gbt and many
more you will receive a prestigious
certificate and ask me anything session
by IBM with five Capstone in different
domains using real data set you will
gain practical experience master classes
by P University and perdu faculty and
IBM experts ensure topnotch education
simply and job assist help you get
noticed by Leading companies so why wait
join now seats are filling fast course
link is in the description box below and
in the pin comment moving forward let's
see component of mops the extent of
mlops in machine learning projects can
vary based on the project needs
sometimes mlops covers everything from
preparing the data to making the final
model other times it might just involve
putting the model into action most
companies use mlops for first one is
explor data analysis looking into the
data to understand it the second one is
getting the data ready and making
valuable features from it data
preparation and feature engineering the
third one is model training and the fine
tuning the fourth one is model model
review and governance fifth one is model
inference and serving model sixth one is
model deployment and monitoring the last
one is teaching the model again
automatically when needed moving forward
let's see some difference between mlops
and devops the first one is mlops
specifically focus on the deployment
monitoring and management of machine
learning models while devops devops Prim
focus on the collaboration and
integration between development and
operation the second one is it is
centered around machine learning AI
application addressing the unique
challenges of deploying and maintaining
models while in devops it applies to
software development in in general
covering all aspects of software
engineering and deployment the third one
is mlops involves automating the end to
end machine learning life cycle from
data pre-processing to data model
monitoring and retraining ha while
devops aims to stream line the
deployment life cycle automate
deployment pipelines improve
collaboration and ensure faster and more
Reliable Software releases and the
fourth one one is mlop employs tools for
model versioning model packaging model
monitoring automated retraining and
model explanation while devops employs
tools like Version Control Systems like
G continuous integration continuous
deployment cicd pipelines configuration
management and infrastructure as scod to
achieve its goal the last one is
efficient model deployment continuous
model performance monitoring
reproductivity collaboration between
data scientists and engineers and
adapting to changing data and
requirements here the f velopment Cycles
continuous integration quick and
automated deployment improve team
collaboration and increase software
reliability in some while devops and
mlops share concept of automation
collaboration and continuous Improvement
devops has a broader focus on software
deployment and deployment while mlops is
tailors to change the challenges
specific to deployment and managing
machine learning models in production
settings both practices aims to enhance
efficiency collaboration and reliability
in their Respec effective domain and we
also have a boot camp for you guys and
if you are one of the aspiring AI ml
Enthusiast looking for online training
and graduating from the best
universities or a professional who
elicits to switch careers in Ai and ml
by learning from the experts then try
giving a short to Simply learns keltech
Ai and machine learning boot camp the
course link or the boot camp link is
mentioned in the description box that
will navigate you to the course page
where you can find a complete overview
of the program being offered today we
are going to discuss the top 10 AI tools
for business in 2023 but before we begin
if you haven't already then consider
getting subscribed to our YouTube
channel and hit the Bell to get notified
and also if you are an aspiring
artificial intelligence engineer looking
for online training and certifications
from the prestigious universities and
collaborating with leading experts to
have the most robust Foundation skills
knowledge and what it takes to be the
best then search no more simply launch
postgraduate program in artificial
intelligence from Pur University in
collaboration with IBM and simply learns
artificial intelligence and machine
learning boot camp from calch University
should be your right choice for more
details use the links in the description
box below with that in mind let's get
started with the first one that is
clickup clickup is an AI powered
assistant that is integrated to the
clickup productivity platform it can
help you with a variety of tasks
including generating text translating
languages answering questions
brainstorming ideas summarizing
documents and much more when it comes to
generating text clickup AI can generate
text for a wide variety of purposes such
as writing blog post creating marketing
copy or summarizing documents and when
it comes to translating languages
clickup AI can translate text between a
variety of languages moving to answering
questions clickup AI can answer your
questions about a variety of topics such
as project management productivity or
business and next we have brainstorming
ideas click up AI can help you
brainstorm ideas for projects products
and marketing campaigns and lastly
summarizing documentations clickup AI
can summarize documents into a concise
and easy to understand format clickup AI
is still under development but it is
already a powerful tool that can help
you save time and improve your
productivity here are some key features
of clickup it is accessible via web
desktop and mobile apps 100 plus role
based AI tools with ready toade prompts
slri command quickly to brainstorm your
ideas access the AI command from the
text tool bar to edit text summarize
long form content with click of a button
it can integrate with click a
productivity platform and can help you
with a wide variety of tasks such as
generating text translating languages
answering questions brainstorming ideas
and summarizing documentations now
moving ahead into the second tool for
today's discussion which is about chat
GPT chat GPT is an AI powered language
model developed by open AI it is capable
of generating humanlike text based on
context and past conversations it is
still under development but it has
already been used for a variety of
purposes such as chatting with humans
generating text translating languages
answering questions let's discuss each
one of these chatting with humans chat
GPT can hold conversations with humans
as bit in a limited capacity it can
answer questions follow instructions and
even tell stories when it comes to
generating text chat gbt can generate
text for a wide variety of purposes such
as writing blog posts creating marketing
copy or summarizing documents it can
also translate languages and answer all
of your questions some of the key
features of chat GPT are as follows it
has a clean and minimalistic UI which
makes it extremely easy to use it has
natural language processing it has
previous conversation details about an
interactive which is two-way dialogues
and then it supports 50 plus languages
that is spoken languages and it has a
vast knowledge of Base spanning multiple
Industries and also has vast knowledge
based subject areas now we will move
into the next tool for today's
discussion that is doll e or doll E2 is
a text to image diffusion model created
by open AI it can generate realistic
images from text descriptions for
example you can give it a prompt like a
cat riding a unicycle on a rainbow which
will generate an image of exact that
dolly is still considered to be under
development but it has already been used
to create some amazing images it has the
potential to be used for a variety of
purposes such as creating art designing
products generating Market materials
educating people when it comes to
creating art Dolly 2 can be used to
create original artwork such as
paintings drawings and sculptures when
it comes to designing products Dolly 2
can be used to design new products such
as clothing furniture or toys when it
comes to generating marketing materials
Dolly 2 can be used to generate
marketing materials such as posters
flyers and advertisements and lastly
when it comes to educating people Dolly
to can be used to create educational
material such as illustrations and diag
Rams some of the most important key
features of dolly2 are it is very
intuitive interface that makes it easy
to use it can generate unique and high
quality images in seconds the native
editing tool lets you generate text
prompts for replacing different elements
in a picture and it has an out painting
feature that lets you expand the canvas
now let's proceed into the fourth tool
for today's discussion that is brecka
bra bricka bra AI is an AI powered app
generator that allows you to create web
applications games and tools without
coding it uses chat gp4 natural language
processing technology to convert text
descriptions into fully functional web
apps with a responsive user interface
breakup braack offers a free plan that
allows users to create up to six apps
per month and paid plans that start at
$68 per month here are some key features
of Bricker Brack it is a time-saving and
coste effective AI tool for hiring
professional and experienced developers
the built-in functional applications
from scratch in minutes just using text
an intuitive drag and drop editor can
let you create customized AI tool to
suit your needs and requirements you can
export your app in file formats such as
HTML CSS and JavaScript it has unlimited
free web app hosting capabilities now we
will be proceeding into the next tool
for today's discussion which is Tom Tom
is an AI powered story telling tool that
helps you create and share immersive
narratives quickly and easily it uses a
combination of machine learning and
natural language processing to generate
presentations outlines and stories with
text images and other media to AI is
easy to use just type a description of
your desired presentation or story into
a text box and Tom AI will generate it
for you you can then customize the
content and style for your liking to AI
is a powerful tool that can be used for
a variety of purposes including creating
presentations for your work or school
writing posts or articles developing
marketing materials pitching ideas to
investors sharing personal stories and
much more to AI is still under
development but it is already a valuable
tool for anyone who wants to create and
share their ideas in a more engaging way
some of the key features of Tom AI are
as follows it can create a complete
presentation with text prompt only adds
specific slides pages to a presentation
with natural language prompts converts
and upload a document into a
presentation with a click of a button it
can integrate with the dolly to to
produce unique AI images for your
presentation embedded live Pages for any
website or external apps and lastly it
has sharing and commenting features for
collaborating with your team now
proceeding into the next tool which is
second brain second brain AI is an AI
powered writing companion that helps you
to write better articles emails tweets
messages and much more it uses the d003
text model from open AI to generate text
and rephrase existing text it also
offers text generation based on a prompt
and creating custom AI typ tasks second
AI is still under development but it has
a wide potential to be a powerful tool
for anyone who wants to improve their
writing skills here are some key
features of second brain it can be
helpful to training your new chatboard
and embedding it on your site page the
chatbot can be trained with data from 90
different languages it quickly and
accurately responds to your customer
queries you can also create multiple
Bots depending on your paid plan and
lastly it is great for entrepreneurs if
you are looking for a way to improve
your writing skills second Brin AI is a
great option to consider it is easy to
use and affordable and it has potential
to help you write better articles email
and other content now let us proceed
into the next tool that is Jasper AI
Jasper AI is an AI writing assistant
that helps you create high quality
content quickly and easily it uses a
variety of AI models to generate text
translate languages write different
kinds of creative content and answer
your queries in an informative way
Jasper AI is a powerful tool that can be
used for a variety of purposes including
writing blog post and articles creating
marketing copies translating languages
answering customer questions generating
creative content such as poems code
scripts musical pieces emails letters
Etc some of the key features of Jasper
AI are it has 50 plus templates to speed
up the content creation proc process it
can generate content in 30 plus
languages it lets you repurpose content
for multiple platforms it has built-in
collaboration tools for working with
your team it can find H content output
to match your Brand's Stone and style
and lastly it can integrate with
grammarly copy space and Surfer SEO
Jasper AI is still under development but
it has already been used for businesses
of all sizes to improve their content
marketing customer service and much more
if you are looking for a way to improve
your content creation Jasper AI is a
great option to consider it is easy to
use and affordable and it has the
potential to help you create better
content that will engage your audience
and help you achieve your business goals
next is plus AI plus AI is an AI tool
that helps businesses automate their
reporting and analytics it uses a wide
variety of AI models to extract insights
from data generate reports and create
dashboards plus AI is a part tool that
can be used for a variety of purposes
including automating reporting and
analytics tasks extracting insights from
data generating reports and dashboards
improving decision making reducing costs
the key features of plus AI are as
follows it is part by the latest in
generative AI it has integration between
Google slice and PowerPoint which is
seamless it can create a presentation
that needs only minor editing it has the
ability to rewrite content on slides
which is a biggest game changer plus AI
is still under development but it has
already been used by businesses of old
sizes to improve their reporting and
analytics capabilities now let's proceed
into the ninth one which is about
fireflies fireflies is an AI powered
note maker that can transcribe summarize
and analyze meetings it uses a
combination of machine learning and
natural language processing to
automatically capture and transcribe
your meeting
and then generate summaries and insights
that you can use to stay on top of your
work here are some things that you can
do with fireflies AI stay on top of your
work by automatically transcribing and
summarizing your meetings get insights
into your team's discussions by
analyzing meeting transcripts share
meeting transcripts and insights with
your team members use meeting
transcripts to create presentations or
reports and some of the key features of
fireflies are as follows it can record
and transcribes calls instantly it has
Chrome extension to capture all your
meetings it has simple to use search
that allows you to review your calls and
easy to use meeting port to invite fir
flight borts to a meeting transcribe
existing audio files instantly inside
the dashboard offer native integration
to dialers zapier and other apis and
lastly it eliminates the task of note
taken if you're looking for AI tool that
can help you improve your produc it
fireflies AI is a great option to
consider it is easy to use and
affordable and it has the potential to
save time and stay on top of your work
now the last tool for today's discussion
which is speechify speechify is an AI
powered text to speech TTS tool that
helps you read and listen to text more
efficiently it can read alloud any text
you provide including PDFs articles
websites and social media posts speech
of five also offers a variety of
features to customize your listening
experience such as adjusting the reading
speed voice and background noise some of
the key features of speechify are it has
web based with chrome and Safari
extensions more than 15 languages are
supported by speech fi it has over 30
voices to select from and it can scan
and convert printed text to speech
speechify is an excellent tool for
anyone who wants to improve their
reading comprehension Focus or
productivity it can also be helpful for
people with dyslexia ADHD or other
learning disabilities AI tools for
business in 2023 but before we begin if
you haven't already then consider
getting subscribed to our YouTube
channel and hit the Bell to get notified
and also if you are an aspiring
artificial intelligence engineer looking
for online training and certifications
from the prestigious universities and
collaborating with leading experts to
have the most robust Foundation skills
knowledge and what it takes to be the
best then search no more simply laun
postgraduate program in artificial
intelligence from her University in
collaboration with IBM and simply learns
artificial intelligence and machine
learning boot camp from CTIC University
should be your right choice for more
details use the links in the description
box below with that in mind let's get
started with the first one that is
clickup clickup is an AI powered
assistant that is integrated to the
clickup productivity platform it can
help you with a variety of tasks
including generating text translating
languages answering questions
brainstorming ideas summarizing
documents and much more when it comes to
generating text clickup AI can generate
text for a wide variety of purposes such
as writing blog post creating marketing
copy or summarizing documents and when
it comes to translating languages
clickup AI can translate text between a
variety of languages more moving to
answering questions clickup AI can
answer your questions about a variety of
topics such as project management
productivity or business and next we
have brainstorming ideas clickup AI can
help you brainstorm ideas for projects
products and marketing campaigns and
lastly summarizing documentations
clickup AI can summarize documents into
a concise and easy to understand format
clickup AI is still under development
but it is already a powerful tool that
can help you save time and improve your
productivity here are some key features
of clickup it is accessible via web
desktop and mobile apps 100 plus
role-based AI tools with ready toade
prompts slri command quickly to
brainstorm your ideas access the AI
command from the text tool bar to edit
text summarize long form content with
click of a button it can integrate with
click a productivity platform and can
help you with a wide variety of tasks
such as generating text translating Lang
languages answering questions
brainstorming ideas and summarizing
documentations now moving ahead into the
secondary tool for today's discussion
which is about chat GPT chat GPT is an
AI powered language model developed by
open AI it is capable of generating
humanlike text based on context and past
conversations it is still under
development but it has already been used
for a variety of purposes such as
chatting with humans generating text
translating languages answering
questions let's discuss each one of
these chatting with humans chat GPT can
hold conversations with humans Alit in a
limited capacity it can answer questions
follow instructions and even tell
stories when it comes to generating text
chat gbt can generate text for a wide
variety of purposes such as writing blog
posts creating marketing copy or
summarizing documents it can also
translate languages and answer all of
your questions some of the key key
features of chat GPT are as follows it
has a clean and minimalistic UI which
makes it extremely easy to use it has
natural language processing it has
previous conversation details about an
interactive which is two-way dialoges
and then it supports 50 plus languages
that is spoken languages and it has a
vast knowledge of Base spanning multiple
Industries and also has vast knowledge
based subject areas now we will move
into the next tool for today's
discussion that is doll e or doll E2 is
a text to image diffusion model created
by open AI it can generate realistic
images from text descriptions for
example you can give it a prompt like a
cat riding a unicycle on a rainbow which
will generate an image of exact that
dolly is still considered to be under
development but it has already been used
to create some amazing images it has the
the potential to be used for a variety
of purposes such as creating art
designing products generating Market
materials educating people when it comes
to creating art Dolly 2 can be used to
create original artwork such as
paintings drawings and sculptures when
it comes to designing products Dolly to
can be used to design new products such
as clothing furniture or toys when it
comes to generating marketing materials
Dolly 2 can be used to generate
marketing materials such as posters
buers and advertisements and lastly when
it comes to educating people Dolly 2 can
be used to create educational material
such as illustrations and diagrams some
of the most important key features of
dolly2 are it is very intuitive
interface that makes it easy to use it
can generate unique and high quality
images in seconds the native editing
tool lets you generate text prompts for
replacing different elements in a
picture and it has an out painting
feature that lets you expand the canvas
now let's proceed into the fourth tool
for today's discussion that is brecka
bra brecka bra AI is an AI powered app
generator that allows you to create web
applications games and tools without
coding it uses chat GPT 4 natural
language processing technology to
convert text descriptions into fully
functional web apps with a responsive
user interface breaker braack offers a
free plan that allows users to create up
to six apps per month and paid plans
that start at $68 per month here are
some key features of Bricker Brack it is
a time-saving and cost effective AI tool
for hiring professional and experienced
developers the built-in functional
applications from scratch in minutes
just using text an intuitive drag and
drop editor can let you create
customized AI tool to suit your needs
and requirements you can export your app
app in file formats such as HTML CSS and
JavaScript it has unlimited free web app
hosting capabilities now we will be
proceeding into the next tool for
today's discussion which is to Tom is an
AI powered storytelling tool that helps
you create and share immersive
narratives quickly and easily it uses a
combination of machine learning and
natural language processing to generate
presentations outlines and stories with
text images and other media to AI is
easy to use just type a description of
your desired presentation or story into
a text box and Tom AI will generate it
for you you can then customize the
content and style for your liking to AI
is a powerful tool that can be used for
a variety of purposes including creating
presentations for your work or school
writing posts or articles developing
marketing materials pitching ideas to
investors sharing personal stories and
much more to AI is still under
development but it is already a valuable
tool for anyone who wants to create and
share their ideas in a more engaging way
some of the key features of Tom AI are
as follows it can create a complete
presentation with text prompt only adds
specific slides pages to a presentation
with natural language prompts converts
an uploaded document into a presentation
with a click of a button it can
integrate with the dolly to to produce
unique AI images for your presentation
embedded live Pages for any website or
external apps and lastly it has sharing
and commenting features for
collaborating with your team now
proceeding into the next tool which is
second brain second brain AI is an AI
powered writing companion that helps you
to write better articles emails tweets
messages and much more it uses the DAV
00003 text model from open AI to
generate text and rephrase existing text
it also offers text generation based on
a prompt and creating custom AI tasks
second AI is still under development but
it has a wide potential to be a powerful
tool for anyone who wants to improve
their writing skills here are some key
features of second brain it can be
helpful to training your new chatboard
and embedding it on your side page the
chatbot can be trained with data from 90
different languages it quickly and
accurately responds to your customer
queries you can also create multiple
Bots depending on your paid plan and
lastly it is great for entrepreneurs if
you want to become an AI expert and gain
handsome salary packages look at the
wide range of AIML courses by simply
learn in collaboration with top
universities across the globe you will
catch the eyes of top Recruiters in the
industry here we have our um it looks a
little bit like Frankenstein our
Frankenstein looking robot today let me
tell you what is machine learning
machine learning works on the
development of computer programs that
can access data and use it to
automatically learn and improve from
experience watch a robot builder
construct house in two days this was
back in July 29th 200 in 16 so that's
pretty impressive this amount of time to
continue to grow in its development and
it's smart enough to leave spaces in the
brick work for wiring and plumbing and
can even cut and shape bricks to size
Amazon Echo relies on on machine
learning and with more data it becomes
more accurate play your favorite music
order pizza from Domino's voice control
your home request rides from Uber have
you ever wondered the difference between
AI machine learning and deep learning
artificial intelligence a technique
which enables machines to mimic human
behavior this is really important
because this is how we are able to gauge
how well our computations or what we're
working on works is the fact that we're
mimicking human Behavior we're using
this to replace human work and make it
more efficient and make it more
streamlined and more accurate and so the
center of artificial intelligence is the
big picture of all this put together IBM
deep blue chess electronic game
characters those are just a couple
examples of artificial intelligence
machine learning a technique which uses
statistical methods enabling machines to
learn from their past data so this means
if you have your input from last time
and you have your answer you use that to
help prove the next guess it makes for
the correct answer IBM Watson Google
search algorithm email spam filters
these are all part of machine learning
and then deep learning which is a subset
of machine learning composing algorithms
that allow a model to trade itself and
perform tasks Alpha go natural speech
recognition these are a couple examples
deep learning is associated with tools
like neural networks where it's kind of
a black box as it learns it changes all
these things that are as a human we'
have a very hard time tracking and it's
able to come up with an answer from that
now let's see how machine learning works
first we start with training the data
once we've trained the data the train we
go into the machine learning algorithm
which then puts the data into a
processing which then goes down to
machine another machine learning
algorithm and then we take new data
because you have to test whatever you
did to make sure it works correctly and
we put that into the same algorithm once
we do that we check our prediction we
check our results and from the
prediction if we've set aside some
training data and we find out it didn't
do a good job predicting it and it gets
a thumbs down as you see then we go back
to the beginning and we retrain the
algorithm and a lot of times it's not
just about getting the wrong answer it's
about continually trying to get a better
answer so you'll see the first time you
might be like oh this is not the answer
I want depending on what domain you're
working in whether it's medical
economical business stocks whatever you
try out your model and if it's not
giving you a good answer you retrain it
if you think you can get a better answer
you retrain it and you keep doing that
until you get the best answer you can
now let's look into the types of machine
learning machine learning is primarily
of three types first one is supervised
machine learning as the name suggest you
have to supervise your machine learning
while you train it to work on its own it
requires labeled training data next up
is unsupervised learning wherein there
will be training data but it won't be
labeled finally there's reinforcement
learning where in the system learns on
its own let's talk about all these types
in detail let's try to understand how
supervised Learning Works look at the
pictures very very carefully the monitor
depicts the model or the system that we
are going to train this is how the
training is done we provide a data set
that contains pictures of a kind of a
fruit say an apple then we provide
another data set which lets the model
know that these pictures we that of a
fruit called
Apple this ends the training phase SP
now what we will do is we provide a new
set of data which only contains pictures
of apple now here comes the fun part the
system can actually tell you what fruit
it is and it will remember this and
apply this knowledge in future as well
that's how supervised Learning Works you
are training the model to do a certain
kind of an operation on its own this
kind of a model is generally used into
filtering spam mails from your email
account as well yes surprise aren't you
so let's move on to unsupervised
learning now let's say we have a data
set which is cluttered in this case we
have a collection of pictures of
different fruits we feed this data to
the model and the model analyzes the
data to figure out patterns in it in the
end it categorizes the photos into three
types as you can see in the image based
on their
similarities so you provide the data to
the system and let the system do the
rest of the work simple isn't it this
kind of a model is used by flip cart to
figure out the product products that are
well suited for you honestly speaking
this is my favorite type of machine
learning out of all the three and this
type has been widely shown in most of
the Sci-Fi movies lately let's find out
how it works imagine a newborn baby you
put a burning candle in front of the
baby the baby does not know that if it
touches the flame its fingers might get
burned so it does that anyway and gets
hurt the next time you put that candle
in front of the baby it will remember
what happened the last time and would
not repeat what it did that's exactly
how reinforcement learning works we
provide the machine with a data set
wherein we ask it to identify a
particular kind of a fruit in this case
an Apple so what it does as a response
it tells us that it's a mango but as we
all know it's a completely wrong answer
so as a feedback we tell the system that
it's wrong it's not a mango it's an
apple what it does it learns from the
feedback and keeps that in mind on the
next time time when we ask a same
question it gives us the right answer it
is able to tell us that it's actually an
apple that is a reinforced response so
that's how reinforcement learning works
it learns from its mistakes and
experiences this model is used in games
like Prince of Persia or Assassin's
Creed or FIFA where in the level of
difficulty increases as you get better
with the games just to make it more
clear for you let's look at a comparison
between supervised and unsupervised
learning firstly the dat dat involved in
case of supervised learning is labeled
as we mentioned in the examples
previously we provide the system with a
photo of an apple and let the system
know that this is actually an apple that
is called label data so the system
learns from the label data and makes
future
predictions now unsupervised learning
does not require any kind of label data
because its work is to look for patterns
in the input data and organize it the
next point is that you get a feedback in
case of supervised learning that is once
you get the output the system tends to
remember that and uses it for the next
operation that does not happen for
unsupervised learning and the last point
is that supervised learning is mostly
used to predict data whereas
unsupervised learning is used to find
out hidden patterns or structures in
data I think this would have made a lot
of things clear for you regarding
supervised and unsupervised learning
artificial intelligence and machine
learning have permed every aspect of Our
Lives if you are a programmer chances
are you have begun utilizing github's
co-pilot an AI tool that transforms
natural language PRS into coding
suggestions streamlining the programming
process as a writer you may have
encountered open AI gp3 or similar Auto
regressive language models that leverage
deep learning to generate text
resembling human language many of us
dedicated a few hours to experimenting
with d to the AIML powered text to image
generator capable of producing intricate
visuals based on the most unusual
request not only is d 2 exponentially
more powerful but it also has the
potential to revolutionize the field of
digital art consider its impact on a
digital artist illustrator or graphic
designer career imagine creating a
highly realistic image within seconds
using an app these technologies have
significant real world applications with
far-reaching implications
let me tell you some fascinating facts
about machine learning according to
recent studies machine learning related
job postings have increased Byers
staggering
344 in the past 5 years companies across
the globe are actively seeking
professionals who can harness the power
of data and build intelligence systems
the average salary is
$190,000 in us and 26 lakhs perom in
India accelerate your career in AI ml
with our comprehensive postgraduate
program in Ai and machine learning gain
expertise in machine learning deep
learning NLP computer vision and
reinforcement learning you will receive
a prestigious certificate exclusive
alumni membership and hackathons and ask
me anything sessions by IBM with three
Capstone and 25 plus industry projects
using real data sets from Twitter Uber
and more you will gain practical
experience master classes by Purdue
faculty and IBM experts ensure Top Notch
education simply learns job assist helps
you get noticed by Leading companies
live sessions on AI trends like chat GPD
generative Ai and explainable AI this
program CS statistics python supervised
and unsupervised learning NLP neural
networks computer vision G caras tensor
flow and many more skills enroll now and
unlock exciting AI ml opportunities the
link is mentioned in the description box
below with that having said now here is
a comprehensive step-by-step guide to
learning machine learning
number one is skills required number two
job rolls in ml number third compan is
hiring ml engineers and the number four
is future scope of ml now starting with
the number one that is fundamentals of
machine learning familiarize yourself
with the fundamentals of machine
learning in the initial stages of
learning to drive we are familiarized
with the different elements varieties
and regulations pertaining to operating
a car it is essential to delve into the
fundamentals of machine learning to
understand what lies ahead and the
required knowledge now moving to number
two that is acquiring Proficiency in
python or our programming language to
excel in the field of artificial
intelligence and machine learning it is
crucial to develop a strong command of
python or our programming language both
Python and R are widely used in the data
science Community due to their
versatility and extensive libraries for
for scientific Computing so now moving
on to the third step that is gain
knowledge of essential python libraries
for machine learning once you have
acquired Proficiency in python as part
of our machine Learning Journey the
subsequent step in mes familiarizing
yourself with essential python libraries
crucial for working with data and
implementing machine learning Solutions
the key python libraries that you should
learn from machine learning are numpy
pandas matplot Li and psychic learn now
moving to step four that is learn and
Implement various machine learning
algorithms after gaining Proficiency in
python as part of our machine Learning
Journey the subsequent Milestone
involves learning various machine
learning algorithms and their
implementation using python listed below
are some of the key machine learning
algorithms that are essential to
learning number one linear regression
number two logistic regression number
three passive aggression number four na
Base number five five support Vector
machines now moving to step five Master
the concepts and implementation of
neural networks once you have acquired
knowledge of python and machine learning
algorithms the next significant step in
the machine learning road map is to
learn neural network architecture and
their implementations using python
outlined below are several crucial
neural network architectures that are
essential to learning number one
artificial neural networks number two
CNN number three
RNN number four long shortterm memory
now moving to step six that is engage
Hands-On projects to apply your
knowledge and reinforce your
understanding after acquiring knowledge
of python machine learning algorithms
and neural network architectures the
next crucial step is the machine
learning road map and is to gain
practical experience by working on
projects that allow you to apply what
you have learned the first project is
Iris flower classification
number two California house price
prediction number three stock price
prediction number four customer
segmentation now we'll move to job roles
in machine learning machine learning as
highlighted earlier has gained immense
popularity for its ability to elevate
human efforts and enhance machine
performance through autonomous learning
this popularity has resulted in
lucrative and sought after career
options within the field including roles
like machine learning engineer so the
duty of a machine learning engineer
encompasses the creation construction
and deployment of machine learning
models collaborating closely with data
scientist and software Engineers they
participate in the development and
execution of machine Learning Systems
according to glasto ml Engineers can
earn up to
$150,000 in us and 11 lakhs perom in
India now moving to data scientist the
role of a data scientist involves
Gathering scrutinizing and interpreting
extensive data sets leveraging machine
learning algorithms they uncover
patterns and glean insights from the
data utilizing this knowledge to inform
decisions and address challenges
according to glasto data scientists earn
$145,000 in us and 13 lakhs perom in
India now moving to NLP Engineers the
specific duties may vary based on the
role and sector but as outlined by
freelancer map an NLP engineer typically
engage in tasks such as designing
natural language processing systems and
addressing speech patterns and AI speech
recognition according to glass store NLP
engineer can earn
$120,000 in us and 10 lakhs perom in
India now moving to computer vision
engineer Engineers specializing in
computer vision operate within the realm
of computer vision employing machine
learning to empower computers to
comprehend and interpret visual data
from the surround ings the
responsibilities include tasks such as
image and video analysis and object
detection according to glassor CV
Engineers earn
$156,000 in us and 8 lakhs perom in
India now moving to business
intelligence developer the main role of
a bi developer is to develop deploy and
maintain bi tools and interfaces they
also responsible for simplifying highly
technical language and complex
information into layman's terms for
everyone else in the company to
understand according to Glass Ro
business intelligence developers earn
$115,000 in us and 7 lakhs perom in
India now we'll see the top companies
hiring for machine learning in Jor
number one is Amazon then we have asena
Google Apple Intel so these are the top
hiring companies that hire machine
learning Engineers now we'll talk about
future of machine learning machine
machine learning has a bright future but
faces several difficulties ml is
predicted to grow increasingly pervasive
as technology develops revolutionizing
sectors including Healthcare Banking and
transportation the work Market will
change due to AI driven automation
necessitating new position and skills
Welcome to our video on skills required
for an ml engineer machine learning has
been the Talk of the Town lately every
organization has realized the potential
of machine learning in in improving
their business objectives and attaining
the Enterprise goals this expanding
demand has led to a lot of people
applying for machine learning jobs and
upskilling themsel in the field of
machine learning you can take up this
growing opportunity in the field of
machine learning and utilize it to learn
yourself a very challenging fulfilling
and high ping job in this video we will
be breaking down in complete detail each
and every skill that you would need in
order to crack the machine learning
engineer job interview well ml is not
just a passing Trend it's a sismic shift
that is reshaping our world and creating
new avenues for Innovation and Discovery
so by embracing a career in ml you
become part of dynamic field that
thrives on solving complex problem
pushing boundaries and making a profound
impact on society so the demand for ML
professional is skyrocketing across
industries from Healthcare and finance
to entertainment and transportation
organization are actively seeking
talented individuals who can harness the
power of AI to drive their business
forward but what skill does it takes to
become an ml engineer how can you embark
on this thrilling Journey we have the
answers to all your questions also
accelerate your career in Ai and ml with
our comprehensive post-graduate program
in Ai and machine learning gain
expertise in machine learning deep
learning NLP computer vision and
reinforcement learning you will receive
a prestigious certificate exclusive
alumini membership and ask me anything
sessions by IBM with three Capstone
projects and and 25 plus industry
projects using real data set from
Twitter Uber and more you will gain
practical experience master classes by
Kelch faculty and IBM experts ensure
top-notch education simply learns job
assist help you get notice by Leading
companies this program covers python
supervised and unsupervised learning NLP
neural networks computer Visions g k tza
flow and many more other skills so
enroll now and unlock exciting Ai and ml
opportunities the link is in the
description box below so without any
further delay let's get started to
become a machine learning engineer you
need a combination of technical skills
non-technical skills and some bonus
skills so here are some essential skills
required to pursue a career as an ml
engineer so first we will talk about
some technical skills to become a ml
engineer so first one in the list is
programming languages strong programming
skills are essential you should be
proficient in at least one programming
languages such as python or R python is
widely used in the ml Community due to
its Rich libraries that is numai pandas
tensorflow and pytorch that supports ml
task and the second on the list is
machine learning algorithms and
techniques you should have a solid
understanding of various ml algorithm
such as linear regression logistic
regression decision trees random Forest
neural network and deep learning
familiarize yourself with the principles
Behind These algorithms their pros and
cons and when to use them so third one
on the list is data pre-processing ml
models require clean and well prepared
data you should know how to handle
missing data deal with normalized and ex
standardized data and the final is
perform feature engineering
understanding data pre-processing
technique is crucial for Effective ml
model training and the fourth one is
data manipulation and Analysis data is
the foundation of ml model you should be
skilled in the data manipulation and
Analysis using libraries like numai and
pandas this includes cleaning and
transforming data explorat data analysis
which is Eda and understanding the
statical properties of the data and the
fifth one is machine learning libraries
and Frameworks familiarity with popular
ml libraries and framework is essential
some some commonly used one include
numai pandas tensorflow and py to this
Library provide pre-implemented ml
algorithms neural network architectures
and tools for model training and
evaluation now that we have seen the
Technical Machine learning engineering
skills let us have a look at the non
Technical Machine learning skills so the
first one is industry knowledge machine
learning projects that effectively
tackle genuine challenges are likely to
achieve great success regardless of the
industry you are involved in it is
crucial to have a comprehensive
understanding of its operation and
identify ways to optimize business
outcomes and the second one on the list
is effective communication effective
communication plays a crucial role in
facilitating these interaction companies
seeking skilled ml engineer value
candidates who can effectively conver
technical discoveries to non-technical
terms like marketing or sales
demonstrating Clarity and fluency in
their explanation moving forward let's
see some bonus skills to become ml
engineer the first one on the list is
reinforcement learning in 2023
reinforcement learning EML as a catalyst
for numerous captivating advancement in
deep learning and artificial
intelligence to pursue a career in
robotics self-driving cars or any other
AI related feed it is crucial to
comprehend this concept and the second
on the list is computer vision computer
vision and machine learning are
fundamental branches of computer science
that can independently fuel highly
Advanced system relying on CV and ml
algorithm however their combination has
a potential to unlock greater even
possibilities and achievements so
remember that the field of ml is
constantly evolving to continuous
learning and staying updated with the
latest development and the research
papers machine learning has improved our
lives in a number of wonderful ways
today let's talk about some of these I'm
Rahul from Simply learn and these are
the top 10 applications of machine
learning first let's talk about virtual
personal assistants Google Assistant
Alexa Cortana and Siri now we've all
used one of these at least at some point
in our lives now these help improve our
lives in a great number of ways for
example you could tell them to call
someone you could tell them to play some
music you could tell them to even
schedule an appointment so how do these
things actually work first they record
whatever you're saying send it over to a
server which is usually in a cloud
decode it with the help of machine
learning in neural networks and then
provide you with an output so if you
ever noticed that these systems don't
work very well without the internet
that's because the server couldn't be
contacted next let's talk about traffic
predictions now say I wanted to travel
from Buckingham Palace to LS cricket
ground the first thing I would probably
do is to get on Google Maps so search
it and let's put it here
so here we have the path you should take
to get to Lodge cricket ground now here
the map is a combination of red yellow
and blue now the blue regions signify a
clear road that is you won't encounter
traffic there the yellow indicate that
they're slightly congested and red means
they're heavily congested so let's look
at the map a different version of the
same map and here as I told you before
red means heavily congested yellow means
slow moving and blue means clear so how
exactly is Google able to tell you that
the traffic is clear slow moving or
heavily congested so this is with the
help of machine learning and with the
help of two important measures first is
the average time that's taken on
specific days at specific times on that
route the second one is the real-time
location data of vehicles from Google
Maps and with the help of sensors some
of the other popular map services are
Bing Maps maps. me and here we go next
up we have social media personalization
so say I want to buy a drone and I'm on
Amazon and I want to buy a DJI mavic Pro
the thing is it's close to one lap so I
don't want to buy it right now but the
next time I'm on Facebook I'll see an
advertisement for the product the next
time I'm on YouTube I'll see an
advertisement even on Instagram I'll see
an advertisement so here with the help
of machine learning Google has
understood that I'm interested in this
particular product hence it's targeting
me with these advertisements this is
also with the help of machine learning
let's talk about e mail spam filtering
now this is a spam that's in my inbox
now how does Gmail know what's spam and
what's not spam so Gmail has an entire
collection of emails which have already
been labeled as spam or not spam so
after analyzing this data Gmail is able
to find some characteristics like the
word lottery or winner from then on any
new email that comes to your inbox goes
through a few spam filters to decide
whether it's spam or not now some of the
popular spam filters that Gmail uses is
content filters header filters General
Blacklist filters and so on next we have
online fraud detection now there are
several ways that online fraud can take
place for example there's identity theft
where they steal your identity fake
accounts where these accounts only last
for how long the transaction takes place
and stop existing after that and man in
the- Middle attacks where they steal
your money while the transaction is
taking place the feed forward neural
network helps determine whether a
transaction is genuine or fraudulent so
what happens with feed forward neur
networks are that the outputs are
converted into hash values and these
values become the inputs for the next
round so for every real transaction that
takes place there's a specific pattern a
fraudulent transaction would stand out
because of the significant changes that
it would cause with the hash values
Stock Market trading machine learning is
used extensively when it comes to Stock
Market trading now you have stock market
indices like nikai they use long
short-term memory neural networks now
these are used to classify process and
predict data when there are time lags of
unknown size and duration now this is
used to predict stock market trends
assisted medical technology now medical
technology has been innovated with the
help of machine learning diagnosing
diseases has been easier from which we
can create 3D models that can predict
where exactly there are lesions in the
brain it works just as well for brain
tumors and I kemic stroke lesions they
can also be used in fetal Imaging and
cardiac analysis now some of the medical
fields that machine learning will help
assist in his disease identification
personalized treatment drug Discovery
clinical research and radiology and
finally we have automatic translation
now say you're in a foreign country and
you see Billboards and signs that you
don't understand that's where aut
automatic translation comes of help now
how does automatic translation actually
work the technology behind it is the
same as the sequence to sequence
learning which is the same thing that's
used with chatbots here the image
recognition happens using convolutional
neural networks and the text is
identified using optical character
recognition furthermore the sequence to
sequence algorithm is also used to
translate the text from one language to
the other finding a suitable job in the
field of machine learning is becoming
increasingly difficult the ideal way to
display your machine learning skill is
in the form of portfolio of data science
and machine learning projects a Sol
portfolio of projects will illustrate
that you can utilize those machine
learning skills in your profile as well
projects like movie reccommendation
system fake news detection and many more
are the best way to improve your early
programming skills you may have the
knowledge but putting it to the use what
is keep you competitive here are 10
machine learning projects that can
increase your portfolio and enable you
to acquire a job as a machine learning
engineer at number 10 we have loan
approval prediction system in this
machine learning project we will analyze
and make prediction about the loan
approval process of any person this is a
classification problem in which we must
determine whether or not the loan will
be approved a classification problem is
a predictive modeling problem that
predict a class label for a given
example of input data some
classification problem include spam
email cancer detection sentiment
analysis and many more you can check the
project link from the description box
below to understand classification
problem and how to build a loan
prediction system at number nine we have
fake news detection system do you
believe in everything you read in social
media isn't it true that not all news is
true but how will you as fake news ml is
the answer you will able to tell the
difference between real and fake news by
practicing this project of detecting
fake news this ml project for detecting
fake news is concerned with the fake
news and the true news on our data set
we create a tfid vectorizer with escalan
the model is then fitted using a passive
aggressive classifier that has been
initialized finally the accuracy score
and the confusion Matrix indicate how
well our model performs the link for the
project is in the description box below
at number eight we have personality
prediction system the idea is based on
determining an individual personality
using machine learning techniques a
person personality influences both his
personal and professional life nowadays
many company are shortlisting applicant
based on their personality which
increases job efficiency because the
person is working on what he is good at
rather than what is compelled to do in
our study we attempted to combine
personality prediction system using
machine learning techniques such as SVD
nabas and logistic regression to predict
a person personality and talent
prediction using phrase frequency method
this model or method allows users to
recognize their personality and
Technical abilities easily to learn
about Moree this project check the link
in the description box below at number
seven we have Parkinson disease system
Parkinson disease is a progressive
central nervous system element that
affects movement and cause tremors and
stiffness it comprises five stages and
affects more than 1 million worldwide
each other in this machine learning
project we will develop an svm model
using python modules psychic learn numpy
and pandas and svm we will import the
data extract the features and label and
scale the features split the data set
design an an model and calculate the
model accuracy and at the end we will
check the Parkinson disease for the
individual to learn about more this
project check the link in the
description box below at number six we
have text to speech converter
application the machine learning domain
of audio is undoubtly cutting as right
now the majority of the application
available today are the commercial the
community is building several audio
specific open source framework and
algorithm other text to speech apis are
available for this project we will
utilize pyttsx3 pyttsx3 is a python text
to speech conversion Library it operates
offline unlike other libraries and is
compatible with python 2 and Python 3
before API various pre-train models were
accessible in Python but changing the
voice of volume was often difficult it
also needed additional computational
power to learn more about this project
check the link in the description box
below at number five we have spe speech
recognition system speech recognition
often known as speech to text is the
capacity of a machine or program to
recognize and transfer word spoken
allowed into readable text MLS speech
recognition uses algorithm that model
speech in terms of both language and
sound to extract the more important
parts of the speech such as words
sentences and acostic modeling is used
to identify the phenomes and the
phonetics in the speech for this project
we will utilize pyttsx3 pyttsx3 is a
python text to speech conversion Library
it operates offline unlike other
libraries and is compatible with python
2 and Python 3 to learn more about this
project check the link in the
description box below at number four we
have sentiment analysis sentiment
analysis also known as opinion mining is
a straightforward process of determining
the author's feeling about a text what
was the user intention when he or she
wrote something to determine what could
be personal information we employ a
variety of natural language processing
and text analysis technology we must
detect extract and quantify such
information from the text to
classification and data manipulation in
this project we will use the Amazon
customer review data set for the
sentiment analysis check the link in the
description box below at number three we
have image classification using CNN deep
learning is a booving field currently
most projects and problem statement used
deep learning is and any sort of work
many of you like myself would choose a
conventional neural network as a deep
learning technique for answering any
computer vision problem statement in
this project we will use CNN to develop
an image processing project and learn
about its cap capabilities and why it
has become so popular we will go over
each stage of creating our CNN model and
our first spectacular project we will
use the CFI 10 data set for image
classification in this project to learn
more about this project check the link
in the description box below at number
two we have face recognition system
currently technology absolutely amazes
people with Incredible invention that
makes life easier and more comfortable
face recognition has shown to be the
least intrusive and fastest form of the
biometric verification over time this
project will use open CV and face
recognition libraries to create a phase
detection system open CV provides a
real-time computer vision tool library
and Hardware we can create amazing
realtime projects using open CV to learn
how to create face recognition system
for you check the link in the
description box below and last but not
the least we have movie recommendation
system almost everyone today use
technology to stream movies and
television show while figuring out what
to stream next can be disheartening
recommendation are often made based on a
viewer history and preferences this is
done through a machine learning and can
be a fun and the easy project for the
beginners new programmers can practice
by coding in either python or R and with
the data from the movie lens data set
generated by more than 6,000 users to
learn how to create movie recommendation
system for yourself or for your loved
ones check the project in the
description box below what are the
different types of machine learning
algorithms machine learning algorithms
are broadly classified into three types
the supervised learning unsupervised
learning and reinforcement learning
supervised learning in turn consists of
techniques like regression and
classification and unsupervised learning
we use techniques like Association and
clustering and reinforcement learning is
a recently developed technique and it is
very popular in gaming some of you must
have heard about alphago so this was
developed using reinforcement learning
primary difference between supervised
learning and unsupervised learning
supervised learning is used when we have
historical data and we have labeled data
which means that we know how the data is
classified so we know the classes if we
are doing classification or we know the
values when we are doing regression so
if we have historical data with these
values which are known as labels then we
use supervised learning in case of
unsupervised learning we do not have
past labeled data historical labeled
data so we use techniques like
Association and clustering to maybe form
clusters or new classes maybe and then
we move from there in case of
reinforcement learning the system learns
pretty much from scratch there is an
agent and there is an environment the
agent is given a certain Target and it
is rewarded when it is moving towards
that Target and it is penalized if it is
moving in a direction which is not
achieving that Target so it's more like
a carrot and stick model so what is the
difference between between these three
types of algorithms supervised
algorithms or supervised learning
algorithms are used when you have a
specific Target value that you would
like to predict the target could be
categorical having two or more possible
outcomes or classes if you will that is
what is classification or the target
could be a a value which can be measured
and that's where we use regression like
for example weather forecasting you want
to find the temperature whereas in
classification you want to find out
whether this is a fraud or not a fraud
or if it is email spam whether it is
Spam or not spam so that is a
classification example so if you know or
this is known as labeled information if
you have the labeled information then
you use supervised learning in case of
unsupervised learning we have input data
but we don't have the labels or what the
output is supposed to be so that is when
we use unsupervised learning techniques
like clustering and Association and we
try to analyze the data in case of
reinforcement learning it allows the
agent to automatically determine the
ideal Behavior within a specific context
and it has to do this to maximize the
performance like for example playing a
game so the agent is told that you need
to score the maximum score possible
without losing lives so that is a Target
that is given to the agent and it is
allowed to learn from scratch play the
game itself multiple times and slowly it
will learn the behavior which will
increase the score and keep the lives to
the maximum that's an example of
reinforcement learning when we look at
our different machine learning
algorithms we can divide them into three
areas supervised
unsupervised reinforcement we're only
going to look at supervised today
unsupervised means we don't have the
answers we're just grouping things
reinforcement is where we give positive
and negative Fe feedback to our
algorithm to program it and it doesn't
have the information till after the fact
but today we're just looking at
supervised because that's where linear
regression fits in in supervised data we
have our data already there and our
answers for a group and then we use that
to program our model and come up with an
answer the two most common uses for that
is through the regression and
classification now we're doing linear
regression so we're just going to focus
on the regression side and in the
regression we have SIMPLE linear
regression we have multiple linear
regression and we have polom linear
regression now on these three simple
linear regression is the examples we've
looked at so far where we have a lot of
data and we draw a straight line through
it multiple linear regression means we
have multiple variables remember where
we had the rainfall and the crops we
might add additional variables in there
like how much food do we give our crops
when do we Harvest them those would be
additional information add into our
model and that's why it' be multiple
linear regression and finally we have
polinomial linear regression that is
instead of drawing a line we can draw a
curved line through it now that you see
where regression model fits into the
machine learning algorithms and we're
specifically looking at linear
regression let's go ahead and take a
look at applications for linear
regression let's look at a few
applications of linear regression
economic growth used to determine the
economic growth of a country or a state
in the coming quarter can also be used
to predict the GDP of a country product
price can be used to predict what would
be the price of a product in the future
we can guess whether it's going to go up
or down or should I buy today housing
sales to estimate the number of houses a
builder would sell and what price in the
coming months score predictions Cricut
fever to predict the number of runs a
player would score in the coming matches
based on the previous performance I'm
sure you can figure out other
applications you could use linear
regression for so let's jump in and
let's understand linear regression and
dig into the Theory understanding linear
regression linear regression is the
statistical model used to predict the
relationship between independent and
dependent variables by examining two
factors the first important one is which
variables in particular are significant
predictors of the outcome variable and
the second one that we need to look at
closely is how significant is the
regression line to make predictions with
the highest possible accuracy if it's
inaccurate we can't use it so it's very
important we find out the most accurate
line we can get since linear regression
is based on drawing a line through data
we're going to jump back and take a look
at some ukian geometry the simplest form
of a simple linear regression equation
with one dependent and one independent
variable is represented by y = m * x + C
and if you look at our model here we
plotted two points on here uh X1 and y1
X2 and Y2 y being the the dependent
variable remember that from before and X
being the independent variable so y
depends on whatever X is m in this case
is the slope of the line where m equals
the difference in the Y 2 minus y1 and
X2 - X1 and finally we have C which is
the coefficient of the line or where
happens to cross the zero axis let's go
back and look at an example we used
earlier of linear regression we're going
to go back to plotting the amount of
crop yield based on the amount of
rainfall and here we have our rainfall
remember we cannot change rainfall and
we have our crop yield which is
dependent on the rainfall so we have our
independent and our dependent variables
we're going to take this and draw a line
through it as best we can through the
middle of the data and then we look at
that we put the red point on the y axis
is the amount of crop yield you can
expect for the amount of rainfall
represented by the Green Dot so if we
have an IDE idea what the rainfall is
for this year and what's going on we can
guess how good our crops are going to be
and we've created a nice line right
through the middle to give us a nice
mathematical formula let's take a look
and see what the math looks like behind
this let's look at the intuition behind
the regression line now before we dive
into the math and the formulas that go
behind this and what's going on behind
the
scenes I want you to note that when we
get into the case study and we actually
apply some python script that this math
that you're going to see here is already
done automatically for you you don't
have to have it memorized it is however
good to have an idea what's going on so
if people reference the different terms
you'll know what they're talking about
let's consider a sample data set with
five rows and find out how to draw the
regression line we're only going to do
five rows because if we did like the
rainfall with hundreds of points of data
that would be very hard to see what's
going on with the mathematics so we'll
go ahead and create our own two sets of
data and we have our independent
variable X and our dependent variable Y
and when X was 1 we got Y = 2 when X was
uh 2 y was 4 and so on and so on if we
go ahead and plot this data on a graph
we can see how it forms a nice line
through the middle you can see where
it's kind of grouped going upwards to
the right the next thing we want to know
is what the means is of each of the data
coming in the X and the Y the means
doesn't mean other than the average so
we add up all the numbers and divide by
the total so 1 + 2 + 3 + 4 + 5 over 5
equal 3 and the same for y we get four
if we go ahead and plot the means on the
graph we'll see we get 3 comma 4 which
draws a nice line down the middle a good
estimate here we're going to dig deeper
into the math behind the regression line
now remember before I said you don't
have to have all these formulas
memorized or fully understand them even
though we're we're going to go into a
little more detail of how it works and
if you're not a math whiz and you don't
know if you've never seen the sigma
character before which looks a little
bit like an e that's opened up that just
means summation that's all that is so
when you see the sigma character it just
means we're adding everything in that
row and for computers this is great
because as a programmer you can easily
iterate through each of the XY points
and create all the information you need
so in the top half you can see where
we've broken that down into pieces and
as it goes through the first two points
it computes the squared value of x the
squared value of y and x * Y and then it
takes all of X and adds them up all of Y
adds them up all of X squ adds them up
and so on and so on and you can see we
have the sum of equal to 15 the sum is
equal to 20 all the way up to x * Y
where the sum equals 66 this all comes
from our formula for calculating a
straight line where y equal the slope *
X plus the coefficient C so we go down
below and we're going to compute more
like the averages of these and we're
going to explain exactly what that is in
just a minute and where that information
comes from it's called the square means
error but we'll go into that in detail
in a few minutes all you need to do is
look at the formula and see how we've
gone about Computing it line by line
instead of trying to have a huge set of
numbers pushed into it and down here
you'll see where the slope m equals and
on the top part if you read through the
brackets you have the number of data
points Point time the sum of x * Y which
we computed one line at a time there and
that's just the 66 and take all that and
you subtract it from the sum of x times
the sum of Y and those have both been
computed so you have 15 * 20 and on the
bottom we have the number of lines times
the sum of X2 easily computed as 86 for
the sum minus I'll take all that and
subtract the sum of X2 and we end up as
we come across with our formula you can
plug in all those numbers which is very
easy to do on the computer you don't
have to do the math on a piece of paper
or calculator and you'll get a slope of
6 and you'll get your C coefficient if
you continue to follow through that
formula you'll see it comes out as equal
to 2.2 continuing deeper into what's
going behind the scenes let's find out
the predicted values of Y for
corresponding values of X using the
linear equation where M = 6 and C = 2.2
we're going to take these values and
we're going to go ahead and plot them
we're going to predict them so y = 6 *
or x = 1 + 2.2 = 2.8 so on and so on and
here the Blue Points represent the
actual y values and the brown points
represent the predicted y values based
on the model we created the distance
between the actual and predicted values
is known as residuals or errors the best
fit line should have the least sum of
squares of these errors also known as e
square if we put these into a nice chart
where you can see X and you can see Y
what we actual values were and you can
see y predicted you can easily see where
we take Yus y predicted and we get an
answer what is the difference between
those two and if we square that Yus y
prediction squared we can then sum those
squared values that's where we get the
64 plus the 36 + 1 all the way down
until we have a summation equals 2.4 so
the sum of squared errors for this Reg
ression line is 2.4 we check this error
for each line and conclude the best fit
line having the least e Square value in
a nice graphical representation we can
see here where we keep moving this line
through the data points to make sure the
best fit line has the least Square
distance between the data points and the
regression line now we only looked at
the most commonly used formula for
minimizing the distance there are lots
of ways to minimize a distance between
the line and the data points like sum of
square squ errors sum of absolute errors
root mean square error Etc what you want
to take away from this is whatever
formula is being used you can easily
using a computer programming and
iterating through the data calculate the
different parts of it that way these
complicated formulas you see with the
different summations and absolute values
are easily computed one piece at a time
up until this point we've only been
looking at two values X and Y well in
the real world it's very rare that you
only have two values when you're
figuring out a solution so let's move on
to the next topic multiple linear
regression let's take a brief look at
what happens when you have multiple
inputs so in multiple linear regression
we have uh well we'll start with the
simple linear regression where we had y
= m + x + C and we're trying to find the
value of y now with multiple linear
regression we have multiple variables
coming in so instead of having just X we
have X1 X2 X3 and instead of having just
one slope each variable has its own
slope attached to it as you can see here
we have M1 M2 M3 and we still just have
the single coefficient so when you're
dealing with multiple linear regression
you basically take your single linear
regression and you spread it out so you
have y = M1 * X1 + M2 * X2 so on all the
way to m to the nth x to the nth and
then you add your coefficient on there
implementation of linear regression now
we get into my favorite part let's
understand how multiple linear
regression works by implementing it in
Python if you remember before we were
looking at a company and just based on
its R&D trying to figure out its profit
we're going to start looking at the
expenditure of the company we're going
to go back to that we're going to
predict his profit but instead of
predicting it just on the R&D we're
going to look at other factors like
Administration cost marketing cost and
so on and from there we're going to see
if we can figure out what the profit of
that company's going to be to start our
coding we're going to begin by importing
some basic libraries and we're going to
be looking through the data before we do
any kind of linear regression we're
going to take a look at the data see
what we're playing with then we'll go
ahead and format the data to the format
we need to be able to run it in the
linear regression model and then from
there we'll go ahead and solve it and
just see how valid our solution is so
let's start with importing the basic
libraries now I'm going to be doing this
in Anaconda Jupiter notebook a very
popular IDE I enjoy it cuz it's such a
visual to look at and it's so easy to
use um just any ID for python will work
just fine for this so break out your
favorite python IDE so here we are in
our Jupiter notebook let me go ahead and
paste our first piece of code in there
and let's walk through what libraries
we're importing first we're going to
import numpy as NP and then I want you
to skip one line and look at import
pandas as PD these are very common tools
that you need with most of your linear
regression the numpy which stands for
number python is usually denoted as NP
and you have to almost have that for
your sklearn toolbox you always import
that right off the beginning pandas
although you don't have to have it for
your sklearn libraries it does such a
wonderful job of importing data setting
it up into a data frame so we can
manipulate it rather easily and it has a
lot of tools also in addition to that so
we usually like to use the pandas when
we can and I'll show you what that looks
like the other three lines are for us to
get a visual of this data and take a
look at it so we're going to import M PL
library. pyplot as PLT and then caborn
as SNS caborn works with the matplot
library so you have to always import
matplot library and then caborn sits on
top of it and we'll take a look at what
that looks like you could use any of
your own plotting libraries you want
there's all kinds of ways to look at the
data these are just very common ones and
the caborn is so easy to use it just
looks beautiful it's a nice
representation that you can actually
take and show somebody and the final
line is the Amber sign map plot Library
inline that is only because I'm doing an
inline IDE my interface in the Anaconda
Jupiter notebook requires I put that in
there or you're not going to see the
graph when it comes up let's go ahead
and run this it's not going to be that
interesting because we're just setting
up variables in fact it's not going to
do anything that we can see but it is
importing these different libraries and
setup the next step is load the data set
and extract independent and dependent
variables now here in the slide you'll
see companies equals p . read CSV and it
has a long line there with the file at
the end 1,000 companies. CSV you're
going to have to change this to fit
whatever setup you have and the file
itself you can request just go down to
the commentary below this video and put
a note in there and simply learn we'll
try to get in contact with you and
Supply you with that file so you can try
this coding yourself so we're going to
add this code in here and we're going to
see that I have companies equals pd.
reader CSV and I've changed this path to
match my computer c/s simplylearn
1000 companies. CSV and then below there
we're going to set the x equals to
companies under the iocation and because
this is companies is a PD data set I can
use this nice notation that says take
every row that's what the colon the
first colon is comma except for the last
column that's what the second part is
where we have a colon minus one and we
want the values set into there so X is
no longer a data set a pandas data set
but we can EAS extract the data from our
pandas data set with this notation and
then why we're going to set equal to the
last row well the question is going to
be what are we actually looking at so
let's go ahead and take a look at that
and we're going to look at the
companies. head which lists the first
five rows of data and I'll open up the
file in just a second so you can see
where that's coming from but let's look
at the data in here as far as the way
the panda sees it when I hit run you'll
see it breaks it out into a nice setup
this is what pandas one of the things
pandas is really good about is a looks
just like an Excel spreadsheet you have
your rows and remember when we're
programming we always start with zero we
don't start with one so it shows the
first five rows 0 1 2 3 4 and then it
shows your different columns R&D spend
Administration marketing spend State
profit it even notes that the top are
column names it was never told that but
pandas is able to recognize a lot of
things that they're not the same as the
data rows why don't we go ahead and open
this file up in a CSV so you can
actually see the raw data so here I've
opened it up as a text editor and you
can see at the top we have R&D spin
comma Administration comma marketing
spin comma State comma profit carriage
return I don't know about you but I'd go
crazy trying to read files like this
that's why we use the pandas you could
also open this up in an Excel and it
would separate it since it is a comma
separated variable file but we don't
want to look at this one we want to look
at something we can read rather easily
so let's flip back and take a look at
that top part the first five row now as
nice as this format is where I can see
the data to me it doesn't mean a whole
lot maybe you're an expert in business
and Investments and you understand what
$165,300 compared to the administration
cost of
$136,800 so on so on helps to create the
profit of1
19226 83 cents that makes no sense to me
whatsoever no pun intended so let's flip
back here and take a look at our next
set of code where we're going to graph
it so we can get a better understanding
of our data and what it means so at this
point we're going to use a single line
of code to get a lot of information so
we can see where we're going with this
let's go ahead and paste that into our
uh notebook and see what we got going
and so we have the visualization and
again we're using SNS which is pandas as
you can see we imported the map plot
library. pyplot as PLT which then the
caborn uses and we imported the caborn
as SNS and then that final line of code
helps us show this in our um inline
coding without this it wouldn't display
and you can display it to a file and
other means and that's the matap plot
library in line with the Amber sign at
the beginning so here we come down to
the single line of code caborn is great
because it actually recognizes the panda
data frame so I can just take the
companies. core for coordinates and I
can put that right into the Seaborn and
when we run this we get this beautiful
plot and let's just take a look at what
this plot means
if you look at this plot on mine the
colors are probably a little bit more
purplish and blue than the original one
uh we have the columns and the rows we
have R and D spending we have
Administration we have marketing
spending and profit and if you cross
index any two of these since we're
interested in profit if you cross index
profit with profit it's going to show up
if you look at the scale on the right
way up in the dark why because those are
the same data they have an exact
correspondence so R&D spending is going
to be the same as R&D spending and the
same thing with Administration cost so
right down the middle you get this dark
row or dark um diagonal row that shows
that this is the highest corresponding
data that's exactly the same and as it
becomes lighter there's less connections
between the data so we can see with
profit obviously profit is the same as
profit and next it has a very high
correlation with R&D spending which we
looked at earlier and it has a slightly
less connection to marketing spending
and even less to how much money we put
into the administration so now that we
have a nice look at the data let's go
ahead and dig in and create some actual
useful linear regression model so that
we can predict values and have a better
profit now that we've taken a look at
the visualization of this data we're
going to move on to the next step
instead of just having a pretty picture
we need to generate some hard data some
hard values so let's see what that looks
like we're going to set up our linear
regression model in two steps the first
one is we need need to prepare some of
our data so it fits correctly and let's
go ahead and paste this code into our
Jupiter notebook and what we're bringing
in is we're going to bring in the
sklearn preprocessing where we're going
to import the label encoder and the one
hot encoder to use the label encoder
we're going to create a variable called
label encoder and set it equal to
capital L label capital E encoder this
creates a class that we can reuse for
transferring the labels back and forth
now about now you should ask what labels
are we talking about let's go take a
look at the data we processed before and
see what I'm talking about here if you
remember when we did the companies. head
and we printed the top five rows of data
we have our columns going across we have
column zero which is R&D spending column
one which is Administration column two
which is marketing spending and column
three is State and you'll see under
State we have New York California
Florida now to do a linear regression
model it doesn't know how to process New
York it knows how to process a number so
the first thing we're going to do is
we're going to change that New York
California and Florida and we're going
to change those to numbers that's what
this line of code does here x equals and
then it has the colon comma 3 in
Brackets the first part the colon comma
means that we're going to look at all
the different rows so we're going to
keep them all together but the only row
we're going to edit is the third row and
in there we're going to take the label
coder and we're going to fit and
transform the X also the third row so
we're going to take that third row we're
going to set it equal to a
transformation and that transformation
basically tells it that instead of
having a uh New York it has a zero or
one or a two and then finally we need to
do a one hot encoder which equals one
hot encoder categorical features equals
three and then we take the X and we go
ahead and do that equal to one hot
encoder fit transform X to array this
final transformation preps our data for
us so it's completely set the way we
need it as just a row of numbers even
though it's not in here let's go ahead
and print X and just take a look what
this data is doing you'll see you have
an array of arrays and then each array
is a row of numbers and if I go ahead
and just do row zero you'll see I have a
nice organized row of numbers that the
computer now understands we'll go ahead
and take this out there because it
doesn't mean a whole lot to us it's just
a row of numbers next on setting up our
data we have avoiding dummy variable
trap this is very important why because
the computer has automatically
transformed our header into the setup
and it's automatically transformed all
these different variables so when we did
the encoder the encoder created two
columns and what we need to do is just
have the one because it has both the
variable and the name that's what this
piece of code does here let's go ahead
and paste this in here and we have x = x
colon comma 1 colon all this is doing is
removing that one extra column we put in
there when we did our one hot encoder
and our label encoding let's go ahead
and run that and and now we get to
create our linear regression model and
let's see what that looks like here and
we're going to do that in two steps the
first step is going to be in splitting
the data now whenever we create a uh
predictive model of data we always want
to split it up so we have a training set
and we have a testing set that's very
important otherwise we'd be very
unethical without testing it to see how
good our fit is and then we'll go ahead
and create our multiple linear
regression model and train it and set it
up let's go ahead and paste this next
piece of code in here and I'll go ahead
and shrink it down a size or two so it
all fits on one line so from the sklearn
module selection we're going to import
train test split and you'll see that
we've created four completely different
variables we have capital x train
capital X test smaller case y train
smaller case y test that is the standard
way that they usually references when
we're doing different uh models usually
see that a capital x and you see the
train and the test and the lowercase Y
what this is is X is our data going in
that's our R&D spin our Administration
our marketing and then Y which we're
training is the answer that's the profit
because we want to know the profit of an
unknown entity that's what we're going
to shoot for in this tutorial the next
part train test split we take X and we
take y we've already created those X has
the columns with the data in it and Y
has a column with profit in it and then
we're going to set the test size equ ALS
.2 that basically means 20% So 20% of
the rows are going to be tested we're
going to put them off to the side so
since we're using a th000 lines of data
that means that 200 of those lines we're
going to hold off to the side to test
for later and then the random State
equals zero we're going to randomize
which ones it picks to hold off to the
side we'll go ahead and run this it's
not overly exciting it's setting up our
variables but the next step is the next
step we actually create our linear
regression model now that we got to the
linear regression model we get that next
piece of the puzzle let's go ahe and put
that code in there and walk through it
so here we go we're going to paste it in
there and let's go ahead and since this
is a shorter line of code let's zoom up
there so we can get a good luck and we
have from the SK learn. linear model
we're going to import linear regression
now I don't know if you recall from
earlier when we were doing all the math
let's go ahead and flip back there and
take a look at that do you remember this
where we had this long formula on the
bottom and we were doing all this suiz
and then we also looked at setting it up
with the different lines and then we
also looked all the way down to multiple
linear regression where we're adding all
those formulas together all of that is
wrapped up in this one section so what's
going on here is I'm going to create a
variable called regressor and the
regressor equals the linear regression
that's a linear regression model that
has all that math built in so we don't
have to have it all memorized or have to
compute it individually and then we do
the regressor do fet in this case we do
X train and Y train cuz using the
training data X being the data in and Y
being profit what we're looking at and
this does all that math for us so within
one click and one line we've created the
whole linear regression model and we fit
the data to the linear regression model
and you can see that when I run the
regressor it gives an output linear
regression it says copy x equals True
Fit intercept equals true in jobs equal
one normalize equals false it's just
giving you some general information on
what's going on with that regressor
model now that we've created our linear
regression model let's go ahead and use
it and if you remember we kept a bunch
of data aside so we're going to do a y
predict variable and we're going to put
in the X test and let's see what that
looks like scroll up a little bit paste
that in here predicting the test set
results so here we have y predict equals
regressor do predict X test going in and
this gives us y predict now because I'm
in Jupiter in line I can just put the
variable up there and when I hit the Run
button it'll print that array out I
could have just as easily done print y
predict so if you're in a different IDE
that's not an inline setup like the
Jupiter notebook you can do it this way
print y predict and you'll see that for
the 200 different test variables we kept
off to the side it's going to produce
200 answers this is what it says the
profit are for those 200 predictions but
let's don't stop there let's keep going
and take a couple look we're going to
take just a short detail here and
calculating the coefficients and the
intercept
this gives us a quick flash at what's
going on behind the line we're going to
take a short detour here and we're going
to be calculating the coefficient and
intercepts so you can see what those
look like what's really nice about our
regressor we created is it already has a
coefficients for us and we can simply
just print regressor do coefficient
uncore when I run this you'll see our
coefficients here and if we can do the
regressor coefficient we can also do The
regressor Intercept let's run that and
take a look at at that this all came
from the multiple regression model and
we'll flip over so you can remember
where this is going into and where it's
coming from you can see the formula down
here where y = M1 * X1 + M2 * X2 and so
on and so on plus C the coefficient so
these variables fit right into this
formula y = slope 1 * column 1 variable
plus slope 2 * column 2 variable all the
way to the m into the n and x to the N
plus C the coefficient or in this case
you you have -
8.89 to the^ of 2 etc etc times the
First Column and the second column and
the third column and then our intercept
is the minus
10309 Point boy it gets kind of
complicated when you look at it this is
why we don't do this by hand anymore
this is why we have the computer to make
these calculations easy to understand
and calculate now I told you that was a
short detour and we're coming towards
the end of our script as you remember
from the beginning I said if we're going
to divide this information we have to
make sure sure it's a valid model that
this model works and understand how good
it works so calculating the R squ value
that's what we're going to use to
predict how good our prediction is and
let's take a look at what that looks
like in code and so we're going to use
this from SK learn. metrix we're going
to import R2 score that's the R squar
value we're looking at the error so in
the R2 score we take our y test versus
our y predict y test is the actual
values we're testing that was the one
that was given to us that we know are
true the Y predict of those 200 values
is what we think it was true and when we
go ahead and run this we see we get a0
9352 that's the R2 score now it's not
exactly a straight percentage so it's
not saying it's 93% correct but you do
want that in the upper 90s oh and higher
shows that this is a very valid
prediction based on the R2 score and if
R squ value of. 91 or 92 as we got on
our model remember it does have a random
generation involved this proves the
model is a good model which means
success yay we successfully trained our
model with certain predictors and
estimated the profit of the companies
using linear regression so now that we
have a successful linear regression
model if you are an aspiring machine
learning engineer looking for online
training and certifications from
prestigious universities and in
collaboration with leading experts then
search no more simply learns
professional certification program in Ai
and machine learning from Pur University
in collaboration with IVM should be your
right choice for more details use the
link in the description box below with
that in mind all right what is logistic
regression as I mentioned earlier
logistic regression is an algorithm for
performing binary classification so
let's take an example and see how this
works let's say your car has not been
serviced for quite a few years and now
you want to find out if the is going to
break down in the near future so so this
is like a classification problem find
out whether your car will break down or
not so how are we going to perform this
classification so here's how it looks if
we plot the information along the X and
Y AIS X is the number of years since the
last service was performed and why is
the probability of your car breaking
down and let's say this information was
this data rather was collected from
several car users it's not just your car
but several car users so that is our
labeled data so the data has been
collected and um for for the number of
years and when the car broke down and
what was the probability and that has
been plotted along X and Y axis so this
provides an idea or from this graph we
can find out whether your car will break
down or not we'll see how so first of
all the probability can go from 0 to one
as you aware probability can be between
zero and one and as we can imagine it is
intuitive as well as the number of years
are on the Lower Side maybe one year 2
years or 3 years till after the service
the chances of your car breaking down
are very limited right so for example
chances of your car breaking down or the
probability of your car breaking down
within two years of your last service
are 0.1 probability similarly 3 years is
maybe3 and so on but as the number of
years increases let's say if it was six
or S years there is almost a certainty
that your car is going to break down
that is what this graph shows so this is
an example of a application of the
classification algorithm and we will see
in little details how exactly logistic
regression is applied here one more
thing needs to be added here is that the
dependent variables outcome is discrete
so if we are talking about whether the
car is going to break down or not so
that is a discrete value the Y that we
are talking about the dependent variable
that we are talking about what we are
looking at is whether the car is going
to break down or not yes or no that is
what we are talking about so here the
outcome is discrete and not a continuous
value so this is how the logistic
regression curve looks let me explain a
little bit what exactly and how exactly
we are going to uh determine the class
at the outcome rather so for a logistic
regression curve a threshold has to be
set saying that because this is a
probability calculation remember this is
a probability calculation and the
probability itself will not be zero or 1
but based on the probability we need to
decide what the outcome should be so
there has to be a threshold like for
example 05 can be the threshold let's
say in this case so any value of the
probability below ow .5 is considered to
be zero and any value above 0.5 is
considered to be one so an output of
let's
say8 will mean that the car will break
down so that is considered as an output
of one and let's say an output of. 29 is
considered as zero which means that the
car will not break down so that's the
way logistic regression works now let's
do a quick comparison between logistic
regression and linear regression because
they both have the term regression in
them so it can cause confusion so let's
try to remove that confusion so what is
linear regression linear regression is a
process is once again an algorithm for
supervised learning however here you're
going to find a continuous value you're
going to determine a continuous value it
could be the price of a real estate
property it could be your hike how much
hike you're going to get or it could be
a stock price these are all continuous
Valu Val these are not discrete compared
to a yes or a no kind of a response that
we are looking for in logistic
regression so this is one example of a
linear regression let's say the HR team
of a company tries to find out what
should be the salary hike of an employee
so they collect all the details of their
existing employees their ratings and
their salary hikes what has been given
and that is the labeled information that
is available and the system learns from
this it is trained and it learns from
this labeled information so that when a
new employees information is fed based
on the rating it will determine what
should be the height so this is a linear
regression problem and a linear
regression example now salary is a
continuous value you can get 5,000
5,500 5,600 it is not discrete like a
cat or a dog or an apple or a banana
these are discrete or a yes or a no
these are discrete values right so so
this where you are trying to find
continuous values is where we use linear
regression so let's say just to extend
on that scenario we now want to find out
whether this employee is going to get a
promotion or not so we want to find out
that is a discrete problem right a yes
or no kind of a problem in this case we
actually cannot use linear regression
even though we may have labeled data so
this is the labeled data So based on the
employee rating these are the ratings
and then some people got the promotion
and this is the ratings for which people
did not get promotion that is a no and
this is the rating for which people got
promotion we just plotted the data about
whether a person has got an employee has
got promotion or not yes no right so
there is nothing in between and what is
the employees rating okay and ratings
can be continuous that is not an issue
but the output is discrete in this case
whether employee got promotion yes no
okay so if we try to plot that and we
try to find a straight line this is how
it would look and as you can see doesn't
look very right because looks like there
will be lot of Errors th root mean
square error if you remember for linear
regression would be very very high and
also the the values cannot go beyond
zero or Beyond one so the graph should
probably look somewhat like this clipped
at 0er and one but still this straight
line doesn't look right therefore
instead of using a linear equation we
need to come up with something different
and therefore the logistic regression
model looks somewhat like this so we
calculate the probability and if we plot
that probability not in the form of a
straight line but we need to use some
other equation we will see very soon
what that equation is then it is a
gradual process right so you see here
people with some of these ratings are
not get getting any promotions and then
slowly uh at certain rating they get
promotion so that is a gradual process
and uh this is how the math behind
logistic regression looks so we are
trying to find the odds for a particular
event happening and this is the formula
for finding the odd so the probability
of an event happening divided by the
probability of the event not happening
so P if it is the probability of the
event happening probability of the
person getting a promotion and divided
by the probability of the person not
getting a promotion that is 1 minus
P so this is how you measure the odds
now the values of the odds range from 0
to Infinity so when this probability is
zero then the odds will the value of the
odds is equal to zero and when the
probability becomes 1 then the value of
the odds is 1 by 0 that will be Infinity
but the probab ility itself remains
between 0 and 1 now this is how an
equation of a straight line Looks So Y
is equal to Beta 0 plus beta 1 x where
beta 0 is the Y intercept and beta 1 is
the slope of the line if we take the
odds equation and take a log of both
sides then this would look somewhat like
this and the term logistic is actually
derived from the fact that we are doing
this we take a log of PX by 1 - PX this
is an extension of the calculation of
odds that we have seen right and that is
equal to Beta 0 plus beta 1 x which is
the equation of the straight line and
now from here if you want to find out
the value of PX we will see we can take
the exponential on both sides and then
if we solve that equation we will get
the equation of PX like this PX is equal
to 1 by 1 + e^ of - beta 0 + beta 1 x
and recall this is is nothing but the
equation of the line which is equal to y
y is equal to Beta 0 + beta 1 x so that
this is the equation also known as the
sigmoid function and this is the
equation of the logistic regression Al
all right and if this is plotted this is
how the sigmoid curve is obtained so
let's compare linear and logistic
regression how they are different from
each other let's go back so linear
regression is solved or used to solve
regression problems and logistic
regression is used to solve
classification problems so both are
called regression but linear regression
is used for solving regression problems
where we predict continuous values
whereas logistic regression is used for
solving classification problems where we
have had to predict discrete values the
response variables in case of linear
regression are continuous in nature
whereas here they are categorical or
discrete in nature and uh the linear
regression helps to estimate the
dependent variable when there is a
change in the independent variable
whereas here in case of logistic
regression it helps to calculate the
probability or the possibility of a
particular event happening and linear
regression as the name suggest is a
straight line that's why it's called
linear regression whereas logistic
regression is a sigmoid function and the
curve is the shape of the curve is s
it's an s-shaped curve this is another
example of application of logistic
regression in weather prediction whether
it's going to rain or not rain now keep
in mind both are used in weather
prediction if we want to find the
discrete values like whether it's going
to rain or not rain that is a
classification problem we use logistic
regression but if we want to determine
what is going to be the temperature
tomorrow then we use linear regression
so just keep in mind that in weather
prediction we actually use both but
these are some examples of logistic
regression so we want to find out
whether it's going to be rain or not
it's going to be sunny or not whe it's
going to snow or not these are all
logistic regression examples a few more
examples classification of objects this
is a again another example of logistic
regression now here of course one
distinction is that these are multiclass
classification so logistic regression is
not used in its original form but it is
used in a slightly different form so we
say whether it is is a dog or not a dog
I hope you understand so instead of
saying is it a dog or a cat or elephant
we convert this into saying so because
we need to keep it to Binary
classification so we say is it a dog or
not a dog is it a cat or not a cat so
that's the way logistic regression can
be used for classifying objects
otherwise there are other techniques
which can be used for performing
multiclass classification in healthcare
logistic regression is used to find the
survival rate of a patient so they take
multiple parameters like trauma score
and age and so on and so forth and they
try to predict the rate of survival all
right now finally let's take an example
and see how we can apply logistic
regression to predict the number that is
shown in the image so this is actually a
live demo I will take you into Jupiter
notebook and um show the code but before
that let me take you through a couple of
slides to explain what we're trying to
do so let's say you have an 8x8 image
and the the image has a number 1 2 3 4
and you need to train your model to
predict what this number is so how do we
do this so the first thing is obviously
in any machine learning process you
train your model so in this case we are
using logistic regression so and then we
provide a training set to train the
model and then we test how accurate our
model is with the test data which means
that like any machine learning process
we split our initial data into two parts
training set and test set with the
training set we train our model and then
with the test set we we test the model
till we get good accuracy and then we
use it for for inference right so that
is typical methodology of uh uh training
testing and then deploying of machine
learning models so let's uh take a look
at the code and uh see what we are doing
so I'll not go line by line but just
take you through some of the blocks so
first thing we do is import all the
libraries and then we basically take a
look at the images and see what is the
total number of images we can display
using mat plot lip some of the images or
a sample of these images and um then we
split the data into training and test as
I mentioned earlier and we can do some
exploratory analysis and uh then we
build our model we train our model with
the training set and then we we test it
with our test set and find out how
accurate our model is using the
confusion Matrix the heat map and use
heat map for visualizing this and I will
show you in the code what exactly is the
confusion Matrix and how it can be used
for finding the accuracy in our example
we got we get an accuracy of about .94
which is pretty good or 94% which is
pretty good all right so what is the
confusion Matrix this is an example of a
confusion Matrix and uh this is used for
identifying the accuracy of a uh
classification model or like a logistic
regression model so the most important
part in a confusion Matrix is that first
of all this as you can see this is a
matrix and the size of the Matrix
depends on how many outputs uh we are
expecting right so the the most
important part here is that the model
will be most accurate when we have the
maximum numbers in its diagonal like in
this case that's why it has almost 93
94% because the diagonal should have the
maximum numbers and the others other
than diagonals the cells other than the
diagonals should have very few numbers
so here that's what is happening so
there is a two here there are there's a
one here but most of them are along the
diagonal this what does this mean this
means that the number that has been fed
is zero and the number that has been
detected is also zero so the predicted
value and the actual value are the same
so along the diagonals that is true
which means that let's let's take this
diagonal right if if the maximum number
is here that means that like here in
this case it is 34 which means that 34
of the images that have been fed or
rather actually there are two
misclassifications in there so 36 images
have been fed which have number four and
out of which 34 have been predicted
correctly as number four and one has
been predicted as number eight and
another one has been predicted as number
nine so these are two
misclassifications okay so that is the
meaning of saying that the maximum
number should be in the diagonal so if
you have all of them so for an ideal
model which has let's say 100% accuracy
everything will be only in the diagonal
there will be no numbers other than zero
in all other cells so that is like a
100% accurate model okay so that's uh
gist of how to use this Matrix how to
use this confusion Matrix I know the
name uh is a little funny sounding
confusion Matrix but actually it is not
very confusing it's very straightforward
so you are just plotting what has been
predicted and what is the labeled
information or what is the actual data
that's also known as the ground truth
sometimes okay these are some fancy
terms that are used so predicted label
and the actual lab that's all letter is
okay yeah so we are showing a little bit
more information here so 38 have been
predicted and here you will see that all
of them have been predicted correctly
there have been 38 zeros and the the
predicted value and the actual value is
is exactly the same whereas in this case
right it has there are I think 37 + 5
yeah 42 have been fed the images 42
images are of Digit three and uh the
accuracy is only 37 of them have been
accurated predicted three of them have
been predicted as number seven and two
of them have been predicted as number
eight and so on and so forth okay all
right so with that let's go into Jupiter
notebook and see how the code looks so
this is the code in in Jupiter notebook
for logistic regression in this
particular demo what we are going to do
is train our model to recognize digits
which are the images which have digits
from let's say 0 to 5 or 0 to 9 and um
and then we will see how well it is
trained and whether it is able to
predict these numbers correctly or not
so let's get started so the first part
is as usual we are importing some
libraries that are required and uh then
the last line in this block is to load
the digits so let's go ahead and run
this code then here we will visualize
the shape of these uh digits so we can
see here if we take a look this is how
the shape is
1797 by 64 these are like 8 by 8 images
so that's that's what is reflected in
this shape now from here onwards we are
basically once again importing some of
the libraries that are required like
numpy and matplot and we will take a
look at uh some of the sample images
that we have loaded so this one for
example create creates a figure uh and
then we go ahead and take a few sample
images to see how they look so let me
run this code and so that it becomes
easy to understand so these are about
five images sample images that we are
looking at 0 1 2 3 4 so this is how the
images this is how the data is okay and
uh based on this we will actually train
our logistic regression model and then
we will test it and see how well it is
able to recognize so the way it works is
the pixel information so as you can see
here this is an 8 by 8 pixel kind of a
image and uh the each pixel whether it
is activated or not activated that is
the information available for each pixel
now based on the pattern of this
activation and non-activation of the
various pixels this will be identified
as a zero for example right similarly as
you can see so overall each of these
numbers actually has a different pattern
of the pixel activation and that's
pretty much that our model needs to
learn for which number what is the
pattern of the activation of the pixels
right so that is what we are going to
train our model okay so the first thing
we need to do is to split our data into
training and test data set right so
whenever we perform any training we
split the data into to training and test
so that the training data set is used to
train the system so we pass this
probably multiple times uh and then we
test it with the test data set and the
split is usually in the form of there
and there are various ways in which you
can split this data it is up to the
individual preferences in our case here
we are splitting in the form of 23 and
77 so when we say test size as
2023 that means 23 % of the entire data
is used for testing and the remaining
77% is used for training so there is a
readily available function which is uh
called train test split so we don't have
to write any special code for the
splitting it will automatically split
the data based on the proportion that we
give here which is test size so we just
give the test size automatically
training size will be determined and uh
we pass the data that we want to split
and the the results will be stored in
xcore train and Yore train for the
training data set and what is xcore
train this are these are the features
right which is like the independent
variable and Yore train is the label
right so in this case what happens is we
have the input value which is or the
features value which is in xcore train
and since this is labeled data for each
of them each of the observations we
already have the label information
saying whether this digit is a zero or a
one or a two so that this this is what
will be used for comparison to find out
whether the the system is able to
recognize it correctly or there is an
error for each observation it will
compare with this right so this is the
label so the same way xcore train Yore
train is for the training data set xcore
test Yore test is for the test data set
okay so let me go ahead and execute this
code as well and then we can go and
check quickly what is the how many
entries are there and in each of this so
xcore train the shape is
1383 by 64 and Yore train has 1383
because there is uh nothing like the
second part is not required here and
then xcore test shape VC is 414 so
actually there are 414 observations in
test and 30 1383 observations in train
so that's basically what these four
lines of code are are saying okay then
we import the uh logistic regression
library and uh which is a part of psyit
learn so we we don't have to implement
the logistic regression process itself
we just call these the function and uh
let me go ahead and execute that so that
uh we have the logistic regression
Library imported now we create an
instance of logistic regression right so
logistic RR is a is an instance of
logistic regression and then we use that
for training our model so let me first
execute this code so these two lines so
the first line basically creates an
instance of logistic regression model
and then the second line where is where
we are passing our data the training
data set right this is our the the
predictors and uh this is our Target we
are passing this data set to train our
model all right so once we do this in
this case the data is not large but by
and large uh the training is what takes
usually a lot of time so we spend in
machine learning activities in machine
learning projects we spend a lot of time
for the training part of it okay so here
the data set is relatively small so it
was pretty quick so all right so now our
model has been trained using the
training data set and uh we want to see
how accurate this is so what we'll do is
we will test it out in probably faces so
let me first try out how well this is
working for uh one image okay I will
just try it out with one image my the
first entry in my test data set and see
whether it is uh correctly predicting or
not so and in order to test it so for
training purpose we use the fit method
there is a method called fit which is
for training the model and once the
training is done if you want to test for
for a particular value new input you use
the predict method okay so let's run the
predict method and we pass this
particular image and uh we see that the
shape is or the prediction is four so
let's try a few more let me see for the
next 10 uh seems to be fine so let me
just go ahead and test the entire data
set okay that's basically what we will
do so now we want to find out how
accurately this has um performed so we
use the score method to find what is the
percentage of accuracy and we see here
that it has performed up to 94% Accurate
okay so that's uh on this part now what
we can also do is we can um also see
this accuracy using what is known as
confusion Matrix so let us go ahead and
try that as well uh so that we can also
visualize how well uh this model has uh
done so let me execute this piece of
code which will basically import some of
the libraries that are required and um
we we basically create a confusion
Matrix an instance of confusion matrix
by running confusion Matrix and passing
these uh values so we have so this
confusion undor Matrix method takes two
parameters one is the Yore test and the
other is the prediction so what is the
ycore test these are the labeled values
which we already know for the test data
set and predictions are what the system
has predicted for the test data set okay
so this is known to us and this is what
the system has uh the model has
generated so we kind of create the
confusion Matrix and we will print it
and this is how the confusion Matrix
looks as the name suggests it is a
matrix and um the key point out here is
that the accuracy of the model is
determined by how many numbers are there
in the diagonal the more the numbers in
the diagonal the better the accuracy is
okay and first of all the total sum of
all the numbers in this whole Matrix is
equal to the number of observations in
the test data set that is the first
thing right so if you add up all these
numbers that will be equal to the number
of observations in the test data set and
then out of that the maximum number of
them should be in the diagonal that
means the accuracy is predic good if the
the numbers in the diagonal are less and
in all other places there are lot of
numbers uh which means the accuracy is
very low the diagonal indicates a
correct prediction that this means that
the actual value is same as the
predicted value here again actual value
is same as the predicted value and so on
right so the moment you see a number
here that means the actual value is
something and the predicted value is
something else right similarly here the
actual value is something and the
predicted value is something else so
that is basically how we read the
confusion Matrix now how do we find the
accuracy you can actually add up the
total values in the diagonal so it's
like 38 + 44 Plus 43 and so on and
divide that by the total number of test
observations that will give you the
percentage accuracy using a confusion
Matrix now let us visualize this
confusion mat Matrix in a slightly more
sophisticated way uh using a heat map so
we will create a heat map with some
We'll add some colors as well it's uh
it's like a more visually visually more
appealing so that's the whole idea so if
we let me run this piece of code and
this is how the heat map looks uh and as
you can see here the diagonals again are
or all the values are here most of the
values so which means reasonably this
seems to be reasonably accurate accurate
and yeah basically the accuracy score is
94% this is calculated as I mentioned by
adding all these numbers divided by the
total test values or the total number of
observations in test data set okay so
this is the confusion Matrix for
logistic
regression all right so now that we have
seen the confusion Matrix let's take a
quick sample and see how well uh the
system has classified and we will take a
few examples of the data so if we see
here we we picked up randomly a few of
them so this is uh number four which is
the actual value and also the predicted
value both are four this is an image of
zero so the predicted value is also zero
actual value is of course zero then this
is the image of nine so this is also
being predicted correctly nine and
actual value is N9 and this is a image
of one and again this has been predicted
correctly as like the actual value okay
so this was a quick demo of logistic
regression how to use logistic
regression to identify images if you are
an aspiring machine learning engineer
looking for online training and
certifications from prestigious
universities and in collaboration with
leading experts then search no more
simply lar professional certification
program in Ai and machine learning from
P University in collaboration with IBM
should be your right choice for for more
details use the link in the description
box below with that in mind there's all
kinds of regression models that come out
of this so when we put them aside TOS
side we have our linear regression which
is a predictive number used to predict a
dependent output variable based on
Independent input variable accuracy is a
measured uh using least squares
estimation so that's where you take uh
you could also use absolute value uh the
least squares is more popular there's
reasons for that mathematically and also
for computer
runtime uh but it does give you an an
accuracy based on the the least Square
estimation the best fit line is a
straight line and clearly that's not
always used in all the regression models
there's a lot of variations on that the
output is a predicted integer value
again this is what we're talking about
we're talking about linear regression
and we're talking about regression it
means the numers coming out linear
usually means were looking for that line
versus a different model and it's used
in business domain forecasting stocks uh
it's used as a basis of of most um uh
predictions with numbers so if you're
looking at a lot of numbers you're
probably looking at a a linear
regression
model uh for instance if you do just the
high lows of the stock exchange and
you're you're going to take a lot more
of that if you want to make money off
the stock you'll find that the linear
regress ression model fits uh probably
better than almost any of the other
models even you know highend neural
networks and all these other different
machine learning and AI models because
they're numbers they're just a straight
set of numbers you have a high value low
value volume uh that kind of thing so
when you're looking at something that
straight numbers um and are connected in
that way usually you're talking about a
linear regression model and that's where
you want to start a listic regression
model used to classify dependent output
variable based on Independent input
variable so just like the linear
regression model and like all of our
machine learning tools you have your
features coming in uh and so in this
case you might have uh label you know an
image or something like that is is
probably the very popular thing right
now labeling broccoli and vegetables or
whatever accuracy is measured using
maximum likelihood estimation the best
fit is given by a curve and we saw that
um we're talking about linear regression
you definitely are talking about a
straight line although there is other
regression models that don't use
straight lines and usually when you're
looking at a logistic regression the
math as you saw was still kind of a
ukian line but it's now got that sigmoid
activation which turns it into um a
heavily weighted curve and the output is
a binary value between zero and one and
it's used for classification image
processing as I mentioned is is what
people usually think of um although they
use it for classific ation of um like a
window of things so you could take a
window of stock history and you could CL
generate classifications based on that
and separate the data that way if it's
going to be that this particular pattern
occurs it's going to be upward trending
or downward
trending in fact a number of stock uh uh
Traders use that not to tell them how
much to bid or what to bid uh but they
use it as to whether it's worth looking
at the stock or not whether the Stock's
going to go down or go up and it's just
a 01 do I care or do I even want to look
at it so let's do a demo so you can get
a picture of what this looks like in
Python code let's predict the price at
which insurance should be sold to a
particular customer based on their
medical history we will also classify on
a mushroom data set to find the
poisonous and non-poisonous
mushrooms and when you look at these two
datas the first one uh we're looking at
the price so the price price is a number
um so let's predict the price which the
insurance should be sold to and the
second one is we're looking at either
it's poisonous or it's not poisonous so
first off before we begin the demo I'm
in the Anaconda Navigator in this one
I've loaded the python
3.6 and using the Jupiter notebook and
you can use jupyter notebook by itself
um you can use the jupyter lab which
allows multiple tabs it's basically the
notebook with tabs on it but the Jupiter
notebook is just fine and it'll go into
uh Google Chrome which is what I'm using
for my Internet Explorer and from here
we open up new and you'll see Python 3
and again this is loaded with python
3.6 and we're doing the linear versus
logic uh regression or logit you'll see
L git T um is one of the one of the
names that kind of pops up when you do a
search on here uh but it is a logic
we're looking at the logistic regression
models
and we'll start with the linear
regression uh because it's easy to
understand you draw a line through stuff
um and so in programming uh we got a lot
of stuff to unfold here in our in our uh
startup as we pre-ad all of our
different
parts and let's go ahead and break this
up we have at the beginning import uh
pandas so this is our data frame uh it's
just a way of storing the data think of
a uh when you talk about data frame
think of a spreadsheet you rows and
columns it's a nice way of viewing the
data and then we have uh we're going to
be bringing in our pre-processing label
en Co coder I'll show you what that is
um when we get down to it it's easier to
see in the data uh but there's some data
in here like um sex it's male or female
so it's not like an actual number it's
either your one or the other that kind
of stuff ends up being encoded that's
what this label encoder is right here we
have our test split
model if you're going to build a model
uh you do not want to use all the data
you want to use some of the data and
then test it to see how good it is and
if it can't have seen the data you're
testing on until you're ready to test it
on there and see how good it is and then
we have our uh logistic regression model
our categorical one and then we have our
linear regression model these are the
two these right here let me just um
um clear all that there we go uh these
two right here are what this is all
about logistic versus uh linear is it
categorical are we looking for a true
false or are we looking for um a
specific
number and then finally um usually at
the very end we have to take and just
ask how accurate is our model did it
work um if you're trying to predict
something in this case we're going to be
doing um uh Insurance costs uh how close
to the insurance cost does it measure
that we expect it to be you know if
you're an insurance company you don't
want to promise to pay everybody's
medical bill and not be able
to and in the case of the mushrooms you
probably want to know just how much at
risk you are for following this model uh
as to far as whether you're going to get
eat a poisonous mushroom and die or
not um so we'll look at both of those
and we'll get talk a little bit more
about the short comings and the um uh
value of these different processes so
let's go ahead and run this this is
loaded the data set on here and then
because we're in Jupiter notebook I
don't have to put the print on there we
just do data set and by and it prints
out all the different data on here and
you can see here for our insurance
because that's what we're starting with
uh we're loading that with our pandas
and it prints it in a nice format where
you can see the age sex uh body mass
index number of children smoker so this
might be something that the insurance
company gets from the doctor it says hey
we're going to this is what we need to
know to give you a quote for what we're
going to charge you for your
insurance and you can see that it has uh
1,338 rows and seven columns you can
count the columns 1 two 3 four five six
seven so there's seven columns on
here and the column we're really
interested in is charges um I want to
know what the charge charges are going
to be what can I expect not a very good
Arrow drawn um what to expect them to
charge on there uh so is this going to
be you know is this person going to cost
me uh $16,814.23
and then there's one other thing you
really need to notice on this data um
and I mentioned it before but I'm going
to mention it again because
pre-processing data is so much of the
work in data science um sex well how do
you how do you deal with female versus
male um are you a smoker yes or no what
does that mean region how do you look at
Region it's not a number how do you draw
a line between Southwest and
Northwest um you know they're they're
objects it's either you're Southwest or
you're Northwest it's not like I'm
southwest I guess you could do longitude
and latitude but the data doesn't come
in like that it comes in as true false
or whatever you know it's either your
Southwest or your
Northwest so we need to do a little bit
of preprocessing of the data on here to
make this
work oops there we go okay so let's take
a look and see what we're doing with
pre-processing and again this is really
where you spend a lot of time with data
science is trying to understand how and
why you need to do that and so we're
going to do uh you'll see right up
here label uh and then we're going to do
the do a label encoder one of the
modules we brought in so this is SK
learns uh label
encoder I like the fact that it's all
pretty much automated uh but if you're
doing a lot of work with the label
encoder you should start to understand
how that
fits um and then we have uh uh label.
fit right here where we're going to go
ahead and do the data set uh. six. drop
duplicates and then for data set sex
we're going to do the label transform
the data sex and so we're looking right
here at um male or female and so it
usually just converts it to a zer1
because there's only two choices on
here same thing with the smoker it's
zero or one so we're GNA transfer the
trans change the smoker
uh 01 on this and then finally we did
region down here region does it a little
bit different we'll take a look at that
and um it's I think in this case it's
probably going to do it because we did
it on this label
transform um with this particular setup
it gives each region a number like 0 1
two three so let's go ahe and take a
look and see what that looks like go and
run
this and you can see that our new data
set um has age that's still a number uh
Sex Is Zero or one uh so zero is female
one is male number of children we left
that alone uh smoker one or zero it says
no or yes on there we actually just do
one for no zero or no yeah one for no
I'm not sure how it organized them but
it turns the smoker into zero or one yes
or no uh and then region it did this as
uh 0 1 2 three so there three
regions now a lot of times in in when
you're working with data science and
you're dealing with uh regions or even
word
analysis um instead of doing one column
and labeling it 0 one two three a lot of
times you increase your features and so
you would have region Northwest would be
one column yes or no region Southwest
would be one column yes or no true
01 uh but for this this this particular
setup this will work just fine on here
now that we spend all that time getting
it set up up uh here's the fun part uh
here's the part where we're actually
using our setup on this and you'll see
right here we have our um why linear
regression uh data set drop the charges
because that's what we want to
predict and so our X I'm sorry our X
linear data set dro the charges because
that's where we're going to predict
we're predicting charges right here so
we don't want that as our input for our
features and our y output is charges
that's what we want to guess we want to
guess what the charges are
and then what we talked about earlier is
we don't want to do all the data at once
so we're going to take
um3 means 30% we're going to take 30% of
our data and it's going to be as the
train as the testing site so here's our
y test and our X test down there um and
so that part our model will never see it
until we're ready to test to see how
good it is and then of course right here
you'll see our training set and this is
what we're going to train it we're going
to trade it on 70% of the data and then
finally the big ones uh this is where
all the magic happens this is where
we're going to create our magic setup
and that is right here our linear model
we're going to set it equal to the
linear regression model and then we're
going to fit the data on
here and then at this point I always
like to pull up um if you if you if
you're working with a new model it's
good to see where it comes from this
comes from the pyit uh learn and this is
the sklearn linear model linear
regression that we imported earlier and
you can see they have different
parameters the basic parameter works
great if you're dealing with just
numbers uh mentioned that earlier with
stock high lows this model will do as
good as any other model out there for do
if you're doing just the very basic high
lows and looking for a linear fit a
regression model fit um and what you one
of the things when I looking at this is
I look for
methods and you'll see here's our fit
that we're using right now and here's
our
predict and we'll actually do a little
bit in the middle here as far as looking
at some of the parameters hidden behind
it the math that we talked about
earlier and so when we go in this and we
go ahead and run this you'll see it
loads the linear regression model and
just has a nice output that says hey I
loaded the linear regression model and
then the second part is we did the fit
and so this model is now trained our
linear model is now trained on the
training
data and so one of the things we can
look at is the um um for idx and colon
name and enumerate X linear train
columns come an interesting thing this
prints out the coefficients uh so in
you're looking at the back end of the
data you remember we had that formula uh
bxxy 1 plus bxx2 plus the plus the uh
intercept uh and so forth these are the
actual coefficients that are in here
this is what it's actually multiplying
these numbers
by and you can see like region gets a
minus value so when it adds it up I
guess a region you can read a lot into
these numbers uh it gets very
complicated there's ways to mess with
them if you're doing a basic linear
regression model you usually don't look
at them too closely uh but you might
start in these and saying hey you know
what uh smoker look how smoker impacts
the cost um it's just massive uh so this
is a flag that hey the value of the
smoker really affects this model and
then you can see here with the body mass
index uh so somebody who is overweight
is probably less healthy and more likely
to have cost money and then of course
age is a factor um and then you can see
down here we have uh sex is than a
factor also and it just it changes as
you go in there negative number it
probably has its own meaning on there
again it gets really complicated when
you dig into the um workings and how the
linear model works on that and so um we
can also look at the intercept this is
just kind of fun um so it starts at this
negative number and then adds all these
numbers to it that's all that means
that's our intercept on there and that
fits the data we have on that and so you
can see right here we can go back and
and oops give me just a second there we
go we can go ahead and predict the
unknown data and we can print that out
and if you're going to create a model to
predict something uh we'll go ahead and
predict it here's our y prediction value
linear model
predict and then we'll go ahead and
create a new data frame in this case
from our X linear test group we'll go
ahead and put the cost back into this
data frame and then the predicted cost
we're going to make that equal to our y
prediction and so when we pull this up
uh you can see here that we have uh the
actual cost and what we predicted the
cost is going to
be there's a lot of ways to measure the
accuracy on there uh but we're going to
go ahead and jump into our mushroom
data and so in this you can see here we
we've run our basic model we built our
coefficients you can see the intercept
the back end you can see how we're
generating a number here uh now with
mushrooms we want to yes or no we want
to know whether we can eat them or not
and so here's our mushroom file we're
going to go and run this and take a look
at the data and again you can ask for a
copy of this file uh send a note over to
Simply
learn.com and you can see here that we
have a class um the cap shape cap
surface and so forth so there's a lot of
feature in fact there's 23 different
columns in here going
across and when you look look at this um
I'm not even sure what these particular
like PE PE I don't even know what the
class is on
this I'm going to guess by the notes
that the class is uh poisonous or
edible so if you remember before we had
to do a little precoding on our data uh
same thing with here uh we have our cap
shape which is b or X or k um we have
cap color uh these really aren't numbers
so it's really hard to do anything with
just a a single number so we need to go
ahead and turn those into a label
encoder which again there's a lot of
different encoders uh with this
particular label encoder it's just
switching it to 0123 and giving it an
integer
value in fact if you look at all the
columns all of our columns are labels
and so we're just going to go ahead and
uh loop through all the columns and the
data and we're going to transform it
into a um label encoder and so when we
run this you can see how this gets
shifted from uh xbxx K to 012 3 4 5 or
whatever it is class is 01 one being
poisonous zero looks like it's
editable and so forth on here so we're
just encoding it if you were doing this
project depending on the results you
might encode it differently like I
mentioned earlier you might actually
increase the number of features as
opposed to laboring at 0 1 2 3 4 five um
in this particular example it's not
going to make that big of a difference
how we encode
it and then of course we're looking for
uh the class whether it's poisonous or
edible so we're going to drop the class
in our x uh Logistics model and we're
going to create our y Logistics model is
based on that class so here's our
XY and just like we did before
we're going to go ahead and split it uh
using 30% for
test 70% to program the model on
here and that's right here whoops there
we go there's our U train and
test and then you'll see here on this
next setup um this is where we create
our model all the magic happens right
here uh we go ahead and and create a
logistics model I've up the max
iterations if you don't change this for
this particular problem you'll get a
warning that says this has not
converged um because then that that's
what it does is it goes through the math
and it goes hey can we minimize the
error and it keeps finding a lower and
lower error and it still is changing
that number so that means it hasn't
conversed yet it hasn't find the lowest
amount of error it can and the default
is 100 uh there's a lot of settings in
here so when we go in here to let me
pull that up from the SK learn uh so we
pull that up from the sklearn
model you can see here we have our
logistic it has our different settings
on here that you can mess with most of
these work pretty solid on this
particular setup so you don't usually
mess a lot usually I find myself
adjusting the um iteration and it'll get
that warning and then increase the
iteration on there and just like the
other model you can go just like you did
with the other model we can scroll down
here and look for our
methods and you can see there's a lot of
methods uh available on here and
certainly there's a lot of different
things you can do with it uh but the
most basic thing we do is we fit our
model make sure it's set right uh and
then we actually predict something with
it so those are the two main things
we're going to be looking at on this
model is fitting and predicting there's
a lot of cool things you can do that are
more advanced uh but for the most part
these are the two which um I use when
I'm going into one of these models and
setting them
up so let's go ahead and close out of
our sklearn setup on there and we'll go
ahead and run this and you can see here
it's now loaded this up there we now
have a uh uh logistic model and we've
gone ahead and done a predict here also
just like I was showing you earlier uh
so here is where we're actually
predicting the data so we we've done our
first two lines of code as we create the
model we fit the model to our training
data and then we go ahead and predict
for our test data now in the previous
model we didn't dive into the test score
um I think I just showed you a graph and
we can go in there and there's a lot of
tools to do this we're going to look at
the uh model score on this one and let
me just go ahead and run the model
score and it says that it's pretty
accurate we're getting a roughly 95%
accuracy well that's good one 95%
accuracy 95% accuracy might be good for
a lot of
things but when you look at something as
far as whether you're going to pick a
mushroom on the side of the trail and
eat it we might want to look at the
confusion Matrix and for that we're
going to put in our y listic test the
actual values of edible and unedible and
we're going to put in our prediction
value and if you remember on here uh
let's see I believe it's poisonous was
one uh zero is edible so let's go ahead
and run that 01 zero is good so here is
um a confusion Matrix and this is if
you're not familiar with these we have
true true true
false true false false false so it says
out of the edible mushrooms we correctly
labeled 121 mushrooms edible that were
edible and we correctly measured
1,113 poisonous mushrooms as poisonous
but here's the
kicker I labeled uh 56 edible mushrooms
as being um poisonous well that's not
too big of a deal we just don't eat them
but I measured 68 mushrooms as being
edible that were poisonous so probably
not the best choice to use this model to
predict whether you're going to eat a
mushroom or not and you'd want to dig a
Little Deeper before you uh start eat
picking mushrooms off the side of the
Trail so a little warning there when
you're looking at any of these data
models looking at the error and how that
error fits in with what domain you're in
domain in this case being edible
mushrooms uh be a little careful make
sure that you're looking at them
correctly so we've looked at uh edible
or not edible we've looked at uh
regression model as far as U the end
values what's going to be the cost and
what our predicted cost is so we can
start figuring out how much to charge
these people for their
insurance and so these really are the
fundamentals of data science when you
pull them together uh when I say data
science talking about your machine
learning
code and hopefully you got a little bit
out of here again you can contact our
simply learn team and get a copy of
these files or get more information on
this if you are an aspiring machine
learning engineer looking for online
training and certifications from
prestigious universities and in
collaboration with leading experts then
search no more simply learns
professional certification program in Ai
and machine learning from per University
in collaboration with IVM should be your
right choice for more details use the
link in the description box below with
that in
mind a confusion Matrix represents a
table layout of the different outcomes
of prediction and results of a
classification problem and helps
visualize its
outcomes and so you see here we have our
uh simple chart predicted and actual the
confusion Matrix helps us identify the
correct predictions of a model for
different individual classes as well as
the errors so you'll see here that the
values predicted by our classifier are
along the rows this is what we're going
to guess it is our our model is guessing
what this is based on its training so
we've already trained the model to um
guess whether it's spam or not spam or
whatever it is you're working on and
then the actual values of our data set
are along the
columns so this is the actual value that
supposed to be people who can speak
English will be classified as positives
so because they have a remember 01 do
you speak English yes no and you could
extend this that they might have do you
speak uh French do you speak whatever
languages and so you might have a whole
lot of classifiers that you would look
at each one of these people who cannot
speak English will be classified as
negatives so there'll be a zero so you
know zero ones the number of times are
actual positive values are equal to
predicted positive values gives us true
positive TP the number of times our
actual negative values are equal to
predictive negative values gives us true
negative
TN the number of times our model wrongly
predicts negative values as
positives gives us a false
positive
FP and you'll see when you're working
with these a lot you know memorizing
that it's false positive you can easily
figure out what that is and pretty soon
you're just looking at the FP or the TP
depending on what you're working
and the number times our model wrongly
predicts positive values as negatives
gives us a false negative
FP now I'm going to do a quick step out
here let's say you're working in the
medical and we're talking about
cancer uh do you really want a bunch of
false negatives you want zero under
false negative uh so when we look at
this confusion Matrix if you have 5%
false positive IES and 5% false
negatives it'd be much better to even
have 20% false positives because they go
in and test it and zero false negatives
the same might be true if you're working
on uh uh say uh a car driving is this a
safe place for the car to go well you
really don't want any false positives
you know yes this is safe right over the
cliff so again when you're working on
the project or whatever it is you're
working on this chart suddenly has huge
value uh we were talking about spam
email how many important emails say from
your banking overdraft charge coming in
that you want to be uh a true a false
negative you don't want it to go in the
spam folder likewise you want to get as
much of the spam out of there but you
don't want to miss anything really
important confusion Matrix metrics are
performance measures which help us find
the accuracy of our classifier there are
four main metrics accuracy precision
recall and F1 score the F1 score is the
one I usually hear the most and accuracy
is usually what you put on your chart uh
when you're sending in front of the
shareholders how accurate is it people
understand accuracy um F1 score is a
little bit more on the math side and so
you got to be a little careful when
you're quoting F1 scores in the when
you're sitting there with all the
shareholders because a lot of them will
just glaze over so confusion Matrix
metrics are performance measures which
help us find the accuracy of our
classifier there are four main metrics
accuracy the accuracy is used to find
the portion of the correctly classified
values it tells us how often our
classifier is right it is the sum of all
True Values divided by the total values
and this makes sense uh again it's one
of those
things I don't want to F you know
depends on what you're looking for are
you looking for uh not to miss any spam
mails are you looking to drive down the
road and not run anybody over Precision
is used to calculate the model's ability
to classify positive values correctly it
answers the question when the model
predicts a positive value how often is
it right it is the true positive divided
by the total number of predicted
positive values again this one uh
depends on what project you're working
on whether this is what you're going to
be focusing on uh so recall it is used
to calculate the model's ability to
predict positive values how often does
the model actually predict the correct
positive values it is the true positives
divided by the total number of actual
positive values and then your F1 score
it is the harmonic mean of recall and
precision it is useful when you need to
take both precision and recall into
account consider the following two
confusion Matrix derived from two
different classifier to figure out which
one performs better we can find the
confusion Matrix for both of them and
you can see we're back to uh does it
classify whether they can speak English
or or non-speaker they speak something
they don't know the English language and
so we put these two uh uh confusion
matrixes out here we can go ahead and do
the math behind that we can look up the
accuracy that's a tpn plus TN over the
TF plus TN plus FP Plus FN and so we get
an accuracy of
8125 and we have a Precision if you do
the Precision which is your TP truth
positive over TP plus
FP uh we get
891 and if we do the recall we'll end up
with the 0 825 that's your TP over TP
plus FN and then of course your F1 score
which is 2 * Precision Time recall over
Precision plus recall and we get the 0
857 and if we do that um with another
model let's say we had two different
models and we're trying to see which one
we want to use uh for whatever reason uh
we might go ahead and compute the same
things we have our accuracy our
precision and our recall and our F1
score and uh as we're looking at this we
might uh look at the accuracy because
that's really what we're interested in
is uh how many people are we able to
classify as being to speak English I
really don't want to know if I you know
I I I I really don't want to know if I
if they're non-speakers um I'd rather
Miss 10 people speaking English instead
of 15 and so you can see from these
charts we probably go with the first
model because it does a better job
guessing who speaks English and has a
higher accuracy because in this case
that is what we're looking
for so uh with that we'll go ahead and
pull up a demo so you can see what this
looks like in the python uh setup in in
the actual coding for this we'll go into
Anaconda Navigator if you're not
familiar with Anaconda uh it's a really
good tool to use as far as doing display
and demos and for quick development um
as a data scientist I just love the
package now if you're going to do
something heavier lifting uh there's
some limitations with anaconda and with
the setup but in general you can do just
about anything in here with your Python
and for this we'll go with Jupiter
notebook uh Jupiter lab is the same as
Jupiter notebook you'll see they now
have integration with py charm if you
work in py charm uh certainly there's a
lot of other Integrations that Anaconda
has and we've opened up um my simply
learned files I work on and create a new
file called confusion Matrix demo and
the first thing we want to note is the
data we're working with uh here I've
opened it up in a word pad or notepad or
whatever uh you can see it's got a row
of uh headers comma separated and then
all the data going down below and then I
saved this in the same file so I don't
have to remember what path I'm working
on uh of course if you have your data
separated and you're working with a lot
of data you probably want to put it in a
different folder or file depending on
what you're doing and the first thing we
want to do is go ahead and import our
tools uh we're going to use the pandas
that's our data frame if you haven't had
a chance to work with the data frame
please review Panda's data frame going
to Simply learn you can pull up the
panda data frame um tutorial on
there and then we're going to use uh the
scit framework which is all denoted as
sklearn and I can just pull this in you
can see here's the um
scikit-learn dorg with a stable version
that you can import into your
Python and from here we're going to use
the train test split for splitting our
data we're going to do some
pre-processing we're going to going do
use the logistic regression model that's
our actual uh machine learning model
we're using and then with this C this
particular setup is about is we're going
to do the accuracy score the confusion
Matrix and the classifier report so let
me go ahead and run that and bring all
that information
in and just like we opened the file we
need to go ahead and load our data in
here uh so we're going to go ahead and
do our pandas read CSV and then just
because we're in jupyter Notebook we can
just put data to read the data in here a
lot of times we'll actually let me just
do this I prefer to do the just the head
of the data the top
part and you can see we have age sex um
I'm not sure what CP stands for test BPS
cholesterol uh so a lot of different
measurements uh if you were in this
domain you'd want to know what all these
different measurements mean I don't want
to focus on that too much because when
we're talking about data science a lot
of times you have no idea what the data
means if you've ever looked up the
breast cancer measurement it's just a
bunch of measurements and numbers uh
unless you're a doctor you're going to
have no idea what those measurements
mean but if it's your specialty in your
domain you better know them so we're
going to go ahead and create Y and it's
going to we're going to set it equal to
the Target uh so here's our Target value
here and it's either one or
zero so we have a classifier if you're
dealing with with one zero true false
what do you have you have a
classifier and then our X is going to be
uh everything except for the Target uh
so we're going to go ahead and drop the
target axis equals one remember that's
columns versus uh the index or rows axis
equal zero would would give you an error
but you would drop like row two and then
we'll go ahead and just print that out
so you can see what we're looking at and
uh here we have um Y data X data uh and
you can see from the X data we have the
X head and we can go ahead and just do
print the Y head
data and run
that so this is all loading the data
that we've done so far uh if there's a
confusion in there go back and rewind
the tape and review it and then we need
to go ahead and split our data into our
XT train X test y train y test and then
keep in mind you always want to split
the data before we do the scaler and the
reason is is that uh you want the scaler
on the training data uh to be set on the
training data data or fit to it but not
on the test data think of this as being
out in the field uh you're not it could
actually alter your results uh so it's
always important to do make sure
whatever you do to the training data or
whatever um uh fit you're doing is
always done on the training not on the
test and then we want to go ahead and
scale the data now we are working with
um linear regression model I I'll
mention this here in a minute when we
get to the actual model uh so some
sometimes you don't need to scale when
you're working with linear regression
models it's not going to change your
result as much as say a neural network
where it has a huge
impath uh but we're going to go ahead
and take here's our XT train X test YT
train y test we create our scaler we go
ahead and scale uh the scale is going to
fit the X
train and then we're going to go ahead
and take our XT train and transform it
and then we also need to take our X test
and transform it based on the scale on
here so that our X is now between that
nice minus one to one and so this is all
uh our pre- dat setup and hopefully uh
all of that looks fairly familiar to you
if you've done a number of our other
classes and you're up to the setup on
here and then we want to go ahead and do
is create our model and we're going to
use the logistic regression model and
from the logistic regression model uh
we're going to go ahead and fit our X
train and Y train and then we'll run our
predicted value on here and so let's go
ahead and run that and so now we are we
actually have like our X test and our
prediction so if you remember
from Matrix we're looking for the actual
versus the prediction and how those
compare and if I take this back up here
you're going to notice that we imported
the accuracy score the confusion Matrix
and the classification
report uh and there's of course our
logistic regression the model we're
using for this and I did mention I was
going to talk a little bit about scaler
and the regression
model the scaler on a lot of your
regression models uh your basic Mass
standard regression models and I'd have
to look it up for the logistic
regression model when you're using a
standard regression model you don't need
to scale the data uh it's already just
built in by the way the model
Works in most cases uh but if you're in
a neural network and there's a lot of
other different setups then you really
want to take this and uh fit that on
there and so we can go in and do the
accuracy uh and this is if you remember
correctly we were looking at the
accuracy with the english- speaking uh
so this is saying our accuracy as to
whether this person is I believe this is
the heart data
set um it's going to be accurate about
85% of the time as far as whether it's
going to predict the person's going to
have um a heart condition or the one as
it comes up with the 01 on there which
would mean at this point that you have
an 85% uh being correct on telling
someone they are extremely high risk for
a heart attack kind of
thing and so we want to go ahead and uh
create our confusion Matrix and let me
just do
that of course the software does
everything for us so we'll go ahead and
run this and you can see right here um
here's our
25 uh prediction uh correct predictions
right
here and if you remember from our slide
I'll just bring this over so it's a nice
visual we have our true positive false
positive uh so we had 25 which were true
that it said hey this person's going to
be high risk at um heart and we had four
that were still high risk that it said
were false um so out of these 25 people
or out of these 29 people and that that
makes sense because you have 085 uh out
of 29 people it was correct on 25 of
them and so uh here's our accuracy score
we were just looking at that our
accuracy is your true positive and your
true negative over all of them so how
true is it there was our accuracy um
coming up here 085 and then we have our
nice Matrix generated from that uh and
you can see right here is a similar
Matrix we had going for from the slide
and this starts to this should start
asking questions at this point um so if
you're in a board meeting or you're
working with this you really want to
start looking at this data here and
saying well is this good enough is uh
this number of people and hopefully
you'd have a much larger data set in my
is my confusion Matrix showing for the
true positive and false positive is that
acceptable for what we're doing uh and
of course if you're going to put
together uh whatever data you're putting
out you might want to separate the uh
true negative false positive false
negative true positive and you can
simply do that uh by doing the confusion
Matrix uh and then of course the Ravel
part lets you um set that up so you can
just split that right up into a nice
tupal and the final thing we want to
show you here in the coding on this part
is the confusion Matrix
metrics and so we can come in here and
just use the Matrix equals
classification report the Y test and the
predict and then we're going to take
that classification report and go ahead
and print that out and you can see here
it does a nice job uh giving you your
accuracy uh your micro average your
weighted average um you have your
precision
your recall your F1 score and your
support all in one window so you can
start looking at this data and saying oh
okay our precisions at
083 uh 087 for getting a a positive and
83 for the negative side for a zero and
we start talking about whether this is a
valid information or not to use and when
we're looking at a heart attack
prediction we're only looking at one
aspect what's the chances of this person
having a heart attack or not um you
might have something where we went back
to languages maybe you also want to know
whether they speak English or Hindi uh
or French and you can see right here
that we can now take our confusion
Matrix and just expand it as big as we
need to depending on how many different
classifiers we're working on if you are
an aspiring machine learning engineer
looking for online training and
certifications from prestigious
universities and in collaboration with
leading experts then search no more
simply learns professional certification
program in Ai and machine learning from
Pur University in collaboration with IBM
should be a right choice for more
details use the link in the description
box below with that in mind what is a
decision tree let's go through a very
simple example before we dig in deep
decision tree is a tree-shaped diagram
used to determine a course of action
each branch of the tree represents a
possible decision occurrence or reaction
let's start with a simple question how
to identify a random vegetable from a
shopping bag so we have this group of
vegetables in here and we can start off
by asking a simple question is it red
and if it's not then it's going to be
the purple fruit to the left probably an
eggplant if it's true it's going to be
one of the red fruits is the diameter
greater than two if false it's going to
be a what looks to be a red chili and if
it's true it's going to be a bell pepper
from the capsicum family so it's a
capsicum problems that decision tree can
solve so let's look at the two different
categories the decision tree can be used
used on it can be used on the
classification the true false yes no and
it can be used on regression where we
figure out what the next value is in a
series of numbers or a group of data in
classification the classification tree
will determine a set of logical if then
conditions to classify problems for
example discriminating between three
types of flowers based on certain
features in regression a regression tree
is used when the target variable is
numerical or continuous in nature we fit
the regression model to the Target
variable using each of the independent
variables each split is made based on
the sum of squared error before we dig
deeper into the mechanics of the
decision tree let's take a look at the
advantages of using a decision tree and
we'll also take a glimpse at the
disadvantages the first thing you'll
notice is that it's simple to understand
interpret and visualize it really shines
here because you can see exactly what's
going on in a decision tree little
effort is required for data preparation
so you don't have to do special scaling
there's a lot of things you don't have
to worry about when using a decision
tree it can handle both numerical and
categorical data as we discovered
earlier and nonlinear parameters don't
affect its performance so even if the
data doesn't fit an easy curved graph
you can still use it to create an
effective decision or prediction if
we're going to look at the advantages of
a decision tree we also need to
understand the disadvantages of a
decision tree the first disadvantage is
overfitting overfitting occurs when the
algorithm captures noise in the data
that means you're solving for one
specific instance instead of a general
solution for all the data High variance
the model can get unstable due to small
variation in data low bias tree a highly
complicated decision tree tends to have
a low bias which makes it difficult for
the model to work with new data decision
tree important terms before we dive in
further we need to look at some basic
terms we need to have some definitions
to go with our decision tree in the
different parts we're going to be using
we'll start with entropy entropy is a
measure of Randomness or
unpredictability in the data set for
example we have a group of animals in
this picture there's four different
kinds of animals and this data set is
considered to have a high entropy you
really can't pick out what kind of
animal it is based on looking at just
the four animals as a big clump of of uh
entities so as we start splitting it
into subgroups we come up with our
second definition which is Information
Gain Information Gain it is the measure
of decrease in entropy after the data
set is split so in this case based on
the color yellow we've split one group
of animals on one side as true and those
who aren't yellow as false as we
continue down the yellow side we split
based on the height true or false equals
10 and on the other side height is less
than 10 true or false and as you see as
we split it the entropy continues to be
less and less and less and so our
Information Gain is simply the entropy
E1 from the top and how it's changed to
E2 in the bottom and we'll look at the
uh deeper math although you really don't
need to know a huge amount of math when
you actually do the programming in
Python because they'll do it for you but
we'll look on the actual math at how
they compute entropy finally we went
under the different parts of our tree
and they call the leaf node Leaf node
carries the classification or the
decision so it's the final end at the
bottom the decision node has two or more
branches this is where we're breaking
break the group up into different parts
and finally you have the root node the
topmost decision node is known as the
root
node how does a decision tree work
wonder what kind of animals I'll get the
jungle today maybe you're the hunter
with the gun or if you're more into
photography you're a photographer with a
camera so let's look at this group of
animals and let's try to classify
different types of animals based on
their features using a decision tree so
the problem statement is to classify the
different types of animals based on
their features using a decision tree the
data set is looking quite messy and the
entropy is high in this case so let's
look at a training set or a training
data set and we're looking at color
we're looking at height and then we have
our different animals we have our
elephants our giraffes our monkeys and
our tigers and they're of different
colors and shapes let's see what that
looks like and how do we split the data
we have to frame the conditions that
split the data in such a way that the
Information Gain is the highest note
gain is the measure of decrease in
entropy after splitting so the formula
for entropy is the sum that's what this
symbol looks like that looks like kind
of like a uh e funky e of K where I
equals 1 to k k would represent the
number of animal the different animals
in there where value or P value of I
would be the percentage of that animal
times the log base 2 of the same the
percentage of that animal let's try to
calculate the entropy for the current
data set and take a look at what that
looks like and don't be afraid of the
math you don't really have to memorize
this math and just be aware that it's
there and this is what's going on in the
background and so we have three giraffes
two tigers one monkey two elephants a
total of eight animals gathered and if
we plug that into the formula we get an
entropy that equals 3 over 8 so we have
three drafts a total of eight times the
log usually they use base two on the log
so log base 2 of 3 over 8 plus in this
case let's say it's the elephants 2 over
8 two elephants over total of 8 * log
base 2 2 over 8 + 1 monkey over total of
8 log base 2 1 over8 and plus 2 over 8
of the Tigers log base 2 over 8 and if
we plug that into our computer our
calculator I obviously can't do logs in
my head we get an entropy equal to
.571 the program will actually calculate
the entropy of the data set similarly
after every split to calculate the gain
now we're not going to go through each
set one at a time to see what those
numbers are we just want you to be aware
that this is a Formula or the
mathematics behind it gain can be
calculated by finding the difference of
the subsequent entropy values after a
split now we will try to choose a
condition that gives us the highest gain
we will do that by splitting the data
using each condition and checking that
the gain we get out of them the
condition that gives us the highest gain
will be used to make the first split can
you guess what that first split will be
just by looking at this image as a human
it's probably pretty easy to split it
let's see if you're right if you guessed
the color yellow you're correct let's
say the condition that gives us the
maximum gain is yellow so we will split
the data based on the color yellow if
it's true that group of animals goes to
the left if it's false it goes to the
right the entropy after the splitting
has to decreased considerably however we
still need some splitting at both the
branches to attain an entropy value
equal to zero so we decide to split both
the nodes using height as a condition
since every Branch now contains single
label type we can say that entropy in
this case has reached the least value
and here you see we have the giraffes
the Tigers the monkey and the elephants
all separated into their own groups this
tree can now predict all the classes of
animals present in the data set with
100% accuracy that was easy use case
loan repayment prediction let's get into
my favorite part and open up some Python
and see what the programming code in the
scripting looks like in here we're going
to want to do a prediction and we start
with this individual here who's
requesting to find out how good his
customers are going to be whether
they're going to repay their loan or not
for his bank and from that we want to
generate a problem statement to predict
if a customer will repay loan amount or
not and then we're going to be using the
decision tree algorithm in Python let's
see what that looks like and let's dive
into the code in our first few steps of
implementation we're going to start by
importing the necessary packages that we
need from Python and we're going to load
up our data and take a look at what the
data looks like so the first thing I
need is I need something to edit my
Python and it in so let's flip on over
and here I'm using the Anaconda Jupiter
notebook now you can use any python IDE
you like to run it in but I find the
jupyter notebooks really nice for doing
things on the Fly and let's go ahead and
just paste that code in the beginning
and before we start let's talk a little
bit about what we're bringing in and
then we're going to do a couple things
in here we have to make a couple changes
as we go through this first part of the
import the first thing we bring in is
numpy as NP that's very standard when
we're dealing with math mathematics
especially with uh very complicated
machine learning tools you almost always
see the numpy come in for your num your
number it's called number python it has
your mathematics in there in this case
we actually could take it out but
generally you'll need it for most of
your different things you work with and
then we're going to use pandas as PD
that's also a standard the pandas is a
data frame setup and you can liken this
to uh taking your basic data and storing
it in a way that looks like an Excel
spreadsheet so as we come back to this
when you see NP or PD those are very
standard uses you'll know that that's
the pandas and I'll show you a little
bit more when we explore the data in
just a minute then we're going to need
to split the data so I'm going to bring
in our train test and split and this is
coming from the sklearn package cross
validation and just a minute we're going
to change that and we'll go over that
too and then there's also the sk. tree
import decision tree classifier that's
the actual tool we're using remember I
told you don't be afraid of the
mathematics it's going to be done for
you well the decision tree classifier
has all that Ma mathematics in there for
you so you don't have to figure it back
out again and then we have SK learn.
metrics for accuracy score we need to
score our our setup that's the whole
reason we're splitting it between the
training and testing data and finally we
still need the sklearn import tree and
that's just the basic tree function is
needed for the decision tree classifier
and finally we're going to load our data
down here and I'm going to run this and
we're going to get two things on here
one we're going to get an error and two
we're going to get a warning let's see
what that looks like so the first thing
we had is we have an error why is this
error here well it's looking at this it
says I need to read a file and when this
was written the person who wrote it this
is their path where they stored the file
so let's go ahead and fix
that and I'm going to put in here my
file path I'm just going to call it full
file name and you'll see it's on my C
drive and this is very lengthy setup on
here where I stored the data 2. CSV
file don't worry too much about the full
pth path because on your computer it'll
be different the data. 2 CSV file was
generated by simply learn if you want a
copy of that you can comment down below
and request it here in the
YouTube and then if I'm going to give it
a name full file name I'm going to go
ahead and change it here to
full file name so let's go ahead and run
it now and see what
happens and we get a
warning when you're coding understanding
these different warnings and these
different errors that come up is
probably the hardest lesson to learn so
let's just go ahead and take a look at
this and use this as a uh opportunity to
understand what's going on here if you
read the warning it says the cross
validation is depreciated so it's a
warning on it's being removed and it's
going to be moved in favor of the model
selection so so if we go up here we have
sklearn Doc crossvalidation and if you
research this and go to sklearn site
you'll find out that you can actually
just swap it right in there with model
selection and so when I come in here and
I run it again that removes a warning
what they've done is they've had two
different developers develop it in two
different branches and then they decided
to keep one of those and eventually get
rid of the other one that's all that is
and very easy and quick to
fix before we go any further I went
ahead and opened up the data from this
file remember the the data file we just
loaded on here the dataor 2. CSV let's
talk a little bit more about that and
see what that looks like both as a text
file because it's a comma separated
variable file and in a spreadsheet this
is what it looks like as a basic text
file you can see at the top they've
created a header and it's got 1 two 3
four five columns and each column has
data in it and let me flip this over cuz
we're also going to look at this uh in
an actual spreadsheet so you can see
what that looks like and here I've
opened it up in the open Office calc
which is pretty much the same as um
Excel and zoomed in and you can see
we've got our columns and our rows of
data little easier to read in here we
have a result yes yes no we have initial
payment last payment credit score house
number if we scroll away
down we'll see that this occupies a,1
lines of code or lines of data with uh
the first one being a column and then
1,000 lines of
data now as a
programmer if you're looking at a small
amount of data I usually start by
pulling it up in different sources so I
can see what I'm working
with but in larger data you won't have
that option it'll just be um too too
large so you need to either bring in a
small amount that you can look at it
like we're doing right now or we can
start looking at it through the python
code so let's go go ahead and move on
and take the next couple steps to
explore the data using python let's go
ahead and see what it looks like in
Python to print the length and the shape
of the data so let's start by printing
the length of the database we can use a
simple Lin function from Python and when
I run this you'll see that it's a th
long and that's what we expected there's
a th lines of data in there if you
subtract the column head and this is one
of the nice things when we did the uh
balance data from the Handa read CSV
you'll see that the header is row zero
so it automatically removes a
row and then shows the data separate it
does a good job sorting that data out
for us and then we can use a different
function and let's take a look at that
and again we're going to utilize the
tools in
Panda and since the balance uncore data
was loaded as a panda data
frame we can do a shape on it and let's
go ahead and run the shape and see what
that looks looks
like what's nice about this shape is not
only does it give me the length of the
data we have a thousand lines it also
tells me there's five columns so we were
looking at the data we had five columns
of data and then let's take one more
step to explore the data using Python
and now that we've taken a look at the
length and the shape let's go ahead and
use the uh Panda module for head another
beautiful thing in the data set that we
can utilize so let's put that on our
sheet here and we have print data set
and balance data. head and this is a
panda's print statement of its own so it
has its own print feature in there and
then we went ahead and gave a label for
our print job here of data set just a
simple print statement and when we run
that and let's just take a closer look
at that let me zoom in
here there we
go pandas does such a wonderful job of
making this a very clean readable data
set so you can look at the data you can
look at the column header you can have
it uh when you put it as a head it
prints the first five lines of the data
and we always start with zero so we have
five lines we have 0 1 2 3 4 instead of
1 2 3 4 5 that's a standard scripting
and programming set is you want to start
with the zero position and that is what
the data head does it pulls the first
five rows of data puts in a nice format
that you can look at and view very
powerful tool to view the data so
instead of having to flip and open up an
Excel spreadsheet or open Office or
trying to look at a word dock where it's
all scrunched together and hard to read
you can now get a nice open view of what
you're working with we're working with a
shape of a thousand long five wide so we
have five columns and we do the full
data head you can actually see what this
data looks like the initial payment last
payment credit scores house number so
let's take this now that we've explored
the data and let's start digging into
the decision tree so in our next step
we're going to train and build our data
tree and to do that we need to First
separate the data out we're going to
separate into two groups so that we have
something to actually train the data
with and then we have some data on the
side to test it to see how good our
model is remember with any of the
machine learning you always want to have
some kind of test set to to weigh it
against so you know how good your model
is when you distribute it let's go ahead
and break this code down and look at it
in pieces so first we have our X and
Y where do X and Y come from well X is
going to be our data and Y is going to
be the answer or the target you can look
at it source and Target in this case
we're using X and Y to denote the data
in and the data that we're actually
trying to guess what the answer is going
to be and so to separate it we can
simply put in x equals the balance of
the data. values the first brackets
means that we're going to select all the
lines in the database so it's all the
data and the second one says we're only
going to look at columns 1 through 5
remember we always start with zero zero
is a yes or no and that's whether the
loan went default or not so we want to
start with one if we go back up here
that's the initial payment and it goes
all the way through the house
number well if we want to look at uh 1
through five we can do the same thing
for Y which is the answers and we're
going to set that just equal to the zero
row so it's just the zero row and then
it's all rows going in there so now
we've divided this into two different
data sets one of them with the data
going in and one with the
answers next we need to split the
data and here you'll see that we have it
split into four different parts the
first one is your X training your X test
your y train your y
test simply put we have X going in where
we're going to train it and we have to
know the answer to train it with
and then we have X test where we're
going to test that data and we have to
know in the end what the Y was supposed
to be and that's where this train test
split comes in that we loaded earlier in
the modules this does it all for us and
you can see they set the test size equal
to3 so that's roughly 30% will be used
in the test and then we use a random
state so it's completely random which
rows it takes out of there and then
finally we get to actually build our
decision tree and they've called it here
clf entropy that's the actual decision
tree or decision tree classifier and in
here they've added a couple variables
which we'll explore in just a minute and
then finally we need to fit the data to
that so we take our clf entropy that we
created and we fit the X train and since
we know the answers for X train are the
Y train we go ah and put those in and
let's go ahead and run this and what
most of these sklearn modules do is when
you set up the variable in this case
when we set the clf entrop equal
decision tree classif fire it
automatically prints out what's in that
decision tree there's a lot of variables
you can play with in here and it's quite
beyond the scope of this tutorial to go
through all of these and how they work
but we're working on entropy that's one
of the options we've added that it's
completely a random state of 100 so 100%
And we have a max depth of three now the
max depth if you remember above when we
were doing the different graphs of
animals means it's only going to go down
three layers before it stops and then we
have minimal samples of leaves five so
it's going to have at least five Leafs
at the end so I'll have at least three
splits I'll have no more than three
layers and at least five end leaves with
the final result at the bottom now that
we've created our decision tree
classifier not only created it but
trained it let's go ahead and apply it
and see what that looks like so let's go
ahead and make a prediction and see what
that looks like we're going to paste our
predict code in here and before we run
it let's just take a quick look at
what's it's doing here
we have a variable y predict that we're
going to do and we're going to use our
variable clf entropy that we
created and then you'll see do predict
and that's very common in the sklearn
modules that they different tools have
the predict when you're actually running
a prediction in this case we're going to
put our X test data in here now if you
delivered this for use an actual
commercial use and distributed it this
should be the new loans you're putting
in here to guess whether the person's
going to be uh pay them back or not in
this case so we need to test out the
data and just see how good our sample is
how good of our tree does at predicting
the loan payments and finally since
Anaconda jupyter notebook is works as a
command line for python we can simply
put the Y predict e in to print it I
could just as easily have put the
print and put brackets around y predict
in to print it out we'll go ahead and do
that it doesn't matter which way you do
it and you'll see right here that it
runs a prediction this is roughly 300 in
here remember it's 30% of a th000 so you
should have about 300 answers in here
and this tells you which each one of
those lines of our test went in there
and this is what our y predict came out
so let's move on to the next step we're
going to take this data and try to
figure out just how good a model we have
so here we go since sklearn does all the
heavy Lifting for you and all the math
we have a simple line of code to let us
know what the accuracy is and let's go
ahead and go through that and see what
that means and what that looks like
let's go ahead and paste this in and let
me zoom in a little bit there we go so
you have a nice full picture and we'll
see here we're just going to do a print
accuracy is and then we do the accuracy
score and this was something we imported
um earlier if you remember at the very
beginning let me just scroll up there
real quick so you can see where that's
coming from from that's coming from here
down here from sklearn docs import
accuracy score and you could probably
run a script make your own script to do
this very easily how accurate is it how
many out of 300 do we get right and so
we put in our y test that's the one we
ran the predict on and then we put in
our y predict in that's the answers we
got and we're just going to multiply
that by a 100 because this is just going
to give us an answer as a decimal and we
want to see it as a percentage and let's
run that and see what it looks like and
if you see here we got an accuracy of
93.6
6667 so when we look at the number of
loans and we look at how good our model
fit we can tell people it has about a
93.6 fitting to it so just a quick recap
on that we now have accuracy setup on
here and so we have created a model that
uses the decision tree algorithm to
predict whether a customer will repay
the loan or not the accuracy of the
model is about 94.6%
the bank can now use this model to
decide whether it should approve the
loan request from a particular customer
or not and so this information is really
powerful we might not be able to as
individuals understand all these numbers
because they have thousands of numbers
that come in but you can see that this
is a smart decision for the bank to use
a tool like this to help them to predict
how good their uh profits going to be
off of the loan balances and how many
are going to default or not if you are
an aspiring machine learning engineer
looking for online training and
certifications from prestigious
universities and in collaboration with
leading experts then search no more
simply learns professional certification
program in Ai and machine learning from
P University in collaboration with IVM
should be your right choice for more
details use the link in the description
box below with that in mind let us dig
deep into the theory of exactly how it
works and let's look at what is random
for us random for us or random decision
Forest is a method that operates by con
constructing multiple decision trees the
decision of the majority of the trees is
chosen by the random Forest as the final
decision and this uh we have some nice
Graphics here we have a decision tree
and they actually use a real tree to
denote the decision tree which I love
and given a random some kind of picture
of a fruit this decision tree decides
that the output is it's an apple and we
have a decision tree to where we have
that picture of the fruit goes in and
this one decides that it's a lemon and
the decision 3 tree gets another image
and it decides it's an apple and then
this all comes together in what they
call the random forest and this random
Forest then looks at it and says okay I
got two votes for apple one vote for
lemon the majority is Apples so the
final decision is apples to understand
how the random Forest works we first
need to dig a little deeper and take a
look at the random forest and the actual
decision tree and how it builds that
decision Tree in looking closer at how
the individual decision trees work we'll
go ahead and continue to use the fruit
example since we're talking about trees
and forests a decision tree is a tree
shaped diagram used to determine a
course of action each branch of the tree
represents a possible decision
occurrence or reaction so in here we
have a bowl of fruit and if you look at
that it looks like um they switch from
lemons to oranges so we have oranges
cherries and apples and the first
decision of the decision tree might be
is a diameter greater than or equal to
three and if it says false it knows that
they're cherries because everything else
is bigger than that so all the cherries
f into that decision so we have all that
data we're training we can look at that
we know that that's what's going to come
up is the color orange well goes hm
orange or red well if it's true then it
comes out as the orange and if it's
false that leaves apples so in this
example it sorts out the fruit in the
bowl or the images of the fruit a
decision tree these are very important
terms to know because these are very
Central to understanding the decision
train when working with them the first
is entropy everything on on the decision
tree and how it makes those decision is
based on entropy entropy is a measure of
Randomness or unpredictability in the
data set uh then they also have
Information Gain the leaf node the
decision node and the root node we'll
cover these other four terms as we go
down the tree but let's start with
entropy so starting with entropy we have
here a high amount of Randomness what
that means is that whatever is coming
out of this decision if it was going to
guess based on this data it wouldn't be
able to tell you whether it's a lemon or
an apple it would just say it's a fruit
uh so the first thing we want to do is
we want to split this apart and we take
the initial data set we're going to set
create a data set one and a data set two
we just split it in two and if you look
at these new data sets after splitting
them the entropy of each of those sets
is much less so for the first one
whatever comes in there it's going to
sort that dat data and it's going to say
okay if this data goes this direction
it's probably an apple and if it goes
into the other direction it's probably a
lemon so that brings us up to
Information Gain it is the measure of
decrease in the entropy after the data
set is split what that means in here is
that we've gone from one set which has a
very high entropy to two lower sets of
entropy and we've added in the values of
E1 for the first one and E2 for the
second two which are much lower and so
that Information Gain is increased
greatly in this example and so you can
find that the information grain simply
equals uh decision E1 minus E2 as we're
going down our list of uh definitions
we'll look at the leaf node and the leaf
node carries the classification or the
decision so we look down here to the
leaf node we finally get to our set one
or our set two when it comes down there
and it says okay this object's gone into
set one if it's it's gone into set one
it's going to be split by some means and
we'll either end up with apples on the
leaf node or a lemon on the leaf node
and on the right it'll either be an
apple or lemons those Leaf nodes or
those final decisions or
classifications uh that's the definition
of leaf node in here if we're going to
have a final Leaf where we make the
decision we should have a name for the
nodes above it and they call those
decision nodes a decision node decision
node has two or more brand
and you can see here where we have the
uh five apples and one lemon and in the
other case the five lemons and one apple
they have to make a choice of which tree
It Goes Down based on some kind of
measurement or information given to the
tree and that brings us to our last
definition the root node the topmost
decision node is known as the root node
and this is where you have all of your
data and you have your first decision it
has to make or the first split in
information
so far we've looked at a very general
image um with the fruit being split
let's look and see exactly what that
means to split the data and how do we
make those decisions on there uh let's
go in there and find out how does a
decision tree work so let's try to
understand this and let's use a simple
example and we'll stay with the fruit we
have a bowl of fruit and so let's create
a problem statement and the problem is
we want to classify the different type
of fruits in the bowl based on different
features the data set in the bowl is
looking quite messy and the entropy is
high in this case so if this bow was our
decision maker it would know what choice
to make it has so many choices which one
do you pick Apple grapes or lemons and
so we look in here we're going to start
with a dra a training set so this is our
data that we're training our data with
and we have a number of options here we
have the color and under the color we
have red yellow purple uh we have a
diameter uh 331 331 and we have a label
Apple lemon Grapes apple lemon grapes
and how do we split the data we have to
frame the conditions to split the data
in such a way that the Information Gain
is the highest it's very key to note
that we're looking for the best gain we
don't want to just start sorting out the
smallest piece in there we want to split
it the biggest way we can and so we
measure this decrease in entropy that's
what they call it entropy there's our
entropy after splitting and now we'll
try to choose choose a condition that
gives us the highest gain we will do
that by splitting the data using each
condition and checking the gain that we
get out of them the conditions that give
us the highest gain will be used to make
the first split so let's take a look at
these different conditions we have color
we have diameter and if we look
underneath that we have a couple
different values we have diameter equals
3 color equals yellow red diameter
equals 1 and when we look at that you'll
see over here we have 1 two 3 four
threes that's a pretty high select ction
so let's say the condition gives us the
maximum gain of three so we have the
most pieces fall into that range so our
first split from our decision node is we
split the data based on the diameter is
it greater than or equal to three if
it's not that's false it goes into the
great bowl and if it's true it goes into
a bowl fold of lmon and apples the
entropy after splitting has decreased
considerably so now we can make two
decisions if you look at there very uh
much less chaos going on there this node
has already attained an entropy value of
zero as you can see there's only one
kind of label left for this Branch so no
further splitting is required for this
node however this node on the right is
still requires a split to decrease the
entropy further so we split the right
node further based on color if you look
at this if I split it on color that
pretty much cuts it right down the
middle it's the only thing we have left
in our choices of color and diameter too
and if the color is yellow it's going to
go to the right bowl and if it's false
it's going to go to the left Bowl so the
entropy in this case is now zero so now
we have three bowls with zero entropy
there's only one type of data in each
one of those bowls so we can predict a
lemon with 100% accuracy and we can
predict the Apple also with 100%
accuracy along with our grapes up there
so we've looked at kind of a basic tree
in our forest but what we really want to
know is how does a random Forest work as
a whole so to begin our um random Forest
classifier let's say we already have
built three trees and we're going to
start with the first tree that looks
like this just like we did in the
example this tree looks at the diameter
if it's greater than or equal to three
it's true otherwise it's false so one
side goes to the smaller diameter one
side goes to larger diameter and if the
color is orange it's going to go to the
right true we're using oranges now
instead of lemons and if it's red it's
going to go to the left false and we
build a second tree very similar but
split differently instead of the first
one being split by a diameter uh this
one when they created it if you look at
that first Bowl it has a lot of red
objects so it says is the color red
because that's going to bring our
entropy down the fastest and so of
course if it's true it goes to the left
if it's false it goes to the right and
then it looks at the shape false or true
and so on and so on and tree three is
the diameter equal to one and it came up
with this because there's a lot of
cherries in this bowl so that would be
the biggest split on there is is the
diameter equal to one that's going to
drop the entropy the quickest and as you
can see it splits it into true if it
goes false and they've added another
category does it grow in the summer and
if it's false it goes off to the left if
it's true it goes off to the right let's
go ahead and bring these three trees so
you can see them all in one image so
this would be three completely different
trees categorizing a fruit and let's
take take a fruit now let's try this and
this fruit if you look at it we've
blackened it out you can't see the color
on it so it's missing data remember one
of the things we talked about earlier is
that a random Forest works really good
if you're missing data if you're missing
pieces so this fruit has an image but
maybe a person had a black and white
camera when they took the picture and
we're going to take a look at this and
it's going to have um they put the color
in there so ignore the color down there
but the diameter equals three we find
out it grows in the summer equals yes
and the shape is a circle and if you go
to the right you can look at what one of
the decision trees did this is the third
one is a diameter greater than equal to
three is the color orange well it
doesn't really know on this one but it
if you look at the value it say true and
it go to the right tree 2 classifies it
as cherries is a color equal red is a
shape a circle true it is a circle so
this would look at it and say oh that's
a cherry and then we go to the other
classifier and it says is the diameter
equal 1 well that's false does it grow
in the summer true so it goes down and
looks at as oranges so how does this
random Forest work the first one says
it's an orange the second one said it
was a cherry and the third one says H
it's an orange and you can guess that if
you have two oranges and one says it's a
cherry uh when you add that all together
the majority of the vote says orange so
the answer is it's classified as an
orange even though we didn't know the
color and we're missing data on it I
don't I know about you but I'm getting
tired of fruit so let's switch and I did
promise you we'd start looking at a case
example and get into some python coating
today we're going to use the case the
iris flower analysis o this is the
exciting part as we roll up our sleeves
and actually look at some python coding
before we start the python coding we
need to go ahead and create a problem
statement wonder what species of Iris do
these flowers belong to let's try to
predict the species of the flowers using
machine learning in Python
let's see how it can be done so here we
begin to go ahead and Implement our
python code and you'll find that the
first half of our implementation is all
about organizing and exploring the data
coming in let's go ahead and take this
first step which is loading the
different modules into Python and let's
go ahead and put that in our favorite
editor whatever your favorite editor is
in this case I'm going to be using the
Anaconda Jupiter notebook which is one
of my favorites certainly there's
notepad Plus plus and eclipse and dozens
of others or just even using the python
terminal window any of those will work
just fine to go ahead and explore this
python coding so here we go let's go
ahead and flip over to our Jupiter
notebook and I've already opened up a
new page for Python 3 code and I'm just
going to paste this right in there and
let's take a look and see what we're
bringing into our python the first thing
we're going to do is from the sklearn
dod sets import load Iris now this isn't
the actual data this is just the module
that allows us to bring in the data the
load Iris and the iris is so popular
it's been around since
1936 when Ronald fiser published a paper
on it and they're measuring the
different parts of the flower and based
on those measurements predicting what
kind of flower it is and then if we're
going to do a random Forest classifier
we need to go ahead and import a random
forest classifier from the SK learned
module so SK learn. emble import random
force classifier and then we want to
bring in two more modules um and these
are probably the most commonly used
modules in Python and data science with
any of the um other modules that we
bring in and one is going to be pandas
so we're going to import pandas as PD PD
is the common term used for pandas and
pandas is basically creates a data
format for us where when you create a
pandas data frame it looks like an Excel
spreadsheet and you'll see that in a
minute when we start digging deeper into
the code panda is just wonderful cuz it
plays nice with all the other modules in
there and then we have numpy which is
our numbers Python and the numbers
python allows us to do different
mathematical sets on here we'll see
right off the bat we're going to take
our NP and we're going to go ahead and
seeed the randomness with it was zero so
np. random. seed is seeding that is zero
this code doesn't actually show anything
we're going to go ahead and run it
because I need to make sure I have all
those loaded and then let's take a look
at the next module on here the next six
slides including this one are all about
exploring the data remember I told you
half of this is about looking at the
data and getting it all set so let's go
ahead and take this code right here the
script and let's get that over into our
Jupiter notebook and here we go we've
gone ahead and uh run the Imports and
I'm going to paste the code down
here and let's take a look and see
what's going on the first thing we're
doing is we're actually loading the iris
data and if you remember up here we
loaded the module that tells how to get
the iris data now we're actually
assigning that data to the variable Iris
and then we're going to go ahead and use
the DF to Define data frame and that's
going to equal PD and if you remember
that's pandas as PD so that's our pandas
and Panda data frame and then we're
looking at Iris data and columns equals
Iris feature names and we're going to do
the DF head and let's run this so you
can understand what's going on
here the first thing you want to notice
is that our DF has created uh what looks
like an Excel spreadsheet and in this
Excel spreadsheet we have set the
columns so up on the top you can see the
four different columns and then we have
the data iris. dat down below it's a
little confusing without knowing where
this data is coming from so let's look
at the bigger picture and I'm going to
go print I'm just going to change this
for a moment and we're going to print
all of virus and see what that looks
like so when I print all of virus I get
this long list list of information and
you can scroll through here and see all
the different titles on there what's
important to notice is that first off
there's a brackets at the beginning so
this is a python
dictionary and in a python dictionary
you'll have a key or a label and this
label pulls up whatever information
comes after it so feature names which we
actually used over here under columns is
equal to an array of SE length seel
width petal length pedal width these are
the different names they have for the
four different columns and if you scroll
down far enough you'll also see data
down here oh goodness it came up right
towards the top and uh data is equal to
the different data we're looking
at now there's a lot of other things in
here like Target we're going to be
pulling that up in a minute and there's
also the names uh the target names which
is further down and we'll show you that
also in a minute let's go Ahad and set
that back to the head and this is one of
the neat features of pandas and Panda
data frames is when you do dfad or the
panda datf frame. head it'll print the
first five lines of the data set in
there along with the headers if you have
them in this case we have the column
headers set to Iris features and in here
you'll see that we have 0 1 2 3 4 in
Python most arrays always start at zero
so when you look at the first five it's
going to be 0 1 2 3 4 not 1 2 3 4 5 so
now we've got our Iris data imported
into a data frame let's take a look at
the next piece of code in here and so in
this section here of the code we're
going to take a look at the Target and
let's go ahead and get this into our
notebook this piece of code so we can
discuss it a little bit more in detail
so here we are in our Jupiter notebook
I'm going to put the code in here and
before I run it I want to look at a
couple things going on so we have uh DF
species and this is interesting because
right here you'll see where I have DF
species in Brackets which is uh the key
code for creating another column and
here we have iris. Target now these are
both in the pandas setup on here so in
pandas we can do either one I could have
just as easily done Iris and then in
Brackets Target depending on what I'm
working on both are um acceptable let's
go ahead and run this code and see how
this changes and what we've done is
we've added the target from the iris
data set as another column on the
end now what species is this is what
we're trying to predict so we have our
data which tells us the answer for all
these different pieces and then we've
added a column with the answer that way
when we do our final setup we'll have
the ability to program our our neural
network to look for these this different
data and know what aosa is or a verac
color which we'll see in just a minute
or virginica those are the three that
are in there and now we're going to add
one more column I know we're organizing
all this data over and over again it's
kind of fun there's a lot of ways to
organize it what's nice about putting
everything onto one data frame is I can
then do a print out and it shows me
exactly what I'm looking at and I'll
show you where you where that's
different where you can alter that and
do it slightly differently but let's go
ahead and put this into our script up to
De now and here we go we're going to put
that down here and we're going to run
that and let's talk a little bit about
what we're doing now we're exploring
data and one of the challenges is
knowing how good your model is did your
model work and to do this we need to
split the data and we split it into two
different parts they usually call it the
training and the testing and so in here
we're going to go ahead and put that in
our database so you can see it clearly
and we've set it DF remember you can put
brackets this is creating another column
is train so we're going to use part of
it for training and this equals NP
remember that stands for numpy random.
uniform so we're generating a random
number between 0 and 1 and we're going
to do it for each of the rows that's
where the length DF comes from so each
row gets a generated number and if it's
less than 75 it's true and if it's
greater than 75 it's false this means
we're going to take 75% of the data
roughly CU there's a Randomness involved
and we're going to use that to train it
and then the other 25% we're going to
hold off to the side and use that to
test it later on so let's flip back on
over and see what the next step is so
now that we've labeled our database for
which is training and which is testing
let's go ahead and sort that into two
different variables train and test and
let's take this code and let's bring it
into our project and here we go let's
paste it on down here and before I run
this let's just take a quick look at
what's going on here is we have up above
we created remember there's our def.
head which prints the first five rows
and we've added a column is train at the
end and so we're going to take that
we're going to create two variables
we're going to create two new data
frames one's called train one's called
test 75% in train 25% in test and then
to sort that out we're going to do that
by doing DF our main original data frame
with the iris data in it and if DF is
train equals true it's going to go in
the train and if DF is train equals
false it goes in the test and so when I
run this we're going to print out the
number in each one let's see what that
looks like and you'll see that it puts
118 in the training module and it puts
32 in the testing module which lets us
know that there was 150 lines of data in
here so if you went and looked at the
original data you could see that there's
150 lines and that's roughly 75% in one
and 25% for us to test our model on
afterward so let's jump back to our code
and see where this goes in the next two
steps STS we want to do one more thing
with our data and that's make it
readable to humans um I don't know about
you but I hate looking at zeros and ones
so let's start with the features and
let's go ahead and take those and make
those readable to humans and let's put
that in our
code let's see here we go paste it in
and you'll see here we've done a couple
very basic things we know that the
columns in our data frame again this is
a panda thing the DF
columns and we know the first four of
them 0 1 2 3 that'd be the first four
are going to be the features or the
titles of those columns and so when I
run this you'll see down here that it
creates an index sea length sea width
petal length and petal width and this
should be familiar because if you look
up here here's our column titles going
across and here's the first
four one thing I want you to notice here
is that when you're in a command line
with whether it's jupyter notebook or
you're running command line in the uh
terminal window if you just put the name
of it it'll print it out this is the
same as doing
print
features and the shorthand is you just
put features in here if you're actually
writing a code and saving the script and
running it by remote you really need to
put the print in there but for this when
I run it you'll see it gives me the same
thing but for this we want to go ahead
and we'll just leave it as feature cuz
it doesn't really matter and this is one
of the fun thing about Jupiter notebooks
is I'm just building the code as we go
and then we need to go ahead and create
the labels for the other part so let's
take a look and see what that for our
final step in prepping our data before
we actually start running the training
and the testing is we're going to go
ahead and convert the species on here
into something the computer understands
so let's put this code into our script
and see where that takes
us all right here we go we set y equal
to pd.
factorize train species of zero so let's
break this down just a little bit we
have our pandas right here PD factorize
what is factorize doing I'm going to
come back to that in just a second let's
look at what trained species is and why
we're looking at the group zero on there
and let's go up here and here is our
species remember this on that we created
this whole column here for species
and then it has Sosa satsa Sosa Sosa and
if you scroll down enough you'd also see
virginica and Vera color we need to
convert that into something the computer
understands zeros and ones so the
trained species of zero because this is
in the format of a of an array of arrays
so you have to have the zero on the end
and then species is just that column
factorize goes in there and looks at the
fact that there's only three of them so
when I run this you'll see that y
generates an array that's equal to in
this case it's a training set and it's
zeros ones and twos representing the
three different kinds of flowers we have
so now we have something the computer
understands and we have a nice table
that we can read and understand and now
finally we get to actually start doing
the predicting so here we go uh we have
two lines of code oh my goodness that
was a lot of work to get to two lines of
code but there is a lot in these two
lines a code so let's take a look and
see what's going on here and put this
into our full script that we're running
and let's paste this in here and let's
take a look and see what this is we have
we're creating a variable CF and we're
going to set this equal to the random
forest classifier and we're passing two
variables in here and there's a lot of
variables you can play with as far as
these two are concerned they're very
standard in jobs all that does is to
prioritize it not something to really
worry about usually when you're doing
this on your own computer you do in jobs
equals 2 if you're working in a larger
or big data and you need to prioritize
it differently this is what that number
does is it changes your priorities and
how it's going to run across the system
and things like that and then the random
state is just how it starts zero is fine
for
here but uh let's go ahead and run
this we also have cf. fit train features
comma Y and before we run it let's talk
about this a little bit more
cf. fit so we're fitting we're training
it we are actually creating our random
Forest classifier right here this is the
code that does everything and we're
going to take our training set remember
we kept our test off to the side and
we're going to take our training set
with the features and then we're going
to go ahead and put that in and here's
our Target the Y so the Y is 01 and two
that we just created and the features is
the actual data going in that we we put
into the training set let's go ahead and
run
that and this is kind of an interesting
thing because it printed out the random
force
classifier and everything around it and
so when you're running this in your
terminal window or in a script like this
this automatically treats us like just
like when we were up here and I typed in
y and I printed out y instead of print y
this does the same thing it treats this
as a variable and prints it out but if
you're actually running your codee that
wouldn't be the case and what it's
printed out is it shows us all the
different variables we can change and if
we go down here you can actually see in
jobs equals 2 you can see the random
State equals zero those are the two that
we sent in there you would really have
to dig deep to find out all these
different meanings of all these
different settings on here some of them
are self-explanatory if you kind of
think about it a little bit like Max
features is auto so all the features
that we're putting in there is just
going to automatically take all four of
them whatever we send it it'll take some
of them might have so many features
because you're processing words there
might be like 1.4 million features in
there because you're doing legal
documents and that's how many different
words are in there at that point you
probably want to limit the maximum
features that you're going to process
and leaf nodes that's the end nodes
remember we had the fruit and we're
talking about the leaf nodes like I said
there's a lot in this we're looking at a
lot of stuff here so you might have in
this case there's probably only think
three leaf nodes maybe four you might
have thousands of leaf nodes at which
point do need to put a cap on that and
say okay it can only go so far and then
we're going to use all of our resources
on processing this and that really is
what most of these are about is limiting
the process and making sure we don't uh
overwhelm a system and there's some
other settings in here again we're not
going to go over all of them warm start
equals false warm start is if you're
programming it one piece at a time
externally since we're not we're not
going to have like we're not going to
continually to train this particular
Learning Tree and again like I said
there's a lot of things in here here
that you'll want to look up more detail
from the
sklearn and if you're digging in deep
and running a major project on here for
today though all we need to do is fit or
train our features and our Target y so
now we have our training model what's
next if we're going to create a
model we now need to test it remember we
set aside the test featur test group 25%
of the data so let's go ahead and take
this code and let's put it into our uh
script and see what that looks like okay
here we go and we're going to run
this and it's going to come out with a
bunch of zeros ones and twos which
represents the three type of flowers the
satsa the virginica and the Versa color
and what we're putting into our predict
is the test features and I always kind
of like to know what it is I am looking
at so real quick we're going to do
test features and remember features is
an array
of SEO length SEO width pedal length
pedal width so when we put it in this
way it actually loads all these
different columns that we loaded into
features so if we did just features let
me just do features in here so you can
see what features looks like this is
just playing with the with Panda's data
frames you'll see that it's an index so
when you put an index in like
this into test features into test it
then takes those columns and creates a
panda data frames from those columns and
in this case we're going to go ahead and
put those into our predict so we're
going to put each one of these lines of
data the 5.0 3.4
1.5.2 and we're going to put those in
and we're going to predict what our new
um Forest classifier is going to come up
with and this is what it predicts it
predicts uh 00001 211 222 and and uh
again this is the flower type satos of
virginica and Versa color so now that
we've taken our test features let's
explore that let's see exactly what that
data means to us so the first thing we
can do with our predicts is we can
actually generate a different prediction
model when I say different we're going
to view it differently it's not that the
data itself is different so let's take
this next piece of code and put it into
our
script so we're pasting it in here and
you'll see that we're doing uh predict
and we've added underscore proba for
probability so there's our cf. predict
probability so we're we're running it
just like we ran it up here but this
time with this we're going to get a
slightly different result and we're only
going to look at the first 10 so you'll
see down here instead of looking at all
of them uh which was uh what 27 you'll
see right down here that this generates
a much larger field on the probability
and let's take a look and see what that
looks like and what that that means so
when we do the predict underscore proba
for probability it generates three
numbers so we had three leaf nodes at
the end and if you remember from all the
theory we did this is the predictors the
first one is predicting a one for Sosa
it predicts a zero for virginica and it
predicts a zero for Versa color and so
on and so on and so on and let's um you
know what I'm going to change this just
a little bit let's look at 10
to 20 just because we
can and we start to get in a little
different of data and you'll see right
down here it gets to this one this line
right here and this line has 0 0.5
0.5 and so if we're going to vote and we
have two equal votes it's going to go
with the first one so it says uh satsa
gets zero votes virginica gets .5 votes
Versa color gets 0.5 votes but let's
just go with the virginica since these
two are equal and so on and so on down
the list you can see how they vary on
here so now we've looked at both how to
do a basic predict of the features and
we've looked at the predict probability
let's see what's next on here so now we
want to go ahead and start mapping names
for the plants we want to attach names
so that it makes a little more sense for
us and that's what we're going to do in
these next two steps we're going to
start by setting up our predictions and
mapping them to the name so let's see
what that looks like and let's go ahead
and paste that code in here and run it
and this goes along with the next piece
of code so we'll skip through this
quickly and then come back to it a
little bit so here's
iris. Target
names and uh if you remember correctly
this was the the names that we've been
talking about this whole time the satsa
virginica versacolor and then we're
going to go ahead and do the prediction
again we run we could have just set a
variable equal to this instead of
rerunning it each time but we we go
ahead and run it again cf. predict test
features remember that Returns the zeros
the ones and the twos and then we're
going to set that equal to predictions
so this time we're actually putting it
in a variable and when I run
this it distributes and it comes out as
an array and the array is sosa Sosa
satsa Sosa Sosa we're only looking at
the first five we could actually do
let's do the first 25 just so we can see
a little bit more on there and you'll
see that it starts uh mapping it to all
the different flower types the Versa col
and the virginica in there and let's see
how this goes with the next one so let's
take a look at the top part of our
species in here and we'll take this code
and put it in our
script and let's put that down here and
paste it there we go and we'll go ahead
and run it and let's talk about both
these sections of code here and how they
go together the first one is our
predictions and I went ahead and did uh
predictions through 25 let's just do
five
and so we have ptosis sitosis ptosis
sitosis that's what we're predicting
from our test model and then we come
down here and we look at test species I
remember I could have just done test.
species. head and you'll see it says
Sosa Sosa Sosa Sosa and they match so
the first one is what our forest is
doing and the second one is what the
actual data is now is we need to combine
these so that we can understand what
that means we need to know how good our
forest is how good it is at predicting
the features so that's where we come up
to the next step which is lots of fun
we're going to use a single line of code
to combine our predictions and our
actuals so we have a nice chart to look
at and let's go ahead and put that in
our script in our Jupiter notebook here
let's see let's go ahead and paste that
in and then I'm going to because I'm on
the Jupiter notebook I can do a control
minus so you can see the whole line
there there we go resize it and let's
take a look and see what's going on here
we're going to create in pandas remember
PD stands for pandas and we're doing a
cross tab this function takes two sets
of data and creates a chart out of them
so when I run it you'll get a nice chart
down here and we have the predicted
species so across the top you'll see the
Sosa vers color virginica and the actual
species Sosa versacolor virginica and so
the way to read this chart and let's go
ahead and take a look on how to read
this chart here when you read this chart
you have satsa where they meet you have
verol where they meet and you have
virginica where they meet and they're
meeting where the actual and the
predicted agree so this is the number of
accurate predictions so in this case it
equals 30 if you had 13 + 5 + 12 you get
30 and then we notice here where it says
virginica but it was supposed to be
versacolor this is inaccurate so now we
have two two inaccurate predictions and
30 accurate predictions so we'll say
that the model accuracy is 93 that's
just 30 ID 32 and if we multiply by 100
we can say that it is 93% accurate so we
have a 93% accuracy with our model I did
want to add one more quick thing in here
on our scripting before we wrap it up so
let's flip back on over to my script in
here we're going to take this uh line of
code from up above I don't know if you
remember it but predicts equals the est.
Target names so we're going to map it to
the names and we're going to run the
prediction and we ran it on test
features but you know we're not just
testing it we want to actually deploy it
so at this point I would go ahead and
change this and this is an array of
arrays this is really important when
you're running these to know that so you
need the double brackets and I could
actually create data maybe let's let's
just do two flowers so maybe I'm
processing more data coming in and we'll
put two flowers in here
and then uh I actually want to see what
the answer is so let's go ahead and type
in PRS and print that out and when I run
this you'll see that I've now predicted
two flowers that maybe I measured in my
front yard as versacolor and
versacolor not surprising since I put
the same data in for each one this would
be the actual uh end product going out
to be used on data that you don't know
the answer
for so that's going to conclude our
scripting part of this if you are an
aspiring machine learning engineer
looking for online training and
certifications from prestigious
universities and in collaboration with
leading experts then search no more
simply learns professional certification
program in Ai and machine learning from
Pur University in collaboration with IVM
should be your right choice for more
details use the link in the description
box below with that in mind by now we
all know machine learning models make
predictions by learning from the past
data available so we have our input
values our machine learning model Builds
on those inputs of what we already know
and then we use that to create a
predicted output is that a dog little
kid looking over there and watching the
black cat cross their path no dear you
can differentiate between a cat and a
dog based on their
characteristics cats cats have sharp
claws uses to climb smaller length of
ears meows and purs doesn't love to play
around dogs they have dle claws bigger
length
barks loves to run around you usually
don't see a cat running around people
although I do have a cat that does that
where dogs do and we can look at these
we can say uh we can evaluate the
sharpness of the claws how sharp are
their claws and we can evaluate the
length of the ears and we can usually
sort out cats from dogs based on even
those two
characteristics now tell me if it is a
cat or a dog not question usually little
kids know cats and dogs by now unless
they live a place where there's not many
cats or dogs so if we look at the
sharpen of the claws the length of the
ears and we can see that the cat has
smaller ears and sharper claws than the
other animals its features are more like
cats it must be a cat sharp claws length
of ears and it goes in the cat group
because K Ann is based on feature
similarity we can do classification
using KNN classifier so we have our
input value the picture of the black cat
it goes into our trained model and it
predicts that this is a cat coming out
so what is is knnn what is the KNN
algorithm K nearest neighbors is what
that stands for it's one of the simplest
supervised machine learning algorithms
mostly used for classification so we
want to know is this a dog or it's not a
dog is it a cat or not a cat it
classifies a data point based on how its
neighbors are classified KNN stores all
available cases and classifies new cases
based on a similarity measure and here
we gone from cats and dogs right into
wine another favorite of mine KNN stores
all available cases and classifies new
cases based on a similarity measure and
here you see we have a measurement of
sulfur dioxide versus the chloride level
and then the different wines they've
tested and where they fall on that graph
based on how much sulfur dioxide and how
much chloride K and KN andn is a
perimeter that refers to the number of
nearest neighbors to include in the
majority of the voting process and so if
we add a new glass of wine there red or
white we want to know what the neighbors
are in this case we're going to put k
equals 5 we'll talk about K in just a
minute a data point is classified by the
majority of votes from its five nearest
neighbors here the unknown point would
be classified as red since four out of
five neighbors are red so how do we
choose K how do we know k equals five I
mean that's was the value we put in
there I said we're going to talk about
it how do we choose the factor K KN andn
algorithm is based on feature similarity
choosing the right value of K is a
process called parameter tuning and is
important for better accuracy so at k
equal 3 we can classify we have a
question mark in the middle as either a
as a square or not is it a square or is
it in this case a triangle and so if we
set k equals to three we're going to
look at the three nearest neighbors
we're going to say this is a square and
if we put k equals to 7 we classify as a
triangle depending on what the other
data is around and you can see as the K
changes depending on where that point is
that drastically changes your answer and
uh we jump here we go how do we choose
the factor of K you'll find this in all
machine learning choosing these factors
that's the face you get it's like oh my
gosh did I choose the right K did I set
it right my values in whatever machine
learning tool you're looking at so that
you don't have a huge bias in One
Direction or the other and in terms of
knnn the number of K if you choose it
too low the bias is based on it's just
too noisy it's it's right next to a
couple things and it's going to pick
those things and you might get a skewed
answer and if your K is too big then
it's going to take forever to process so
you're going to run into processing
issues and resource issues so what we do
the most common use and there's other
options for choosing K is to use the
square root of n so N is a total number
of values you have you take the square
root of it in most cases you also if
it's an even number so if you're using
uh like in this case squares and
triangles if it's even you want to make
your K value odd that helps it select
better so in other words you're not
going to have a balance between two
different factors that are equal so
usually take the square root of N and if
it's even you add one to it or subtract
one from it and that's where you get the
K value from that is the most common use
and it's pretty solid it works very well
when do we use KNN we can use KNN when
data is labeled so you need a label on
it we know we have a group of pictures
with dogs dogs cats cats data is Noise
free and so you can see here when we
have a class and we have like
underweight 140 23 Hello Kitty normal
that's pretty confusing we have a high
variety of data coming in so it's very
noisy and that would cause an issue data
set is small so we're usually working
with smaller data sets where I you might
get into gig of data if it's really
clean doesn't have a lot of noise
because KNN is a lazy learner I.E it
doesn't learn a discriminative function
from the training set so it's very lazy
so if you have very complicated data and
you have a large amount of it you're not
going to use the KNN but it's really
great to get a place to start even with
large data you can sort out a small
sample and get an idea of what that
looks like using the knnn and also just
using for smaller data sets KNN works
really good how does a KNN algorithm
work consider a data set having two
variables height in centimeters and
weight in kilograms and each point is
classified as normal or underweight so
we can see right here we have two
variables you know true false or either
normal or they're not they're
underweight on the basis of the given
data we have to classify the below set
as Norm normal or underweight using KNN
so if we have new data coming in this
says 57 kg and 177 cm is that going to
be normal or underweight to find the
nearest neighbors we'll calculate the
ukian distance according to the ukan
distance formula the distance between
two points in the plane with the
coordinates XY and ab is given by
distance D equals the square Ro T of x -
a^ 2 + y - b^ 2 and you can remember
that from the two edges of a triangle
we're Computing the third Edge since we
know the X side and the yide let's
calculate it to understand clearly so we
have our unknown point and we placed it
there in red and we have our other
points where the data is scattered
around the distance D1 is a square OT of
170 - 167 2 + 57 - 51 2ar which is about
6.7 and distance two is about 13 and
distance three is about 13.4 similarly
we will calculate the ukian distance of
unknown data point from all the points
in the data set and because we're
dealing with small amount of data that's
not that hard to do and it's actually
pretty quick for a computer and it's not
a really complicated Mass you can just
see how close is the data based on the
ukian distance hence we have calculated
the ukian distance of unknown data point
from all the points as showing where X1
and y1 equal 57 and 170 whose class we
have to classify so now we're looking at
that were saying well here's the ukian
distance who's going to be their closest
neighbors now let's calculate the
nearest neighbor at k equals 3 and we
can see the three closest neighbors puts
them at normal and that's pretty
self-evident when you look at this graph
it's pretty easy to say okay what you
know we're just voting normal normal
normal three votes for normal this is
going to be a normal weight so majority
of neighbors are pointing towards normal
hence as per KNN algorithm the class of
57170 should be normal so recap of KNN
positive integer K is specified along
with a new sample we select the K
entries in our database which are
closest to the new sample we find the
most common classification of these
entries this is the classification we
give to the new sample so as you can see
it's pretty straightforward we're just
looking for the closest things that
match what we got so let's take a look
and see what that looks like in a use
case in Python so let's dive into the
predict diabetes use case so use case
predict diabetes the objective predict
whether person will be diagnosed with
diabetes or not we have a data set of
768 people who were or were not
diagnosed with diabetes and let's go
ahead and open that file and just take a
look at that data and this is in a
simple spreadsheet format the data
itself is comma separated very common
set of data and it's also a very common
way to get the data and you can see here
we have columns a through I that's what
1 2 3 4 5 6 7 eight um eight columns
with particular attribute and then the
ninth column which is the outcome is
whether they have diabetes as a data
scientist the first thing you should be
looking at is insulin well you know if
someone has insulin they have diabetes
because that's why they're taking it and
that could cause issue on some of the
machine learning packages but for a very
basic setup this works fine for uh doing
the KNN and the next thing you notice is
it didn't take very much to open it up
um I can scroll down to the bottom of
the data there's
768 it's pretty much a small data set
you know at 76 9 I can easily fit this
into my ram on my computer I can look at
it I can manipulate it and it's not
going to really tax just a regular
desktop computer you don't even need an
Enterprise version to run a lot of this
so let's start with importing all the
tools we need and before that of course
we need to discuss what IDE I'm using
certainly you can use any particular
editor for python but I like to use for
doing uh very basic visual stuff the
Anaconda which is great for doing demos
with the Jupiter note book and just a
quick view of the Anaconda Navigator
which is the new release out there which
is really nice you can see under home I
can choose my application we're going to
be using python 36 I have a couple
different uh versions on this particular
machine if I go under environments I can
create a unique environment for each one
which is nice and there's even a little
button there where I can install
different packages so if I click on that
button and open the terminal I can then
use a simple pip install to install
different packages I'm working with
let's go ahead and go back under home
and we're going to launch our notebook
and I've already you know kind of like
uh the old cooking shows I've already
prepared a lot of my stuff so we don't
have to wait for it to launch because it
takes a few minutes for it to open up a
browser window in this case I'm going
it's going to open up Chrome because
that's my default that I use and since
the script is pre-done you'll see you
have a number of windows open up at the
top the one we're working in and uh
since we're working on the KNN predict
whether a person will have diabetes or
not let's go and put that title in there
and I'm also going to go up here and
click on sell actually we want to go
ahead and first insert a cell below and
then I'm going to go back up to the top
cell and I'm going to change the cell
type to markdown that means this is not
going to run as python it's a markdown
language so if I run this first one it
comes up in nice big letters which is
kind of nice remind us what we're
working on and by now you should be
familiar with doing all of our Imports
we're going to import the pandas as PD
import numpy is NP pandas is the pandas
data frame and numpy is a number array
very powerful tools to use in here so we
have our Imports so we've brought in our
pandas our numpy our two general python
tools and then you can see over here we
have our train test split by now youed
should be familiar with splitting the
data we want to split part of it for
training our thing and then training our
particular model and then we want to go
ahead and test the remaining data just
see how good it is pre-processing a
standard scaler pre-processor so we
don't have a bias of really large
numbers remember in the data we had like
number pregnancies isn't going to get
very large where the amount of insulin
they take and get up to 256 so 256
versus 6 that will skew results so we
want to go ahead and change that so
they're all uniform between minus one
and one and then the actual tool this is
the K neighbors classifier we're going
to use and finally the last three are
three tools to test all about testing
our model how good is it we just put
down test on there and we have our
confusion Matrix our F1 score and our
accuracy so so we have our two general
python modules we're importing and then
we have our six module specific from the
sklearn setup and then we do need to go
ahead and run this so these are actually
imported there we go and then move on to
the next step and so in this set we're
going to go ahead and load the database
we're going to use pandas remember
pandas is PD and we'll take a look at
the data in Python we looked at it in a
simple spreadsheet but usually I like to
also pull it up so that we can see what
we're doing so here's our data set
equals pd. read CSV that's a pandas
command and the diabetes folder I just
put in the same folder where my IPython
script is if you put in a different
folder You' need the full length on
there we can also do a quick length of
uh the data set that is a simple python
command Len for length we might even
let's go ahead and print that we'll go
print and if you do it on its own line
link. data set in the jupyter notebook
it'll automatically print it but when
you're in most of your different setups
you want to do the print in front of
there and then we want to take a look at
the actual data set and since we're in
pandas we can simply do data set head
and again let's go ahead and add the
print in there if you put a bunch of
these in a row you know the data set one
head data set two head it only prints
out the last one so I usually always
like to keep the print statement in
there but because most projects only use
one data frame Panda's data frame doing
it this way doesn't really matter the
other way works just fine and you can
see when we hit the Run button button we
have the 768 lines which we knew and we
have our pregnancies it's automatically
given a label on the left remember the
head only shows the first five lines so
we have zero through four and just a
quick look at the data you can see it
matches what we looked at before we have
pregnancy glucose blood pressure all the
way to age and then the outcome on the
end and we're going to do a couple
things in this next step we're going to
create a list of columns where we can't
have zero there's no Sy thing is zero
skin thickness or zero blood PR pressure
zero glucose uh any of those you'd be
dead so not a really good Factor if they
don't if they have a zero in there
because they didn't have the data and
we'll take a look at that because we're
going to start replacing that
information with a couple of different
things and let's see what that looks
like so first we create a nice list as
you can see we have the values talked
about glucose blood pressure skin
thickness uh and this is a nice way when
you're working with columns is to list
the columns you need to do some kind of
transformation on uh very common thing
to do and then for this particular setup
we certainly could use the there's some
Panda tools that will do a lot of this
where we can replace the na but we're
going to go ahead and do it as a data
set column equals data set column.
replace this is this is still pandas you
can do a direct there's also one that
that you look for your n a lot of
different options in here but the Nan
npn an is what that stands for is is non
doesn't exist so the first thing we're
doing here is replacing the zero with a
numpy none there's no data there that's
what that says that's what this is
saying right here so put the zero in and
we're going to replace zeros with no
data so if it's a zero that means the
person's well hopefully not dead
hopefully they just didn't get the data
the next thing we want to do is we're
going to create the mean which is the in
integer from the data set from the
column do mean where we skip Naas we can
do that that is a panda's command there
the skip na so we're going to figure out
the mean of that data set and then we're
going to take that data set column and
we're going to replace all the
npnn with the means why did we do that
and we could have actually just uh taken
this step and gone right down here and
just replaced zero and Skip anything
where except you could actually there's
a way to skip zeros and then just
replace all the zeros but in this case
we want to go ahead and do it this way
so you can see that we're switching this
to a non-existent value then we're going
to create the mean well this is the
average person so if we don't know what
it is they did not get the data and the
data is missing one of the tricks is you
replace it with the average what is the
most common data for that this way you
can still use the rest of those values
to do your computation and it kind of
just brings that particular value of
those missing values out of the equation
let's go ahead and take this and we'll
go ahead and run it doesn't actually do
anything so we're still preparing our
data if you want to see what that looks
like we don't have anything in the first
few lines so it's not going to show up
but we certainly could look at a r row
let's do that let's go into our data set
with printed data set and let's pick in
this case let's just do glucose and if I
run this this is going to print all the
different glucose levels going down and
we thankfully don't see anything in here
that looks like missing data at least on
the ones it shows you can see it skipped
a bunch in the middle because that's
what it does if you have too many lines
in Jupiter notebook it'll skip a few and
and go on to the next in a data set let
me go and remove this and we'll just
zero out that and of course before we do
any processing before proceeding any
further we need to split the data set
into our train and testing data that way
we have something to train it with and
something to test it on and you're going
to notice we did a little something here
with the panda database code there we go
my drawing tool we've added in this
right here off the data set and what
this says is that the first one in
pandas this is from the PD pandas it's
going to say within the data set we want
to look at the iocation and it is all
rows that's what that says so we're
going to keep all the rows but we're
only looking at zero column 0 to 8
remember column 9 here it is right up
here we printed in here as outcome well
that's not part of the training data
that's part of the answer yes it's
column 9 but it's listed as eight number
eight so 0o to 8 is nine columns so uh
eight is the value and when you see it
in here zero this is actually 0 to 7even
it doesn't include the last one and then
we go down here to Y which is our answer
and we want just the last one just
column 8 and you can do it this way with
this particular notation and then if you
remember we imported the train test
split that's part of the SK learn right
there and we simply put in our X and our
y we're going to do random State equals
zero you don't have to necessarily seat
it that's a seed number I think the
default is one when you seed it I'd have
to look that up and then the test size
test size is 0.2 that simply means we're
going to take 20% of the data and put it
aside so that we can test it later
that's all that is and again we're going
to run it not very exciting so far we
haven't had any print out other than to
look at the data but that is a lot of
this is prepping this data once you prep
it the actual lines of code are quick
and easy and we're almost there with the
actual Runing of our KNN we need to go
ahead and do a scale the data if you
remember correctly we're fitting the
data in a standard scaler which means
instead of the data being from you know
5 to 303 and one column and the next
column is 1 to six we're going to set
that all so that all the data is between
my - one and one that's what that
standard scaler does keeps it
standardized and we only want to fit the
scaler with the training set but we want
to make sure the testing set is the X
test going in is also transformed so
it's processing it the same so here we
go with our standard scaler we're going
to call it scor X for the scaler and
we're going to import the standard
scaler into this variable and then our X
train equals score x.f fit transform so
we're creating the scaler on the X train
variable and then our X test we're also
going to transform it so we've trained
and transformed the X train and then the
X test isn't part of that training it
isn't part of that of training the
Transformer it just gets transformed
that's all it does and again we're going
to go and run this and if you look at
this we've now gone through these steps
all three of them we've taken care of
replacing our zeros for key columns that
shouldn't be zero and we replace that
with the means of those columns that way
that they fit right in with our data
models we've come down here and we split
the data so now we have our test data
and our training data and then we've
taken and we scaled the data so all of
our data going in now no we don't TR we
don't train the Y part the Y train and Y
test that never has to be trained it's
only the data going in that's what we
want to train in there then Define the
model using K neighbors classifier and
fit the train data in the model so we do
all that data prep and you can see down
here we're only going to have a couple
lines of code where we're actually
building our model and training it
that's one of the cool things about
Python and how far we've come it's such
an exciting time to be in machine
learning because there's so many
automated tools let's see before we do
this let's do a quick length of and
let's do y we want let's just do length
of Y and we get 768 and if we import
math we do math.
Square t let's do y train there we go
it's actually supposed to be X train
before we do this let's go ahead and do
import math and do math square root
length of Y test and when I run that we
get
12.49 I want to see show you where this
number comes from we're about to use 12
is an even number so if you know if
you're ever voting on things remember
the neighbors all vote don't want to
have an even number of neighbors voting
so we want to do something odd and let's
just take one away we'll make it 11 let
me delete this out of here that's one of
the reasons I love Jupiter notebook
because you can flip around and do all
kinds of things on the fly so we'll go
ahead and put in our classifier we're
creating our classifier now and it's
going to be the K neighbors classifier n
neighbors equal 11 remember we did 12
minus 1 for 11 so we have an odd number
of neighbors P equals 2 because we're
looking for is it are they diabetic or
not and we're using the ukian metric
there are other means of measuring the
distance you could do like square square
means values all kinds of measure this
but the ukan is the most common one and
it works quite well it's important to
evaluate the model let's use the
confusion Matrix to do that and we're
going to use the confusion Matrix
wonderful tool and then we'll jump into
the F1 score and finally accuracy score
which is probably the most commonly used
quoted number when you go into a meeting
or something like that so let's go ahead
and past that in there and we'll set the
cm equal to confusion Matrix y test y
predict so those are the two values
we're going to put in there and let me
go ahead and run that and print it out
and the way you interpret this is you
have the Y predicted which would be your
title up here we could do uh let's just
do p r d predicted across the top and
actual going down actual it's always
hard to to write in here actual that
means that this column here down the
middle that's the important column and
it means that our prediction said 94
and prediction and the actual agreed on
94 and 32 this number here the 13 and
the 15 those are what was wrong so you
could have like three different if
you're looking at this across three
different variables instead of just two
you'd end up with the third row down
here and the column going down the
middle so in the first case we have the
the and I believe the zero is a 94
people who don't have diabetes the
prediction said that 13 of those people
did have diabetes and were at high risk
and and the 32 that had diabetes it had
correct but our prediction said another
15 out of that 15 it classified as
incorrect so you can see where that
classification comes in and how that
works on the confusion Matrix then we're
going to go ahead and print the F1 score
let me just run that and you see we get
a 69 in our F1 score the F1 takes into
account both sides of the balance of
false positive
where if we go ahead and just do the
accuracy account and that's what most
people think of is it looks at just how
many we got right out of how many we got
wrong so a lot of people when you're a
data scientist and you're talking to
other data scientists they're going to
ask you what the F1 score the F score is
if you're talking to the general public
or the U decision makers in the business
they're going to ask what the accuracy
is and the accuracy is always better
than the F1 score but the F1 score is
more telling it lets us know that
there's more false positives than we
would like on here but 82% not too bad
for a quick flash look at people's
different statistics and running an
sklearn and running the KNN the K
nearest neighbor on it so we have
created a model using KNN which can
predict whether a person will have
diabetes or not or at the very least
whether they should go get a checkup and
have their glucose checked regularly or
not the print accurac score we got the 0
818 was pretty close to what we got and
we can pretty much round that off and
just say we have an accuracy of 80%
tells us that is's a pretty fair fit in
the model if you are an aspiring machine
learning engineer looking for online
training and certifications from
prestigious universities and in
collaboration with leading experts then
search no more simply learn professional
certification program in Ai and machine
learning from Pur University in
collaboration with IBM should be your
right choice for more details use the
link in the description box below with
that in mind and find out why support
Vector machine so in this example
example last week my son and I visited a
fruit shop dad is that an apple or a
strawberry so the question comes up what
fruit did I just pick up from the Fruit
Stand after a couple of seconds you
could figure out that it was a
strawberry so let's take this model a
step further and let's uh why not build
a model which can predict an unknown
data and in this we're going to be
looking at some sweet strawberries or
crispy apples we wanted to be able to
label those two and decide what the
fruit is and we do that by having data
already put in so we already have a
bunch of strawberries we know our
strawberries and they're already labeled
as such we already have a bunch of
apples we know our apples and are
labeled as such then once we train our
model that model then can be given the
new data and the new data is this image
in this case you can see a question mark
on it and it comes through and goes it's
a strawberry in this case we're using
the support Vector Machine model svm is
a supervised learning method that looks
at data and sort it into one of two
categories and in this case we're
sorting the strawberry into the
strawberry side at this point you should
be asking the question how does the
prediction work before we dig into an
example with numbers let's apply this to
our fruit scenario we have our support
Vector machine we've taken it and we'
taken labeled sample of data
strawberries and apples and we draw on a
line down the middle between the two
groups this split now allows us to take
new data in this case an apple and a
strawberry and place them in the
appropriate group based on which side of
the line they fall in and that way we
can predict the unknown as colorful and
tasty as the fruit example is let's take
a look at another example with some
numbers involved and we can take a
closer look at how the math Works in
this example we're going to be
classifying men and women and we're
going to start with a set of people with
a different height and a different
weight and to make this work we'll have
to have a sample data set of female
where you have their height and weight
174 65 17488 and so on and we'll need a
sample data set of the male they have a
height 179 90 180 to 80 and and so on
let's go ahead and put this on a graph
so you have a nice visual so you can see
here we have two groups based on the
height versus the weight and on the left
side we're going to have the women on
the right side we're going to have the
men now if we're going to create a
classifier let's add a new data point
and figure out if it's male or female so
before we can do that we need to split
our data first we can split our data by
choosing any of these lines in this case
we drawing two lines through the data in
the middle that separates the men from
the women but to predict the gend of a
new data point we should split the data
in the best possible way and we say the
best possible way because this line has
a maximum space that separates the two
classes here you can see there's a clear
split between the two different classes
and in this one there's not so much a
clear split this doesn't have the
maximum space that separates the two
that is why this line best splits the
data we don't want to just do this by
eyeballing it and before we go further
we need to add some technical terms to
this we can also say that the distance
between the points and the line should
be as far as possible in technical terms
we can say the distance between the
support vector and the hyperplane should
be as far as possible and this is where
the support vectors are the extreme
points in the data set and if you look
at this data set they have circled two
points which seem to be right on the
outskirts of the women and one on the
outskirts of the men and hyperplane has
a maximum distance to the support
vectors of any class now you'll see the
line down the middle and we call this
the hyperplane because when you're
dealing with multiple Dimensions it's
really not just a line but a plane of
intersections and you can see here where
the support vectors have been drawn in
dash lines the math behind this is very
simple we take D+ the shortest distance
to the closest positive point which
would be on the Min side and D minus is
the shortest distance to the closest
negative point which is on the women's
side the sum of D+ and D minus is called
the distance margin or the distance
between the two support vectors that are
shown in the dash lines and then by
finding the largest distance margin we
can get the optimal hyperplane once
we've created an optimal hyperplane we
can easily see which side the new data
fits in and based on the hyperplane we
can say the new data point belongs to
the male gender hopefully that's clear
how that works on a visual level as a
data scientist you should also be asking
what happens if the hyper plane is not
optimal if we select a hyperplane having
low margin then there is a high chance
of
misclassification this particular svm
model the one we discussed so far is
also called referred to as the
lsvm so far so clear but a question
should be coming up we have our sample
data set but instead of looking like
this what if it look like this where we
have two sets of data but one of them
occurs in the middle of another set you
can see here where we have the blue and
the yellow and then blue again on the
other side of our data line in this data
set we can't use use a hyper plane so
when you see data like this it's
necessary to move away from a 1D view of
the data to a two-dimensional view of
the data and for the transformation we
use what's called a kernel function the
kernel function will take the 1D input
and transfer it to a two-dimensional
output as you can see in this picture
here the 1D when transferred to a
two-dimensional makes it very easy to
draw a line between the two data sets
what if we make it even more complicated
how do we perform an svm for this type
of data set here you can see we have a
two-dimensional data set where the data
is in the middle surrounded by the green
data on the outside in this case we're
going to segregate the two classes we
have our sample data set and if you draw
a line through it's obviously not an
optimal hyperplane in there so to do
that we need to transfer the 2D to a 3D
array and when you translate it into a
three-dimensional array using the kernel
you can see where you can place a hyper
plane right through it and easily split
the data before we start looking at a
programming example and dive into the
script let's look at the advantage of
the support Vector machine we'll start
with high dimensional input space or
sometimes referred to as the curse of
dimensionality we looked at earlier on
Dimension two Dimension three dimension
when you get to a thousand dimensions a
lot of problems start occurring with
most algorithms that have to be adjusted
for the svm automatically does that in
high dimensional space one of the high
dimensional space one high dimensional
space that we work on is sparse document
vector this is where we tokenize the
words and documents so we can run our
machine learning algorithms over them
I've seen ones get as high as 2.4
million different tokens that's a lot of
vectors to look at and finally we have
regularization parameter the
regularization parameter or Lambda is a
parameter that helps figure out whether
we're going to have a bias or
overfitting of the data whether it's
going to be overfitted to very specific
instance or it's going to be biased to a
high or low value with the svm it
naturally avoids the overfitting and
bias problems that we see in many other
algorithms these three advantages of the
support Vector machine make it a very
powerful tool to add to your repertoire
of machine learning tools now we did
promise you a used case study we're
actually going to dive into some Python
Programming and so we're going to go
into a problem statement and start off
with the zoo so in the zoo example we
have um family members going to the zoo
and we have the young child going dad is
that a group of crocodiles or alligators
well that's hard to differentiate and
zoos are a great place to start looking
at science and understanding how things
work especially as a young child and so
we can see the parents sitting here
thinking well what is the difference
between a crocodile and an alligator
well one crocodiles are larger in size
alligators are smaller in size snout
width the crocodiles have a narrow snout
and alligators have a wider snout and of
course in the modern day and age the
father sitting here is thinking how can
I turn this into a lesson for my son and
he goes let us support Vector machine
segregate the two groups I don't know if
my dad ever told me that but that would
be funny now in this example we're not
going to use actual measurements and
data we're just using that for imagery
and that's very common in a lot of
machine learning algorithms and setting
them up but let's roll up our sleeves
and we'll talk about that more in just a
moment as we break into our python
script so here we arrive in our actual
coding and I'm going to move this into a
python editor in just a moment but let's
talk a little bit about what we're going
to cover first we're going to cover in
the code the setup how to actually
create our svm and you're going to find
that there's only two lines of code that
actually create it and the rest of it is
done so quick and fast that it's all
here in the first page and we'll show
you what that looks like as far as our
data because we're going to create some
data I talked about creating data just a
minute ago and so we'll get into the
creating data here and you'll see this
nice correction of our two blobs and
we'll go through that in just a second
and then the second part is we're going
to take this and we're going to bump it
up a notch we're going to show you what
it looks like behind the scenes but
let's start with actually creating our
setup I like to use the Anaconda Jupiter
notebook because it's very easy to use
but you can use any of your favorite
python editors or setups and go in there
but let's go ahead and switch over there
and see what that looks like so here we
are in the Anaconda python notebook or
anaconda Jupiter notebook with python
we're using Python 3 I believe this is
3.5 but it should be work in any of your
3x versions and uh you'd have to look at
the sklearn and make sure if you're
using a 2X version an earlier version
let's go and put our code in there and
one of the things I like about the
Jupiter notebook is up to view and I'm
going to go ahead and toggle the line
numbers on to make it a little bit
easier to talk about and we can even
increase the size CU this is edited in
in this case I'm using Google Chrome
Explorer and that's how it opens up for
the editor although anyone any like I
said any editor will work now the first
step is going to be our Imports and
we're going to import four different
parts the first two I want you to look
at are line one and line two are numpy
as NP and matplot library. pyplot as PLT
now these are very standardized Imports
when you're doing work the first one is
the numbers python we need that because
part of the platform we're using uses
app for the numpy array and I'll talk
about that in a minute so you can
understand why we want to use a numpy
array versus the standard python array
and normally it's pretty standard setup
to use NP for numpy the map plot library
is how we're going to view our data so
this has uh you do need the NP for the
SK learn module but the map plot library
is purely for our use for VIs
visualization and so you really don't
need that for the svm but we're going to
put it there so you have a nice visual
aid and we can show you what it looks
like that's really important at the end
when you finish everything so you have a
nice display for everybody to look at
and then finally we're going to I'm
going to jump one ahead to line number
four that's the SK learn. dat sets.
samples generator import make blobs and
I told you that we were going to make up
data and this is a tool that's in the SK
learn and makeup data I personally don't
want to go to the zoo get in trouble for
jumping over the fence and probably get
eaten by the crocodiles or alligators as
I work on measuring their snouts and
width and length instead we're just
going to make up some data and that's
what that make blobs is It's a Wonderful
tool if you're ready to test your your
uh setup and you're not sure about what
data you're going to put in there you
can create this blob and it makes it
real easy to use and finally we have our
actual svm the sklearn import svm on
line three so that covers all our
Imports we're going to create remember I
used the make blobs to create data and
we're going to create a capital x and a
lowercase y equals make blobs in samples
equals 40 so we're going to make 40
lines of data it's going to have two
centers with a random State equals 20 so
each each each group is going to have 20
different pieces of data in it and the
way that looks is that we'll have under
X um an XY plane so I have two numbers
under X and Y will be zero or one that's
the two different centers so we have yes
or no in this case alligator or
crocodile that's what that represents
and then I told you that the actual SK
learner the svm is in two lines of code
and we see it right here with clf equals
svm SVC kernel equals linear and I set c
equal to one although in this example
since we are not uh regularizing the
data because we want it to be very clear
and easy to see I went ahead you can set
it to a thousand a lot of times when
you're not doing that but for this thing
linear because it's a very simple linear
example we only have the two dimensions
and it'll be a nice linear hyper plane
will'll be a nice linear line instead of
a full plane so we're not dealing with a
huge amount of data and then all we have
to do is do cf. fit X comma Y and that's
it clf has been created and then we're
going to go ahead and display it and I'm
going to talk about this display here in
just a second but let me go ahead and
run this code and this is what we've
done is we've created two blobs you'll
see the blue on the side and then kind
of an orangish uh on the other side
that's our two sets of data they
represent one represents crocodiles and
one represents alligators and then we
have our measurements in this case we
have like the width and length of the
snout and I did say I was going to come
up here and talk just a little bit about
our plot and you'll see PLT that's what
we imported we're going to do a scatter
plot that means we're just putting dots
on there and then look at this notation
I have the capital x and then in
brackets I have a colon comma zero
that's from numpy if you did that in a
regular array you'll get an error in a
python array you have to have that in a
numpy array it turns out that our make
blobs returns a numpy array and this
notation is great because what it means
is the first part is the colon means
we're going to do all the rows that's
all the data in our blob we created
under capital x and then the second part
has a comma zero we're only going to
take the first value and then if you
notice we do the same thing but we're
going to take the second value remember
we always start with zero and then one
so we have column zero and column one
and you can look at this as our XY plots
the first one is the X plot and the
second one is the Y plot so the first
one is on the bottom 0 2 4 6 8 and 10
and then the second one X of the one is
the four 5 6 7 8 9 10 going up the left
hand side s equal 30 is just the size of
the dot so we can see them instead real
tiny dots and then cmap equals plt.com
paired and you'll also see the C equals
y That's the color we're using two
colors 01 and that's why we get the nice
blue and the two different colors for
the alligator and the cross Cod now you
can see here that we did this the actual
fit was done in two lines of code a lot
of times there'll be a third line where
we regularize the data we set it between
like minus one and one and we reshape it
but for this it's not necessary and it's
also kind of nice because you can
actually see what's going on and then if
we wanted to we wanted to actually run a
prediction let's take a look and see
what that looks like and to predict some
new data and we'll show this again as we
get towards the end of digging in deep
you can simply assign your new data and
in this case I am giving it a uh width
and length 34 and a width and length 56
and note that I put the data as a set of
brackets and then I have the brackets
inside and the reason I do that is
because when we're looking at data it's
designed to process a large amount of
data coming in we don't want to just
process one line at a time and so in
this case I'm processing two lines and
then I'm just going to print and you'll
see cf. predict new data so the clf and
the predict part is going to give us an
answer and let's see what that looks
like and you'll see 01 so predicted the
first one the 34 is going to be on the
one side and the 56 is going to be on
the other side so one came out as a
alligator and one came out as a
crocodile now that's pretty short
explanation for the setup but really we
want to dig in and see what going on
behind the scenes and let's see what
that looks like so the next step is to
dig in deep and find out what's going on
behind the scenes and also put that in a
nice pretty graph we're going to spend
more work work on this and we did
actually generating the original model
and you'll see here that we go through a
few steps and I'll move this over to our
editor in just a second we come in we
create our original data it's exactly
identical to the first part and I'll
explain why we redid that and show you
how not to redo that and then we're
going to go in there and add in those
lines we're going to see what those
lines look like and how to set those up
and finally we're going to plot all that
on here and show it and you'll get a
nice graph with the what we saw earlier
when we were going through the theory
behind this where it shows the support
vectors and the hyper plane and those
are done where you can see the support
vectors as the dash lines and the solid
line which is the hyperplane let's get
that into our Jupiter notebook before I
scroll down to a new line I want you to
notice line 13 it has Plot show and
we're going to talk about that here in
just a second but let's scroll down to a
new line down here and I'm going to
paste that code in and you'll see that
the plot show has moved down below let's
scroll up a little bit and if you look
at the top here of our new Section 1 two
3 and four is the same code we had
before and let's go back up here and
take a look at that we're going to fit
the values on our svm and then we're
going to plot scatter it and then we're
going to do a plot show if you are an
aspiring machine learning engineer
looking for online training and
certifications from prestigious
universities and in collaboration with
leading experts then search no more
simply learns professional certification
program in Ai and machine learning from
P University in collaboration with IVM
should be your right choice for more
details use the link in the description
box below with that in mind what is
naive Bays let's start with a basic
introduction to the Baye theorem named
after Thomas baze from the 1700s who
first coin this in the western
literature naive Bay classifier works on
the principle of conditional probability
as given by the bay theorem before we
move ahead let us go through some of the
simple Concepts and the probability that
we will be using let us consider the
following example of tossing two coins
here we have two quarters and if we look
at all the different possibilities of
what they can come up as we get that
they can come up as head heads they come
up as head ta tell head and tell tell
when doing the math on probability we
usually denote probability as a p a
capital P so the probability of getting
two heads equals 1/4 you can see in our
data set we have two heads and this curs
once out of the four possibilities and
then the probability of at least one
tail occurs 3/4 of the time you'll see
on three of the coin tosses we have
tails in them and out of four that's
3/4s and then the probability of the
second coin being head given the first
coin is tail is 1/2 and the probability
of getting two heads given the first
coin is a head is 1/2 we'll demonstrate
that in just a minute and show you how
that math works now when we're doing it
with two coins it's easy to see but when
you have something more complex you can
see where these Pro these formulas
really come in and work so the Baye
theorem gives us the conditional
probability of an event a given another
event B has occurred in this case the
first coin toss will be B and the second
coin toss a this can be confusing
because we've actually reversed the
order of them and go from B to a instead
of a to B you'll see this a lot when you
work in probabilities the reason is
we're looking for event a we want to
know what that is so we're going to
label that a since that's our focus and
then given another event B has occurred
in the Baye theorem as you can see on
the left the probability of a occurring
given B has occurred equals the
probability of B occurring given a has
occurred times the probability of a over
the probability of B this simple formula
can be moved around just like any
algebra formula and we could do the
probability of a after a given B times
probability of b equals the probability
of B given a times probability of a you
can easily move that around and multiply
it and divide it out let us apply base
theorem to our example here we have our
two quarters and we'll notice that the
first two probabilities of getting two
heads and at least one tail we compute
directly off the data so you can easily
see that we have one example HH out of
four 1/4 and we have three with TS in
them giving us 3/4 or 34
75% the second condition the second uh
set three and four we're going to
explore a little bit more in detail now
we stick to a simple example with two
coins because you can easily understand
the math the probability of throwing a
tail doesn't matter what comes before it
and the same with the head so still
going to be 50% or 1/2 but when that
come when that probability gets more
complicated let's see you have a D6 dice
or some other instance then this formula
really comes in handy but let's stick to
the simple example for now in this
sample space let a be the event that the
second coin is had and B be the event
that the first coin is Tails again we
reversed it because we want to know what
the second event is going to be so we're
going to be focusing on a and we write
that out as a probability of a given B
and we know this from our formula that
that equals the prob probability of B
given a times the probability of a over
the probability of B and when we plug
that in we plug in the probability of
the first coin being Tails given the
second coin is heads and the probability
of the second coin being heads given the
first coin being over the probability of
the first coin being Tails when we plug
that data in and we have the probability
of the first coin being Tails given the
second coin is heads times the
probability of the second coin being
heads over the probability of the first
coin being tails you can see it's a
simple formula to calculate we have 1 12
* 1/2 over 1/2 or 1 12 = .5 or 1/4 so
the base theorem basically calculates
the conditional probability of the
occurrence of an event based on prior
knowledge of conditions that might be
related to the event we will explore
this in detail when we take up an
example of online shopping further in
this tutorial understanding naive bays
and machine learning like with any of
our other machine learning tools it's
important to understand where the naive
Bas fits in the hierarchy so under the
machine learning we have supervised
learning and there is other things like
unsupervised learning there's also
reward system This falls under the
supervised learning and then under the
superi learning there's classification
there's also regression but we're going
to be in the classification side and
then under classification is your naive
Bay let's go ahead and glance into where
is naive Bay used let's look at some of
the Ed scenarios for it as a classifier
we use it in face recognition is this
Cindy or is it not Cindy or whoever or
it might be used to identify parts of
the face that they then feed into
another part of the face recognition
program this is the eye this is the nose
this is the mouth weather prediction is
it going to be rainy or sunny medical
recognition news prediction it's also
used in medical diagnosis we might
diagnose somebody is either as high risk
or not as high risk for cancer or heart
disease or other ailments and news
classification you look at the Google
news and it says well is this political
or is this world news or a lot of that's
all done with the naive Baye
understanding naive Baye classifier now
we already went through a basic
understanding with the coins and the two
heads and two tails and head tail tail
heads Etc we're going to do just a quick
review on that and remind you that the
ni Bay classifier is based on the Baye
theorem which gives a conditional
probability of in event a given event B
and that's where the probability of a
given b equals the probability of B
given a Time probability of a over
probability of B remember this is an
algebraic function so we can move these
different entities around we can
multiply by the probability of B so it
goes to the left hand side and then we
could divide by the probability of a
given B and just as easily come up with
a new formula for the probability of B
to me staring at these algebraic
functions kind of gives me a slight
headache it's a lot better to see if we
can actually understand how this data
fits together in a table and let's go
ahead and start applying it to some
actual data so you can see what that
looks like so we're going to start with
the shopping demo problem statement and
remember we're going to solve this first
in uh table form so you can see what the
math looks like and then we're going to
solve it in Python and in here we want
to predict whether the person will
purchase a product are they going to buy
or don't buy very important if you're
running a business you want to know how
to maximize your profits or at least
maximize the purchase of the people
coming into your store and we're going
to look at a specific combination of
different variables in this case we're
going to look at the day the discount
and the free delivery and you can see
here under the day we want to know
whether it's uh on the weekday you know
somebody's working they come in after
work or maybe they don't work weekend
you can see the bright colors coming
down there celebrating not being in work
or holiday and did we offer a discount
that day yes or no did we offer free
delivery that day yes or no and from
this we want to know whether the
person's going to buy based on these
traits so we can maximize them and find
out the best system for getting somebody
to come in and purchase our goods and
products from our store now having a
nice visual is great but we do need to
dig into the data so let's go ahead head
and take a look at the data set we have
a small sample data set of 30 rows we're
showing you the first 15 of those rows
for this demo now the actual data file
you can request just type in below under
the comments on the YouTube video and
we'll send you some more information and
send you that file as you can see here
the file is very simple columns and rows
we have the day the discount the free
delivery and did the person purchase or
not and then we have under the day
whether it was a weekday a holiday was
it the weekend this is a pretty simple
set of data and long before computers
people used to look at this data and
calculate this all by hand so let's go
ahead and walk through this and see what
that looks like when we put that into
tables also note in today's world we're
not usually looking at three different
variables in 30 rows nowadays because
we're able to collect data so much we're
usually looking at 27 30 variables
across hundreds of rows the first thing
we want to do is we're going to take
this data and uh based on the data set
containing our three inputs dat discount
and free delivery we're going to go
ahead and populate that to frequency
tables for each attribute so we want to
know if they had a discount how many
people buy and did not buy uh did they
have a discount yes or no do we have a
free delivery yes or no on those dates
how many people made a purchase how many
people didn't and the same with the
three days of the week was it a weekday
a weekend a holiday and did they buy yes
or no as we dig in deeper to this table
for our Baye theorem let the event buy
ba a now remember when we looked at the
coins I said really want to know what
the outcome is did the person buy or not
and that's usually event a is what
you're looking for and the independent
variables discount free delivery and day
BB so we'll call that probability of B
now let us calculate the likelihood
table for one of the variables let's
start with day which includes weekday
weekend and holiday and let us start by
summing all of our rows so we have the
uh weekday row and out of the weekdays
there's 9 plus 2 so is 11 weekdays
there's eight weekend days and and 11
holidays wow it's a lot of holidays and
then we want to sum up the total number
of days so we're looking at a total of
30 days let's start pulling some
information from our chart and see where
that takes us and when we fill in the
chart on the right you can see that 9
out of 24 purchases are made on the
weekday 7 out of 24 purchases on the
weekend and eight out of 24 purchases on
a holiday and out of all the people who
come in 24 out of 30 purchase you can
also see how many people do not purchase
on the week dat two out of six didn't
purchase and so on and so on we can also
look at the totals and you'll see on the
right we put together some of the
formulas the probability of making a
purchase on the weekend comes out 11 out
of 30 so out of the 30 people who came
into the store throughout the weekend
weekday and holiday 11 of those
purchases were made on the weekday and
then you can also see the probability of
them not making a purchase and this is
done for doesn't matter which day of the
week so we call that probability of no
buy would be 6 over 30 or02 so there's a
20% chance that they're not going to
make a purchase no matter what day of
the week it is and finally we look at
the probability of BF a in this case
we're going to look at the probability
of the weekday and not buying two of the
no buys were done out of the weekend out
of the six people who did not make
purchases so when we look at that
probability of the week day without a
purchase is going to be 33 or
33% let's take a look at this at
different probabilities and uh based on
this likelihood table let's go ahead
calculate conditional probabilities as
below the first three we just did the
probability of making a purchase on the
weekday is 11 out of 30 or roughly 36 or
37% 367 the probability of not making a
purchase at all doesn't matter what day
of the week is roughly 02 or 20% and the
probability of a weekday no purchase is
roughly two out of six so two out of six
of our no purchases were made on the
weekday and then finally we take our P
of a B if you looked we've kept the
symbols up there we got P of probability
of B probability of a probability of B
if a we should remember that the
probability of a if B is equal to the
first one times the probability of no
per buys over the probability of the
weekday so we could calculate it both
off the uh table we created we can also
calculate this by the formula and we get
the 367 which equals or 33 * 2 over 367
which equals. 179 or roughly uh 17 to
18% and that'd be the probability of no
purchase done on the weekday and this is
important because we can look at this
and say as the probability of buying on
the weekday is more than the probability
of not buying on the weekday we can
conclude that customers will most likely
buy the product on a weekday now we've
kept our chart simple and we're only
looking at one aspect so you should be
able to look at the table and come up
with the same information or the same
conclusion that should be kind of
intuitive at this point next we can take
the same setup we have the frequency
tables of all three independent
variables now we can construct the
likelihood tables for all three of the
variables we're working with we can take
our day like we did before we have
weekday weekend and holiday we filled in
this table and then we can come in and
also do that for the discount yes or no
did they buy yes or no and we fill in
that full table so now we have our
probabilities for a discount and whether
the disc account leads to a purchase or
not and the probability for free
delivery does that lead to a purchase or
not and this is where it starts getting
really exciting let us use these three
likelihood tables to calculate whether a
customer will purchase a product on a
specific combination of Day discount and
free delivery or not purchase here let
us take a combination of these factors
day equals holiday discount equals yes
free delivery equals yes let's dig
deeper into the math and actually see
what this looks like and we're going to
start start with looking for the
probability of them not purchasing on
the following combinations of days we're
actually looking for the probability of
a equal no buy no purchase and our
probability of B we're going to set
equal to is it a holiday did they get a
discount yes and was it a free delivery
yes before we go further let's look at
the original equation the probability of
a if B equals the probability of B given
the condition a and the probability
times probability of a over the
probability of B occurring now this is
basic algebra so we can multiply this
information together so when you see the
probability of a given B in this case
the condition is b c and d or the three
different variables we're looking at and
when you see the probability of B that
would be the conditions we're actually
going to multiply those three separate
conditions out probability of you'll see
that in just a second in the formula
times the full probability of a over the
full probability of B so here we are
back to this and we're going to have let
a equal no purchase and we're looking
for the probability of B on the
condition a where a sets for three
different things remember that equals
the probability of a given the condition
B and in this case we just multiply
those three different variables together
so we have the probability of the
discount times the probability of free
delivery times the probability is the
day equal a holiday those are our three
variables of the probability of a if B
and then that is going to be multiplied
by the probability of them not making a
purchase and then we want to divide that
by the total probabilities and they're
multiplied together so we have the
probability of a discount the
probability of a free delivery and the
probability of it being on a holiday
when we plug those numbers in we see
that one out of six were no purchase on
a discounted day two out of six were a
no purchase on a free delivery day and
three out of six were a no no purchase
on a holiday those are our three
probabilities of a of B multiplied out
and then that has to be multiplied by
the probability of a no purchase and
remember the prop probability of a no
buy is across all the data so that's
where we get the 6 out of 30 we divide
that out by the probability of each
category over the total number so we get
the 20 out of 30 had a discount 23 out
of 30 had a yes for free delivery and 11
out of 30 were on a holiday we plug all
those numbers in we get
178 so in our probability math we have
a178 if it's a no buy for a holiday a
discount and a free delivery let's turn
that around and see what that looks like
if we have a purchase I promise this is
the last page of math before we dig into
the python script so here we're
calculating the probability of the
purchase using the same math we did to
find out if they didn't buy now we want
to know if they did buy and again we're
going to go by the day equals a holiday
discount equals yes free pre delivery
equals yes and let a equal bu now right
about now you might be asking why are we
doing both calculations why why would we
want to know the no buys and buys for
the same data going in well we're going
to show you that in just a moment but we
have to have both of those pieces of
information so that we can figure it out
as a percentage as opposed to a
probability equation and we'll get to
that normalization here in just a moment
let's go ahead and walk through this
calculation and as you can see here the
probability of a on the condition of b b
being all three categories did we have a
discount with a purchase did we have a
free delivery with a purchase and did we
is a day equal to Holiday and when we
plug this all into that formula and
multiply it all out we get our
probability of a discount probability of
a free delivery probability of the day
being a holiday times the overall
probability of it being a purchase
divided by again multiplying the three
variables out the full probability of
there being a discount the full
probability of being a free delivery
and the full probability of there being
a day equal holiday and that's where we
get this 19 over 24 * 21 over 24 * 8
over 24 times the P of a 24 over 30
divided by the probability of the
discount the free delivery times the day
or 20 over 30 23 over 30 * 11 over 30
and that gives us our
986 so what are we going to do with
these two pieces of data we just
generated well let's go ahead and go
over them we have a probability of
purchase equals 986 we have a
probability of no purchase equals 178 so
finally we have a conditional
probabilities of purchase on this day
let us take that we're going to
normalize it and we're going to take
these probabilities and turn them into
percentages this is simply done by
taking the sum of probabilities which
equals
98686 plus. 178 and that equals the
1.64 if we divide each probability by
the sum we get the percentage and so the
likelihood of a purchase is
84.7% and the likelihood of no purchase
is
15.29% given these three different
variables so it's if it's on a holiday
if it's a with a discount and has free
delivery then there's an 84.7 1% chance
that the customer is going to come in
and make a purchase hooray they
purchased our stuff we're making money
if you're were owning a shop that's like
is the bottom line is you want to make
some money so you can keep your shop
open and have a living now I promised
you that we were going to be finishing
up the math here with a few pages so
we're going to move on and we're going
to do two steps the first step is I want
you to understand why you want to why
you want to use the naive Bays what are
the advantages of naive bays and then
once we understand those advantages we
just look at that briefly then we're
going to dive in and do some python
coding advantages of naive Baye
classifier so let's take a look at the
six advantages of the naive Bay
classifier and we're going to walk
around this this lovely wheel looks like
an origami folded paper the first one is
very simple and easy to implement
certainly you could walk through the
tables and do this by hand you got to be
a little careful because the notations
can get confusing you have all these
different probabilities and I certainly
mess those up as I put them on you know
is it on the top or the bottom got to
really pay close attention to that when
you put it into python it's really nice
because you don't have to worry about
any of that you let the python handle
that the python module but understanding
it you can put it on a table and you can
easily see how it works and it's a
simple algebraic function it needs less
training data so if you have smaller
amounts of data this is great powerful
tool for that handles both continuous
and discret data it's highly scalable
with number of predictors and data
points so as you can see you just keep
multiplying different probabilities in
there and you can cover not just three
different variables or sets you can now
expand this to even more categories
number five it's fast it can be used in
realtime predictions this is so
important this is why it's used in a lot
of our predictions on online shopping
carts uh referrals spam filters is
because there's no time delay as it has
to go through and figure out a neural
network or one of the other mini setups
where you're doing classification and
certainly there's a lot of other tools
out there in the machine learning that
can handle these but most of them are
not as fast as the naive bays and then
finally it's not sensitive to irrelevant
features so it picks up on your
different probabilities and if you're
short on date on one probability you can
kind of it automatically adjust for that
those formulas are very automatic and so
you can still get a very solid
predictability even if you're missing
data or you have overlapping data for
two completely different areas we see
that a lot in doing census and studying
of people and habits where they might
have one study that covers one aspect
and another one that overlaps and
because the two overlap they can then
predict the unknowns for the group that
they haven't done the second study on or
vice versa so it's very powerful in that
it is not sensitive to the irrelevant
features and in fact you can use it to
help predict features that aren't even
in there so now we're down to my
favorite part we're going to roll up our
sleeves and do some actual programming
we're going to do the use case text
classification now I would challenge you
to go back and send us a note on the
notes below underneath the video and
request the data for the shopping cart
so you can plug that into python code
and do that on your own time so you can
walk through it since we walk through
all the information on it but we're
going to do a python code doing text
classification very popular for doing
the naive Bays so we're going to use our
new tool to perform a text
classification of news headlines and
classify news into different topics for
a News website as you can see here we
have a nice image of the Google news and
then related on the right subgroups I'm
not sure where they actually pulled the
actual data we're going to use from it's
one of the standard sets but certainly
this can be used on any of our news
headlines and classification so let's
see how it can be done using using the
naive Bay classifier now we're at my
favorite part we're actually going to
write some python script roll up our
sleeves and we're going to start by
doing our Imports these are very basic
Imports including our news group and
we'll take a quick glance at the Target
names then we're going to go ahead and
start training our data set and putting
it together we'll put together a nice
graph because it's always good to have a
graph to show what's going on and once
we've trade it and we've shown you a
graph of what's going on then we're
going to explore how to use it and see
what that looks like now I'm going to
open up my favorite editor or inline
editor for python you don't have to use
this you can use whatever your editor
that you like whatever uh interface IDE
you want this just happens to be the
Anaconda Jupiter notebook and I'm going
to paste that first piece of code in
here so we can walk through it let's
make it a little bigger on the screen so
you have a nice view of what's going on
uh and we're using Python 3 in this case
3.5 so this would work in any of your 3x
if you have it set up correctly should
also work in a lot of the 2x you just
have to make make sure all the the
versions of the modules match your
python version and in here you'll notice
the first line is your percentage met
plot library in line now three of these
lines of code are all about plotting the
graph this one let's The Notebook notes
and this is inline setup that we want
the graphs to show up on this page
without it in a notebook like this which
is an Explorer interface it won't show
up now a lot of Ides don't require that
a lot of them like on if I'm working on
one of my other setups it just has a pop
up and the graph pops up on there so you
have a that setup also but for this we
want the matte plot library in line and
then we're going to import numpy as NP
that's number python which has a lot of
different formulas in it that we use for
both of our sklearn module and we also
use it for any of the upper math
functions in Python and it's very common
to see that is NP nump as NP the next
two lines are all about our graphing
remember I said three of these were
about graphing well we need our matplot
library. pyplot as PLT and you'll see
that PLT is a very common setup as is
the SNS and just like the NP and we're
going to import caborn as SNS and we're
going to do the SNS do set now caborn
sits on top of P plot and it just makes
a really nice heat map it's really good
for heat maps and if you're not familiar
with heat maps that just means we give
it a color scale the term comes from the
brighter red it is the hotter it is in
some form of data and you can set it to
whatever you want and we'll see that
later on so those you'll see that those
three lines of code here are just
importing the graph function so we can
graph it and as a data scientist you
always want to graph your data and have
some kind of visual it's really hard
just to shove numbers in front of people
and they look at it and it doesn't mean
anything and then from the SK learn. dat
sets we're going to import the fetch 20
news groups very common one for
analyzing tokenizing words and setting
them up and exploring how the words work
and how do you categorize different
things when you're dealing with
documents and then we set our data equal
to fetch 20 news groups so our data
variable will have the data in it and
we're going to go ahead and just print
the target names data. Target names and
let's see what that looks like and
you'll see here we have alt atheism comp
Graphics comp osms windows.
miscellaneous and it goes all the way
down to talk politics. miscellaneous
talk religion. miscellaneous these are
the categories they've already assigned
to this news group and it's called Fetch
20 because you'll see there's I believe
there's 20 different topics in here or
20 different categories as we scroll
down now we've gone through the 20
different categories and we're going to
go ahead and start defining all the
categories and set up our data so we're
actually here going to go ahead and get
it get the data all set up and take a
look at our data and let's move this
over to our Jupiter notebook and let's
see what this code does first we're
going to set our categories now if you
noticed up here I could have just as
easily set this equal to to data. Target
names because it's the same thing but we
want to kind of spell it out for you so
you can see the different categories it
kind of makes it more visual so you can
see what your data is looking like in
the background once we've created the
categories we're going to open up a
train set so this training set of data
is going to go into fetch 20 news groups
and it's a subset in there called train
and categories equals categories so
we're pulling out those categories that
match and then if you have a train set
you should also have the testing we have
test equals fetch 20 news groups subset
equals test and categories equals
categories let's go down one SI so it
all fits on my screen there we go and
just so we can really see what's going
on let's see what happens when we print
out one part of that data so it creates
train and under train it creates train.
dat and we're just going to look at data
piece number five and let's go ahead and
run that and see what that looks like
and you can see when I print train. dat
number five under train it prints out
one of the Articles this is article
number five you can go through and read
it on there and we can also go in here
and change this to test which should
look identical because it's splitting
the date up into different groups train
and test and we'll see test number five
is a a different article but another
article in here and maybe you're curious
and you want to see just how many
articles are in here we could do length
of train. data and if we run that you'll
see that the training data has
11,314 articles so we're not going to go
through all those articles that's a lot
of articles but um we can look at one of
them just you can see what kind of
information is coming out of it and what
we're looking at and we'll just look at
number five for today and here we have
it rewarding the Second Amendment IDs VT
line 58 lines 58 in article uh Etc and
you can scroll all the way down and see
all the different parts to there now
we've looked at it and that's pretty
complicated when you look at one of
these articles to try to figure out how
do you wait this if you look down here
we have different words and maybe the
word from well from is probably in all
the Articles so it's not going to have a
lot of meaning as far as trying to
figure out whether this article fits one
of the categories or not so trying to
figure out which category it fits in
based on these words is where the
challenge comes in now that we've viewed
our data we're going to dive in and do
the actual predictions this is the
actual naive base and we're going to
throw another model at you or another
module at you here in just a second we
can't go into too much detail but it
deals specifically working with words
and text and what they call tokenizing
those words so let's take this code and
let's uh skip on over to our Jupiter
notebook and walk through it and here we
are in our jupyter notebook let's paste
that in there and I can run this code
right off the bat it's not actually
going to display anything yet but it has
a lot going on in here so the top we had
the print module from the earlier one I
didn't know why that was in there so
we're going to start by importing our
necessary packages and from the sklearn
features extraction. Tech next we're
going to import TF IDF vectorizer I told
you we're going to throw a module at you
we can't go too much into the math
behind this or how it works you can look
it up the notation for the math is
usually tf.idf and that's just a way of
weighing the words and it weighs the
words based on how many times they're
used in a document how many times or how
many documents they're used in and it's
a well-used formula it's been around for
a while it's a little confusing to put
this in here uh but let's let it know
that it just goes in there and waits the
different words in the document for us
that way we don't have to wait and if
you put a weight on it if you remember I
was talking about that up here earlier
if these are all emails they probably
all have the word from in them from
probably has a very low weight it has
very little value and telling you what
this document's about same with words
like in in article in articles in cost
of un maybe cost might or where words
like criminal weapons destruction these
might have a heavier weight because you
describe a little bit more what the
article is doing well how do you figure
out all those weights in the different
articles that's what this module does
that's what the tfidf vectorizer is
going to do for us and then we're going
to import our SK learn. naive Bas and
that's our multinomial NB multinomial
naive Bas pretty easy to understand that
where that comes from and then finally
we have the sky learn pipeline import
make pipeline now the make pipeline is
just a cool piece of code because we're
going to take the information we get
from the tfidf vectorizer and we're
going to pump that into the multinomial
in B so a pipeline is just a way of
organizing how things flow it's used
commonly you probably already guessed
what it is if you've done any businesses
they talk about the sales pipeline if
you're on a work crew or project manager
you have your pipeline of information is
going through or your projects and what
has to be done in what order that's all
this pipeline is we're going to take the
tfid vectorizer and then we're going to
push that into the multinomial NB now
we've designated that as the variable
model we have our pipeline model and
we're going to take that model and this
is just so elegant this is done in just
a couple lines of code model.fit and
we're going to fit the data and first
the train data and then the train Target
now the train data has the different
articles in it you can see the one we
were just looking at and the train.
Target is what category they already
categorized that that particular article
as and what's Happening Here is the
train data is going into the tfid
vectorizer so when you have one of these
articles it goes in there it weights all
the words in there so there's thousands
of words with different weights on them
I remember once running a model on this
and I literally had 2.4 million tokens
go into this so when you're dealing like
large document bases you can have a huge
number of different words and then takes
those words gives them a weight and then
based on that weight based on the words
and the weights and then puts that into
the multinomial in B and once we go into
our naive Bay we want to put the train
Target in there so the train data that's
been mapped to the tfid vectorizer is
now going through the multinomial in B
and then we're telling it well these are
the answers these are the answers to the
different documents so this document
that has all these words with these
different weights from the first part is
going to be whatever category it comes
out of maybe it's the um Talk show or
the article on religion miscellaneous
once we fit that model we can then take
labels and we're going to set that equal
to model. predict most of the sklearn
use the term predict to let us know that
we've now trained the model and now we
want to get some answers and we're going
to put our test data in there because
our test data is the stuff we held off
to the side we didn't train it on there
and we don't know what's going to come
up out of it and we just want to find
out how good our labels are do they
match what they should be now I've
already run this through there's no
actual output to it to show this is just
setting it all up this is just trending
our model creating the labels so we can
see how good it is and then we move on
to the next step to find out what
happened to do this we're going to go
ahead and create a confusion Matrix and
a heat map so the confusion Matrix which
is confusing just by its very name it's
basically going to ask how confused is
our answer did it get it correct or did
it Miss some things in there or have
some missed labels and then we're going
to put that on a heat map so we have
some nice colors to look at to see how
that plots out let's go ahead and take
this code and see how that uh take a
walk through it and see what that looks
like so back to our Jupiter notebook
going to put the code in there and let's
go ahead and run that code take it just
a moment and remember we had the inline
that way my graph shows up on the inline
here and let's walk through the code and
then we'll look look at this and see
what that means so make it a little bit
bigger there we go no reason not to use
a whole screen too big so we have here
from sklearn metrics import confusion
Matrix and that's just going to generate
a set of data that says I the prediction
was such the actual truth was either
agreed with it or is something different
and it's going to add up those numbers
so we can take a look and just see how
well it worked and we're going to set a
variable mat equal to confusion Matrix
we have our test Target our test data
that was not part of the training very
important in data science we always keep
our test data separate otherwise it's
not a valid model if we can't properly
test it with new data and this is the
labels we created from that test data
these are the ones that we predict it's
going to be so we go in and we create
our SN heat map the SNS is our caborn
which sits on top of the PIP plot so
when we create a SNS do heat map we take
our confusion Matrix and it's going to
be uh matt. T and do we have other
variables that go into the SNS do heat
map we're not going to go into detail
what all the variables mean The
annotation equals true that's what tells
it to put the numbers here so you have
the 166 the 1 the 0 0 01 format d and c
bar equals false have to do with the uh
format if you take those out you'll see
that some things disappear and then the
X tick labels and the Y tick labels
those are our Target names and you can
see right here that's the alt atheism
comp Graphics comp osms windows.
miscellaneous and then finally we have
our PLT dox label remember the SNS or
the caborn sits on top of our map plot
Library our PLT and so we want to just
tell it X label equals a true is is true
the labels are true and then the Y label
is prediction label so when we say a
true this is what it actually is and the
prediction is what we predicted and
let's look at this graph CU that's
probably a little confusing the way we
rattled through it and what I'm going to
do is I'm going to go ahead and flip
back to the slides because they have a
black background they put in there that
helps it shine a little bit better so
you can see the graph a little bit
easier so in reading this graph what we
want to look at is how the color scheme
has come out and you'll see a line right
down the middle diagonally from upper
left to bottom right what that is is if
you look at the labels we have our
predicted label on the left and our true
label on the right those are the numbers
where the predict
and the true come together and this is
what we want to see is we want to see
those lit up that's what that heat map
does as you can see that it did a good
job of finding those data and you'll
notice that there's a couple of red
spots on there where it missed you know
it's a little confused when we talk
about talk religion miscellaneous versus
talk politics miscellaneous social
religion Christian versus Alt atheism it
mislabeled some of those and those are
very similar topics so you could
understand why it might mislabel them
but overall it did a pretty good job if
we're going to create these models we
want to go ahead and be able to use them
so let's see what that looks like to do
this let's go ahead and create a
definition a function to run and we're
going to call this function let me just
expand that just a notch here there we
go I like mining big letters predict
category we want to predict the category
we're going to send it s a string and
then we're sending it train equals train
we have our training model and then we
had our pipeline model equals model this
way we don't have to resend these
variables each time the definition knows
that because I said train equals train
and I put the equal for model and then
we're going to set the prediction equal
to the model. predicts so it's going to
send whatever string we send to it it's
going to push that string through the
pipeline the model pipeline it's going
to go through and uh tokenize it and put
it through the TF IDF convert that into
numbers and weights for all the
different documents and words and then
it'll put that through our naive Bay and
from it we'll go ahead and get our
prediction we're going to predict what
value it is and so we're going to return
train. Target names predict of zero and
remember that the train. target names
that's just categories I could have just
as easily put uh categories in their
predict of zero so we're taking the
prediction which is a number and we're
converting it to an actual category
we're converting it from um I don't know
what the actual numbers are let's say 0
equals alt atheism so we're going to
convert that zero to the word or uh one
maybe it equals comp Graphics so we're
going to convert number one into comp
Graphics that's all that is and then we
got to go ahead and and then we need to
go ahead and run this so I load that up
and then once I run that we can start
doing some predictions I'm going go
ahead and type in predict category and
let's just do predict category Jesus
Christ and it comes back and says it's
social religion Christian that's pretty
good now note I didn't put print on this
one of the nice things about the Jupiter
notebook editor and a lot of inline
editors is if you just put the name of
the variable out as returning the
variable train. Target names it'll
automatically print that for you in your
own ID you might have to put in print
let's see where else we can take this
and maybe you're a space science buff so
how about sending load to
International Space
Station and if we run that we get
science
space or maybe you're a uh automobile
buff and let's do um oh they were going
to tell me Audi is better than BMW but
I'm going to do BMW is better than an
Audi so maybe you're car buff and we run
that and you'll see it says recreational
I'm assuming that's what recc stands for
Autos so I did a pretty good job
labeling that one how about uh if we
have something like a caption running
through there president of India and if
we run that it comes up and says talk
politics
miscellaneous so when we take our
definition or our function and we run
all these things through Kudos we made
it we were able to correctly classify
texts into different groups based on
which category they belong to using the
naive base classifier now we did throw
in the pipeline the TF IDF vectorizer we
threw in the graphs those are all things
that you don't necessarily have to know
to understand the naive Bay setup or
classifier but they're important to know
one of the main uses for the naive Bays
is with the tfidf tokenizer or
vectorizer where tokenizes the word and
adds labels and we use the pipeline
because you need to push all that data
through and it makes it really easy and
fast you don't have to know those to
understand naive Bays but they certainly
help for understanding the industry in
data science and we can see our
categorizer or naive Bay classifier we
were able to predict the category
religion space motorcycles Autos
politics and properly classify all these
different things we pushed into our
prediction and our train model if you
want to become an eii expert and gain
handsome salary packages look at the
wide range of eiml courses by simply
learn in collaboration with top
universities across the globe you will
catch the eyes of top Recruiters in the
industry go a little deeper into
hierarchial clustering let's consider we
have a set of cars and we have a group
similar we want to group similar ones
together so So Below we have you'll see
four different cars down there and uh we
get two clusters of car types sedan and
SUV so if you're just looking at it you
can probably think oh yeah we'll put the
sedans together and the SUVs together
and then at last we can group everything
into one cluster so we just have just
cars so you can see as we have this we
make a nice little tree this is very
common when you see anybody talks about
hierarchial clustering this is usually
what you see and what comes out of it we
terminate when we are left with only one
cluster so we have as you can see we
bring them all together we have one
cluster we can't bring it together
anymore because they're all together
hierarchial clustering is separating
data into different groups based on some
measure of similarity so we have to find
a way to measure what makes them alike
what makes them different a glomera
clustering is known as bottom up
approach remember I said think of that
as bringing things together you see I
think the Latin term aglo is together
because you have your glomerate rocks
where all the different pieces of rocks
are in there so we want to bring
everything together that's a bottom up
and then divisive is we're going to go
from the top down so we take one huge
cluster and we start dividing it up into
two clusters into three four five and so
on digging even deeper into how
hierarchial clustering Works let's
consider we have a few points on a plane
so this plane is 2D so we have an XY
coordinates kind of makes it easy we're
going to start with measure the distance
so we want to figure a way to compute
the distance between each point each
data point is a cluster of its own
remember if we're going from the bottom
up a glomera then we have each point
being its own cluster we try to find the
least distance between two data points
to form a cluster and then once we find
those with the least distance between
them we start grouping them together so
we start forming clusters of multiple
points this is represented in a tree
like structure called dendogram so
there's another key word dendogram and
you can see it is it's just the branch
we've looked at before and we do the
second group the same so it gets its own
uh dendogram and the third gets its own
dendogram and then we might group two
groups together so now those two groups
are all under one dendogram because
they're closer together than the P1 and
P2 and then we terminate when we are
left with one cluster so we finally
bring it all together you can see on the
right how we've come up all the way up
to the top whoops and we have the gray
hierarchial box coming in there and
connecting them so we have just one
cluster and that's a good place to
terminate because there is no way we can
bring them together any further so how
do we measure the distance between the
data points I mean this is really where
it starts getting interesting up until
now you can kind of eyeball and say hey
these look together but when you have
thousands of data points how are we
going to measure those distances and
there is a lot of ways to get the
distance measure so let's go a and take
a look at that distance measures will
determine the similarity between two
elements and it will influence the shape
of the Clusters and we have ukian
distance measure we have square ukian
distance measure which is almost the
same thing but with less computations
and we have the Manhattan distance
measure which will give you slightly
different results and we have the cosine
distance measure which again is very
similar to the ukian playing with
triangles and sometimes it can compute
faster depending on what kind of data
you're looking at so let's start with
the citan distance measure the most
common is we want to know the distance
between the two points so if we have
Point p and point Q the E cidian
distance is the ordinary straight line
it is the distance between the two two
points in ukian space and you should
recognize D equals in this case we're
going to sum all the points so if there
was more than one point we could figure
out the distance to the not more than
one points this is the sum of more than
two Dimensions so we can have the
distance between each of the different
dimensions squared and that will give us
and then take the square root of that
and that gives us the actual distance
between them and this should look
familiar from ukian geometry maybe you
haven't played too much with multiple
Dimensions so this summation symbol
might not look familiar to you but it's
pretty straightforward as you add the
distance between each of the two
different points squared so if your y
difference was 2 - 1 squared would be
two and then you take the difference
between the X again squared and if there
was a z coordinates it would be you know
Z1 minus Z2 squared and then take the
square root of that and sum it all or
sum it all together and then take the
square root of it so to make it compute
faster since the difference in distances
whether one is is farther apart or
closer together than the other we can do
What's called the squared ukian distance
measurement this is identical to the
ukian measurement but we don't take the
square root at the end there's no reason
to certainly gives us the exact distance
but as far as doing calculations as to
which one's bigger or smaller than the
other one it won't make a difference so
we'll just go with the so we just get
rid of that Divi that final square root
computes faster and it gives us the
pretty much ukian squared distance on
there now the Manhattan distance
measurement is a simple sum of
horizontal and vertical components or
the distance between two points measured
along axes at right angles now this is
different because you're not looking at
the direct line between them and in
certain cases the individual distances
measured will give you a better result
now generally that's not true most times
you go with ukian squared method because
that's very fast and easy to see but the
Manhattan distance is you measure just
the Y value and you take the absolute
value Val of it and you measure just the
X difference you take the absolute value
of it and just the Z and if you had more
you know different dimensions in there a
b c d e f however many dimensions you
would just take the absolute value of
the difference of those dimensions and
then we have the cosine distance
similarity measures the angle between
the two vectors and as you can see as
the two vectors get further and further
apart the cosine distance gets larger so
it's another way to measure the distance
very similar to the ukan so you're still
looking at the same kind of measurement
so should have a similar result as the
first two but keep in mind the Manhattan
will have a very different result and
you can end up with a bias with the
Manhattan if your data is very skewed if
one set of values is very large and
another set of values is very small but
that's a little bit beyond the scope of
this it's just important to know that
about the Manhattan distance so let's
dig into the agglomerative clustering
and agglomerative clustering begins with
each element as a separate cluster and
then we merge them into a larger cluster
how do we represent a cluster of more
than one point so we're going to kind of
mix the distance together with the
actual agglomerative and see what that
looks like and we're actually going to
have three key questions that are going
to be answered here so how do we
represent a cluster of more than one
point so we want to look at the math
what it looks like mathematically and
geometrically how do we determine The
Nearness of clusters when to stop
combining clusters always important to
have your computer script or your
whatever you're working on have a
termination point so it's not going on
eternally we've all done that if you do
any kind of computer programming or or
writing of script let's assume that we
have six data points in a ukian space so
again we're dualing with X Y and Z in
this case just X and Y so how do we
represent a cluster of more than one
point let's take a look at that and
first we're going to make use of
centroids very common terminology in a
lot of machine machine learning
languages when we're grouping things
together so we're going to make use of
Centro which is the average of its
points and you can see here we're going
to take the one two and the 21 and we're
going to group them together because
they're close and if we were looking at
all the points we'd look for those that
are closest and start with those and
we're going to take those two we're
going to compute a point in the middle
and we'll give that point5 1.5 1.5 and
that's going to be the centroid of those
two points and next we start measuring
like another group of points we got 4150
when they're pretty close together so
we'll go ahead and set up a centroid of
those two points in this case it would
be the
4.5 and 05 would be the measurements on
those two points and once we have the
centroid of the two groups we find out
that the next closest point to a
centroid is over on the left and so
we're going to take this and say oh 0 0
is closest to the 1.5 1.5 centr so let's
go ahead and group that together and we
compute a a new centroid based on those
three points so now we have a centroid
of 1.1 or one comma 1 and then we also
do this again with the last point the 53
and it computes into the first group and
you can see our dagram on the right is
growing so we have each of these points
are become connected and we start
grouping them together and finally we
get a centroid of that group too and
then finally the last thing we want to
do is combine the two groups by their
centroids and you can see here we end up
with one large group and it'll have its
own centroid although usually they don't
compute the last centroid we just put
them all together so when do we stop
combining clusters well hopefully it's
pretty obvious to you in this case when
they all got to be one but there are
actually many approaches to it so first
pick a number of clusters K up front and
this is done in the fact that we don't
want to look at 200 in clusters we only
want to look at the top five clusters or
something like that so we decide the
number of clusters required in the
beginning and we terminate when we reach
the value K so so if you looked back on
our clustering let me just go back a
couple screens you'll see how we
clustered these all together and we
might want just the two clusters and so
we look at just the top two or maybe we
only want three clusters and so we would
compute which one of these has a wider
spread to it or something like that
there's other computations to know how
to connect them and we'll look at that
in just a minute but to note that when
we pick the K value we want to limit the
information that's coming in so that can
be very important especially if you're
feeding it into another algorithm that
requires three values or you set it to
four values and you need to know that
value coming in so we might take the
clustering and say okay only three
clusters that's all we want for K so the
possible challenges this only makes
sense when we know the data well so when
you're clustering with K clusters you
might already know that domain and know
that that makes sense but if you're
exploring brand new data you might have
no idea how many clusters you really
need to explore that data with let's
consider the value of K to be two so in
this case in our previous example we
stop when we are left with two clusters
and you can see here that this is where
they came together the best while still
keeping separate the data the second
approach is stop when the next merge
would create a cluster with low cohesion
so we keep clustering till the next
merge of clusters creates a bad cluster
low cohesion setup on there that means
the point is so close to being between
two clusters it doesn't make sense to
bring them together but how is cohesion
defined oh let's dig a little deeper
into cohesion the diameter of a cluster
so we're looking at the actual diameter
of our cluster and the diameter is the
maximum distance between any pair of
points in the cluster we terminate when
the diameter of a new cluster exceeds
the threshold so as that diameter gets
bigger and bigger we don't want the two
circles or clusters to overlap and we
have radius of a cluster radius is the
maximum distance of a point from
centroid we terminate when the radius of
a new cluster exceeds the threshold
again we're not we don't want things to
over overlaps when it crosses that
threshold and it's overlapping with
other data we stop so let's look at
divisive clustering remember we went
from the bottom up now we want to go
from the top down divisive clustering
approach begins with a whole set and
proceeds to divide it into smaller
clusters so we start with a single
cluster composed of all the data points
we split it into different clusters this
can be done using monothetic divisive
methods what is monothetic divisive
method and we'll go backwards and let's
consider the example we took in the
agglomerative clustering to understand
this so we consider a space with six
points in it just like we did before
same points we had before and we name
each point in the cluster so we have in
this case we just gave it a letter value
a b CDE e f since we follow top down
approach in divisive clustering obtain
all possible splits into two columns so
we want to know where you could split it
here and we could do like a Ab split and
a CDE EF split
we could do BCE ADF and you can see this
starts generating a huge amount of data
ABCDEF and so for each split we can
compute cluster sum of squares and we
can see here the actually have the
formula out for us B J12 = N1 of
absolute value of xus absolute value of
x^ SAR so again we're Computing all the
different distances in there squared
back to your kind of ukian distances on
that and so we can actually compute B AJ
between clusters one and two and we have
the mean of the cluster and the grand
mean depending on number of members in
the cluster and we select the cluster
with the largest sum of squares let's
assume that the sum of squared distance
is largest for the third split we had up
above and that's where we split the ABC
out and if we split the ABC out we're
left with the DF on the other side we
again find the sum of square distances
and split it into clusters so we go from
ABC we might find that the a splits into
BC and D into EF and again you start to
see that hierarchial dendogram coming
down as we start splitting everything
apart and finally we might have a splits
in b and c and then each one gets their
own DF and it continues to divide until
we get little nodes at the end and every
data has its own point or until we get
to K if we have set a k value so we've
kind of learned a little bit about the
background and some of the math in
hierarchial clustering let's go ahead
and dive into a demo and and our demo
today is going to be for the problem
statement we're going to look at us oil
so a US oil organization needs to know
it cells in various States in us and
cluster of the states based on the sales
so what are the steps involved into
setting this problem up so the steps
we're going to look at and this is
really useful for just about any
processing of data although I believe
we're going to be doing this in R today
we're going to import the data set so
we'll explore our data a little bit
there create a scatter plot it's it's
always good to have a visual if you can
once you have a visual you can know if
you're really far off in the model you
choose to Cluster the data in or how
many splits you need and then we're
going to normalize the data so we're
going to fix the data up so it processes
correctly we'll talk more detail about
normalization when we get there and then
calculate the ukian distance and finally
we'll create our dagram so it looks nice
and pretty and we have something we can
show to our shareholders so that they
have something to go on and know why
they gave us all that money and salary
for the year so we go ahead and open up
R and we're actually using R Studio
which is the really has some nice
features in it it automatically sets up
the three Windows where we have our
script file on the upper left and then
we can execute that script and it'll
come down and put it into the console
bottom left and execute it and then we
have our plots off to the right and I've
got it zoomed in hopefully not too large
a font but large enough that you can see
it and let's just go ahead and take a
look at some of the script going in
here it's clustering analysis and we're
going to work we'll call it my data and
we're going to assign it in R this is a
symbol for assigning and we're going to
go read
CSV read CSV
file and we'll put that in Brackets and
let's before we go any further let's
just look at the data outside of R it's
always nice to do if you
can and the file is going to be called
called utilities. CSV this would also be
the time to get the full path so you
have the right path to your file and
remember that you can always Post in the
comments down
below and when you post down there just
let us know you want to connect up with
simply learn so that they can get you
this file so you can get the same file
we're working on and you can repeat it
and see how this works this is
utilities. CSV it's a comma separated
variable file so it's pretty
straightforward and you can see here
they have the city fixed charge and a
number of different features to the data
and so we have our Ro cost load demand
cells nuclear fuel cost on here and then
going down the other side we have US
cities Arizona Boston they have Central
us so I guess they're grouping a number
of areas together the Commonwealth area
you can see down here Nevada New England
Northern us Oklahoma the Pacific region
and so on so we have a nice little chart
of different data they brought in
and so I'm going to take that complete
path that ends in the utilities.
ccsv and we're going to import that file
let me just enlarge this all the way
oops I had an extra set of brackets here
somehow maybe I missed a set of
brackets this can happen if you're not
careful you can get brackets on one side
and not the other or in this case I got
double brackets on each side there we go
and then the magic hot keys in this case
are your controll enter which will let
me go ahead and run the script and so
I've now loaded the data and as you can
see I went ahead and shrunk the plot
since we're going to be looking at the
window down below and we can simply
convert the data to a string now all of
us do this automatically the first time
we say hey just print it all out as a
string and then we get this huge mess of
stuff that doesn't make a whole lot of
sense so and you can see here they have
uh you can probably kind of pull it
together as looking at at it but let's
go ahead and just do the head I we'll do
the head of my
data there we go and control enter on
that and the head shows the first five
rows you'll see this in a lot of
different scripts in R it's you type in
head and then in Brackets you put your
data and it comes through and list the
first five rows as you can see below and
it shows Arizona Boston Central and it
has the same columns we just looked at
so we have the fixed charge the RO the
cost the load the
D demand I'm not an expert in oil so I'm
not even sure what D demand is cells I'm
guessing nuclear how much of it supplied
by nuclear and the actual fuel
cost and then the different states that
it's in or different areas and one of
the wonders of R is all these cool easy
to ous tools that are so quick so we'll
do
Pairs and pairs creates a nice graph so
let me go ahead and run this uh whoops
the reason it gave me an error is
because I forgot to resize it so let me
bring my plot way out so we can see it
and let's run that again and you'll see
here that we have a nice graph of the
different data and how it plots together
how the different points kind of come
together this is neat because if you
look at this I would look at this and
say hey this is a great candidate for
some kind of clustering and the reason
is is when I look at any any two pairs
let's go down to say cells and fuel cost
towards the bottom right and when you
look at them where they cross over you
sort of see things how they group
together but it's a little confusing you
can't really pinpoint how they group
together you could probably look at
these two and say yeah there's pretty
good commonalities there and if you look
at any of the other pairs you'll start
seeing some patterns there also and so
we really want to know what are the
patterns on all of them put together not
just any two of them but the whole setup
let me go ahead and Shrink my um this
down for just a second just a notch here
and let's create a uh scatter plot oops
and this is simply just use the term
plot in Brackets and then which values
do we want to plot and if we remember
when we looked at the data earlier let
me just go back this
way in this case let's go ahead and
compare two just two values to see how
they
look
and we'll do fuel cost and cells and
it's in my data so we got to let her
know which two columns we're looking at
next to each other and it will open up
our plot thing and then go ahead and
execute that and we can see on those
closeup of what we were just looking at
in the
pairs and if I was eyeballing this I
would say oh look there's a kind of a
cluster up here of five items and this
one it's hard to tell which cluster it'd
be with but maybe it's six you go in the
top one and you have a middle CL cluster
and a bottom cluster maybe two different
clusters so you can sort of group them
together fuel cost and the cells and see
how they connect with each other and
again that's only two different values
we're looking at so in the long run we
want to look at all of them and then the
people in the back they sent me the
script so we can go ahead and add labels
so with my data text fuel cost cells the
labels equals
City position four these are numbers you
can kind of play with till they look
nice
and oops again I forgot to resize my
plot it doesn't like having it too small
we'll run
that I mistype something in
here oh I did a lowercase D and they did
a capital D there we go so now we can go
in here and do this with my data and you
can see a little hard to see on my
screen with all the the different things
in there it plots the actual cities so
we can now see where all the cities are
in connection with in this case fuel
cost and cells so you have a nice label
to go with the
graph and then we can also go ahead and
plot in this case let's do oh the r o r
oops and we'll do
that also with the cells and do my data
remember to leave it lowercase this time
so when we plot those two we'll come
over here and it's going to I'm
surprised it didn't give me an
error and then we'll also add in the U
with statement so we put some nice
labels in
there and it's going to be the same as
before except instead of doing the fuel
cost cells we want the r cells and we'll
execute
that oops and of course it gives me an
error because I shrunk it down so let's
redo those two
again there we go and we can see now we
have the RO with cells and we'd probably
get a slightly different clustering here
if I was looking at these cities they're
probably different than what we had
before but you could probably look at
this and say h these kind of go together
and those kind of go together but again
we're going to be looking at all of them
instead of just one or
two and so at this point we want to dive
into the next step we're going to start
looking at a little bit of coding or
scripting
here this is very important because
we're going to be looking at
normalization we put that in there
normalization and if you've done any of
the other machine learning skills and
setup this should start to look very
normal in your pre-processing of data
whether you're in r or python or any of
these scripts we really want to make
sure you normalize your data so it's not
biased remember we're dealing with
distances and if I have let's say the RO
is even look at this graph here on the
right you can see where my RO varies
between 8 and 14 that's a very small
variable and our cells varies between
4,000 and
16,000 so you can imagine the distance
between 4,000 and 8,000 which is the
distance of 4,000 versus 8 to 10 versus
two the cells is going to dominate so if
we do any kind of special work on this
it's going to look at cells and it's
going to Cluster them just by the cells
alone and then Ro might have a little
tiny effect of two versus 4,000 we want
to level the playing
field turns out there's actually a
number of ways in script to
normalize so I'm just going to put in
the code that they put together in the
back for me and let's talk about it a
little bit so we have Z we're going to
sign it to my
data and it's go ahead
and we're going to do a little reshaping
across all
rows or I mean across all columns each
of the rows is going to have a little
reshaping there and then we're going to
get M which stands for means and we're
going to apply it to my data so again we
want to go ahead and create a um the
most common variable in
there and then s is going to be SD
stands for standard
deviation so instead of just doing a lot
of times what they do with normalization
of data is we just reshape the data
everything between zero and one so that
if the lower end is eight that now
becomes zero and the upper end is 14
that now becomes
one that doesn't help if it is not a
linear set of data so with this we're
going to look for the means and the
standard deviation for reshaping the
data and that way the most common values
now become the kind of like the center
point and then the standard deviation is
how big the spread so we want the
standard deviation to be equal amongst
all of them and then finally we go and
take Z and with the Z we're going to
reassign it and we're going to scale the
original my data which we re shaped
based on M and based on the standard
deviation and the two in here that just
means we're looking at everything in
kind of a XY kind of
plot and we can quickly run these contrl
enter contrl enter contrl enter contrl
enter so so now we have Z which is a
scaled version of my
data and now we can go ahead and
calculate the ukan
distance oops CC
you there we
go and in um R this is so easy once
you've got to here we've done all that
pre- dat
processing we'll call it
distance and we'll assign this
to dist so
D is the computation for getting the
ukian distance and we can just put Z in
there because we've already reformatted
and scaled Z to fit what we want let me
go ahead and just hit enter on that and
I'm going to widen my left hand side
again I'm always curious what does this
data look like so let's just type in
distance which will print the variable
down below oops have to hit control
enter and this prints out a huge amount
of
information as you can see just kind of
streams down
there and let's go ahead and enlarge
this and I don't know about you but when
I look at something like this it doesn't
mean a whole lot to me other than I see
2 3 4 5 six and then you kind of have
the top part 6 17 18 so I imagine this
is like a huge chart is what we're
looking at
and we can go ahead and use print oh
distance
[Applause]
digits equal three and let's run
that oops I keep forgetting that it has
to go through the graph on the right and
we see a different slightly different
output in here let me just open this up
so we can see what we're looking at and
by cutting down the distance you can
start to see the patterns here of it's
looking at the different distances uh so
if I go to the top we have the distance
between one and two one and three one
and four one and five one and six and
then two and three and so on obviously
distance between itself is zero and it
doesn't repeat the data so we don't care
to see two versus one again because we
already know the distance between one
and
two and so we have a nice view of all
the distances in the chart and that's
what we're looking at right here and
it's a little easier to read that's why
we did the print statement up here to do
digits equals three make it a little bit
smaller we could even just do digits oh
let's just do two see what that looks
like we might lose some data on this one
if it's uh if something's way off but we
have a nice setup and we can see the
different distances and that's what we
computed here between each of the
points and then the whole reason we're
doing this is to get ourselves a nice
dagram going a nice clustering dagram
we'll do a couple of these looking at
different
things we'll take a variable HC DOL and
we're going to assign it
H
cluster and then
distance that easy we've already
computed the distances so the H
clustering does all the work for us and
let me hit enter on there so now we have
our HCL which is assigned the H
clustering computation based on
distances and a part and then I'm going
to expand my graph because we would like
to go ahead and see what this looks like
and we can simply plot
that and hit the control enter so it
runs runs and look at that we have a
really nice clustering dendogram except
when I look at it the first thing I
notice is it really shows like numbers
down below now if you were a shareholder
and some data scientist came up to you
and said look at this this is what it
means you'd be looking at that going
what the heck does 3941 19118 mean so
let's give it some words
there so let's do the same plot with our
HC
L HC there we
go and let's add in labels and this is
just one of the commands in plots so we
have labels equals my
data and then under my data we want to
know the city and we'll have it hang
minus one that's just the instructions
to make it pretty so we'll run
that oops I accidentally ran just hang
my one let me try that again there we go
okay so now you can see what it's done
and the and the hang my one turns it
sideways that way we can see Central
which is Central us and Kentucky and we
start to actually get some information
off our clustering setup and the
information you start looking at is that
we put all the information together you
probably want to look at Central America
and Kentucky together Oklahoma and Texas
has a lot of commonality as does Arizona
and Southern us and you can even group
group all five of those Florida Oklahoma
Texas Arizona and Southern us these
regions for some reason share a lot of
similarities and so we want to start
asking what those similarities are but
this gives us a place to look it says
hey these things really go together you
should be grouping these together as far
as your cells and what's what you're
doing and then one of the things you
might want to do is there's also we can
do the dagram average and this changes
how we do the
clustering so it looks very similar like
we had before there's our HCL we're
going to assign it we're going to do an
H cluster we're still doing it on
distance oops distance and this time
we're going to set the method to average
so we can change the methodology in
which it computes the values and before
if you remember correctly we did median
median is a little bit different than
means we did the most common one and
then we want the average of the median
and let's go ahead and run
that and then we can
plot and here's our
HCL oops there we go here's our HCL and
I can run that plot and you can see this
changed a little bit so our way it
computes and groups things looks a
little different than it did before and
let's go and put the cities back in
there and do the hang
control copy let me just real quickly
copy that down
here because we want the same
labels and again you can see Nevada
Idaho puet I remember we were looking at
um Southern us and Arizona Texas and
Oklahoma Florida so the grouping really
hasn't changed too much so we still have
a very similar grouping it's almost kind
of flipped it as far as the uh distance
based on average has but this is
something you could actually take to the
shareholders and say hey look these
things are connected and at which point
you want to explore a little deeper as
to why they're connected because they're
going to ask you okay how are they
connected and why do we
care but that's a little bit beyond the
actual scope of what we're working on
today but we are going to cover
membership what's called a clustering
membership on
there and let's create a member we'll
just call it Member One oops there
member. one and we're going to sign to
this we're going to do cut
tree and cut
tree it limits it so what that means is
I take my HC
dot l in
here oops there we go L and so I'm
taking the cluster I just built and we
want to take that cluster and we want to
limit it to just a depth the three so we
go ahead and do that and run that one
oops let me go run there we go so now
I've created my member
one and then whoops let me just move
this out of the way we're going to do an
Aggregate and we're going to use Z
remember Z from above and we're going to
turn Member One into a list and then
we're going to aggregate that together
based on the mean let me go ahead and
hit enter run
thats I did member L it's actually
member
one there we
go and if we take a look at this we now
have our group one fixed charge and then
all your
different columns listed there and most
of them should come up kind of looking
between zero and one but you'll see a
lot of variation because we're varying
it based on the means so it's a means a
standard deviation not just forcing it
in between zero and one
which is a much better way usually to
normalize your data than just doing the
01
setup and finally we can actually uh
look at the actual
values and the same chart we just did
oops I made a mistake on there with my
data there we go
okay and again we now have our actual
data instead of looking at just the if
you looked up here it's all between zero
and one and when we look down here we
now have some actual connections and how
far distance this different data is uh
again becomes more of a domain issue in
understanding the oil company and what
these different values means and you can
look at these as being the distances
between different items so a little bit
different View and you have to really
dig deep into this data what we really
want you to take away from this is the
dagram and the charts that we did
earlier and that is the cluster output
and our nice dagram so this would be
stage one in data analysis of the cells
again you'd have to have a lot more
domain experience to find out what all
the individual numbers we looked at mean
and what the distance is and what's
difference between Central America and
Kentucky and why they're similar and why
it groups all of Central Kentucky
Florida Oklahoma Texas Arizona and
Southern us together into one large
group so that'd be way beyond the scope
of this but you can see how we start to
explore data and we start to see things
in here where things are grouped
together and ways we might not have seen
before and this is a good start for
understanding and giving advice for
sales and marketing maybe Logistics City
development there's all kinds of things
that kind of come together in the
hierarchial clustering as you begin to
explore data and we just want to point
out that we get three clusters of
regions with the highest cells region
with average cells region with the
lowest cells again those are some of the
things that they clustered it around and
you could actually see where things are
going on or lacking you know as this
case if you're the lowest sales no one
wants to be in the region of the lowest
sales if you are an aspiring machine
learning engineer looking for online
training and certifications from
prestigious universities and in
collaboration with leading experts then
search no more simply learn professional
certification program in Ai and machine
learning from Pur University in
collaboration with IBM should be a right
choice for more details use the link in
the description box below with that in
mind regression algorithms is used to
find out the connection between
dependent and independent variables
dependent variables are nothing but a
variable that we are trying to predict
or forecast and independent variables
are the factor that influence the
analysis it is usually used to make
projections medical researchers
frequently utilize linear regression to
study the connection between patient
blood pressure and dosage researcher
make give patient different quantities
of a specific medication and track the
how their blood pressure changes a
regression model might use dosage as a
predictor variable and blood pressure as
a response variable there are some
popular regression algorithms that come
under supervis machine learning first
one is linear regression regression
trees nonlinear regression basian linear
regression after discussing what is
supervised learning and its types let's
move forward to see what actually
regession analysis
is a St technique known as regression
analysis is used to simulate the
relationship between dependent and
independent variables using one or more
independent variables regression
analysis more precisely anbl us to
comprehend and when other independent
variables are held constant changes in
the dependent variables value that
correspond to an independent variables
are changing prediction include values
that are continuous or real such such as
temperature age pay and cost there are
some following concept or terminologies
that must be understood in order to
completely grasp regression analysis the
first one is dependent variable in a
regression analysis the primary variable
we wish to predict or comprehend is the
dependent variable here dependent
variables are also known as Target
variable the next one is independent
variable the term independent variables
also known as predictor refers to the
elements that impact the dependent
variables or are used to forecast their
values and the third one is outliers an
observation that is extremely high or
extremely low in relation to the other
observed values is referred as an
outlier an outlier should be avoided as
it might affect the outcomes the fourth
one is multicolinearity the situation is
set to as occurring when independent
variables have a higher correlation with
one another than other variables it
shouldn't be included in the data set
because it causes issues when
determining which variable has the
greatest impact and the fifth one is
overfitting and underfitting overfitting
is the problem that occurs whenever
system problems well well with the
training data set but poorly with the
test data set underfitting is the term
used when an algorithm does not perform
well even with the training data set
after understanding what regression
analysis is and its terminologies let's
move forward and see why we use
regression analysis including those
involving the together sales marketing
Trends and other factors in this
situation we need regression analysis
that can make forecast more precisely so
let's understand the concept of
regression analysis using an example
business frequently use linear
regression to understand the
relationship between advertising
spending and income for instance they
might run a simple linear model with
advertising as a response variable and
revenue as a predictor variable the
regression model would look like this is
the V equals to Beta 0 plus B1 add
spending where there are no advertising
the coefficient zero would indicate the
whole expected Revenue when advertising
expenditures are increased by one unit
coefficient will represent the typical
change in the total revenue that is $1
if beta 1 is negative then higher
advertising expenditures will result in
lower Revenue if if beta 1 is nearly
zero advertising expenditures have
little impact on revenue and if beta 1
is positive it would imply that more
advertising expenditures are linked to
higher
income the amount of beta 1 will
determine whether a corporation decide
to or decrease its advertising
budget regression means creating a graph
that connects variable the best fit the
given data point the dection analysis
model can then make prediction about
data using the plot regression analysis
is defined as showing a line or a curve
that goes through all the data points on
the target predictive graph in such a
way that vertical distance between the
data points and the regression is
smallest whether a model has captured a
strong link between determined by the
distance between data points and the
line there are some more Ed Case of
regression analysis like we can use
temperature and other variable to
predict or forecast rain regression
analysis can be used for identification
of market trends for predicting traffic
accidents by reckless driving we can use
regression analysis for the same and
many more so by understanding why we
need regression analysis moving forward
let's see some popular regression
algorithms in
detail the first one is linear
regression linear regression is one of
the most famous and straightforward
machine learning algorithm utilized for
predictive analysis linear regression
shows the linear connection between
dependent and independent factors the
line of the equation is y = to mx + b
here y stand for the response variable
or a dependent variable whether X is for
the predictor variable or an independent
variable here m is the estimated slope
and B is for the estimated intercept and
the next one is regression trees a
regression trees is work through a cycle
known as binary recursive which is
iterative interaction that divides the
information into segments of branches
and afterward keep splitting the data
into a smaller group as the technique
climbs each branch this tree is used for
the dependent variable with continuous
values for example a regression tree is
name food which is divided into segments
veg and nonv further it keep splitting
into smaller groups and the third one is
nonlinear regression nonlinear
regression is a type of regression
examination wherein information is to
fit a model and afterward communicated
with a numerical function simple linear
regression relates two factors X and Y
with a straight line Y = MX Plus B while
nonlinear regression relates the two
factor in a nonlinear relationship
nonlinear regression can be predict
population growth over time or the
relationship between GTP and a country's
time and the fourth one is basan linear
regression the algorithm is a way to
deal with a linear regression in which
statical examination is attempted inside
the setting of basian inference linear
regression and basian regression can
generate the same prediction and with
the help of basan processing we can
retrieve the complete variety of
differential solution instead of a point
estimate after seeing some regression
analysis algorithm let's move forward
and see advantages and disadvantages of
different regression analysis
models and the first model is linear
regression model the advantages are work
well irrespective of data set size and
the second one is gives information
about the relevance of features and the
disadvantages are the assumptions of
linear regression and the next one is
polinomial regression the advantages are
works on any size of the DAT set and the
second one is works very well on
nonlinear problems and disadvantages are
we need to choose the right polinomial
degree for the variance stateof the next
one is support Vector regression
algorithms and the advantages are easily
adaptable works very well on nonlinear
problems and disadvantages are difficult
to understand and not well known and the
next one is decision tree regression the
advantages of decision trees are no need
to apply scaling and the second one is
works very well on both linear and
nonlinear problems and disadvantages are
poor results on a small dat set and
overfitting can easily occur and the
last one is random Forest regression the
advantages are powerful and second one
is accurate and disadvantages are no
interpretability and the second one is
overfitting can easily occur after
seeing advantages and disadvantages of
different types of regression analysis
model let's move forward and see some
applications of regression
analysis the first one is forecasting
regression analysis is frequently used
in business to predict potential
opportunities and dangers for examples
demand analysis predicts how many items
a buyer is likely to purchase demand
however is not only the dependent
variables when it comes to business much
more than just direct income can be
predicted using regression analysis and
the second one is capm capital asset
pricing model the linear regression
model is a key component of the capital
asset pricing model capm which
determines the relationship between an
asset expected return and the associated
Market risk premium financial analyst
typically use it for forecast corporate
return and operational performance when
doing financial analysis and the third
one is comparing with competition it can
be used to evaluate how how well a
business is doing financially in
comparison to a certain rival it can
also be used to figure out how the stock
prices of two companies are related to
one another this can be extended to find
the correlation between two competing
companies it might help the company
identifying the effectors affecting the
sales and contrast to the comparable
company these methods can help small
business succeed quickly within a short
period of time and the last one is
identifying problem regression is a
beneficial for identifying incorrect
judgment as well as for providing
factual support for the management for
instance a retail store manager might
believe that extending the hours of
operation will greatly increase sales if
you are an aspiring machine learning
engineer looking for online training and
certifications from prestigious
universities and in collaboration with
leading experts then search no more
simply learns professional certification
program in Ai and machine learning from
Pur University in collaboration with
with IBM should be your right choice for
more details use the link in the
description box below with that in
mind so let's go ahead and talk about
overfitting uh when we talk about
overfitting it's a scenario where the
machine learning model tries to learn
from the details along with the noise
and the data tries to fit each data
point on the curve uh you can see that
um if you plug in your coordinates
you're just going to get the whatever
it's fited every point on the data
stream there's no average there's no two
points that might have the me know y
might have two different answers cuz uh
if the wind blows a certain way um and
your efficiency of your car maybe you
have a headwind so your car might alter
how efficient it is as it goes and so
there's going to be this variance on
here and this says no you can't have any
variance what you know the this is it's
going to be exactly this it can't be any
you can't be the same speed or the same
car and have a slightly different
efficiency
so as the model has very less
flexibility it fails to predict new data
points and thus the model rejects every
new data point during the
prediction uh so you'll get like a
really high error on
here and so uh reasons for overfitting
uh data used for training is not cleaned
and contains noise garbage values in it
you can spend so much time cleaning your
data and it's so important it's so
important that if you have if you have
some kind of something wrong with the
data coming in it needs to be addressed
whether it's a source of the data maybe
they use in medical different measuring
tools uh so you now have to adjust for
data that came in from hospital a versus
Hospital b or even off of machine a and
machine B that's testing something and
those those numbers are coming in
wrong the model has a high variance uh
again wind is a good example I was
talking about that with the car
you may have a 100 tests but because the
wind's blowing it's all over the
place uh size of training data used is
not enough so a small amount of data is
going to also cause this problem you
only have a few points and you try to
plot everything the model is two complex
uh this comes up a lot we put too many
pieces together and how they interact
can't even be track um and and so you
have to go back back break it up and
find out actually what correlates and
what doesn't
so what is underfitting a scenario where
machine learning models can neither
learn the relationship between the data
points nor predict or classify a new
data point and you can see here we have
our efficiency of our car and our line
drawn and it's just going to be way off
for both the training and the predicting
data as the model doesn't fully learn
the patterns it accepts every new data
point during the the prediction so
instead of looking for a general pattern
uh we just kind of accept
everything data used for training is not
cleaned and contains noise garbage and
values again underfitting and
overfitting same issue you got to clean
your
data the model has a high bias uh we've
seen this in all kinds of things
from uh the mo the most common is the
driving cars to facial identification
and or whatever it is the model itself
when they build it might have a bias
towards one thing and this would be an
underfitted model would have that bias
because it's averaged it out so if you
have um five people from India and 10
people from um Africa and 20 people from
the US you've created a bias uh because
it's looking at the 20 people and you
only have a small amount of data to work
with size of training data use is not
enough uh that goes with the size I was
just talking about so we have a model
with a high bias we have size of
training data used is not enough the
model is too
simple again this a one straight line
through all the data when it needs has a
slight shift to it for other
reasons so what is a good fit uh a
linear curve that best fits the data is
neither overfitting or underfitting
models but is just right and of course
we have the nice examples here where we
have overfitting lines going up and down
every Point's trying to be include
included
underfitting um the line really is off
from where the data is and then a good
fit is got to get rid of that minimize
that um error coming through so this is
all exciting but what does this look
like so we really need to jump in and
put a code together and see what this
looks like when we're programming for
this uh demo we'll bring up our trusty
Anaconda and go into Jupiter notebook
for
python move myself out of the way here
uh and so we're going to start off this
is going to be a demo on overfitting and
underfitting using
Python and let's start with our uh
Imports now if you've been through
enough of these tutorials we don't want
to spend a huge amount of time on what
we're bringing in and what we're doing
uh so you should be up on doing this
with python and how to bring in your
different modules we're going to bring
in the sklearn or the uh s kit
processing SK learn. neural network
Import in
MLP regressor so there's our regressor
model right there uh that's going to be
our linear regression model and we have
our metrics mean absolute error if you
remember we had our U that's how we
figure out how well it fits is how far
off that error is based on the um mean
square error value msse
and then of course uh numpy because we
just like to work with numpy it's a
great data array uh we always import it
as MP that's the most common way of
doing it and then we have SK learn model
selection import validation curve so
we're going to look at a validation
curve to see how good our models are and
then we have the uh data set we'll use
the um very famous Iris data set and
that's embedded in the S kit so the SK
key learn data sets have a load Iris in
there and then we have the mat plot
Library cuz if you're doing any kind of
demo or showing this off to your
shareholders we want to have something
nice to display it
on and then we have sklearn model
selection we're going to import import K
fold and we'll talk about that when we
get to it uh and then we're going to go
ahead and do um just for our numpy we're
going to do like a random seed for
random numbers and then for our plot
style we'll use the GG plot that's just
some backend setup um you could even
probably leave the plot style out uh
depending depending on what version of
um you're using of um depending on what
version you're using a map plot
library and then we'll go ahead and run
this uh it's not going to do anything
that we can visibly see cuz it's just
loading those modules and then we also
want to load our Iris uh data in here
and the iris data has an iris data and
Iris Target uh we're going to load that
as X and
Y and just so you can have an idea what
we're talking about we're going to ahead
and
print um X and just the first bit of X
we'll just do the top of X and we'll
also print y so you can see what the top
of Y looks like uh print
y uh and since we're in numpy we're
going to go ahead and do our own thing
if this was uh of course some pandas we
could just do the head of it and see
what it looks like and you can see here
we've loaded this up and in X we have
these different measurements that they
take of the flower the Irish flowers
from this particular data set and what
kind of flower it is it's going to be a
zero a one or a two is actually what the
target comes out of even though that
doesn't show in
here and so we're going to come in here
and we're going to use the K folds cross
validation with 20
folds um and a good catch that this was
a model selection we're we're we're
going through and we're selecting
different parts of the data in here here
we use kold cross validation with 20
folds k equal 20 to evaluate the
generalization efficiency of the model
within each fold we will then estimate
the training and test error using the
training and test sets
respectfully so here we have our KF
equals KF uh kfold here's our splits on
the top then we need to go ahead and
have our list training error we're going
to create an array for that we're going
to list our test testing error and for
train index and test index in
kf. spit X xtrain Y uh X train and X
test we're going to go ahead and split
up our our data our X values and the
same thing with the yv values so now we
have an X train and X test a y train and
a y
test and then here's our model our m m
uh MLP
regressor that's your linear regression
model in there and we have used a
multi-layer Perron MLP so this is a
neural network uh multi-layer Patron
that's what the MLP is for regressor
means that it is dealing with numbers
we're not categorizing
things
um and then let's go ahead uh kind of
went off the screen here we'll just go
ahead and bring that down it's a class
of feed forward artificial neural
networks uh they kind of loosely call it
Ann don't get caught up in the Ann nnn
mnn
there's NN is neural network and then
everybody puts their own flavor on it
depending on what they're doing uh so if
you see the NN you know you're dealing
with a neural
network so we go ahead and fit our data
here's our model.fit we have XT train
and Y train and the YT train data we're
going to predict equals uh model.
predict XT Trin so here's our prediction
of what it's going to be so we've
trained it and we predicted it we've
trained our our the train data and then
we have our y train and then we have our
y test and the Y test equals a model
predict X
test now notice what we did here is
we're going to use our model to predict
what we think y should be but this is
the training Set uh so we've trained it
with this data and now we want to see
how good our model fits our training
data and then we want to see how well it
fits our testing data so we take our
fold training error mean absolute error
y train
y train data predict and we're going to
do our full testing error the mean
absolute error of Y test and Y test data
predict and we do this and here's our
mean absolute error there's our um uh a
little bit different connotation but
that's that's taking the Square value um
and finding the in this case just using
the absolute value so instead of the
Square value we get rid of the minus and
pluses by using an absolute value and we
find the average of that and that works
the same way as doing the squared value
and then we take our list training error
and we're going to go and just depend it
for each uh each one of these runs we go
through so every time we fold the
data think of it like this uh we want to
go ahead and take a piece of data that's
going to be one piece of the data and
we're going to look
at each section and we want to go
through each section to see how well it
does and splits it up this way we have a
nice picture uh when we're looking at it
from a distance I do this a lot when I
do X and when I split my X train and my
y train I'll take 2/3 of the data and
then 1/3 of the data and then I'll
switch it and I'll do three different
models so I can really see how well it
tests out and how that averages out this
is the same thing but with the uh with
the K fold we're doing and we're doing
it across 20
sections we'll go ahead and run
this and when we run this it's not too
exciting because we're just loading up
the data and appending it into our list
and so we want to take with this uh is
we're going to go ahead and plot it and
this is where we can really see what's
going on this is where where it gets
exciting uh so we take it we're going to
create uh a couple subplots uh that way
we have a nice setup down here we're
splitting it up into a couple different
uh
graphs and let's go ahead and run this
and then we'll walk through it a little
bit uh so our subplot comes in um
there's our subplot and then our PLT
plot we're going to do in there range
one uh we're going to ahead and do the
splits plus one in PR list training
error uh Ravel um this is of course just
a code to how we properly set it up on
there so that it sees it correctly and
then we have our xlabel which is our
number fold uh plot the Y label training
error plot the title training error
across folds plot the tight layout plot
the subplot so we're going to move on to
this is one 121 one 122 there's are one
just one and
two it has to do with how it how it
layers it on there for doing multiple
plots because you can do all kinds of
cool things with our plot our pip plot
Library uh and again we're going to go
ahead and do the same thing for the
error and we end up with our training
error cross folds and our testing eror
across
folds and so you can see these different
folds how they kind of Spike and how
they look and so we're talking about
overfitting or underfitting we're
comparing these two graphs and if one of
them is more off than the other one uh
if you're looking at these two graphs
you got to say hey is this one uh um
overfit or underfit and this is always a
good question to ask I mean what do we
got going here is that overfit or is
that underfit and I would say based on
these two graphs and the um training
data uh the training data is more
sporadic than the testing
data so I would look at this and say hey
uh might need to be fit a little bit
better um maybe we don't have enough
data with the iris we probably don't
something else is going on here so it's
a little underfit maybe a different
model would fit better um I would not
use a neural network model for this I
would actually use just a basic linear
reg uh linear model on
this lot of different choices uh but
this gives you an idea what we're
looking at is how chaotic are these two
is it getting better or is it getting
worse if at some point the training data
gets so much better than the testing
data you know you've overfitted and
that's where you start running into the
overfitting this to me looks like it's
underfit so that concludes underfitting
and overfitting if you are an aspiring
machine learning engineer looking for
online training and certifications from
prestigious universities and in
collaboration with leading experts then
search no more simply learns
professional certification program in Ai
and machine learning from per University
in collaboration with IBM should be your
right choice for more details use the
link in the description box below with
that in mind machine learning has
improved our lives in a number of
wonderful ways today let's talk about
some of these I'm Rahul from Simply
learn and these are the top 10
applications of machine learning first
let's talk about virtual personal
assistants Google Assistant Alexa
Cortana and Siri now we've all used one
of these at least at some point in our
lives now these help improve our lives
in a great number of ways for example
you could tell them to call someone you
could tell them to play some music you
could tell them to even schedule an
appointment so how do these things
actually work first first they record
whatever you're saying send it over to a
server which is usually in a cloud
decode it with the help of machine
learning and neural networks and then
provide you with an output so if you
ever noticed that these systems don't
work very well without the internet
that's because the server couldn't be
contacted next let's talk about traffic
predictions now say I wanted to travel
from Buckingham Palace to lods cricket
ground the first thing I would probably
do is to get on Google Maps so search
it
and let's put it
here so here we have the path you should
take to get to Lodge cricket ground now
here the map is a combination of red
yellow and blue now the blue regions
signify a clear road that is you won't
encounter traffic there yellow indicate
that they are slightly congested and red
means they're heavily congested so let's
look at the map a different version of
the same map and here as I told you
before red means heavily congested
yellow means slow moving and blue means
clear so how exactly is Google able to
tell you that the traffic is clear slow
moving or heavily congested so this is
with the help of machine learning and
with the help of two important measures
first is the average time that's taken
on specific days at specific times on
that route the second one is the
real-time location data of vehicles from
Google Maps and with the help of sensors
some of the other popular map services
are Bing Maps maps.me and here we go
next up we have social media
personalization so say I want to buy a
drone and I'm on Amazon and I want to
buy a DJI mavic Pro the thing is it's
close to one lap so I don't want to buy
it right now but the next time I'm on
Facebook I'll see an advertisement for
the product next time I'm on YouTube
I'll see an advertisement even on
Instagram I'll see an advertisement so
here with the help of machine learning
Google has understood that I'm
interested in this particular product
hence it's targeting me with these
advertisements this is also with the
help of machine learning let's talk
about email spam filtering now this is a
spam that's in my inbox now how does
Gmail know what's spam and what's not
spam so Gmail has an entire collection
of emails which have already been
labeled as spam or not spam so after
analyzing this data Gmail is able to
find some characteristics like the word
lottery or winner from then on any new
email that comes to your inbox goes
through a few spam filters to decide
whether it's spam or not now some of the
popular spam filters that Gmail uses is
content filters header filters General
Blacklist filters and so on next we have
online fraud detection now there are
several ways that online fraud can take
place for example there's identity theft
where they steal your identity fake
accounts where these accounts only last
for how long the transaction takes place
and stop existing after that and man in
the middle attacks where they steal your
money while the transaction is taking
place the feed forward neural network
helps determine whether a transaction is
genuine or fraudulent so what happens
with feed for power in unit networks are
that the outputs are converted into hash
values and these values become the
inputs for the next round so for every
real transaction that takes place
there's a specific pattern a fraudulent
transaction would stand out because of
the significant changes that it would
cause with the hash values Stock Market
trading machine learning is used
extensively when it comes to Stock
Market trading now you have stock market
indices like nikai they use long
short-term memory neural networks now
these are used to classify process and
predict data when there are time lags of
unknown size and duration now this is
used to predict stock market trends
assisted medical technology now medical
technology has been innovated with the
help of machine learning diagnosing
diseases has been easier from which we
can create 3D models that can predict
where exactly there are lesions in the
brain it works just as well for brain
tumors and ischemic stroke lesions they
can also be used in fetal Imaging and
cardiac analysis now some of the medical
fields that machine learning will help
assist in is disease identification
personalized treatment drug Discovery
clinical research and radiology and and
finally we have automatic translation
now say you're in a foreign country and
you see Billboards and signs that you
don't understand that's where automatic
translation comes of help now how does
automatic translation actually work the
technology behind it is the same as the
sequence to sequence learning which is
the same thing that's used with chat
Bots here the image recognition happens
using convolutional neural networks and
the text is identified using optical
character recognition furthermore the
sequence to sequence algorithm is also
used to translate the text from one
language to the other so first we will
import some major libraries of python so
here I will write
import pandas as
PD and
import numpy as
NP then
import
cbor as
SNS okay then
import s
skar do model
selection
Port train
underscore test dcore
split before that I will
import mat plot
Li do p
plot as
PLT okay
then I will write here from
Escalon dot
matrix
import
accuracy
4 then
from
eSalon dot
matrix
import
classification
report and import R then import
string
okay then press enter so it is
saying okay here I have to write
from everything seems
good loading let's
see okay till then numai is a python
Library used for working with arrays it
also has function for working with
domain of linear algebra and
matrices it is an open source project
and you can use it
freely number stand for for numerical
python pandas so panda is a software
Library written for Python programming
language for data manipulation and
Analysis in particular it offers data
structure and operation for manipulating
numerical tables and time
series then cbon an open source python
Library based on M plot lib is called
cbon it is utilized for data exploration
and data visualization with data frames
and the pandas Library cbon function
with ease then matplot lip for Python
and its numerical extension numpy met
plot lip is a cross platform for the
data visualization and graphical
charting package as a result it presents
a strong open source suitable for metlab
the apis for met plot lib allow
programmers to incorporate graphs into
gii applications then this train test
split we may build our training data and
the test data with the aid of Escalon
train test split function
this is so because the original data set
often serves as both the training data
and the test data starting with a single
data set we divide it into two data sets
to obtain the information needed to
create a model like H and test accuracy
score the accuracy score is used to gge
the model's Effectiveness by calculating
the ratio of total true positive to
Total to negative across all the model
prediction this re regular expression
the functions in the model allow you to
determine whether a given text fits a
given regular expression or not which is
known as
R okay then string a collection of
letters words or other character is
called a string it is one of the basic
data structure that serves as the
foundation of manipulating data the Str
Str class is a built-in string class in
Python because python strings are
immutable they cannot be modified after
they have been formed formed okay so now
let's import the data set we will be
going to import two data set one for the
fake news and one for the True News or
you can say not fake news okay so I will
write here BF
uncore P equals
to PD
do read underscore
CSV or what can I say d fake
okay _
fake
okay then fake dot CSV you can download
this data set from the description box
below then data dot true equals to pd.
read CSV
sorry
CSC then fake news sorry true through.
CSV okay then press
enter so these are the two data set you
can download these data set from the
description box below so let's see the
board data set okay then I will write
here
dataor fake
do
head so this is the fake data okay
then data underscore
true
Dot and this is the two
data okay this is not fake so if you
want to see your top five rows of the
particular data set you can use head and
if you want to see the last five rows of
the data set you can use tail instead of
head
okay so let me give some space for the
better
visual so now we will insert column
class as a Target feature okay then I
will write here data let go
fake class
equals
to0 then data underscore
true and
plus = to
1
okay
then I will write here data underscore
fake dot shape and data underscore true
through dot
shape okay then press
enter so the shape method return the
shape of an array the shape is a tle of
integers these number represent the
length of the corresponding array
dimension in other words a tle
containing the quantities of entries on
each axis is an array shape Dimension so
what's the meaning of
shape in the fake word in this data set
we have 2 3 48 one rows and five columns
and in this data set true we have 21 417
rows and five column okay so these are
the rows column rows column for the
particular data
set so now let's
move and let's remove the last 10 rows
for the manual testing okay then I will
write here data underscore
fake let's go
manual
testings to dataor
fake do
tail for the last 10 rows I have to
write here
10 okay so for
I in
range 2 3
4 8 1 soorry
zero comma 2
3
470 comma
-1
okay
then DF underscore not DF
data underscore
fake dot drop
one instead of one I can write here
I
comma X is equal
to0 in
place go to
true then data
not
here data
underscore same I will right for I will
copy from
here and I will paste it here and I will
make the particular changes so here I
can write
true here I can write
true
okay then I have to change a
number
2
1
416
WR 2 1 40
6 -
1
same
so press
enter x equal
to0 since X maybe you mean d0 or of
this okay we will put here double
course
I'm putting
this. drop i z in place
okay also WR equals toal
to yeah
so okay axis is not
defined
now it's working
so let me
see now
data
underscore pick do
shape
okay and data dot
true data underscore true
dot
shape as you can
see 10 rows are deleted from each data
set yeah so I will write here data
underscore fake underscore
manual
testing
class =
to0 and data
underscore true
underscore manual underscore
testing CR equals
to
1
okay just ignore this
warning
then let's
see data
underscore
bore
manual
testing.
head as you can see we have this and
then data dot sorry underscore true
underscore
manual
testing dot at
10 this is this is the uh true data
set so here I will merge data let us go
merge
to PD Dot
concat concat is used for the
conation dataor
fake data
underscore comma
XIs =
to0 then data underscore
merge Dot
head the top 10
rows
yeah as you can see the data is merged
here okay first it will come for the
fake news and then with the for the True
News then let's merge true and fake data
frames okay
we did this
and let's Mery column then data do
merge dot columns or let's see the
columns it has not defined what data
underscore
merge these are the column same title
Tex subject date class
okay
now let's remove those columns which are
not required for the further process so
here I will write data
underscore or equals to data underscore
merge
prop title we don't need
then subject we don't need
then so
one so let's check some null
values it's giving
error of
this
that's good then
data dot is
null
some
Center so no null values okay then let's
do the random shuffling of the data
frames okay
for that we have to write here data
equals to data do
sample
one
then data okay
data do
head okay now you can see here the
random shuffling is
done and and one for the true data set
and zero for the fake news one
okay then let me write here data
dot
reset underscore
index
Place equals to
True data dot
drop
fora X is = to
1 then comma in
place equals to
True
okay then let me see columns now data
dot
columns so here we have two columns only
rest we have deleted
okay let me see data dot
add yeah everything seems
good let's proceed further and let's
create a function to process the text
okay for that I will write here
but okay you can use any
name
text then text equals
to text.
lower okay and
text to r dot for the
substring
remove these
things uh from
the datas okay so for that I'm writing
here
comma
okay then text equals to r dot
substring
comma comma
text okay
then I have to write text equals
to do
substring
www dot
s+
comma comma
text okay then text equals
to I do
subring
then
comma okay then text equals to R do
substring
then but
percentage
s again percentage for ar. SK
function right here
string do
punctuation
comma then comma then
text
right then text equals to R do
substring and
N
comma text equals to r dot
subring right
here and again
D then
again
okay then
comma then again text here okay then at
the end I have to write here return text
so everything like uh this this type of
special character will be removed from
the data set okay let's run this let's
see yeah so here I will addite DF sorry
not DF
data
data
then
text was to
data
okay dot
apply to the function name wordp what
opt
okay press enter yeah so now let's uh
Define the dependent and independent
variables okay x equals to
data
text and y equal
to
data
class okay then splitting training and
testing
data okay sorry so here I will write
xcore
train comma xcore
test uh then Yore
train comma Yore test equals to train
underscore test testore
split then X comma
y comma
test let go size equals to
0.25 okay press
enter so now let's convert X to vectors
for that I have to write
here that is
X
so here I will write from
Escalon dot
feature
exraction dot text
import D
vectorizer
okay then
vectorization
to T
FID
vectorizer okay
then
XV
underscore
train equals
to factorization
R1
factorization do
fit then
transform xcore
train okay then X Vore test equals
to
factorization
dot
transform xor
test okay then press
enter
uh so now let's see our first model
logistic
regression so here I will write
from eSalon
dot linear underscore
model okay
import
logistic
gr then lot go
to
logistic
regression have to write here l
dot fit
then XV
dot not DOT train
comma X Vore
test okay
Center XV
TR okay here I have to write y
train
okay and press
enter will work so here I will write
prediction
underscore linear
regression equ
to
lr.
predict Vore
test okay let's see the accuracy
score for that I have to write LR do
score then XV uncore
test comma
Yore
test okay let's see the accuracy so here
as you can see accuracy is quite good
98% now let's
print the
classification
put Yore test
comma prediction of linear regression
okay so this is you can see Precision
score then F1 score then support value
accuracy okay so now we will uh do this
same for the decision free gradient
boosting classifier random for Forest
classifier okay then we will do model
testing then we will predict the
school okay so now for the decision tree
classification so for that I have to
import from
skar dot
tree
import
decision
three
classifier
okay then at the short form I will write
here I will copy it from
here
then okay then I have to write the same
as this so I will copy it from
here
and
yeah
let's change linear
regression to se Tre
classific
okay then I will write here
same go
DT to DT do predict
Vore
test it's
be still loading it's it will take
time
okay till then let me write here for the
accuracy.
score
vcore
test comma
y
okay let's wait
okay
run the accuracy so as you can see
accuracy is good than this linear
regression okay logistic
regression yeah so let
me show you the let me
predict
print so this is the accuracy score this
is the all the
report
yeah now let's move for the uh gradient
boosting classifier okay for that I
write from
Escalon dot
assemble
Port
radiant
boosting
classifier
classifier I will write here
GB equals to let me copy it from
here
okay we give here random let Co
State equals to
zero wait wait wait wait so I will write
here GB dot
fit
X Vore train comma ycore train okay then
press
enter here I will write predict _
GB
to GB do Pit sorry
redit
three DOT
test _
test till then it's loading so I will
write here uh for the score then I will
add GB do
score then Vore test
comma Yore test okay so let's wait it is
running this
part till then let me write for the
printing
this okay it's taking
time taking time still taking
time what if I will run this it's not
coming because of
this yeah it's done now so you can see
the
accuracies uh not good
than decision Tre but yeah it is also
good
994 something okay
so now let's check for the last one
random
Forest first I will
do for the random Forest we have to
write from Escalon
dot
symbol
import
random
Forest
classifier
okay and here I will write
RF to
right I will copy it from
here
then random
State equals
to Z
then RF dot
fit tore
train
Yore
train okay then press
enter and predict
underscore
RC
RF equals
to RF dot
predict Vore test
okay till then I will write here it's
still loading it will take time so till
then I will write for the score score
accuracy
score XV uncore test comma Yore
test okay then I will write here till
then
print
classification
p and Yore
test
comma will take time little
bit
so uh it run the accuracy score is 99 it
is also
good so now I will write the code for
the model testing so I will get back to
you
but after writing the code
so so I have made two functions one for
the output label and one for the manual
testing okay so it will predict the all
the from the all models from
the repeat so it will
predict the the news is fake or not from
all the models okay so for that let me
write here uh
news to
string
put
okay then I will write here manual
underscore
testing
so
here I will you can add any news from
the you can copy it from the Internet or
whatever from wherever you want so I'm
just copying from the internet okay from
the Google the news which is not fake
okay I'm adding which is not fake
because I already know I searched on
Google so I'm entering this so just run
it let's see what is
showing okay string input object is not
callable okay let me check this
first yeah I have to give here s Str
only yeah let
check okay I have to add here again the
script yeah manual testing is not
Define let me see manual
testing
okay I have to edit
something it is just GB and it is just
RF GBC is not defined okay okay so what
I have to do I have to remot this
this okay everything seems
sorted
now as I said to you I just copied this
news from the internet I already know
the news is not fake so it is showing
not a fake news Okay so now what I will
do I will
copy one fake news from the
internet and let's see it is detecting
it or not
okay so let me run
this and let me add the news for
this
so all the models are predicting right
it is a fake news or you can add your
own script like this is the fake news
okay I hope you guys understand till
here so I hope you guys must have
understand how to detect a fake news
using machine learning you can you can
copy it any news from the internet and
you can check it is fake or not okay or
if your model is predicting right or not
if you are an aspiring machine learning
engineer looking for online training and
certifications from prestigious
University ities and in collaboration
with leading experts then search no more
simply learns professional certification
program in Ai and machine learning from
Pur University in collaboration with IBM
should be your right choice for more
details use the link in the description
box below with that in mind open CV open
source computer vision library is an
open source computer vision and machine
learning software Library it is WR in
C++ but as a binding for various
programming languages such as python
Java MLB open c was designed with the
goal of providing a common
infrastructure for computer vision
applications and to accelerate the use
of machine learning perception in
commercial product open CV is widely
used in a variety of Industries
including robotics automative and
Healthcare it supported by a large
community of developers researchers and
users who contribute to its development
and provide support to its users it is
supported by a large community of
developers researchers and users who
contribute to its development and
provide support to it users so now let's
see what is object detection object
detection is a computer vision
technology that involves identifying and
localizing the object of interest within
an image or a video it is a challenging
task as it involves not only recognizing
the presence of an object but also
detecting its precise location and size
within the image or video object
detection algorithm typically used deep
learning techniques such as CNN to
analyze the image or video for
identifying the objects these algorithm
can also determine the boundaries of the
object by by drawing a bounding box
around them so after understanding what
is object detection now let's move on to
the programming part so this is a Kel
python kernel here we will change this
name so here I will write object
detection
demo okay
so first we will import some major
Library like open CV so for that we will
write import CV2 and the next one is
import M plot
lib
pyplot as PLT so why we are writing PLT
because we can't write again and again M
plot li. P plot okay it's a long one so
we can write a shout from
PLT so yeah so let's run this so what is
opencv open CV is an open source
software library for computer vision and
machine learning the open CV full form
is open source Source computer vision
library was created to provide a shared
infrastructure for application for
computer vision and to speed up the use
of machine learning perception in
consumer products open CV as a BSD
license software make it simple for
companies to use and change the code
okay so there are some predefined
packages and libraries that make our
life simple and open CV is one of them
second one is M BL Li mat BL Li is a
easy to use and an amazing visualize
library in Python it is built on numpy
array and designed to work with broader
CPI stack and consist of several plots
like line bath scatter histogram and
many others okay so moving forward we
will import our file okay so here I will
write
config file equals to this is our file
name SSD
uncore
mobilet V3
large
Coco
202 14.
DB okay so you can find this file on the
description box
below Frozen model equals
to I explain you every single thing
inference
graph. PB okay so let me run it first
mobile net as a name applied the mobile
net model is designed to use in mobile
application and its T oflow first mobile
computer vision model mobile net use
depthwise separable convolutions it
significantly deduces the numbers of
parameter when compared to the network
with regular convolutions with the same
depth in the NS this result in the the
lightweight of the deep neural network
so mobilet is a class of CNN that was
open source by Google and therefore this
gives us an excellent starting point for
training our classifiers that are
insanely small and insanely fast okay so
what is this large Coco this is a data
set Coco data set like with applications
such as object detection segmentation
and captioning the Coco data set is
widely understood by the
state-of-the-art of neural network its
versatility and the multi-purpose scene
variation serve best to train a computer
vision model and Benchmark its
performance okay so what is coko the
common object in context is one of the
most popular large scale label images
data set available for public use it
represent a handful of object we
encounter on a daily basis and contains
image Inn notations in 80 categories I
will show you the categories I have with
over 1.5 million object instances okay
so modern day AI driven solution are
still not capable of producing absolute
accuracy and result which comes down to
the fact that Coco data set is a major
Benchmark for CV to train test and
polish refine models for faster scaling
of The annotation Pipeline on the top of
that the Coco data set is a supplement
to transfer learning where the data used
for one model serves as a starting point
for the another so what is frozen
Insurance graph like freezing is the
process to identify and save all the
required graphs like weights and many
others in a single file that can usually
use a typical transl model contains four
files and this contains a complete graph
okay so forward let's create one model
here I will write model to CV2 do
DNN
model and then config
file here I'm giving the parameters two
parameters like frozen model and config
file score here yeah we run it first
okay there is
error okay so error is CV2 DNN detection
model return is
result an exception set the question
comes what is the meaning of deduction
model or DNN deduction model so this
class represent the high level API for
object detection networks detection
model allows to set parameters for
pre-processing input image detection
model creates net from file and with
trained weights and config sets it
processing input runs forward pass and
return the result
deductions okay moving forward let's set
the class
labels okay
class
labels file
name Al labels.
dxt I will put this file on the
description box below you can download
from there
open file
name pass
labels
this
Tri
so here I created one array of name
class labels so this is the file name
what I'm doing I'm putting this label
file into these class labels okay so
here if I will
print class
labels so these are the 80 categories in
the Coco data
set okay this person bicycle car
motorbike aerplane bus train these all
are the a
categories I will put this file label.
txt in the description box below you can
download from there okay
fine so let's print the length of the
Coco data set or you can see class
labels this 80 as you can see I have
already told
you the length will be
8 so here let's set this some model
input size scaling mean and all so I
will write here model dot set
input
size 320 comma
320 model do
set
input
scale 1
0/
127.5 okay I will explain you don't
worry model do
set
set
input
[Music]
mean
127.5 comma
127.5 comma
127.5 okay okay and then model do
set
input
B will
be what is set input
size
okay so set input size is a size of new
frame a shape of the new blob less than
zero okay so this is the size of the new
frame the second one is set input scale
the set input scale is a scale factor of
the value for the frame or you can say
the parameter will be the multiplier of
the frame values or you can say
multiplier for the frame values okay so
input mean so it set the mean value for
the frame the frame in which the photo
will come the video will come or my
webcam will come so it set the mean
value for the frame or the four
parameters mean is scalar with the mean
values which are subtracted from the
channels you can say and the last one is
set input swap RB so it set the flag
swap RB for the every frame we don't
have to put every time a single frame
for a particular image it will be set
the true for the all the images okay so
parameters will be swap RB flag which
indicates the swap first and the last
channels so moving forward we will Port
one
image
I am
read do
jpg
dot IM am
show so this is the size of 320 by 320
okay so first thing is you can download
this the random pi from the Google I
took from Google itself so now what we
will do we will set the class
index the confidence
value
value the B box B box is the boundary
box which I will create for the
particular person cycle motorbike and
the car okay equals to
model
the confidence
threshold threshold is used
for if my model will confirm it the
particular image which is the texting is
correct it will print the name
okay so let me
print
print
class class index is coming one 2 3
4 okay so one means
person two means Bicycle three means car
and four means motorbike this is the
class index index for particular label
what I will do I will print the
boxes
font
scale = to 3 and the font equals to CB2
dot font
her
plane
for
class index and the confidence and the
boxes
and dot
pattern
this
that box the boundary
box
okay then I will write here CV2 do
rectangle
make the
rectangle set the
image and
boxes 5 comma 0 comma 0 this is the
color of the box and this will be the
thickness okay then I will write CV2 do
putut
text image
then class
labels I will write class index minus
one because always index start with zero
that's fine and the box
is
z
and comma
boxes
1
4 okay
F
comma
scale
font
scale
coloral
to this will be the text color 0 comma
255 comma
0 and the
thickness three let me run
it uh no error okay now PLT dot I am
show then see V2 do
CVT
color
then
cvt2 dot
color color then
BGR to
brg that is why we wrote swap RB equals
to true because every time we will
convert BGR to B
RG sorry GB RGB so we don't have to
write again and again it will convert
all the files into RGB okay run
it okay as you can see the motorbike is
coming bicycle is coming the person is
coming the car will come the car is
coming okay so it's detecting the
right for the image
now we will do this for the video and
for the
webcam we are done with this image one
and then now I will write
here okay so this is we'll do for the
video for the video I will write here
cap equals to capture you can write any
name so CV2
do video
capture so you can take any random video
I took this
pixels
George can comment
down share the
link
have dot is
open so here I will write
cap equals
to
CV2 sorry
CV2 dot
video
capture
zero and and if
not cap dot is
open
then then
raut
output
error
can't open this the
video can't open the
video here everything will be the same
font
scale equals to
three okay font equals to CV2
dot
font
her
okay so here I will write while
true comma
frame equals to cap do sheet this is for
the reading of the
file the same I will write class
index comma
confidence
my boundary
boxum model do
detect FL and the
confidence
threshold to
0.55 okay everything is the same we did
before so here I will
print
class
index okay so here I will write
if
and of the class
index does not equals to
zero then what to perform is here I have
to write
for class
index comma
confidence comma
boxes
inip
class index do
flatten flatten is a layers
okay
confides
flatten e
box and
if
class index is greater than equals to
80
then what to do then I will copy from
here okay the same thing I have to write
here
so here I will write CV2 dot I'm
sure this will be the in the frame
object
detection by simply learn
and
frame so if CV2 dot vit
key
to
and
zero
FFX =
to o
d q okay
then here I will write
break will be break when it get into two
the weight key will be two okay I will
tell you what is the weight
key here I will write cap dot
release and CV2
dot
destroy
all
windows
okay so now let me
run let's see error there will be error
okay see Python programming language
modules let me run it
again
the
keys
okay video is
here the video is here as you can see
see bicycle the person the person the
bus car traffic light the person person
so our object detection for the video is
coming right okay person okay person
traffic light
bus this is how you can do for the video
okay so now let's we will do for the
webcam
live so this is for the video
so if we want to do for the webcam we
okay so we need to just
change one one thing only we have to
change instead of giving the file we
have to write one here okay the rest
will be the
same got it so I have to just shut down
my webcam so let me shut down the webcam
and get back to
you as you can see
this is a 320 by 320 box
so so this is coming right okay so I if
I will show this the mobile phone is
coming right now okay so this is how you
can do the correct object detection if
you are an aspiring machine learning
engineer looking for online training and
certifications from prestigious
universities and in collaboration with
leading experts then search no more
simply learns professional certification
program in Ai and machine learning from
P University in collaboration with IVM
should be your right choice for more
details use the link in the description
box below with that in mind so first we
will open command prom to write a
command to open jupyter notebook so here
we will write
Jupiter
notebook then press enter so it will
take some time
yeah so this is the landing page of
jupyter notebook and here we have to new
then Python
3 so this is how the Jupiter notebook UI
look lies so at first we will import
some major libraries of python which
help in creating a mass detection system
so here I will write
import CV2 comma OS and press enter then
I will give the path then data path
equals to
C then
SL users
L
SL
SLP 0
9375 then
slash
desktop then slash then
pH
mask
detection then slash data
set okay
so here it will be slash my bad sorry
and
slash okay let look
fine so here I will write
categories equals to OS
do
list directory L
di then I will assign data underscore
path then I will create some
labels equals
to uh here I will
write I will write here
I
for I in
range then here here I will give
length of
categories
yeah then press enter Then here I will
write
label then D directory equals
to directory
then zip then
categories
comma
labels so here I will write
print then label
directory okay then I will
print
categories then I will print
labels
okay then press enter
yeah so here CV means capturing video
the CV2 function in open CV can read
video video capture by using pass zero
as a function parameter we can access
our webcam we may pass rtsp URL in the
function parameter to capture CCTV
footage which is quite helpful for video
analysis then OS this this OS module the
OS module in Python has function for
adding and deleting folders retrieving
their contents changing the directory
locating the current directory and more
before you can communicate with the
underlying operation system you must
import the OS
module and this OS list directory to
retrieve a list of all files and folders
in the given directory use Python os.
list directory method the list of files
or directory in the current working
directory will be returned if no
directory is
specified then label the the TK inter
visit class called a label is used to
show text or an image the user views the
label they cannot be interact with it so
moving forward I will write code so just
stay with me after that I will explain
you line wise
okay so here I will WR
write that image uncore
size = to
100 then I will create two classes
data then
Target okay
arrays then I will write here for
category in categories
folder
underscore
path equals to OS do
path then dot
join then
data underscore
path comma
category okay then
IMG image names equals to os.
list directory then folder
path just stay with me I will explain
you line
wise okay so here I will write
for
IMG
name in
IMG
names IMG uncore
paath equals to os.
paath
dot
join then I will write here
folder I
repeat I repeat so here we write folder
underscore
path then IMG uncore
name so IMG equals to
CV2 do I am
read then IMG Gore
path so here I will write
try that gray equals to
CV2 dot
CVT then Capital C
color then IMG comma
CV2
Dot
then
color then underscore
BGR to
RGB
okay so I will go for the gray
one CVT color
vgr
tog
yeah so I will write here
resize okay not in capital letter
resize equals to
CV2 dot
resize then
gray comma
IMG
size comma IMG
size why this two IMG size because like
length and uh like width and breadth
would be
shame like 100 so press enter so I will
write data
dot
aen then
resized then
Target do
append then
label
then
directory of
category
okay
so I will write here
accept
exception s
e so here I will print
then
exception comma
e okay then press enter let's see there
should be no
error so it is loading let's
wait okay no
error so image size should be like 100
by 100 100 so that is why I wrote here
100 the IM size 100 by 100 and I made
two arrays like for one for data then
one for
targets
so this for this gray equals to CV2 do
CVT color image so converting the image
into gray scale okay then this line
resize equals to C CV2 do resize gray
IMG so resizing the gray scale into like
100 by 100
since we need a fixed common size for
all the images in the data set
okay then this target do append label
direct so appending the image like and
the label categories into the list list
what is list data set so like for the
last print exception e so if any
exception raised the exception will be
printed here and the pass to the next
image
okay so moving forward but let's import
numai and save this data and the target
okay so I will write here
import
numpy as
NP
right then I will write here
data equals to NP do
array then again
data by 250
5.0 okay then again data equals to NP
dot
reshape then
data
comma
data do
Shi than
zero comma IMG
size comma IMG
size comma
1 okay then press enter Then for
Target let me put do it this like this
then Target equals to NP do
array then
Target
okay okay
okay so here I will write
from kiras
dot
utils
import
NPS okay so here I will write
newcore
Target equals to NP underscore
utils dot
2 underscore
categories categorical I guess
categorical then
Target
okay then press
enter so here I will save NP do
save this
data comma
data okay and np.
save this
target comma new
Target
okay so press
enter
yeah so numai numai is a python Library
used for working with ARR it also has a
function for working with in the domain
of linear algebra and matrices it is an
open source project and you can use it
freely numai stand for numerical Python
and the scas a Python interface for
artificial neutral network is provided
by the open source software package
known as kasas the tensorflow library
interface is provided by the kasas a
number of backends were supported by
kasas up until version 2.3 including Tor
flow Microsoft
cognitive so this is the part of the
data processing this all this part of
the data processing so let me write here
data pre-processing
preprocessing so the data pre-processing
part is done now we will create the
another file of this python for the
training CNN CNN means convocation
neural network okay test
data
comma
Trainor
Target comma
testore
Target equals to
train underscore
split then we will split on data and
Target comma test size should
be
0.1 okay so now I will give presenter
okay train split train split is not
defined
okay it is
train test
split
yeah so now I will write
checkpoint
equals to
model
checkpoint then
in
model I repeat so I will write more
model comma
EPO then 0
3D do
model
okay so here I write comma
monitor equals to Value
loss
underscore loss
comma
bubos equals
to0 comma
save
best
only equals
to through comma
mode equals to mode should be
Auto
okay then I will press enter
then here I will write
history equals to model
dot
bit to
train underscore
data
comma okay train underscore
Target
comma apocs = to
20 comma call
backs callback equals
to
checkpoint comma
validation validation underscore
split equals to 0.2 the best ratio okay
so let me press enter okay there is one
error model. fit got and
X okay there should be a spelling
mistake
eepo
PS to
20
do model it is
fine then
monitor go to Value
loss then buos equal to
Zer save best only equals to true then
more
okay
so here I have to
write maybe it will so it will take some
time to go till 20 okay so it will
download one by one so we will wait for
a
while
so the model checkpoints are completed
so here I will write
print then
model dot
evaluate okay here I will write
testore
data comma
test underscore Target
then press
enter okay so I hope you guys understand
till here if you have any question or
any query regarding any code just put as
in comments our team will shortly
provide you the correct solution okay so
moving forward we will do we will create
another file for detecting mask okay so
I will go here and then new file
python so I will write here
detecting
okay yeah so I will import some
liabilities here kasas dot
models import
load
models we will
import
CV2 and we will
import
numai as
NP so
preser okay num by as
NP okay so
presenter so what I will do I will write
here model equals to load underscore
model
then OKAY kasas model from kasas cannot
import
name okay
so it is model only yeah so here I will
write model equals to load
model so I will write here
model
07 do
model
okay okay
my so here I will write face underscore
classifier equals to
CB2 dot
cascate casate
classifier so this is one file for
frontal phas default so you can find
this file on the description box below
so I will write
here our c
SK
underscore
frontal
pH underscore
default do
XML okay so
Source equals
to
CV2 dot
video
capture it will open our camera so
labels underscore
directory equals
to
zero
mask and one
for no
mask
okay so
color directory equals
to like no mask for red and mask for
green okay so WR
zero then 0 comma
2 55 comma
0 then again comma for one there should
be 0 comma 0 comma
255
okay so I hope you guys understand till
here so if you have any question or any
query regarding any code so just put as
in comments our team will shortly
provide you the correct
solution
so the code is
written so let me run this for this
first I have to like so first I have to
close my this person on screen
camera so the code is running
fine now it is showing no mask
now it is showing
Mark so I hope you guys understand till
here if you have any queries any
question regarding any code just put as
a comment our team will shortly provide
you the correct solution if you are an
aspiring machine learning engineer
looking for online training and
certifications from prestigious
universities and in collaboration with
leading experts then search no more
simply Larn professional certification
program in Ai and machine learning from
Pur University in collaboration with IVM
should be your right choice for more
details use the link in the description
box below with that in mind we'll talk
about interview questions for machine
learning now this video will probably
help you when you're attending
interviews for machine learning
positions and the attempt here is to
probably consolid a 30 most commonly
asked uh questions and to help you in
answering these questions we tried our
best to give you the best possible
answers but of course what is more
important here is rather than the
theoretical knowledge you need to kind
of add to the answers or supplement your
answers with your own experience so the
responses that we put here are a bit
more generic in nature so that if there
are some Concepts that you are not clear
this video will help you in kind of of
getting those Concepts cleared up as
well but what is more important is that
you need to supplement these responses
with your own practical experience okay
so with that let's get started so one of
the first questions that you may face is
what are the different types of machine
learning now what is the best way to
respond to this there are three types of
machine learning if you read any
material you will always be told there
are three types of machine learning but
what is important is you would probably
be better of emphasizing that there are
actually two main types of machine
learning which is supervised and
unsupervised and then there is a third
type which is reinforcement learn so
supervised learning is where you have
some historical data and then you feed
that data to your model to learn now you
need to be aware of a keyword that they
will be looking for which is labeled
data right so if you let's say past data
or historical data the impact may not be
so much you need to emphasize on labeled
data so what is label data basically
let's say if you're trying to do train
your model for classification you need
to be aware of for your existing data
which class each of the observations
belong to right so that is what is
labeling so it is nothing but a fancy
name you must be already aware but just
make it a point to throw in that keyword
labeled so that will have the right in
fact okay so that is what is supervised
learning when you have existing labeled
data which you then use to train your
model that is known as supervised
learning and unsupervised learning is
when you don't have this labeled data so
you have data it is not labeled so the
system has to figure out a way to do
some analysis on this okay so that is
unsupervised learning and you can then
add a few things like what what are the
ways of performing
supervised learning and unsupervised
learning or what are some of the
techniques so supervised learning we we
perform or we do uh regression and
classification and unsupervised learning
uh we do clustering okay and clustering
can be of different types similarly
regression can be of different types but
you don't have to probably elaborate so
much if they are asking uh for uh just
the different types you can just mention
these and just at a very high level you
can but if they want you to elaborate
give examples then of course I think
there is a different question for that
we will see that later then the third so
we have supervised and we have
unsupervised and then reinforcement you
need to provide a little bit of
information around that as well because
it is sometimes a little difficult to
come up with a good definition for
reinforcement learning so you may have
to little bit elaborate on how
reinforcement learning works right so
reinforcement learning works in in such
a way that it basically has two parts to
it one is the agent and the environment
and the agent basically is working
inside of this environment and it is
given a Target that it has to achieve
and uh every time it is moving in the
direction of the target so the agent
basically has to take some action and
every time it takes an action which is
moving uh the agent towards the Target
right towards a goal a Target is nothing
but a goal okay then it is rewarded and
every time it is going in a direction
where it is away from the go goal then
it is punished so that is the way you
can a little bit explain and uh this is
used primarily or very very impactful
for teaching the system to learn games
and so on examples of this are basically
used in alphago you can throw that as an
example where alphago used reinforcement
learning to actually learn to play the
game of Go and finally it defeated the
go world champion all right this much of
information that would be good enough
okay then there could be a question on
overfitting uh so the question could be
what is overfitting and how can you
avoid it so what is overfitting so let's
first try to understand the concept
because sometimes overfitting may be a
little difficult to understand
overfitting is a situation where the
model has kind of memorize the data so
this is an equivalent of memorizing the
data so we can draw an analogy so that
it becomes easy to explain this now
let's say you're teaching a child about
some recognizing some fruits or
something like that okay and you're
teaching this child about recognizing
let's say three fruits apples oranges
and pineapples okay so this is a a small
child and for the first time you're
teaching the child to recognize fruits
then so what will happen so this is very
much like that is your training data set
so what you will do is you'll take a
basket of fruits which consists of
apples oranges and pineapples okay and
you take this basket to this child and
there may be let's say hundreds of these
fruits so you take this basket to this
child and keep showing each of this
fruit and then first time obviously the
child will not know what it is so you
show an apple and you say hey this is
Apple then you show maybe an orange and
say this is orange and so on and so and
then again you keep repeating that right
so till that basket is over this is
basically how training work in machine
learning also that's how training works
so till the basket is completed maybe
100 fruits you keep showing this child
and then the process what has happened
the child has pretty much memorized
these so even before you finish that
basket right by the time you are halfway
through the child has learned about
recognizing the Apple orange and
pineapple now what will happen after
halfway through initially you remember
it made mistakes in recognizing but
halfway through now it has learned so
every time you show a fruit it will
exactly 100% accurately it will identify
it will say the child will say this is
an apple this is an orange and if you
show a pineapple it will say this is a
pineapple right so that means it has
kind of memorized this data now let's
say you bring another basket of fruits
and it will have a mix of maybe apples
which were already there in the previous
set but it will also have in addition to
Apple it will probably have a banana or
maybe another fruit like a jack fruit
right so this is an equivalent of your
test data set which the child has not
seen before some parts of it it probably
has seen like the apples it has seen but
this banana and jack fruit it has not
seen so then what will happen in the
first round which is an equivalent of
your training data set towards the end
it has 100% it was telling you what the
fruits are right Apple was accurately
recognized orange were was accurately
recognized and pineapples were
accurately recognized right so that is
like a 100% accuracy but now when you
get another a fresh set which were not a
part of the original one what will
happen all the apples maybe it will be
able to recognize correctly but all the
others like the jack fruit or the banana
will not be recognized by the child
right so this is an analogy this is an
equivalent of overfitting so what has
happened during the training process it
is able to recognize or reach 100%
accuracy maybe very high accuracy okay
and we call that as very low loss right
so that is the technical term so the
loss is pretty much zero and accuracy is
pretty much 100% whereas when you use
testing there will be a huge error which
means the loss will be pretty high and
therefore the accuracy will be also low
okay this is known as overfitting this
is basically a process where training is
done training process is it goes very
well almost reaching 100% accuracy but
while testing it really drops down now
how can you avoid it so that is a
extension of this question there are
multiple
ways of avoiding overfitting there are
techniques like what do you call
regularization that is the most common
technique that is used uh for uh
avoiding overfitting and within
regularization there can be a few other
subtypes like drop out in case of neural
networks and a few other examples but I
think if you give example or if you give
regularization as the technique probably
that should be sufficient so so there
will be some questions where where the
interviewer will try to test your
fundamentals and your knowledge and
depth of knowledge and so on and so
forth and then there will be some
questions which are more like trick
questions that will be more to stump you
okay then the next question is around
the methodology so when we are
performing machine learning training we
split the data into training and test
right so this question is around that so
the question is what is training set and
test set in machine learning model and
how is the split done so the question
can be like that so in machine learning
when we are trying to train the model so
we have a three-step process we train
the model and then we test the model and
then once we are satisfied with the test
only then we deploy the model so what
happens in the train and test is that
you remember the labeled data so let's
say you have th000 records with labeling
information now one way of doing it is
you use all the Thousand records for
training and then maybe right which
means that you have exposed all this
thousand records during the training
process and then you take a small set of
the same data and then you say okay I
will test it with this okay and then you
probably what will happen you may get
some good results all right but there is
a flaw there what is the flaw this is
very similar to human beings it is like
you are showing this model the entire
data as a part of training okay so
obviously it has become familiar with
the entire data so when you're taking a
part of that again and you're saying
that I want to test it obviously you
will get good results so that is not a
very accurate way of testing so that is
the reason what we do is we have the
label data of this thousand records or
whatever we set aside before starting
the training process we set aside a
portion of that data and we call that
test set and the remaining we call as
training set and we use only this for
training our model now the training
process remember is not just about aing
one round of this data set so let's say
now your training set has 800 records it
is not just one time you pass this 800
records what you normally do is you
actually as a part of the training you
may pass this data through the model
multiple times so this thousand records
may go through the model maybe 10 15 20
times till the training is perfect till
the accuracy is high till the errors are
minimized okay now so which is fine
which means that here that is what is
known as the model has seen your data
and gets familiar with your data and now
when you bring your test data what will
happen is this is like some new data
because that is where the real test is
now you have trained the model and now
you are testing the model with some data
which is kind of new that is like a
situation like a realistic situation
because when the model is deployed that
is what will happen it will receive some
new data not the data that it has
already seen right so this is a
realistic test so you put some new data
so this data which you have set aside is
for the model it is new and if it is
able to accurately predict the values
that means your training has worked okay
the model got drained properly but let's
say while you're testing this with this
test data you're getting lot of errors
that means you need to probably either
change your model or retrain with more
data and things like that now coming
back to the question of how do you split
this what should be the ratio there is
no fixed uh number again this is like
individual preferences some people split
it into 50/50 50% test and 50% training
Some people prefer to have a larger
amount for training and a smaller amount
for test so they can go by either 6040
or 7030 or some people even go with some
odd numbers like
6535 or uh
63333 which is like 1/3 and 2/3 so there
is no fixed rule that it has to be
something the ratio has to be this you
can go by your individual preferences
all right then you may have questions
around uh data handling data
manipulation or what do you call data
management or Preparation so these are
all some questions around that area
there is again no one answer one single
good answer to this it really varies
from situation to situation and
depending on what exactly is the problem
what kind of data it is how critical it
is what kind of data is missing and what
is the type of corruption so there a
whole lot of things this is a very
generic question and therefore you need
to be little careful about responding to
this as well so probably have to
illustrate this again if you have
experience in doing this kind of work in
handling data you can illustrate with
example saying that I was on one project
where I received this kind of data these
were the columns where data was not
filled or these were the this many rows
where the data was missing that would be
in fact a perfect way to respond to this
question but if you don't have that
obviously you have to provide some good
answer I think it really depends on what
exactly the situation is and there are
multiple ways of handling the missing
data or correct data now let's take a
few examples now let's say you have data
where some values in some of the columns
are missing and you have pretty much
half of your data having these missing
values in terms of number of rows okay
that could be one situation another
situation could be that you have records
or data missing but uh when you do some
initial calculation how many records are
corrupt or how many rows or observations
as we call it has this missing data
let's assume it is very minimal like 10%
okay now between these two cases how do
we so let's assume that this is not a
mission critical situation and in order
to fix this 10 % of the data the effort
that is required is much higher and
obviously effort means also time and
money right so it is not so Mission
critical and it is okay to let's say get
rid of these records so obviously one of
the easiest ways of handling the data
part or missing data is remove those
records or remove those observations
from your analysis so that is the
easiest way to do but then the downside
is as I said in as in the first case if
let's say 50% of your data is like that
because because some column or the other
is missing so it is not like every in
every place in every Row the same column
is missing but you have in maybe 10% of
the records column one is missing and
another 10% column two is missing
another 10% column 3 is missing and so
on and so forth so it adds up to maybe
half of your data set so you cannot
completely remove half of your data set
then the whole purpose is lost okay so
then how do you handle then you need to
come up with ways of filling up this
data with some meaningful value right
that is one one way of handling so when
we say meaningful value what is that
meaningful value let's say for a
particular column you might want to take
a mean value for that column and fill
wherever the data is missing fill up
with that mean value so that when you're
doing the calculations your analysis is
not completely way off so you have
values which are not missing first of
all so your system will work number two
these values are not so completely out
of whack that your whole analysis goes
for a TOS right there may be situations
where where if the missing values
instead of putting mean maybe a good
idea to fill it up with the minimum
value or with a zero so or with a
maximum value again as I said there are
so many possibilities so there is no
like one correct answer for this you
need to basically talk around this and
illustrate with your experience as I
said that would be the best otherwise
this is how you need to handle this
question okay so then the next question
can be how can you choose a classifier
based on on a training set data size so
again this is one of those questions uh
where you probably do not have like a
one size fits all answer first of all
you may not let's say decide your
classifier based on the training set
size maybe not the best way to decide
the type of the classifier and uh even
if you have to there are probably some
thumb rules which we can use but then
again every time so in my opinion the
best way to respond to this question is
you need to try out few classifiers
irrespective of the size of the data and
you need to then decide on your
particular situation which of these
classifiers are the right ones this is a
very generic issue so you will never be
able to just by if somebody defines a a
problem to you and somebody even if if
they show the data to you or tell you
what is the data or even the size of the
data I don't think there is a way to
really say that yes this is the classifi
that will work here no that's not the
right way so you need to still you know
test it out get the data try out a
couple of classifiers and then only you
will be in a position to decide which
classifier to use you try out multiple
classifiers see which one gives the best
accuracy and only then you can decide
then you can have a question around
confusion Matrix so the question can be
explain confusion Matrix right so
confusion Matrix I think the best way to
explain it is by by taking an example
and drawing like a small diagram
otherwise it can really become tricky so
my suggestion is to take a piece of pen
and paper and uh explain it by drawing a
small Matrix and confusion Matrix is
about to find out this is used
especially in classification uh learning
process and when you get the results
when the our model predicts the results
you compare it with the actual value and
try to find out what is the accurate
accuracy okay so in this case let's say
this is an example of a confusion Matrix
and it is a binary Matrix so you have
the actual values which is the labeled
data right and which is so you have how
manys and how many no so you have that
information and you have the predicted
values how many yes and how many no
right so the total actual values the
total yes is 12 + 11 13 and they are
shown here and the actual value NOS are
9 + 3 12 okay so that is what this
information here is so this is about the
actual and this is about the predicted
similarly the predicted values there are
yes are 12 + 3 15 yeses and no are 1 + 9
10 NOS okay so this is the way to look
at this confusion Matrix okay and uh out
of this what is the meaning convey so
there are two or three things that needs
to be explained outright the first thing
is for a model to be accurate the values
across the diagonal should be high like
in this case right that is one number
two the total sum of these values is
equal to the total observations in the
test data set so in this case for
example you have 12 + 3 15 + 10 25 so
that means we have 25 observations in
our test data set okay so these are the
two things you need to First explain
that the total sum in this Matrix the
numbers is equal to the size of the test
data set and the diagonal values
indicate the accuracy so by just by
looking at it you can probably have a
idea about is this uh an accurate model
is the model being accurate if they're
all spread out equally in all these four
boxes that means probably the accuracy
is not very good okay now how do you
calculate the accuracy itself right how
do you calculate the accuracy itself so
it is a very simple mathematical
calculation you take some of the
diagonals right so in this case it is 9
+ 12 21 and divide it by the total so in
this case what will it be let's me uh
take a pen so your your diagonal values
is equal to if I say d is equal to 12 +
9 so that is 21 right and the total data
set is equal to right we just calculated
it is 25 so what is your accuracy it is
21 by your accuracy is equal to 21 by 25
and this turns out to be about
85% right so this is 85% so that is our
accuracy okay so this is the way you
need to explain draw a diagram Give an
example and maybe it may be a good idea
to be prepared with an example so that
it becomes easy for you don't have to
calculate those numbers on the fly right
so couple of uh hints are that you take
some numbers which are with which add up
to 100 that is always a good idea so you
don't have to really do this complex
calculations so the total value will be
100 and then diagonal values you divide
once you find the diagonal values that
is equal to your percentage okay all
right so the next question can be a
related question about false positive
and false negative so what is false
positive and what is false negative now
once again the best way to explain this
is using a piece of paper and Pen
otherwise it will be pretty difficult to
to explain this so we use the same
example of the confusion Matrix and uh
we can explain that so A confusion
Matrix looks somewhat like this and um
when we just take yeah it looks somewhat
like this and we continue with the
previous example where this is the
actual value this is the predicted value
and uh in the actual value we have 12 +
1 13 yeses and 3 + 9 12 Nos and the
predicted values there are 12 + 3 15
yeses and uh 1 + 9 10 NOS okay now this
particular case which is the false
positive what is a false positive first
of all the second word which is positive
okay is referring to the predicted value
so that means the system has predicted
it as a positive but the real value so
this is what the false comes from but
the real value is not positive okay that
is the way you should understand this
term false positive or even false
negative so false positive so positive
is what your system as predicted so
where is that system predicted this is
the one positive is what yes so you
basically consider this row okay now if
you consider this row so this is this is
all positive values this entire row is
positive values okay now the false
positive is the one which where the
value actual value is negative predicted
value is positive but the actual value
is negative so this is a false positive
right and here is a true positive so the
predicted value is positive and the
actual value is also postive positive
okay I hope this is making sense now
let's take a look at what is false
negative false negative so negative is
the second term that means that is the
predicted value that we need to look for
so which are the predicted negative
values this row corresponds to predicted
negative values all right so this row
corresponds to predicted negative values
and what they are asking for false so
this is the row for predicted negative
values and the actual value is this one
right this is predicted negative and the
actual value is also negative therefore
this is a true negative so the false
negative is this one predicted is
negative but actual is positive right so
this is the false negative so this is
the way to explain and this is a way to
look at false positive and false
negative same way there can be true
positive and true negative as well so
again positive the second term you will
need to use to identify the predicted
row right so if we say true positive
positive we need to take for the
predicted part so predicted positive is
here okay and then the first term is for
the actual so true positive so true in
case of actual is yes right so true
positive is this one okay and then in
case of actual the negative now we are
talking about let's say true negative
true negative negative is this one and
the true comes from here so this is true
negative right nine is true negative the
actual value is also negative and the
predicted value is also negative okay so
that is the way you need to explain this
the terms false positive false negative
and true positive true negative then uh
you might have a question like what are
the steps involved in the machine
learning process or what are the three
steps in the process of developing a
machine learning model right so it is
around the methodology that is applied
so basically the way you can probably
answer in your own words but the way the
model development of the machine
learning model happens is like this so
first of all you try to understand the
problem and try to figure out whether it
is a classification problem or a
regression problem based on that you
select a few algorithms and then you
start the process of training these
models okay so you can either do that or
you can after due diligence you can
probably decide that there is one
particular algorithm that which is most
suitable usually it happens through
trial and error process but at some
point you will decide that okay this is
the model we are going to use okay so in
that case we have the model algorithm
and the model decided and then you need
to do the process of training the model
and testing the model and this is where
if it is supervised learning you split
your data the label data into training
data set and test data set and you use
the training data set to train your
model and then you use the test dat dat
set to check the accuracy whether it is
working fine or not so you test the
model before you actually put it into
production right so once you test the
model you're satisfied it's working fine
then you go to the next level which is
putting it for production and then in
production obviously new data will come
and uh the inference happens so the
model is readily available and only
thing that happens is new data comes and
the model predicts the values whether it
is regression or classification now so
this can be an iterative process so it
is not a straightforward process where
you do the training do the testing and
then you move it to production now so
during the training and test process
there may be a situation where because
of either overfitting or or things like
that the test doesn't go through which
means that you need to put that back
into the training process so that can be
an iterative process not only that even
if the training and test goes through
properly and you deploy the model in
production there can be a situation that
the data that actually comes the real
data that comes with that this model is
failing so in which case you may have to
once again go back to the drawing board
or initially it will be working fine but
over a period of time maybe due to the
change in the nature of the data once
again the accuracy will deteriorate so
that is again a recursive process so
once in a while you need to keep
checking whether the model is working
fine or not and if required you need to
tweak it and modify it and so on and so
forth so net net this is a continuous
process of um tweaking the model and
testing it and making sure it is up to
date then you might have question around
deep learning so because deep learning
is now associated with AI artificial
intelligence and so on so can be as
simple as what is a deep learning so I
think the best way to respond to this
could be deep learning is a part of
machine learning and then then obviously
the the question would be then what is
the difference right so deep learning
you need to mention there are two key
parts that interviewer will be looking
for whenu you're defining deep learning
so first is of course deep learning is a
subset of machine learning so machine
learning is still the bigger let's say
uh scope and deep learning is one one
part of it so then what exactly is the
difference deep learning is primarily
when we are implementing these our
algorithms or when we are using neural
networks for doing our training and
classification and regression and all
that right so when we use neural network
then it is considered as deep learning
and the term deep comes from the fact
that you can have several layers of
neural networks and these are called
Deep neural networks and therefore the
term deep you know deep learning uh the
other difference between machine
learning and deep learning which the
interviewer may be wanting to hear is
that in case of machine learning the
feature engineering is done manually
what do we mean by feature engineering
basically when we are trying to train
our model we have our training data
right right so we have our training
label data and uh this data has several
let's say if it is a regular table it
has several columns now each of these
columns actually has information about a
feature right so if we are trying to
predict the height weight and so on and
so forth so these are all features of
human beings let's say we have sensus
data and we have all this so those are
the features now there may be probably
50 or 100 in some cases there may be 100
such features now all of them do not
contribute to our model right so we as a
data scientist we have to decide whether
we should take all of them all the
features or we should throw away some of
them because again if we take all of
them number one of course your accuracy
will probably get affected but also
there is a computational part so if you
have so many features and then you have
so much data it becomes very tricky so
in case of machine learning we manually
take care of identifying the features
that do not contribute to the learning
process and thereby we eliminate those
features and so on right so this is
known as feature engineering and in
machine learning we do that manually
whereas in deep learning where we use
neural networks the model will
automatically determine which features
to use and which to not use and
therefore feature engineering is also
done automatically so this is a
explanation these are two key things
probably will add value to your response
all right so the next question is what
is the difference between or what are
the differences between machine learning
and deep learning so here this is a a
quick comparison table between machine
learning and deep learning and in
machine learning learning enables
machines to take decisions on their own
based on past data so here we are
talking primarily of supervised learning
and um it needs only a small amount of
data for training and then works well on
lowend system so you don't need large
machines and most features need to be
identified in advance and manually coded
so basically the feature engineering
part is done manually and the problem is
divided into parts and solved
individually and then combined so that
is about the machine learning part in
deep learning deep learning basically
enables machines to take decisions with
the help of artificial neural network so
here in deep learning we use neural l so
that is the key differentiator between
machine learning and deep learning and
usually deep learning involves a large
amount of data and therefore the
training all also requires usually the
training process requires high-end
machines uh because it needs a lot of
computing power and the Machine learning
features are or the feature engineering
is done automatically so the neural
networks takes care of doing the feature
engineering as well and in case of deep
learning therefore it is said that the
problem is handled end to end so this is
a quick comparison between machine
learning and deep learning in case you
have that kind of a question then you
might get a question around the uses of
machine learning or some real life
applications of machine learning in
modern business the question may be
worded in different ways but the the
meaning is how exactly is machine
learning used or actually supervised
machine learning it could be a very
specific question around supervised
machine learning so this is like give
examples of supervised machine learning
use of supervised machine learning in
modern business so that could be the
next question so there are quite a few
examples or quite a few use cases if you
will for supervised machine learning the
very common one is email spam detection
so you want to train your application or
your system to detect between spam and
non-spam so this is a very common
business application of supervised
machine learning so how does this work
the way it works is that you obviously
have historical data of of your emails
and they are categorized as spam and not
spam so that is what is the labeled
information and then you feed this
information or the all these emails as
an input to your model right and the
model will then get trained to detect
which of the emails are to detect which
is Spam and which is not spam so that is
the training process and this is
supervised machine learning because you
have labeled data you already have
emails which are tagged as spam or not
spam and then you use that to train your
model right so this is one example now
there are a few industry specific
applications for supervised machine
learning one of the very common ones is
in healthcare Diagnostics in healthcare
Diagnostics you have these images and
you want to train models to detect
whether from a particular image whether
it can find out if the person is sick or
not whether a person has cancer or not
right so this is a very good example of
super wied machine learning here the way
it works is that existing images it
could be x-ray images it be MRI or any
of these images are available and they
are tacked saying that okay this x-ray
image is defective or the person has an
illness or it could be cancer whichever
illness right so it is stacked as
defective or clear or good image and
defective image something like that so
we come up with a binary or it could be
multiclass as well saying that this is
defective to 10% % this is 25% and so on
but let's keep it simple you can give an
example of just a binary classification
that would be good enough so you can say
that in healthcare Diagnostics using
image we need to detect whether a person
is ill or whether a person is having
cancer or not so here the way it works
is you feed labeled images and you allow
the model to learn from that so that
when New Image is fed it will be able to
predict whether this person is having
that illness or not having cancer or not
right so I think this would be a very
good example for supervised machine
learning in modern business all right
then we can have a question like so
we've been talking about supervised and
um unsupervised then so there can be a
question around semi-supervised machine
learning so what is semi-supervised
machine learning now semi-supervised
learning as the name suggests it falls
between supervised learning and
unsupervised learning but for all
practical purposes it is considered as a
part of supervised learning and the
reason this has come into existence is
that in supervised learning you need
labeled data so all your data for
training your model has to be labeled
now this is a big problem in many
Industries or in many under many
situations getting the labeled data is
not that easy because there's a lot of
effort in labeling this data let's take
an example of the diagnos tic images we
can just let's say take X-ray images now
there are actually millions of x-ray
images available all over the world but
the problem is they are not labeled so
the images are there but whether it is
defective or whether it is good that
information is not available along with
it right in a form that it can be used
by a machine which means that somebody
has to take a look at these images and
usually it should be like a doctor and
uh then say that okay yes this image is
clean and this image is cancerous and so
on and so forth now that is a huge
effort by itself so this is where
semi-supervised learning comes into play
so what happens is there is a large
amount of data maybe a part of it is
labeled then we try some techniques to
label the remaining part of the data so
that we get completely labeled data and
then we train our model so I know this a
little long winding explanation but
unfortunately there is no uh quick and
easy definition for semi-supervised
machine learning this is the only way
probably to explain this concept we may
have another question as um what are
unsupervised machine learning techniques
or what are some of the techniques used
for performing unsupervised machine
learning so it can be worded in
different ways so how do we answer this
question so unsupervised learning you
can say that there are two types
clustering and Association and
clustering is a technique where similar
objects are put together and there are
different ways of finding similar
objects so their characteristics can be
measured and if they have in most of the
characteristics if they are similar then
they can be put together this is
clustering then Association you can I
think the best way to explain
Association is with an example in case
of Association you try to find out how
the items are linked to each other so
for example if somebody bought a maybe a
laptop the person has also purchased a
mouse so this is more in an e-commerce
scenario for example so you can give
this as an example so people who are
buying laptops are also buying the mouse
so that means there is an association
between laptops and mouse or maybe
people who are buying bread are also
buying butter so that is a Association
that can be created so this is
unsupervised learning one of the
techniques okay all right then we have
very fundamental question what is the
difference between supervised and
unsupervised machine learning so machine
learning these are the two main types of
machine learning supervised and unised
and in case of supervised and again here
probably the keyword that the person may
be wanting to hear is labelled data now
very often people say we have historical
data and if we run it it is supervised
and if we we don't have historical data
yes but you may have historical data but
if it is not labeled then you cannot use
it for supervised learning so it is it's
very key to understand that we put in
that keyword labeled okay so when we
have labeled data for training our model
then we can use supervised learning and
if we do not have labeled data then we
use unsupervised learning and there are
different algorithms available to
perform both of these types of uh
trainings so there can be another
question a little bit more theoretical
and conceptual in nature this is about
inductive machine learning and deductive
machine learning so the question can be
what is the difference between inductive
machine learning and deductive machine
learning or somewhat in that manner so
that the exact phrase or exact question
can vary they can ask for examples and
things like that but that could be the
question so let's first understand what
is inductive and deductive training
inductive training is induced by
somebody and you can illustrate that
with a small example I think that always
helps so whenever you're doing some
explanation try as much as possible as I
said to give examples from your work
experience or give some analogies and
that will also help a lot in explaining
as well and for the interviewer also to
understand so here we'll take an example
or rather we will use an analogy so
inductive training is when we induce
some knowledge or the learning process
into a person without the person
actually experiencing it okay what can
be an example so we can probably tell
the person or show a person a video that
fire can burn the F burn his finger or
fire can cause damage so what is
happening here this person has never
probably seen a fire or never seen
anything getting damaged by fire but
just because he has seen this video he
knows that okay fire is dangerous and if
fire can cause damage right so this is
inductive learning compared to that what
is deductive learning so here you draw
conclusion or the person draws
conclusion out of experience so we will
stick to the analogy so compared to the
showing a video Let's assume a person is
allowed to play with fire right and then
he figures out that if he puts his
finger it's burning or if throws
something into the fire it burns so he
is learning through experience so this
is known as deductive learning okay so
you can have applications or models that
can be trained using inductive learning
or deductive learning all right I think
uh probably that explanation will be
sufficient the next question is are KNN
and K means clustering similar to one
another or are they same right because
that the letter K is kind of common
between them okay so let us take a
little while to understand what these
two are one is KNN and another is K
means K stands for K nearest neighbors
and K means of course is the clustering
mechanism now these two are completely
different except for the letter K being
common between them K andn is completely
different K means clustering is
completely different KNN is a
classification process and therefore it
comes under supervised learning whereas
K means clustering is actually a
unsupervised okay when you have k andn
when you want to implement KN andn which
is basically K nearest neighbors the
value of K is a number so you can say k
is equal to three you want to implement
KN andn with K is equal to 3 so which
means that it performs the
classification in such a way that how
does it perform the classification so it
will take three nearest objects and
that's why it's called nearest neighbor
so basically based on the distance it
will try to find out its nearest objects
that are let's say three of the nearest
objects and then it will check whether
the class they belong to which class
right so if all three belong to one
particular class obviously this new
object is also classified as that
particular class but it is possible that
they may be from two or three different
classes okay so let's say they are from
two classes and then if they are from
two classes now usually you take a odd
number you assign odd number to so if
there are three of them and two of them
belong to one class and then one belongs
to another class so this new object is
assigned to the class to which the two
of them belong now the value of K is
sometimes tricky whether should you use
three should you use five should you use
seven that can be tricky because the
ultimate classification can also vary so
it's possible that if you're taking K as
three the object is probably in one
particular class but if you take K is
equal to 5 maybe the object will belong
to a different class because when you're
taking three of them probably two of
them belong to a class one and one
belong to class two whereas when you
take five of them it is possible that
only two of them belong to class one and
three of them belong to class two so
which means that this object will belong
to class two right so you see that so it
is the class allocation can vary
depending on the value of K now K means
on the other hand is a clustering
process and it is unsupervised where
what it does is the system will
basically identify how the objects are
how close the object are with respect to
some of their features okay and but the
similarity of course is the the letter K
and in case of K means also we specify
its value and it could be three or five
or seven there is no technical limit as
such but it can be any number of
clusters that uh you can create okay so
based on the value that you provide the
system will create that many clusters of
similar objects so there is a similarity
to that extent that K is a number in
both the cases but actually these two
are completely different processes we
have what is known as KN based
classifier and people often get confused
thinking that KN base is the name of the
person who found this uh classifier or
who developed this classifier which is
not 100% true base is the name of the
person b y s is the name of the person
but naive is not the name of the person
right so naive is basically an English
word and that has been added here
because of the nature of this particular
classifier na based classifier is a
probability based classifier and uh it
makes some assumptions that presence of
one feature of a class is not related to
the presence of any other feature of
maybe other classes right so which is
not a very strong or not a very what do
you say accurate assumption because
these features can be related and so on
but even if we go with this assumption
this whole algorithm works very well
even with this assumption and that is
the good side of it but the term comes
from there so that is the explanation
that you can give then there can be
question around reinforcement learning
it can be paraphrased in multiple ways
one could be can you explain how a
system can play a game of chess using
reinforcement learning or it can be any
game so the best way to explain this is
again to talk a little bit about what
reinforcement learning is about and then
elaborate on that to explain the process
so first of all reinforc learning has an
environment and an agent and the agent
is basically performing some actions in
order to achieve a certain goal and this
goals can be anything either if it is
related to game then the goal could be
that you have to score very high score a
high value High number or it could be
that your uh number of lives should be
as high as possible don't lose lives so
these could be some of them more
advanced examples could be for driving
in the automotive industry self-driving
cars they actually also make use of
reinforcement learning to teach the car
how to navigate through the roads and so
on and so forth that is also another
example now how does it work so if the
system is basically there is an agent
and environment and every time the agent
takes a step or performs a task which is
taking it towards the goal the final
goal let's say to maximize the score or
to minimize the number of lives and so
on or minimize the Ts for example it is
rewarded and every time it takes a step
which goes against that goal right
contrary or in the reverse Direction it
is penalized okay so it is like a carrot
and stick system now how do you use this
to create a game of chess so to create a
system to play a game of chess now the
way this works is and this could
probably go back to this Alpha go
example where alphao defeated a human
Champion so the way it works is in
reinforcement learning the system is
allowed for example if in this case
we're talking about Chess so we allow
the system to first of all watch playing
a game of chess so it could be with a
human being or it could be the system
itself there are computer games of Chess
right so either this new learning system
has to watch that game or watch a human
being play the game because this is
reinforcement uh learning is pretty much
all visual so when you're teaching the
system to play a game the system will
not actually go behind the scenes to
understand the logic of your software of
this game or anything like that it is
just visually watching the screen and
then it learns okay so reinforcement
learning to a large extent works on that
so you need to create a mechanism
whereby your model will be able to watch
somebody playing the game and then you
allow the system also to start playing
the game so it pretty much starts from
scratch okay and as it moves forward it
it it's at right at the beginning the
system really knows nothing about the
game of chess okay so initially it is a
clean slate it just starts by observing
how you're playing so it will make some
random moves and keep losing badly but
then what happens is over a period of
time so you need to now allow the system
or you need to play with the system not
just one 2 three four or five times but
hundreds of times thousands of times
maybe even hundreds of thousands of
times and that's exactly how alpha go
has done it played millions of games
between itself and the system right so
for the game of chess also you need to
do something like that you need to allow
the system to play chess and then learn
on its own over a period of repetition
so I think you can probably explain it
to this much to this extent and it
should be uh sufficient
now this is another question which is
again somewhat similar but here the size
is not coming into picture so the
question is how will you know which
machine learning algorithm to choose for
your classification problem now this is
not only classification problem it could
be a regression problem I would like to
generalize this question so if somebody
asks you how will you choose how will
you know which algorithm to use the
simple answer is there is no way you can
decide exactly saying that this is the
algorithm I'm going to use in a variety
of situations there are some guidelines
like for example you will obviously
depending on the problem you can say
whether it is a classification problem
or a regression problem and then in that
sense you are kind of restricting
yourself to if it is a classification
problem there are you can only apply a
classification algorithm right to that
extent you can probably let's say limit
the number of algorithms but now within
the classification algorithms you have
decision trees you have svm you have
logistic regression is it possible to
outright say yes so for this particular
problem since you have explained this
now this is the exact algorithm that you
can use that is not possible okay so we
have to try out a bunch of algorithms
see which one gives us the best
performance and best accuracy and then
decide to go with that particular
algorithm so in machine learning a lot
of it happens through trial and error
there is uh no real possibility that
anybody can just by looking at the
problem or understanding the problem
tell you that okay in this particular
situation this is exactly the algorithm
that you should use then the questions
may be around application of machine
learning and this question is
specifically around how Amazon is able
to recommend other things to buy so this
is around recommendation engine how does
it work how does the recommendation
engine work so this is basically the
question is all about so the
recommendation engine again Works based
on various inputs that are are provided
obviously something like uh you know
Amazon website or e-commerce site like
Amazon collects a lot of data around the
customer Behavior who is purchasing what
and if somebody is buying a particular
thing they're also buying something else
so this kind of Association right so
this is the unsupervised learning we
talked about they use this to associate
and Link or relate items and that is one
part of it so they kind of build
association between items saying that
somebody buying this is also buying this
that is one part of it then they also
profile the users right based on their
age their gender their geographic
location they will do some profiling and
then when somebody is logging in and
when somebody is shopping kind of the
mapping of these two things are done
they try to identify obviously if you
have logged in then they know who you
are and your information is available
like for example your age maybe your
gender and where you're located what you
purchased earlier right so all this is
taken and the recommendation engine
basically uses all this information and
comes up with recommendations for a
particular user so that is how the
recommendation engine works and with
that we have come to the end of this IML
complete full course I hope you found it
valuable and entertaining please ask any
questions about the topics covered in
this video in the comment section below
our experts will assist you in
addressing your problems thank you for
watching stay safe and keep learning
staying ahead in your career requires
continuous learning and upskilling
whether you're a student aiming to learn
today's top skills or a working
professional looking to advance your
career we've got you covered explore our
impressive catalog of certification
programs in cuttingedge domains
including data science cloud computing
cyber security AI machine learning or
digital marketing designed in
collaboration with leading universities
and top corporations and delivered by
industry experts choose any of our
programs and set yourself on the path to
Career Success click the link in the
description to know
more hi there if you like this video
subscribe to the simply learn YouTube
channel and click here to watch similar
videos to nerd up and get certified
click
here