welcome to this new video by simply
learn Docker and devops team up for a
streamlined software development Docker
containers ensure consistency and
portability while devops focus on
collaboration Automation and CI CD
resulting in Faster more efficient
development and deployment here in this
Docker crash course we will understand
what is Docker how to install Docker in
Ubuntu and Windows 10 we will see some
advanced doku concepts what is Docker
container introduction to Docker swarm
moving ahead we will see what is Docker
decompose Docker commands and how to
dockerize an application we will see
what is the difference between
kubernetes and Docker Docker and virtual
machine and wrap up this video with
Docker interview question and answers
learning is good but getting certified
is the best on that note if you are
looking to get certified in devops and
become a Docker expert have a look at
postgraduate program in devops by simply
learn this program offers Blended
learning that combines live online
devops certification classes with
interactive Labs that will give you
hands-on experience you will gain
expertise and tools like terraform
meowing ansible Jenkins Docker junit git
and more so hurry up and enroll now we
prefer one year of Prior experience to
enroll in this course so why wait enroll
the course from the link in the
description box so let's take a slow
scenario of a developer and a tester
before you had the world of Docker a
developer would actually build the code
and then they send it to the tester but
then the code wouldn't work on their
system encoders will work on the other
system due to the differences in
computer environments so what could be
the solution to this well you could go
ahead and create a virtual machine to be
the same of the solution in both areas
we think Docker is an even better
solution so this kind of break out what
the main big differences are between
Docker and virtual machines as you can
see between the left and the right hand
side both look to be very similar what
you'll see however is that on the docker
side what you'll see as a big difference
is that the guest OS for each container
has been illuminated Docker is
inherently more lightweight but provides
the same functionality as a virtual
machine so let's step through some of
the pros and cons of a virtual machine
versus Docker so first of all a virtual
machine occupies a lot more memory space
on the host machine in contrast Docker
occupies significantly less memory space
the boot up time between both is very
different Docker just boots up faster
the performance of the docker
environment is actually better and more
consistent than the virtual machine
Docker is also so very easy to set up
and very easy to scale the efficiencies
therefore are much higher with a Docker
environment versus a virtual machine
environment and you'll find it is easier
to Port Docker across multiple platforms
than a virtual machine finally the space
allocation between Docker and a virtual
machine is significant when you don't
have to include the guest OS you're
eliminating a significant amount of
space and the dock environment is just
inherently smaller so after Docker as a
developer you can build out your
solution and send it to a tester and as
long as we're all running in the docker
environment everything will work just
great so let's step through what we're
going to cover in this presentation
we're going to look at the devops tools
and where Docker fits within that space
we'll examine what Docker actually is
and how Docker works and then finally
we'll step through the different
components of the docker environment so
what is devops devops is a collaboration
between the development team the
operation team allowing you to
continuously deliver Solutions and
applications and services that both
delight and improve the efficiency of
your customers if you look at the Venn
diagram that we have here on the left
hand side we have development on the
right hand side we have operation and
then there's a crossover in the middle
and that's where the devops team sits if
we look at the areas of integration
between both groups developers are
really interested in planning code
building and testing and operations want
to be able to efficiently deploy operate
a monitor when you can have both groups
interacting with each other on these
seven key and elements then you can have
the efficiencies of an excellent devops
team so planning and code base we use
tools like jit and gearer for building
we use Gradle and Maven test thing we
use selenium the integration between Dev
and Ops is through tools such as Jenkins
and then the deployment operation is
done with tools such as Docker and Chef
finally nagias is used to monitor the
entire environment so let's step deeper
into what Docker actually is so Docker
is a tool which is used to automate the
deployment of applications in a
lightweight container so the application
can work efficiently in different
environments now it's important to note
that the container is actually a
software package that consists of all
the dependencies required to run the
application so multiple containers can
run on the same Hardware the containers
are maintained in isolated environments
they're highly productive and they're
quick and easy to configure so let's
take an example of what Docker is by
using a house that may be rented for
someone using Airbnb so in the house
there are three rooms and only one
cupboard and kitchen and the problem we
have is that none of the guests are
really ready to share the cupboard and
kitchen because every individual has a
different preference when it comes to
how the cupboard should be stocked and
how the kitchen should be used this is
very similar to how we run software
applications today each of the
applications could end up using
different Frameworks so you may have a
framework such as rails perfect and
flask and you may want to have them
running for different applications for
different situations this is where
Docker will help you run the
applications with the suitable
Frameworks so let's go back to our
Airbnb example so we have three rooms
and a kitchen and cupboard how do we
resolve this issue well we put a kitchen
and cupboard in each room we can do the
same thing for computers Docker provides
the suitable Frameworks for each
different application and since every
app application has a framework with a
suitable version this space could also
then be utilized for putting in software
and applications that are long and since
every application has its own framework
and suitable version the area that we
had previously stored for a framework
can be used for something else now we
can create a new application in this
instance a fourth application that uses
its own resources you know what with
these kinds of abilities to be able to
free up space on the computer it's no
wonder Docker is the right choice so
let's take a closer look to how Docker
actually works so when we look at Docker
and we call something Docker we're
actually referring to the base engine
which actually is installed on the host
machine that has all the different
components that run your Docker
environment and if we look at the image
on the left hand side of the screen
you'll see that Docker has a client
server relationship there's a client
installed on the hardware there is a
client that contains the docker product
and then there is a server which
controls how that Docker client is
created the communication that goes back
and forth to be able to share the
knowledge on that Docker client
relationship it's done through a rest
API this is fantastic news because that
means that you can actually interface
and program that API so we look here in
the animation we see that the docker
client is constantly communicating back
to the server information about the
infrastructure and it's using this rest
API as that Communication channel the
docker server then will check out the
requests and the interaction necessary
for it to be the docker Daemon which
runs on the server itself well then
check out the interaction and the
necessary operating system pieces needed
to be able to run the container okay so
that's just an overview of the docker
engine which is probably where you're
going to spend most of your time but
there are some other components that
farm on the infrastructure for Docker
let's dig into those a little bit deeper
as well so what we're going to do now is
break out the four main components that
comprise of the docker environment the
four components are as follows the
docker client and server which we've
already done a deeper dive on Docker
images Docker containers and the docker
registry so if we look at the structure
that we have here on the left hand side
you see the relationship between the
docker client and the docker server and
then we have the rest API in between now
if we start digging into that rest API
particularly the relationship with the
docker Daemon on the server we actually
have our other elements that form the
different components of the docker
ecosystem so the docker client is
accessed from your terminal window so if
you are using Windows it's going to be
Powershell on Mac it's going to be your
terminal window and it allows you to run
the docker Daemon and the registry
service when you have your terminal
window open so you can actually use your
terminal window to create instructions
on how to build and run your Docker
images and containers if we look at the
images part of our registry here we
actually see that the image is really
just a template with the instructions
used for creating the containers which
you use within Docker the docker image
is built using a file called the docker
file and then once you've created that
Docker file you'll store that image in
the docker Hub or registry and that
allows other people to be able to access
the same structure of a Docker
environment that you've created the
syntax of creating the image is fairly
simple it's something that you'll be
able to get your arms around very
quickly and essentially what you're
doing is you're creating the option of a
new container you're identifying what
the image will look like what are the
commands that are needed and the
arguments for within those commands and
once you've done that you have a
definition for what your image will look
like so if we look here at what the
container itself looks like is that the
container is a standalone executable
package which includes applications and
their dependencies it's the instructions
for what your environment will look like
so you can be consistent in how that
environment is shared between multiple
developers testing units and other
people within your devops team now the
thing that's great about working with
Docker is that it's so lightweight that
you can actually run multiple Docker
containers in the same infrastructure
and share the same operating system this
is its strength it allows you to be able
to create those multiple environments
that you need for multiple projects that
you're working on interestingly though
within each container that container
creates an isolated area for the
applications to run so while you can run
multiple containers in an infrastructure
each of those containers are completely
isolated they're protected so that you
can actually control how your Solutions
work there now as a team you may start
off with one or two developers on your
team but when a project starts becoming
more important and you start adding in
more people to your team you may have 15
people that are offshore you may have 10
people that are local you may have 15
Consultants that are working on your
project you have a need for each of
those developers or each person on your
team to have access to that Docker image
and to get access to that image we use a
Docker registry which is an Open Source
server-side service for hosting and
distributing the images that you have
defined you can also use Docker itself
as its own default Retreat and Docker
Hub now something that has to be bear in
mind though is that for publicly shared
images you may want to have your own
private images in which case you would
do that through your own registry so
once again public repositories can be
used to host the docker images which can
be accessed by anyone and I really
encourage you you to go out to dogger
and see the other Docker images that
have been created because there may be
tools there that you can use to speed up
your own development environments now
you will also get to a point where you
start creating environments that are
very specific to the solutions that you
are building and when you get to that
point you'll likely want to create a
private repository so you're not sharing
that knowledge with the world in general
now the way in which you connect with
the docker registry is through simple
pull and push commands that you run
through terminal window to be able to
get the latest information so if you
want to be able to build your own
container what you'll start doing is
using the pull commands to actually pull
the image from the docker repository and
the command line for that is fairly
simple in terminal window you would
write Docker pull and then you put in
the image name and any tags associated
with that image and use the command
pause so in your terminal window you
would actually use a simple line of
command once you've actually connected
to your Docker environment and that
command will be Docker pull with the
image name and any Associated tags
around that image what that will then do
is pull the image from the docker
repository whether that's a public
repository or a private one now in
Reverse if you want to be able to update
the docker image with a new information
you do a push command where you take the
script that you've written about the
docker container that you define and
push it to the repository and as you can
imagine the commands for that are also
fairly simple in terminal window you
would write Docker push the image name
any Associated tags and then that would
then push that image to the docker
repository again either a public or a
private repository so if we recap the
docker file creates a Docker image
that's using the build commands Docker
image then contains all the information
necessary for you to be able to execute
the project using the docker image any
user can run run the code in order to
create a Docker container and once a
Docker image is built it's uploaded to a
registry or to a Docker Hub where it can
be shared across your entire team and
from the docker Hub users can get access
to the docker image and build their own
new containers so the five key takeaways
here so with a virtual machine you're
able to create a virtualized environment
to run an application on an operating
system with Docker it allows you to
focus on just running the application
and doing it consistently it improves
the ability for teams to be able to
share environments that are consistent
from Team to team it's highly productive
and it's really quick and easy to
configure the architecture of Docker is
really primarily built out of four
components of which the one that you'll
use the most is the client server
environment where as a developer you
have a client application running on
your local machine and then you connect
with a server environment where you're
getting the latest information about
that container that you're building a
solution for and then finally what we
see with the workflow improvements with
Docker is that the goal is to be able to
be more efficient to be able to be more
consistent with your development
environments and be able to push out
those environments whether it goes to a
test person to a business analyst or
anybody else on your devops team so they
have a consistent environment that looks
and acts exactly like your production
environment and can be eventually pushed
out to a production environment using
tools such as puppet or Chef so you're
creating a consistent operations
environment so what is Docker Docker is
a tool which is used to automate the
deployment of applications in
lightweight containers so when I say
containers I mean a software package
that consists of all the dependencies
required to run the particular
application and when you deploy
containers you're basically providing
the capability for the application to
run in any kind of environment so
Dockers have multiple features some of
which are that multiple containers can
run on the same Hardware it has very
high productivity when compared to
Virtual machines it maintains isolated
applications and has a very quick and
easy configuration process so now let's
begin with the demo we'll be installing
Docker on an Ubuntu system so this is my
system I'll just open the terminal so
the first thing you can start with is
removing any Docker installation that
you probably already have present in
your system if you want to start from
scratch so this is the command to do so
sudo app get removed docker
dock engine docker.io enter your
password
and Docker is removed so now we'll start
from scratch and we'll install Docker
once again before that I'll just clear
my screen
okay so before I install Docker let me
just ensure that all these softwares on
my system currently is in its latest
state
so sudo app get update
great so that's done next thing we'll
actually install our docker
so type in sudo apt
get
install
docker
now as you can see here there's an error
that's occurred so sometimes it's
possible that due to the environment of
the machine that you're working in this
particular command does not work
in which case there's always another
command that you can start with
just type Docker install
and that by itself will give you the
commands you can use to install docker
so as it says here sudo apt install.io
is a command that we will need to
execute to install docker
and after that we'll execute the sudo
snap install Docker so sudo apt install
Docker dot IO first
and this will install your docker
after that's done we will have pseudo
snap install Docker so snap install
Docker installs a newly created snap
package
there are basically some other
dependencies for Docker that you'll have
to install
of course since this is the installation
process for the entire Docker i o it
will take some time
great so our Docker is installed the
next thing we do as I mentioned earlier
is that we need to install all the
dependency packages so the command for
that is
sudo snap install talk
enter your password
so with that we have completed the
insulation process for Docker but we'll
perform a few more stages where we will
test if the installation has been done
right
so before we move on with the testing
for Docker let's once again just check
the version that we have installed
so for that the command is Docker
version
and as you can see Docker version
17.12.1 CE has been installed next thing
we do is we pull an image from the
docker hub
so Docker run
hello
world
now hello world is a Docker image which
is present on the docker Hub Docker Hub
is basically a repository that you can
find online so with this command the
docker image hello world has been pulled
onto your system
so let's see if it's actually present on
your system now the command to check
this
is pseudo Docker images
and as you can see here hello world
repository this is present on our system
currently so the image has been
successfully pulled onto the system and
this means that a Docker is working now
we'll try out another command
pseudo Docker PS minus a this displays
all the containers that you have pulled
so far so as you can see here there are
three hello world images displayed and
all of them are in exited state so I did
this demo previously too which is why
the two hello worlds which is created
two minutes ago is also displayed here
and the first hello world which has been
created a minute ago is the one we just
did for this demo now as you have
probably noticed that all the hello
world images over here all these
containers are in the exited state so
when you give the option for Docker PS
minus a where minus a stands for all it
displays all the containers whether they
are in exited or running state if you
want to see only those containers which
are in their running State you can
simply execute sudo Docker PS
pseudo docker
yes
and as you can see no container is
visible here because none of them are in
running State and with that we come to
an end of our Docker installation now
Docker is something which is available
for most of the operating systems
different different platforms so it
supports both the Unix and the windows
platform as such so Linux through
various commands we can do the
installation but in the case of Windows
you have to download the exe file and a
particular installer from the docker Hub
websites you can simply Google it and
you will get a kind of link from where
you will be able to download the package
so let's go to the Chrome and try to
search on for the windows string
particular installer you will get a link
from Docker Hub you download it you get
the stable version you get the edge
version whichever version you want you
wish to download you can download it so
let's go back to the Chrome so here you
have the docker desktop for Windows so
you can go for the stable or you can go
for the edge right so you also have the
comparison that what is the difference
between these two versions right so
um the particular Edge version is
something which is getting releases
every month and the stable version is
getting the releases every quarter so
they are not doing much of the changes
to the stable version as compared to the
edge there so you just have to double
click on the installer and that will
help you to do the installation of the
process so let's get started so you just
click on the get in stable version so
when you do that the particular
installer is going to install now it's
going to take like around 300 MB there
so that's the kind of install which is
available so once the installer is
downloaded so what you can do is that
you can actually go ahead and you can
proceed with the doing the double click
on this installer when you double click
on that you have to proceed with some of
the steps like you know from the GUI
itself you are going to proceed with
these steps so we'll wait for 10 to 20
seconds more and then the installer will
be done and then we can do the double
click and the installation will proceed
so another thing is that uh there is a
huge difference between the installer
like for example in case of Unix they
installed is a little bit less but in
case of windows it's a GUI is also
involved and there are a lot of binaries
which is available there so that's the
reason why you know the huge size is
there now it's available for free that's
for sure and it also requires the
Windows 10 professional or Enterprise
64-bit there so um if you are working on
some previous uh version of operating
systems like Windows 7 and all you have
the older version called Docker toolbox
so they used to call it as like Docker
toolbox earlier but now they are calling
it as in Docker desktop with a new
Docker Windows 10 support as such here
so another couple of seconds and then
the installer will be done and then we
will be able to proceed with the
installation
so let's see that how much progress is
there to the download so we'll click on
the downloads and here still we have
some particular installations or some
download going on so we'll wait for some
time and once the installation is done
then we'll go back and we'll proceed
with installation
so couple of seconds
so it's almost done so I'll just click
on this one you can go to the directory
to the downloads and you can double
click on that also but if you want to do
the installation you can click on this
one also and it will ask for the
approval yes or no you have to provide
now once that is done so um a desktop a
kind of a GUI component will open there
so it will start proceeding with the
installation so it's asking whether you
want to add a desktop the shortcut to
desktop so you can say okay I'm going to
click on OK so it will unpack the files
all the files which is required for
Docker to successfully install that is
getting unpacked over here so it will
take some time to do the installation
because it's doing a lot of work here so
you can just wait for till the execution
of the installer to be completed and
once the installer is done you can open
your command line and start working on
the docker
so taking some time to extract the files
now it's asking us to you know do the
close and do the restart so once that is
done you will be able to proceed further
and you can just you know run the
command line and any Docker command if
you can run so that will give you the
response whether the docker is installed
or not so you can see here that Docker
is you know something which is installed
so you can run like Docker version you
will be able to get a version of the
client when you do the restart of the
machine then add that moment of time the
docker server will also be started and
then this particular error message will
go off right now the docker Daemon is
not up and running because the
installation requires a restart and when
you close on this one and go for the
restart the machine will be restarted
here so this is the way that how exactly
we can go for a Docker installation and
we can go on that part so let's look at
some of the more advanced concepts
within the docker environment and we're
going to look at two Advanced components
one is Docker compose and the second is
Docker swarm so let's look at docket
compose Docker compose is really
designed for running multiple containers
as a single service and it does this by
running each container in isolation but
allowing the containers to interact with
each other as was stated earlier on you
would actually write the compose
environment using yaml as the language
in the files that you would create so
where would you use something like
Docker compose so an example would be if
you are running an Apache server with my
SQL database and you need to create a
additional containers to run additional
services without the need to start each
one separately and this is where you
would write a set of files using Docker
compose to be able to help balance out
that demand so let's now look at Docker
swarm so Docker swarm is a service that
allows you to be able to control
multiple Docker environments within a
single platform so what you actually are
looking at doing is within your Docker
swamp is we're treating each node as a
Docker Daemon and we're actually having
an API that's interacting with each of
those nodes there are two types of nodes
that you're going to be getting
comfortable working with one is the
manager node and the second is the
worker node and as you'd expect the
manager node is the one sending out the
instructions to all of the worker nodes
but there is a two-way communication
that is happening the communication
allows for the manager node to be able
to manage the instructions and then
listen to and receive updates from the
working node so if anything happens
within this environment the major node
can react and adjust the architecture of
the a worker node so it's always in sync
it was really great for a large scaled
environments so finally let's go through
what are some of the basic commands
you'd use within Docker and once we've
gone through all these basic commands
we'll actually show you a demo of how
you'd actually use them as well so if
we're going to go in probably the first
command is to install Docker and so if
you have yam installed you just do yum
install Docker and you'll install Docker
onto your computer to start the docker
Daemon is you want to do system CTL
start Docker the command to remove a
Docker image is Docker RMI and then the
image ID itself and that's not the image
name that's the actual alphanumeric ID
number that you want to grab the command
line to download a new image is Docker
pull and then the name of the image
you'd want to pull and by default you're
going to be pulling from the docker
default registry that will then connect
to your Docker Daemon and download the
images from that registry the command
line to run an image is Docker run and
then the image ID and then we have the
if we wanted to pull specifically from
Docker Hub then we would have a Docker
pull and then the image name and colon
It's tag to pull build an image from a
Docker file you would do Docker build
Dash T and then the image name and colon
tag to shut down the container you do
Docker stop container ID the access for
running a container is Docker exec it
container ID bash so we've gone through
all the different commands but let's
actually see how they would actually
look and we're going to go ahead and do
a demo so welcome to this demo where
we're going to go ahead and put together
all of the different commands that we
have outlined in the presentation for
Docker first is just to list all of the
docker images that we have so we do sudo
Docker images and we enter in password
and this will Now list out the images
that we've created already and we have
three images there
so let's go ahead and pull a Docker
image so to do that we'll we'll go ahead
and type sudo docker
and actually we don't want to do image
we want to select pull and then the name
of the image that we want to pull which
is going to be my SQL
and by default this is actually going to
go ahead and use the latest MySQL
command MySQL image that we have so it's
now going to head and pull this image
it's going to take a few minutes
depending on your internet connection
speed it's kind of a large file that has
to be downloaded so we'll just wait for
that to download
you can see the others have completed
just move this last file to download
almost there once that's done we're
going to go ahead and do as we'll
actually run the docking container and
create a new container using the image
that we just downloaded but we have to
wait for this to download First
all right so the image has been pulled
from Docker hub
and let's go ahead and create the new
Docker container so we're going to do
sudo docker
run Dash D
Ash p
0.0.0.0
[Music]
colon 80
column 80.
and then put in MySQL
call on latest so we have the latest
version
and we have our new token
and that shows our new Docker container
has been created
now let's go ahead and see if the
container is running
and we'll do sudo docker
PS
to list all the running containers and
what we see is that the container is not
listed there which means it's probably
not running so let's go ahead and list
out all of the images that we have
within Docker so we can see whether it's
actually listed there so we'll do PS
Dash a and yes there we are we can see
that we do have our new container my SQL
latest and it was created 36 seconds ago
but it's in the exited mode so what we
have to do is we have to change that
status so it's actually running
so let's change that to running state
we'll do sudo
docker
run
Dash
it
Dash Dash
name
and we can name it
SL
ql
MySQL
slash bin slash
[Music]
and that's now going to be in the root
and we'll exit out of that and now if we
list out the docking containers we
should see it is now an active container
pseudo docker
start
and then we'll start the scene and then
and we should now see it
there we are it's now in the running
State excellent
and we can see that it was updated six
seconds ago
we're gonna go ahead and we're going to
clear the screen
okay now what we want to do is remove
the docker container so we're going to
do is check the list of images that we
have
and so sudo Docker images
here are the images that we have and we
have my sqls listed and what we want to
do is delete my SQL and to do that we're
going to type in sudo Docker RM Dash F
image MySQL
run that command and what we'll find is
the image now there's no such image oh
okay so what we actually have to do is
we have to go and see that the image is
now gone it's been removed excellent
it's exactly what we wanted to see
and we can also delete an image by its
image ID as well
however if an image is running and
active we have to kill that image first
so I'm going to go ahead and we're going
to select the image ID we'll copy that
and it's going to
replace that it won't be able to
actually run correctly because the image
is active so what we have to do now is
stop the image and then we can kill it
so it's in the running state
so we have to
so we do pseudo Docker kill
and kill SL and that will kill the
container and now we'll see that the
container has gone
and now we can delete the image
and that's going to be the image gone
with image ID but boom easy peasy
okay let's go ahead on to the next
exercise which is to
so here we are we've listed all of the
uh containers and they're all gone so
let's go into the next exercise final
exercise which is to actually create a
batch image and we're going to do a
batch HTTP image so let's go ahead and
write that out so it's going to be
Docker run
Dash did
dash dash name
white is that's going to be the name of
this HTTP service Dash p
ad 8080 colon 80 Dash V
open quotes
dollar sign PWD
close quotes
colon
slash USR
slash local
slash Apache 2
slash htd dogs
slash
httpd semicolon 2.4
let me run that
open in our password again
so what we see is the port is already
being used so let's go ahead and see
which ports let's go see if we can
change the port or see what supports are
running so let's do pseudo images and
see which ports are being used because
so the the port or the name hasn't been
put in correctly so pseudo Docker images
PS
pseudo docker PS Dash a
and yep that's Port 80 there
so clear the screen
so we're going to change the container
name because I think we actually have
the wrong container name here so let's
go and change that and we'll paste that
in and voila there we go
now working and we'll just double check
and make sure everything's working
correctly so to do that we'll go into
our web browser and we'll type in as
soon as Firefox opens up
type in localhost
colon 8080
which was the the port that we created
and there we are it's a list of all the
files which shows that the server is up
and running
so let's have a look at what we have in
our current environment so today when
you actually have your standard machine
you have the infrastructure you have the
host operating system and you have your
applications and then when you create a
virtual environment what you're actually
doing is you're actually creating
virtual machines but those virtual
machines actually are now sitting within
a hypervisor solution that sits still on
top of your host operating system and
infrastructure and with a Docker engine
what we're able to do is we're able to
actually reduce significantly the
different elements that you would
normally have within a virtualized
environment so we're able to get rid of
the the bins and the so we're able to
get rid of the guest OS and we're able
to eliminate the hypervisor environment
and this is really important as we
actually start working and creating
environments that are consistent because
we want to be able to make it so it's
really easy and stable for the
environment that you have within your uh
Dev and opt environment now critical is
getting rid of that hypervisor element
it's just a lot of overhead so let's
have a look at a container as an example
so here we actually have a couple of
examples on the right hand side we have
different containers we have one
container that's running Apache Tomcat
with Java a second container is running
SQL server and microsoft.net environment
the third container is running python
with mySQL these are all running just
fine within the docker engine and
sitting on top of a host OS which could
be Linux it really could be any host OS
within a consistent infrastructure and
you're able to have a solution that can
be shared easily amongst your teams so
let's have a look at an example that
you'd have today if a company is doing a
traditional Java application so you have
your developers working in JBoss on his
system and he's coding away and he has
to get that code over to a test and now
what will happen is that tester will
then typically in your traditional
environment government then have to
install JBoss on their machine and get
everything running and cool and
hopefully set up identically to the
developer chances are they probably
won't have it exactly the same but
they're trying to get it as close as
possible and then at some point you want
to be able to test this within your
production environment so you send it
over to a system administrator who would
then also have to install JBoss on their
environment as well yeah this just seems
to be a whole lot of duplication so why
go through the problem of installing
JBoss three times and this is where it
things get really interesting because
the challenge you have today is that
it's very difficult to almost impossible
to have identical environments if you're
just installing software locally on
devices the developers probably got a
whole bunch of development software they
could be conflicting with the JBoss
environment the tester has similar
testing software but probably doesn't
have all the development software and
certainly the system administrator won't
have all the tools of the developer and
tester have their own tools and so what
you want to be able to do is kind of get
away from The Challenge you have of
having to do local installations on
three different computers and in
addition what you see is that this uses
up a lot of effort because when you're
having to install software over and over
again you just keep repeating doing
really basic foundational challenges so
this is where Docker comes in and Docker
is the tool that allows you to be able
to share environments from one group to
another group without having to install
software locally on a device you install
all of the code into your Docker
container and simply share the container
so in this presentation we're going to
go through a few things we're going to
cover what Docker actually is and then
we're going to dig into the actual
architecture of Docker and kind of go
through what Docker container is and how
to create a Docker container and then
we'll go out through the benefits of
using Docker containers and then the
commands and finalize everything out
with a brief demo so what is docker so
Docker is as you'd expect because all
the software that we cover in this
series is an open source solution and it
is a container solution that allows you
to be able to containerize all of the
necessary files and applications needed
to run the solution you're building so
you can share it from different people
in your team whether it's a developer a
tester or system administrator and this
allows you to have a consistent
environment from one group to the next
so let's kind of dig into the
architecture so you understand why
Docker runs effectively so the docker
architecture itself is built up of two
key elements there is the docker client
and then there is a rest API connection
to a Docker Daemon which actually hosts
the entire environment within the docker
host and the docker demon you have your
different containers and each one has a
link to a Docker registry the docker
client itself is a rest service so as
you'd expect a rest API and that sends
command line to the docker Daemon
through a terminal window or command
line interface window and we'll go
through some of these demos later on so
you can actually see how you can
actually interact with Docker the docker
demo then checks the request against
um the docking components and then
performs the service that you're
requesting now the docker image itself
all it really is a collection of
instructions used to create container
and again this is consistent with all
the devops tools that we have the devops
tools that we're looking to use
throughout this series of videos are all
environments that can be scripted and
this is really important because it
allows you to be able to duplicate and
scale the environments that you want to
be able to build very quickly and
effectively the agile container itself
has all of the applications and the
dependencies of those applications in
one package you kind of think of it as a
really effective and efficient zip file
it's a little bit more than that but
it's one file that actually has
everything you need to be able to run
all of your Solutions the actual Docker
registry itself is an environment for
being able to host and distribute
different Docker images among your team
so say for instance you had a team of
developers that were working on multiple
different solutions so say you have a
team of developers and you have 50
developers and they're working on five
different applications you can actually
have the applications themselves the
containers shared in the docker registry
so each of those teams at any time check
out and have the latest container of
that latest image of the code that
you're working on so let's dig into what
actually is in the container so the
important part of a docking container is
that it has everything you need to be
able to run the application it's like a
virtualized environment it has all your
Frameworks and your libraries and it
allows the teams to be able to build out
and run a exactly the right environment
that the developer intended what's
interesting though is the actual
applications then will run in isolation
so they're not impacting other
applications that using dependencies on
other libraries or files outside of the
container because of the architecture it
really uses a lot less space and because
it's using less space it's a much more
lightweight architecture so the files
and the actual folder itself is much
smaller it's very secure highly portable
and the boot up time is incredibly fast
so let's actually get into how you
actually create a Docker container so
the docker container itself is actually
built through command line and it's
built of a file and Docker image so the
actual Docker file is a text file that
contains all the instructions that you
would need to be able to create that
Docker image and then we'll actually
then create all of the project code
within side of that image then the image
becomes the item that you would share
through the docker registry you would
then use the command line and we'll do
this later on select Docker run and then
the name of the image to be able to
easily and effectively run that image
locally and again once you've created
the document you can store that in the
docker registry making it available to
anybody within your network so something
to bear in mind is that Docker itself
has its own registry called Docker Hub
and that is a public restrict so you can
actually go out and see other doc images
that have been created and access those
images as your own company you may want
to have your own private repository so
you want to be able to go ahead and
either do that locally through your own
repository or you can actually get a
licensed version of Docker Hardware you
can actually then share those files now
something that's also very interesting
to know is that you can have multiple
versions of a Docker image so if you
have a different version control
different release versions and you want
to be able to test and write code for
those different release versions because
you may have different setups you can
certainly do that within your Docker
registry environment okay so let's go
ahead and we're going to create a Docker
image using some of our basic Docker
commands and so there are essentially
really you know kind of just two
commands that you're going to be looking
for one is the build command another one
is to actually put it into your registry
which is a push command so if you want
to get a image from a Docker registry
then you want to use the pull command
and a pull command simply pulls the
image from the registry and in this
example using ngi next as our registry
and we can actually then pull the image
down to our test environment on our
local machine so we're actually running
the container within our Docker
application on a local machine we're
able to then have the image run exactly
as it would in production and then you
can actually use the Run command to
actually use the docker image on your
local machine so just a you know a few
interesting tidbits about the docking
container once the container is created
a new layer is formed on top of the
docker image layer is called the
container layer each container has a
separate read write container layer and
any changes made in that docking
container is then reflected upon that
particular container layout and if you
want to delete the container layer the
container layer also gets deleted as
well so you know why with using Docker
and containers be of benefit to you well
you know some of the things that are
useful is that containers have no
external dependencies for the
applications they run once you actually
have the container running locally it
has everything it needs to be able to
run the application so there's no having
to install additional pieces of software
such as the example we gave with JBoss
at the beginning of the presentation now
the containers are really lightweight so
it makes it very easy to share the
containers amongst your teams whether
it's a developer whether it's a tester
whether it's somebody on your operations
environment it's really easy to share
those containers amongst your entire
team different data volumes can be
easily reused and shared among multiple
containers and again this is another big
benefit and this is a reflection of the
lightweight nature of your containers
the container itself also runs in
isolation which means that it is not
impacted by any dependencies you may
have on your own local environment so
it's a completely sandboxed environment
so some of the questions you might ask
us you know can you run multiple
containers together without the need to
start each one individually and you know
what yes you can with Docker compose
docking compose allows you to run
multiple containers in a single service
and again this is a reflection on the
lightweight nature of containers within
the docker environment so we're going to
end our presentation patient by looking
at some of the basic commands that you'd
have within Docker so we have here on
the left hand side we have a Docker
container and then the command for each
item we're actually going to go ahead
and use some of these commands and the
demo that we're going to do after this
presentation you'll see that in a moment
but just you know some of the basic
commands we have are committing the
docker image into the Container kill is
a you know standard kill command to you
know terminate one or more of the
running containers so they stop working
then restart those containers but
suddenly you can look at all the image
all of the commands here and try them
out for yourself so we're going to go
ahead and start a demo of how to use the
basic commands to run Docker so to do
this we're going to open up terminal
window or command line depending whether
you're running Linux PC or Mac and we're
going to go ahead and the first thing we
want to do is see what our Docker image
lists are so we can go sudo Docker
images and this will give us well first
we'll enter in our password so let's go
enter that in and this will now give us
a list of our Docker images and here are
the dock images I have already been
created in the system and we can
actually go ahead and actually see the
processes that are actually running so
I'm going to go ahead and open up this
window a little bit more but this will
show you the actual processes and the
containers that we actually have and so
on the far left hand side you see under
names we have learned simply learn be
unscore cool these are all just
different ones that we've been working
on so let's go ahead and create a Docker
image so I'm going to do sudo
docker
run
Dash D Dash p
0.0.0.0 go on a t colon 80.
Ubuntu and this will allow us to go
ahead and run an Ubuntu image and this
will run the latest image and what we
have here is a hash number and this hash
number is a unique name that defines the
container that we've just created and we
can go ahead and we can check to make
sure that the container actually is
present so we're going to do
pseudo docker.ps and this actually show
us down there so it's not in a running
state right now but that doesn't mean
that we don't have it so let's list out
all the containers that are both running
and in the exits see so let's do sucker
PS Dash a and this lists all the
containers that I have running on my
machine
and this shows all the ones that have
been in the running State and in the
exit State and here we see one that we
just created about a minute ago and it's
called learn
and these are all running Ubuntu and
this is the one that we had created just
a few seconds ago
let's open it up and
there we go so let's change that to that
new Doc container to a running state so
scroll down and we're going to type sudo
docker
run
Dash it Dash Dash
name my
um so this is going to be the new
container name it's going to be my
Docker so this is how we name our Docker
environment
and we'll put in the image name which is
Ubuntu
and dash bin Dash Bash
and it's now in our root and we'll exit
out of that
so now we're going to go ahead and start
the new my Docker container so sudo
docker
start
my
and we'll get the container image which
will be my docker
my docker
return and that started that Docker
image and let's go ahead and check
against the other running Docker images
to make sure it's running correctly so
sudo docker
PS
and there we are underneath name on the
right hand side you have to see my
Docker along with the other Docker
images that we created now it's been
running for 13 seconds
quite fast so we want to rename the
container let's use the command sudo
docker
rename we can take another Docker image
this says grab this one and we'll put it
in rename
and we'll rename and put in the old
container name which is image and then
we'll put in the new container name and
let's call it
purple
so now the container image that had
previously been called image is now
called Purple
so do sudo Docker PS
to list all of our Docker images
and if we scroll up and there there we
go purple
how easy is that to rename an image
and we can go ahead and use this command
if we want to stop container so we're
going to write sudo docker
stop
and then we'll have to put in the
container name
and we'll put in my Docker the container
that we originally created
and that image has now stopped
and let's go ahead and prove that we're
going to list out all the docker images
and what you see is that it's not listed
in the active images it's uh not on the
list on the far right hand side
but if we go ahead and we can list out
all of the docker images so you actually
see it's still there as an image it's
just not in an active State just what's
known as in an exit state
so here we go
and there's my ducko it's in an exit
state so that happened 27 seconds ago
so if you want to to remove a container
we can use the following command
so sudo docker
RM
for remove
my docker and that will remove it from
the exited state
and we're going to go ahead and we're
going to double check that
and yep
yep that's not not listed there under
exit State anymore
it's gone
and there we go
there that's where it used to be all
right let's go back
so if we want to exit a container in the
running state so we do sudo kill and
then the name of the container
I think one of them is called yellow
let's just check and see if that's going
to kill it
oops no I guess we don't have one called
yellow so let's find out name of a
container that we actually have
so sudo Docker kill oh we're going to
list out of the ones that are running oh
okay there we go now yellow isn't in
that list so let's take I know let's
take simply learn and so we can actually
go ahead and let's write sudo Docker
kill simply learn
and that will actually kill an active
Docker container
boom there we go
and we list out all the active
containers you can actually see now
that's they simply learn container is
not active anymore
and these are all the basic commands for
Docker container
if you are looking to get certified in
devops and become a Docker expert how
simply learned so hurry up and enroll
now let's dig into what a Docker swarm
is so a Docker swarm is essentially a
tool that allows you to very easily
create and schedule multiple Docker
nodes so really two or more Docker nodes
and you can have quite a large number of
Docker nodes in a single swarm each node
itself is actually a Docker demon and
that demon is able to interact with the
docker API and have all the benefits of
being a full Docker environment one of
the other advantages you have is that
the each dock container within the Swarm
can then be deployed and managed but as
a node in that entire clustered
environment so what we have here is a
breakdown of the five key elements
within a Docker environment you have the
docker container you have the Daemon the
docker images and the docker client and
Docker register history so the docker
Daemon itself is what does all the work
interacting with the actual host
operating system to be able to manage
the docker containers so if we look here
here we have set up an environment where
we have three docket containers being
run with Docker and what we want to be
able to do is be able to interact with
the environment because what would
happen if we actually have something
change in our environment so we have the
environment set up as a Docker swarm and
one of our containers fails what we're
able to do is use the Swarm to be able
to correct that failure so the docker
swarm manager is able to come in and
reschedule containers and as you would
imagine the actual swarm note has full
backups and full redundancy for any kind
of failures that would happen and we'll
do all of this work through command line
interface so let's go through some of
the features that you have within the
actual Docker swarm itself so a key
feature for Docker swarm is that it is
fully decentralized which means that it
makes it very easy for teams to be able
to access and manage the environment as
you would expect as well is that the
communication that happens between the
manager and client nodes within the
swamp is highly secure and of course
this is really just a fundamental that
you should have for any kind of solution
but it is good to know that Docker swarm
has that built in there is also Auto
load balancing within your environment
you can actually script that into how
you write out and structure your swarm
environment and that load balancing then
also allows you to then convert that
swarm environment into a highly scalable
infrastructure and then rollback task
allows you to be able to roll back
environments to previous safe
environments so say something does get
pushed out and something breaks you're
able to immediately roll back into a
safe environment so each of the
containers are pushed out and and are
controlled using services and they
actually happen to be breast Services
which make it very easy for you to be
able to integrate within your
environment and each of the different
Services contains a group of containers
of the same image Now by having this
structure it allows you to be able to
scale your application appropriate to
the demands on your server so if you
have a service that needs to have a
significantly larger number of services
for it to run you can actually scale
that appropriately and they can be
either Geo or demand based one of the
requirements for setting up a double
swarm is you must have at least one node
deployed so the way that the
architecture is set up is that you have
a major node and a client node and there
must be one of each for the entire
environment to be able to work
effectively so you know here we are just
jumping ahead of sales a little bit we
have two types of nodes in a Docker
swamp we have the manager node and we
have the worker node which is the client
that does the actual execution of the
tasks the manager note very similar to
other systems that we've talked about on
simply learn allows you to actually
control and manage the actual tasks that
are being executed by the worker nodes
and the worker nodes as you can imagine
and then actually execute the
instructions that the manager are
sending out to it so here we have a
situation where we can illustrate what
would happen with a manager node sending
out commands to different worker nodes
the manager is fully aware of the status
of the entire swarm environment at all
times this is because of the two-way
secure communication that's going from
the manager to the worker environment
the workers as you'd expect are
accepting tasks that are being sent from
the manager so the manager sends out a
task saying that you need to be running
as a MySQL environment then the work and
node will then convert to a MySQL
environment and all of these
environments are scripted and controlled
by you as the manager the actual worker
nodes themselves actually have a client
agent and that client will then
communicate all different states of the
infrastructure back to the manager so
that anytime the manager node is in full
control of the entire swarm the major is
the controller of this environment and
so you always want to be able to ensure
that the manager has full access to all
the work that's happening within the
Swan and this allows you as the manager
to be able to control your swarm and
very quickly be able to react to any
changes without having to rely on manual
installations of software and hardware
and as we covered earlier on there is a
rest API that uses the communication
over HTTP from the manager to the worker
node it's interesting to note that it is
a rest API because if you wanted to you
could actually integrate that API into
custom applications and even then create
automated Docker images to be created on
demand from third-party solutions that
you may want to create so one of the
things that's a really a big Advantage
with having a swarm is that once you've
actually created the Swarm any of the
services that you create can be accessed
by any node of the same cluster one of
the things you do have to do though is
you have to specify what container image
that you're going to use when you're
creating a new servers and you can do
that either through a centralized Docker
Hub environment or through your own
private Docker Hub environment one of
the things that's interesting is that
you can set up commands and services to
be either Global or replicated a global
Service will allow you to run a service
consistently on every node within the
Swarm whereas a replicated service will
only push out functionality and tasks to
specific worker nodes within a swarm so
you may be asking yourself hey is it a
service and task the same thing no the
kind of are not in the docker world and
the difference is is a service is a
description of a task for the state
whereas the actual task is the work that
needs to get done and that's the
differentiator here so what you can do
as a Docker user is you actually create
services and then you can then Define
when you want them to start as tasks now
what's interesting is that when you do
assign a task to a node that same task
cannot be assigned to another node also
what's interesting is that you can
actually have multiple managers within a
Docker swarm environment if you do
however go down this path you have to
elect one manager to be the primary
manager and the other managers to be
secondary managers in many ways those
secondary managers are really similar in
concept to worker nodes in which they
have the capability of a manager but
they are dependent on that single
primary manager to be able to provide
the right instructions and for services
and tasks to the entire warm environment
so if we kind of recap some of these we
have a command line interface which
allows us to create and connect via apis
and that those apis that we connect to
in our store environment allows us to do
orchestration via tasks and services the
task allocation allows us to allocate
work to tasks via their IP address which
allows them to execute them on the work
and then the worker nodes themselves
have to connect to the manager node to
be able to check when tasks come in so
that they are keeping a consistent
communication back and forth across the
entire swarm and then the final stage is
to actually execute the tasks that have
been assigned from the manager node to
the work node so that you have a
successful execution of the solution
you're looking to build right so with
that said let's go ahead and we're going
to do a demo so we're going to go ahead
and do a demo and showing how you can
run with Docker Swan for this you'll
need to have both a virtual environment
running your manager and you're working
in environment so here we have our
worker and here we have our manager so
we're just going to open up terminal
window and we're going to go into the
manager for terminal window so the
following command here is used to
initialize Docker Swan which is going to
type this in so sudo Docker swarm in it
and we put the IP address for the
network we're connecting to
open in our password
and here we have now connected
and this is our manager node and we want
to be able to highlight is the specific
token which identifies the docker swarm
environment and this is our token right
here so we're going to copy that
because we'll need to use that for our
worker environment so here we are in the
worker environment and we'll use the
token that we just copied
um as a way to be able to connect to the
manager and connect to the docker swarm
environment
and so sudo
I'll paste in the docker swarm and we'll
join that I will now here we shows that
we have joined the Swarm environment as
a worker
and now we're in the manager we can
actually show that we actually have
that worker in the environment so
there's Docker node LS to list out all
of the
items in that environment
sorry we actually did then the wrong
area so we're on the right pseudo Docker
node LS to list out all the nodes in the
Swarm and there you are you see we've
actually connected the worker to the
manager node and both are active and the
manager is the leader and the work of
virtualbox is in the Swarm
okay we're going to go ahead and create
a new service so Docker Swan create and
we're going to just change the name to
hello world for the new service and
we'll use the Alpine image from
docker.com
takes just a moment for it to run
there we go
and there we go Services converged
so what we have now is a new service
that has been created
let's go ahead and we'll list out the
services
and here we have our new Hello World
Service
and we go ahead and check the docking
containers and we're going to Docker PS
and here's our Alpine it's running the
latest image
and that's the docker container ID
and we can see that it was just updated
and so we're going to do is use the same
command in the worker node
and make sure everything is working over
there
so go over to our worker paste that in
so what we see here is there is no image
or container created because it's not in
the worker node it would be in the
manager node
so we can go ahead and create the new
service
and we'll make the Mode Global and that
way the service is available across all
of the Swarm
so we're just going to execute that work
takes a moment
and
there we are
done work has been completed and we have
the two different IDs which shows that
two different Services running and if we
go back to the working mode and we run
sudo.gov PS again and there we are now
we see that we have the hello world
new virtual environment has been added
with the image of Alpine and we also
have the new token ID for that
if we want to kill a node from the Swarm
we can use the following command which
is suitable Docker swamp leave dash dash
force and that will force the node to
leave the Swarm so one of the questions
that will come up though is what if you
want to use two containers in a single
service and an example of this might be
like a web server that is using a
database but the database may be in one
container and the web server may be in
another container which is very typical
for your own web and database
infrastructure so very similar I'm
scenario but how would you do this with
dark app so by using just Docker itself
it's actually very difficult to do this
you can do it but it's just very time
consuming and just not a very good use
of your productivity but with Docker
compose you can actually do the whole
process quite quickly and easily and
hence the reason why we have Docker
compose to allow you to have two or more
Docker containers running in the same
environment being able to communicate as
as if they were running in a production
environment so let's dig into what
Docker compose is so let's consider a
scenario that you'd have today so Mitra
is a fashion website similar to Amazon
and you'd go to Mantra with your web
browser and on that website you would go
through a number of activities such as
logging into your account browsing a
catalog you'd have your checkout
application server and you go through
the table process of building out an
application behind each of these
services are different products such as
you know to have an account database you
have a product database different
checkout processes and these would all
be run behind the scenes and each of
these can be considered a micro service
so the more microservices you build into
your environment the more complexity
you're adding and the more of value it
would be to have each of these services
in their own container but as a
developer you want to be able to jump
from one container to another container
so your login account can then be passed
on to your product catalog and then be
able to move through the whole process
so the environment that you would see
today would be small like what we're
showing right now where you'd have your
server and your database and you may
have a server running on an Apache
Tomcat or a SQL Server you'd have
different databases and you'd want to be
able to have each of those in their own
containerized environment so the way
that duck compose runs is that it works
as a single service so you have the
docker compose running multiple
containers but the perception is of a
single service run the thing that's
great though is that each of those
containers will run in there in
isolation and but they can interact with
each other so unlike a normal Docker
environment where you have multiple
Docker images running completely in
isolation of each other these are images
that will run in isolation but can
interact with each other the docker
compose files are very easy to right
they're all written with a scripting
language called yaml and yaml is an XML
based language it actually stands for
yet another markup language but it's an
XML based language and it's very easy to
use the great thing about learning yaml
is that a lot of Open Source tools in
the devops environment use yaml as the
scripting way and then finally in Docker
compose as a user you can actually go
and and Trigger all your services within
the containers to start with a single
camera on as you can imagine this
dramatically reduces the amount of work
that you as a developer have to do so
let's take an example of if you're
running a container one with a nijex
server and one with a redis database uh
you can create a yaml Docker compose
file that actually has the instructions
on the containers needed to build out
both environments and then you can run
each of those environments separately
but have them connect as a single
service so the benefits of duck and
compose is that you have a single host
deployment environment you can run this
all on one piece of Hardware you can
have a quick and easy configuration
using yaml scripts the productivity that
you get has developers significantly
increases because you're not having to
have time wasted trying to configure
just traditional Docker containers by
themselves you now have a way of being
able to interact with those containers
and then finally security is at the
center of all of these containers so
each of those containers are completely
isolated from each other and they are
controlled with the same level security
that you'd have with a traditional
Docker image but there is a question
here around Docker compose isn't it
similar in concept to Docker swarm
because at a high level it kind of seems
like both do the same thing and the
answer is well you actually know they're
different there are similarities but
there are positive differences and
particularly for a developer these
differences will really start coming out
in scale as you start working on your
Solutions so let's look at Docker
compose so Docker compose will allow you
to create multiple containers on a
single host and that's the important
part of being a single host which is
maybe your development PC with Docker
swarm it also allows you to create
multiple containers however you have to
manage those multiple containers on
multiple hosts which makes a lot of
sense if you're running an operations
environment but not so much if you're
doing a development environment and
Docker compose is scripted with yaml
which is very easy for you to be able to
control your scripts Docker swarm
doesn't have a scripting technology like
yaml so it's a little bit harder to work
with so what we have here is just some
basic commands that allow you to get up
and started working with Docker compose
in our next demo we're actually going to
take you through how to use all of those
commands so we're going to go ahead and
do a demo on how to use Docker compose
and the first thing you want to do is
check that you have Docker installed so
what we're going to do is we're going to
open up the command window and we're
going to type in Docker you'll be able
to see now that we have Docker installed
if you don't have dog installed go ahead
and install Docker you can go to Google
Search to install Docker and it'll give
you the instructions for that we also
have videos that we've already done if
once you have dock installed then you
want to do a Google search on Docker
compose or go to the docker.com website
and select for the compose those section
and look for the install Docker compose
instructions so we already have another
tab which has the instructions already
installed you'll see on the left hand
side there are a number of Docker tools
you want to expand the docker compose
section select install compose and then
on the right hand side you'll have
different install options for Mac
windows and Linux if you're installing
on those platforms make sure you're
copying the correct command line so
there's a couple command ones you want
to copy copy the first one over and we
paste that in and it's going to go ahead
and download all of the files for
compose it takes a while but we have to
get another file so while it's
downloading we'll go get the other
command line just go back and grab it
real quick there's a little faster but
we'll copy that over there was a second
command instruction and post that
command yep there's the command right
there and that will apply the
appropriate binary sorry and once you've
done that you can actually go ahead and
validate that you have the correct
version of Docker compose installed and
for that you want to write the command
Docker Dash compose space dash dash
version and that will then give you a
version build once you know that you
have a version build that means that you
have everything installed correctly on
your machine so you can see here that we
have a version number which means we
have everything installed you might see
a different version number depending on
how old the video is when you're
watching it but they're using very
content at the moment I'll just go ahead
and clear the screen and now what we're
going to do is create a folder where
we're actually going to install the yaml
file that will have the instructions for
our docking compose environment so we're
moving the cursor to the desktop and on
the desktop we're going to create a new
folder and using the MK derive command
and we'll call this one Docker compose f
for file and then we'll move the cursor
into that new folder by using CT command
and now we are in the folder called
Docker compose F and you can go look at
your desktop and you'll see it's there
and now you want to select and pound
touch Docker Dash compose Dot yml and
they'll actually create the new yaml
file for you and you can use the ls list
command LS dot code compose.yaml and
they'll allow you actually now to step
in and edit that file so let's go ahead
and hit return and they'll allow us to
go in and edit the file so now I'm in
the editor and I'm going to write some
instructions
um so I'm I'm going to put a couple of
mistakes in these instructions so we can
see how Docker pose catches those
mistakes and so but most this is going
to be fairly correct so you can just
type this out so we're going to start
with Services colon then we do web colon
and then we create image colon and we'll
do ninjix and then we'll do a database
colon and and we call the image redis
and so what you can see because we have
two reference images we're actually
calling two different images two
different containers that will be
created by Docker Hub so what we're
going to do is uh where we can actually
show you um how that's pulling in you
can go to Docker Hub and you can
actually see the references to the
images that we've just created so go to
Docker Hub which is Hub hub.docker.com
and you'll see that uh ninjix redis
those are the default names for the
images so you could do other images like
HTP for Apache web server if you wanted
to there are a lot of images in Docker
Hub so certainly have a lot of fun
building uh tools from that environment
so we're going to save those files in
the docker image and we're just going to
do colon WQ to exit this screen I save
the text and let's go and run our yaml
file and you'll see the two different
different error messages that will come
up so we're going to do cat Docker Dash
compose Dot yml and here we have a
display of the text that we just wrote
and now let's go check the validity of
the file that we just created so we'll
do Docker Dash compose config and we
should get an error message and there we
are error message and the reason why we
have an error message is because the
spacing is not correct in the yaml file
itself so let's go to the actual yaml
file that we created let's go through
and we're going to use uh just the file
finder and you'll actually open up the
yaml file in your text and see your
favorite text editor and here we have
the code that created it and let's use
the correct spacing so that everything
is spaced out appropriately in the image
and so this is the spacing that you'd
expect to see a space a line between
surfaces and the first image and the
first image is a tab in and then it
referenced what the image is and let's
go ahead and clear the screen and we'll
see whether or not we've got that all
correct
um this clear screen and we're going to
check that everything is running
correctly and we're seeing we're still
getting an error message and the reason
why we're getting another error is that
there's still a config error in this the
actual yaml file so one of the things
you have to do when you're creating a
yaml file for Docker composers you
actually have to have the appropriate
version number in there so what we have
to do is we have to go and find which is
the appropriate version and there are
three major versions one two and three
depending on the age of Docker that you
have installed on your computer so if
you have the very latest version of
Docker 18 and newer you would put 3.7 as
the version number and what we're going
to do is we're going to just check on
the version that we have installed and
so just clear the screen and then we'll
just do Docker version and what we have
is our version of Dockers 7
18.12 now this is the version of Docker
not Docker compose it's your main Docker
environment and so we go back to our web
browser we'll actually see that version
17 should align with version number 3.5
yep there we are version 3.5 so what we
want to do is go back into our yaml file
and the very first line that we want to
put in above the surfaces is the version
number and that version number we're
going to put in is 3.5 so we type in
version version and we'll put colon and
then version number is in in quotes and
you save that and now what we can
actually do is we can actually go ahead
and use the single line instruction to
run both of these uh two images that
we've created because everything is
configured correctly or just one note
you'll see that the version number is
actually at the bottom of the list and
that's just the way Docker compose works
it pulls out the services and the images
that you'd create and then put the image
the version number at the bottom of the
list so in the presentation we talked
about how you can use one line to
trigger two or more images and so to do
that we're going to do sudo Docker Dash
compose R Dash D and it's going to
create the two environments we have the
web environment and the database
environment and let's use pseudo Docker
PS to actually list out that all the
processes that are running right now and
there we are we actually see all of the
images are running we have the names the
commands the images and everything's up
and running and if we want to close both
of those images we can actually use a
single line of command as well and that
command will be pseudo Docker Dash
compose down and that closes both of the
images and we can see that all the
databases and the web server says images
have been closed and sudo Docker PS will
actually lists that there is nothing
running and that's how to use Docker
compose so uh we are going to talk about
what is a Docker what is a Docker file
what is this intercept Docker file and
how to build a custom Docker image with
the help of Docker file and in the end
we are going to see a particular demo
related to the docker files over here
now let's talk about what is the docker
now Docker is in configuration
management tool which we are pretty much
using to prepare the automation of the
deployment of the software of the
applications now in this case we make
use of the docker containers using which
we can host our applications and within
this Docker containers we are typically
hosting both the application source code
and the defenses all together so both
the dings are getting packaged all
together into a single unit called
Docker containers and getting hosted
onto the server
these containers are very efficient in
work because they are not taking that
much utilization and resources and at
the same time they can be shared across
different environments also so the same
container can be easily deployed across
different different
environments as such
now it helps us to maintain the isolated
application it uh helps in having a high
productivity there and it also you know
helps us to see to resolve all the
problems which is related to the
dependencies for your software because
we are packaging up both the source code
and the dependencies all together in the
single unit in the single entity and
that's where you will be able to get a
particular final solution called as in
Docker container over here so Docker
container is in complete package using
which you can have like both the
applications and the dependencies
bundled up together into a single entity
and that's where we call it as in Docker
container
now what exactly is in a Docker that uh
you know let's see a particular
difference between the virtual machine
and the docker here now if you see on
the left hand side we have the host
operating system uh which is available
there let's say I have a laptop in this
one we have a Windows 10. now we have a
hypervisor on top of that now hypervisor
is in software which is used to manage
the virtual machines it's a software
which is available there which takes up
all the requests from the virtual
machines process it and then I'll give
the response back so hypervisor acts
like a medium between the virtual
machine and the host operating system
resources so it's going to give that a
particular virtual environment to the
virtual machines and then we have a
virtual machines where in fact a guest
operating system is there then
application binaries then the
application is hosted so this guest
operating system actually takes up a lot
of utilization and these was the old
mechanism where we used to you uh follow
this mechanism for hosting our
applications
so all the applications are pretty much
getting deployed in the same manner on
the form of the virtual machines here
right now if you talk about the new
method which we are talking about right
now is in Docker so here we have host
operating system but instead of
hypervisor we have now the docker engine
which is available there now this Docker
engine is something which is making sure
working in the same way that how the
hypervisor is behaving but it's much
lighter as compared to the hypervisor so
Docker is very lightweight as compared
to the hypervisor here and here we don't
have the concept of the heavy duty guest
operating system over here so the guest
operating systems which we are using in
case of Docker containers are very small
in size
the size of guest operating systems is
is in GBS in case of virtual machine but
in case of Docker containers it's pretty
much available in the MB size which is
there so the size of the guest operating
system in case of the virtual machine is
very small as compared to the virtual
machine and that's the biggest benefit
because again it will help the
containers to have a low utilization low
consumptions and that improves the
overall performance of the overall
mechanisms and how exactly the new
method of application deployment really
helps us to go for an a particular
Advanced mechanism so that brings on a
very good and the efficient
way of doing the deployment of the
applications
now let's talk about what exactly is in
dockerfile here now uh Docker file is a
concept which is being used as in build
script so it's being used so that we can
have a build a script there so this will
definitely helps uh to see that how we
can go for a build of a Docker image a
custom Docker image now when we are
using the docker file concept so it
really helps to prepare a kind of Docker
image
so doc image is the end result when you
are going for the mechanism when you're
going for the docker file build up here
so Docker file is a build script which
is available for preparing the custom
Docker containers because not every
Docker image is available on Docker Hub
so some containers you have to use some
images you have to use to work on that
part
now dockerfile is a very simple
straightforward text file which is
available there and it's basically used
to build up custom Docker images so
Docker file is basically used to prepare
a once so when you're running the docker
build so it's actually going to build up
a particular custom Docker image and
that will be given to you so Docker
image is the whole build process which
is being done so it's a custom process
which is available and being done over
here in this case
so you got a Docker file you run the
docker build command and that's how you
will be able to get a Docker image so
whatever these steps you are having in
the docker file that will be processed
one by one and as a end result you will
be able to get a full flash Docker image
over here in this case right now let's
talk about the syntax of the docker file
so Docker file is equal to commands
commands and the arguments so whatever
the things you want to put up so in the
docker file you can do that and as a
part of the execution so you will be
able to get a particular end result
now it's a kind of a mechanism where we
feel that how the execution and the
modifications can be done so uh the
comments which we are trying to do over
here is that you know we can put up the
whatever comments we want to put in the
case of Docker files we can do that
comments is a standard process in
preparing any particular file or any
kind of code because it helps us to
understand that for what reason we are
preparing that particular line then uh
we have the uh particular run statements
the commands are there so you can see
that we are running a particular command
called Eco so using the Run attribute we
are basically going ahead and running a
particular command into our system as
part of our Docker file over here
now how to build up the docker images
using the docker file so um you know
Docker file is uh just a template which
is available there where we are putting
up that what are the different steps we
want to follow and these steps when we
follow one by one step by step so that's
where we will be able to get a
particular final Docker container over
here so Docker image is getting
converted as in Docker container and we
are getting a Docker container as an end
result
now Docker file is being built up and
stored in the version control system and
this particular Docker files when we are
processing So It prepares a respective a
specific uh image layer over here and
this image layer is something which is
uh benefiting us that how the executions
can be done and how we can manage on
that part so
that really helps us to see that how the
executions and the things can be managed
and that can help us to see that how the
management of the things can be
performed as such
so uh Docker file is a very good concept
which is there using which you can
prepare any kind of custom Docker image
and it can be very easily handled also
and it's a fully automated mechanism you
can use it inside the cicd pipelines for
automating the docker images right now
its Docker image is a layer by layer
mechanism which is there in which we
have multiple layers each and every
layer we are doing some specific entries
some specific tasks or some you know
changes we are trying to deploy in part
of Earth is layers right so uh you can
use the particular Docker build command
to perform the executions and you will
be able to get any particular response
and output will be given back to you so
Docker file is basically helping you to
see that how the executions can be done
and you will be able to get an end
result over here so the docker image is
the final result which we are getting
when we are talking about the docker
file here
right so these are all read-only formats
which is available you cannot modify
these directly but yes with the help of
Docker file you can do whatever the
changes you want to deploy on these
Docker images
right so uh this is a small example
where we are using a base image as an
Ubuntu 18.04 so we are using the from
attribute to specify that yes we are
going to use the Ubuntu 18.04 k then we
are pulling our uh the file into the uh
directory we are transferring a
particular file inside the docker
container and then we are running our
make Command and then CMD attribute is
there using which we can run the command
on the runtime on the docker container
so these are a very basic example but
eventually when we are done with this
Docker image we will be able to get a
four layer uh Docker image over here
because there are four steps and then
hence it will be providing you a
particular four layers in your final
Docker image over here so layer by layer
these changes will be done during the
build process and in fact using the
docker history command you can see that
layers these layers information also
right so layer 1 is Ubuntu 18.04 layer 2
is pull and uh the third layer is there
where we can run the uh make command to
you know run this file which we have
pulled on and then the finally the last
one is there where we are running like
using the CMD parent so the difference
between run and CMD is that CMD will run
only when you run a Docker container so
run attribute will be executed during
the docker build part so this is a
pretty much is one way that how we can
go for the execution of a Docker file
and how you know we can go for the build
up of a custom Docker image over here so
these are the for some couple of
attributes which we are using in the
docker file here
right now let's talk about the entry
point over here now entry point allows
you to specify a command along with the
parameters right so uh let's say that
you want to run a particular command uh
over here in the docker container so
entry point is something which you can
use to refer to that so application is
the command which you're trying to run
and then you can give some arguments to
that and argument one is what you're
trying to run over here so entry point
is very important because that
eventually helps to see that how the
executions can be done and the
implementations will be done over here
in this case
right so you can see like you can run a
command called Eco and then you can use
the parameters also it's not like only
the commands can be used but you can in
fact use the arguments also now add is
an attribute which is available in the
docker file which is primary used to see
to transfer the files from the host
machine inside the docker image
now let's see that you are trying to
prepare your own custom Docker image
apart from the dependencies you need the
source code also right so the source
code needs to be transferred inside the
docker image and for that reason we can
use the add attribute so add attribute
requires a source and destination Source
will always be the path on which uh on
the testing on the host machine the
files are present so we can give that
and destination is the folder which is
available inside the particular Docker
container where we want to store the
files so you can use the example over
here just like this you can use and you
can give two entries one is the source
one is the destination and once that is
done so you will be able to see your
files being stored up in the destination
directory once the final Docker image is
prepared
then we have a and b attribute uh now e
and B is something which is used to set
up some environment variables which is
required of course for the docker image
for the application which is running
inside the docker image now environment
variables are something which you can
hardcore during the docker build one but
using the when you are running like when
you're creating the docker container at
that time also you can write the these
environment variables so environment
variables are very important because uh
first during the build process you can
hard code some values but if you really
want to change them during the runtime
that also is being supported by the
environment variables so
most of the times we use it as in kind
of a keep value pair just like in in
Linux we use we used to set up a
particular environment variable so the
key value pair is being used in case of
ENB attribute maintainer is there where
we can describe that what is the details
about the author these are the fields
which is available in the top so where
we can give the the author name and the
author you know details like email ID
and all so that we know that who is the
person who is maintaining this a
particular file and we can reach out to
them in case any issues are there for
the docker files am right so that is
what we have here in this one so now we
are going to see that how we can go for
a small demo in Docker file and we can
go for the implementation of this
particular Docker file and prepare a
custom Docker image so let's just wait
on that part
so let's see some demo here now in this
one we have got two examples one is the
python one is the node.js one so first
of all what we need to do is that we
need to process the python one here so
this is the sample Docker file which we
have got here for the python now all
this one requires a python file which is
already present on the server and we are
going to transfer we are going to copy
this content over here that will this is
the specific Docker file content which
is available to us and all we need to do
is that we need to put this content
across the server so that we will be
able to build our custom Docker image so
let's go back to the server so here we
have already got the python file which
is present so we have to create a Docker
file here which will have the steps
related to the docker image preparation
over here so we will be including some
steps over here that these are the
different steps we want to configure we
want to set up over here in this one so
python 3.6.4 is the version of the
Python which we want to configure and
then we are going to transfer some files
into slash app we are also setting up
the working directory AS Slash app and
ultimately we are running a command
called install flask to do the
installation of flask package and then
we are starting the specific
applications on that part so I'm going
to save this file and once I save this
file so all we need to do is that we
need to create a custom Docker image
right so right now there is no image
which is available over here and it's in
complete FD installs so I'm going to run
a command called docker
build
hyphen e python hyphen amp dot so Docker
build is the typical command but hyphen
T is for the tag that what kind of
Docker image you want to build and then
now we are giving a image name and then
we are specifying dot over there now
once we are done with that so it will
pick up all the things one by one the
basic Docker image is being pulled down
then some sequence of steps is being
executed so it's a step-by-step process
which is happening over here in this
case and we are then transferring some
files in the slash app and then
ultimately we are going for the PIP
install flask so it's installing those
packages and at the end the final Docker
image will be prepared and will be given
back to us so we will be able to get
this uh Docker images over here when we
run that command we will be able to find
out that we got our custom Docker image
also available and as you can see that
the base of Darker image is also
available in this one but yes we have
added up like around 10 MB of work we
have added on top of this python a base
Docker image here so that's how we are
able to do the processing over here in
this case in this setup so that's what
we can do to prepare a custom
environment of python with the help of
the docker file here now let's see that
we will be proceeding on with the
node.js to see that if the node.js kind
of setup also can be done now for
node.js again of course we got the
specific Docker file which is present so
we can process that but let's create a
new folder over there so I'm going to
create for a new node.js folder so that
we can create the work all together in
this one so I'm going to run a command
called npminit which is the comma to
initialize any empty project in case of
node now you can typically bring on some
real code also or you can say that I
want to process an empty a project so
you can give the configurations you can
give the custom names but I want to
process with these as default name
itself so ultimately it's going to
create an entries like uh you will be
able to see like all these uh values
which we are not providing these
standards value will be picked up for
the uh specific code and at the end you
will be able to get a package.json file
created over here in this one now you
can actually bring down the steps that
how you can perform or you can build up
a custom node.js based Docker image now
these Docker files is giving you the
capability that you can convert a
full-fledged application which is
Deployable on the virtual machine you
can actually convert them in the format
of the docker containers over here in
this one so that's what we are going to
get over here with this uh particular
setup or these mechanisms so let's go
back to the document and see that what
exactly we can copy the docker file so
we have got this Docker file over here
so all we can do is that we can copy
this one so once we copy this uh
specific file here so all we can do is
that we can transfer these files into
the onto the server we can convert it
into the docker file there so that we
will be able to set up the
configurations and we can perform the
setup around that so Docker file is
again we want to create and we want to
set up on that here so let's go back to
the server
so here we want to create a Docker file
and we want to set up the files inside
that these are the different files which
is being available which is being
configured over here in this one so we
are running like node server.js which is
a master file so we are doing an exposed
of 8080 then of course we have copy
commands which is there then npm install
command is there and then package.json
file is also there now what you need to
really do is that you need to go for a
server.js file which is available in
which you can configure that how the
setup needs to be done and how the
configurations should be performed there
because that's a command that's a script
which is going to be executed within the
contain so let's copy the content now
I'm going to copy this content over here
which is like a sample node.js code
which is available so I'm just copy this
one and transfer it to the server
so server.js file we have to create here
just like we did in case of the Python
because this is the source we want to
transfer inside the docker container so
this is what we have got now I'm going
to transfer like uh I want to change
this port value from 5000 to 8080
because that's what it's configured in
the docker file of course you can
configure 5000 also depending on your
requirement you can do that
configurations now you have got uh these
two files over here in this one so all
you need to do is that you need to run
like Docker build
hyphen T node.js hyphen app and Dot so
it will automatically pick up like what
exactly it will uh you know go for the
node.js basic Docker image over here so
that will be processed over here in this
one so step by step it will be able to
do the configurations
so first of all it will download the
basic Docker image which is um quite uh
more in size over here in this one once
it's downloaded onto your system then it
will proceed further with the build
activity on processing the docker file
performing the execution so all that
stuff will be performed with this help
of this Docker build process over here
so it's going to process it one by one
in this one
and it's going to the next step where
it's transferring some of the files like
creating some working directory
transferring the files uh putting up
there and running the npm install
command and uh then ultimately it will
expose the port and then it will have
the CMD attribute configured over there
so this is basically typically trying to
portray it how a particular Docker image
can be built up and of course once the
build is Success you can validate it
with the help of Docker images here so
this is the app Docker image which has
been prepared of 911 MB and we have got
it like a final package where the
runtime is also there and the code base
is also available now you can pretty
much
experience it like with other actual
Source cores like you can generate the
download some pre-configured node.js
websites or the source codes there and
you can try to run them onto the docker
container so that you can get the full
uh the complete idea about that how it
goes on but right now the uh these are
the basic Docker image which you can
prepare and you can build up over here
in this one so that's how it really
works on for you if you are looking to
get certified in devops and become a
Docker expert have a look at
postgraduate program in devops by simply
learn so hurry up and enroll now now
let's see that what are the different
commands we have for the docker here now
the very first one is that what are the
different images related commands which
is available there the very first one is
the command which we typically use to
prepare a custom Docker image now I can
easily go to the docker Hub and I can
pull down a Docker image which is
available there but let's assume that
you are working on application source
code so you want to prepare container
for that now that Docker container you
will not be able to have it on the
docker Hub you have to prepare on your
own
right so in order to prepare your own
Docker image all you have to do is that
you have to run Docker build command now
in this case we have to prepare a Docker
file which will act like a Docker a kind
of a Docker build script just like in
case of Maven we have power XML file and
in case of and we have build.xml file so
here we have the docker file which is
available there and this Docker file
makes sure that whatever the steps are
mentioned in this Docker file in the
build script it's going to perform a
series of steps and the build will be
done so Docker build is the command
hyphen T is the tag that what exactly
name uh you know whatever the image you
are building so what exactly tag name
you're trying to provide over here so
here we are using a tag name called my
image and then after the column we are
providing that what version we want to
go here 1.0 2.0 5.0 so whatever the
version you feel that you want to you
know refer you can do that
now this is the exact way that how in
the actual world the docker images are
getting built up by the respective
vendors let's say that Jenkins is one L
Jenkins wants to prepare a Docker image
so they will be preparing the docker
file in the same manner and prepare
these Docker images and publish it to
the docker Hub so all the docker images
are built up the custom Docker images
are built up with the help of Docker
file so Docker build is the command
which will process the docker file and
give you a particular Docker image in
the end
now how to list down all the docker
images so Docker list uh LS is something
which is there so this will eventually
help you to list down all the docker
images which is listed on or which is
available into your system so that will
be listed out over there for you if you
feel that you want to remove a
particular image in that case you can go
for Docker image RM command and uh
whatever the image you want to remove
you can give the respective image name
and the tag name there so Docker image
RM command is there using which you can
actually see that how the image can be
deleted from the host over here in this
one right so whatever the ways whatever
the you know mechanism is there so we
are making sure that how the image can
be built up and how we can go for the
built up and a particular image can be
deployed over here in this case so in
order to delete it in Docker image so
you can go for Docker image RM command
to remove a Docker image completely from
the docker host
so these are some couple of commands
which is available there using which you
can actually interact or manage the
docker image because in order to prepare
in order to run a Docker container we
require a source and that Source we are
going to get it from the docker image so
now we are going to see that how we can
execute these commands so first of all
what we need to do is that we need to
see that how Docker build works for that
we require a Docker file so I am going
to have a simple Docker file over here
in which we are going to have a basic
document selected as an Ubuntu after
that we are going to go for the one
attribute where we are going to do the
installation of a specific package like
let's say git and accordingly you can
whatever software you want to prepare
you want to set up you can do that and
put up the commands over here in this
Docker file so I'm going to save this
file and after that what I need to do is
that I need to run like Docker images to
find out that how many images are there
there is no images present so I'm going
to build a Docker image over here with
the name of
simply learn or whatever the name you
want to give you can uh give it over
here in this case it's not any kind of
extrated word so you can give the actual
Docker image name also that what for
what purpose you are preparing this
image and with that what will happen
that it will take up the uh values
whatever attributes you have put up into
the docker file so that will be
processed and that will be picked up and
once that uh Docker file has been
processed it's been uh you know prepared
you will be able to see that yes a final
product or a final Docker image is
finalized and prepared over here in this
one so we just have to wait for the
whole process to complete and once my
Docker image is prepared I will be able
to have a particular setup done or
achieved over here so that's what we
require over here in this one to be
performed
now once the installation is done so you
will be able to see like the final
Docker images being prepared over here
so it says successfully built which
means that uh the docker image is
prepared now if you run like Docker
images you will be able to find out that
this image is available over here in
this one now you are seeing like two
images because one is like the basic
image which is being pulled on because
we are using a Docker file to do that to
process that one there
now in case you want to remove it so all
you need to do is that you need to run
like docker
image RM and simply learn
so if you do that what will happen that
your Docker image which is related to
Simply learn will be wiped off from your
system and in case you want to do like
removal of the Ubuntu also so that also
you can do over here with the help of
Docker images
RM 2.2 and that will be removed from
your system so your Docker images will
be removed from your system using the a
particular command called Docker image
and then RM command to remove the
document from your system from your host
here so that's how you deal with the
different Docker images uh commands over
here in this one now let's go back to
the document now the next section over
here is the docker container commands
over here so all the commands related to
Docker containers is what we are going
to talk about over here now the very
first one is the uh particular container
so in order to create a container so you
can run the command called Docker create
and the image because for a container
you require a Docker image that's a
prerequis so once the docker image is
prepared you can just simply run Docker
create command and that will help you to
create a container onto your system now
if you want to run a particular
um you want to create a container and
you want to run it out also at the same
time so in that situation you can go for
Docker run command the main difference
between the docker create and Docker and
command is that the docker run command
is going to run a container and it's
going to give you a response from the
command but the docker create command is
going to create only the container so
once the container is up and running so
that's only being done by the docker
create command
but the docker run command is going to
actually do the uh preparation of the
container also and at the same time you
are being given as in particular
response also like you know what a
command you want to run so that response
also you will be able to get here and if
you feel that you want to rename an
existing container so for that you can
go for the docker rename command so
existing container you can rename it
with a new container and that's how the
particular rename of an existing
container will be done with the onto the
docker host here so Docker rename is the
command using which you can rename an
existing container and you can get a new
container over here in this case so
these are some of the options some of
the ways that how you can interact with
the containers how you can manage them
onto your Docker host over here so you
can run it you can create it and you can
rename the containers also so now what
we are going to do is that we're going
to see that how we can work on this
particular container commands here so
first of all I'm going to quickly pull
down a Jenkins image so that I can
create a container out of that so it's
in kind of a 600 round MB Docker image
which is available there now you can
also have a custom Docker image prepared
using the docker file or you can have a
ready-made Docker image pulled down from
the docker Hub also so this is going to
pull the latest version of the Jenkins
from the docker Hub so that we will be
able to proceed further on that and we
will be able to continue with the
hosting so we will be converting or
running this uh specific Docker images
in the form of Docker container so that
we will be able to have a instance of
Docker container over here running in
this one so that's what we are trying to
do here so we will just wait for this a
pull activity to complete because it's
an heavy Docker image so it may take
some time to download and once it's
download you will be able to see that
docker
images is showing a specific Docker
image of Jenkins here now you're going
to create a container out of that so
you're going to run a command called
Docker create and Jenkins over here
so that will help us to do uh the
creation of a Docker container and once
that is done so you will be able to see
like using Docker PS command that if any
container is up and running and if you
go for dockerps hyphen a you will be
able to see that yes a particular Docker
container is in created mode now this is
not in running mode because you see that
when I ran the docker PS command it was
not showing it in the running mode
because I just created a Docker
container here and the created status
also shows that yes the docker container
is created but it did not run over here
so if you want to run it so definitely
you can say like docker start and uh
you can give the container name and that
will help me to start my container over
here but of course you if you feel that
you want it directly create a container
and uh you know you don't want to create
the container only you want to start it
also so in that case you can go for the
docker run command so Docker run command
is something which can be used instead
of uh doing the execution so you can say
like Docker run
Jenkins and what it will do it will
definitely help you to get the container
created onto of the Jenkins over here in
this case it's not like only creating a
container but in fact it's actually
starting the container also so you can
see that it started over here into your
screen itself that whole Jenkins gets
initialized over here in this one so
this is the way that how you can
initialize or you can create a container
now I can just do a control C over here
because it says that and Jenkins is up
and running so you can start using this
one according to your requirement over
here right so I'm just doing Ctrl C so
that it can be stopped over there
right now
so this is the way that how uh typically
we can create the containers now we got
like two containers over here and one is
up and running and one is something
which I just closed just now over here
now if you feel that you want to rename
a container let's say like this is a
container which is available but it's
not having a meaningful name now I want
to convert into a meaningful name like
Dev underscore Champions so that you can
easily do with the help of a Docker
rename command because that will rename
the docker container name so that you
will be able to connect on to that and
you can go for the processing so that's
what we can get over here in this one so
this is how you can do the various
management of the resources over here so
let's go back to the main content now if
you feel that you want to remove all the
stop containers because what happens
that sometimes we simply stop the
container but we do not remove it from
the docker host because
um we are not going for the cleanup now
clean cleanup is very required because
these containers are actually taking up
the disk space also so we need to make
sure that we should be using we should
be having like minimum amount of disk
space utilization so for that what we
need really need to do is that we need
to run Docker container prune command so
this will basically help you to prune or
do the cleanup uh Purge of all the
containers which is running on the host
machine here
and that will definitely help us to
remove all the containers which is in
stop status so that will be simply
removed over here in this one so uh this
really completely removes all the
stopped containers onto the docker host
over here in this scenario now second
one is that it creates a new image from
a container's file changes now the way
what happens that normally what we
follow is that we use the docker build
command to do the build process right so
Docker build command is being used in
that particular way now if you really
want to go for a preparation of a new
you know image so all you need to do is
that you need to create a new image from
a container file so you can run the
command called Docker container commit
command and you know whatever container
is there so that will be saved or
preserved as in Docker image because
sometimes we also do some ad hoc changes
or modifications on the Fly inside the
docker container by going inside the
docker container and doing the steps so
we really need to preserve those steps
and that is the reason why we are
converting a Docker container in the
form of a Docker image here so that's
what we are trying to perform here so
Docker container is something which is
uh being uh done over here in such a way
that whatever the steps whatever changes
we are trying to do so that needs to be
performed over here in this case so
Docker container uh commit command is
being used so that we can get a
particular you know final image as part
of the container so the container will
be converted in the form of a Docker
image over here
and the last one is that how we can
remove an existing container so we can
go for the docker RM command which is
available there using which you will be
able to go ahead and you can get a
particular container removed from your
existing system so Docker RM command is
pretty much there using which we can see
that how we can remove an existing
container from your system as such what
exactly now we are going to see that how
we can perform the various commands on
the containers here
so the very first command which we want
to do here is that uh you know in order
to see that how we can actually remove
all these top containers but for this
one what we really need to do is that we
need to first of all run some couple of
containers so I'm going to quickly run a
container called docker
or run hyphen D httpd so I'm going to
run a particular web server over here
and it should be able to give me a
proper container up and running so I can
just double check our value that it as
with the help of the docker PS command
so Docker PS command should give me the
proper confirmation that whether the
container is in up and running over here
or not so you can see that it's up and
running from last 11 seconds here now we
got a command where you know for example
we have got some stopped containers but
again you have to manually remove them
one by one now if you don't want to do
that stuff if you feel that whatever the
stock containers which is available that
should be automatically removed from
your system then we have a command
called Docker container prone now for
this what we really need to do is that
we need to actually stop this container
because again it will be only picked up
like whatever the containers which is
available in these top settings only
those will be picked up so I have copied
the container name here in this case and
I need to stop it so that I will be able
to remove it with the prone command over
here in this one and then I can run like
docker
container front command which will
basically remove all the containers
which is in these top status and once
that is done you will be able to see
like that is something which is
completely gone from your system so this
is the way that how it can not only take
care of these top containers but it's
actually being done so that you can
remove the multiple containers with a
single command itself over here so this
is not something which will be executed
one by one on a different container but
yes it will definitely take care of
doing the uh changes and Performing the
activities
now another thing is that how we can
actually do the uh commit of a
particular Docker image let's say that
we got a Docker container which is uh
running and there are some changes like
some Dynamic changes which is available
there so you want to say this uh
particular container as a Docker image
so that whatever the changes temporary
changes are there so that will be also
saved as a Docker image so all you can
do is that let's say that again run a
particular container
of httpd here so this will help me to
get a particular container from the HTTP
perspective available of to us so Docker
PS command will give me the option
now here I have got a container in which
it's running now if I want to save this
container let's say that I went inside
they did some modification did some
temporary changes now I want to refer it
I want to use it so how you can do that
so for that what you really need to do
is
you need to run like docker
commit that's the command or you can say
Docker because you're going to do a
Docker container commit over here so the
command of the container is what we are
going to do so Docker container commit
and then we're going to see like what is
the container so for container we will
be yes again copying uh that particular
container ID and we'll say like custom
image column version one so we will be
giving a custom name or a name of a
Docker image and a tag over here in this
one so this will basically save my
Docker container which is available in
this one in the format of a particular
Docker image in a format of a Docker
image over here in this case so as you
can see that almost both of them are
having the same change but
the container which we got over here in
this one so this whatever the changes I
you want to wish you want to do you can
do that and you can then put it up into
the uh form of a Docker image a final
Docker image and at last if you want to
remove it so Docker RM and container ID
now this again Docker RM will come
remove only those containers which is in
stop status so if you want to forcefully
remove it so you will be able to use it
with the help of hyphen f which is the
forceful remove which will remove the
container completely from your system
and once it's removed from your system
you will be able to see like there is no
container running up and running over
here in this one so Docker PS hyphen a
is not something which is up and running
over here in this one so the completely
completely got uh wiped out from your
system so that's how you basically work
on these commands on this uh particular
container related commands here so let's
go back to the main content
right so these are the commands where we
can actually do the management like how
to stop the containers how to remove the
containers and how to get a new you know
Docker image out of the container here
now let's see that how we can list down
all the running containers if you feel
that you want to list down all the
running containers then you can go for
the docker containers LS command and
that will eventually help you to list
down all the containers which is
available there which is uh being there
into your system then Docker stock
command is there to stop a container
so whatever container which is there in
the running status so that will be start
with the help of the docker stop command
and then the restart command is there
using which you can do a restart of a
container into your system
right so once the container is restart
so whatever the application running
inside the container so that will also
be restarted over here in this one so
these are the different options which is
available using which we can explore
over here in this one somewhat uh more
commands about the container so again I
am going to run a container over here of
httpd here so that I can show you some
couple of more commands over here now we
can run like Docker PS or you can say
like Docker container LS will be list
down on all all the containers which is
available to you which is being you know
present in this case now if you want to
do any kind of operations like let's say
if I want to stop it now the moment you
do the stop over here so you will be
able to uh stop the container which is
up and running for you and uh you can
come uh you know whatever the
application is being deployed that will
also not be available to you because
that's simply uh completely gone for you
now you can see that Docker 99 list is
not able to show you that particular
option that whether it's uh being shown
or not because it's in stop status so in
fact you can do a you know start or you
can do a restart so both the operations
you can pretty much do over here so that
you will be able to have a complete
activity implemented so restart is also
something which is available which can
definitely help you to see that how the
things really work and how you can go
for the container uh you know the
management over here in this case so you
can do container document in a LS
command to list down what are the
containers which is available then you
can do a restart you can do a stop also
in fact start is also there which can
help you to see that how the containers
is a really available so this is the way
that how you can manage like uh you know
even the restarts and all because this
really helps you to even do the restart
of bounces to the application which is
present inside this container so that
also you will be able to see with this
particular component here
so that let's go back to the main
content and then um you know killing a
running container so if you really want
to kill a container so for that we can
go for the docker kill because Docker
stop is also going to stop the container
but if you want to forcefully kill it so
in that case you can go for the docker
kill command to do that particular kill
on the containers and then if you really
want to go inside the container and you
want to attach your terminal so for that
you can go for the docker attach command
and with this what will happen that you
can go inside the container you can do
whatever the changes you want to perform
you can do that and you can again come
back outside that also so normally
typically we use it so that we can see
some errors of uh in the logs which is
coming up in the containers and if any
kind of issues are coming up for the
applications we should be able to know
that part now the last one is the
particular weight Docker weight is there
so that will block a container and it
will go in the weight status so Docker
weight and the container ID or the
container name you can give so with that
the particular container will go into
the block status let's see somewhat more
uh commands over here so we can run like
Docker container LS again to find out
like what container is available so you
can see that this particular container
is available over here in this one now
if you see like if I want to get a
attached to this container if I want to
attach my input and output a stream with
this container so you can run like
Docker attach and the container ID so
your terminal will be stuck over here
because it's taking up whatever the
progress which is going to be done in
the container that will be in you know a
kind of attached to your terminal over
here and you will be getting the output
but the moment you come out of this like
you say Ctrl C so you will see that you
know it's basically sending the request
to the container that is getting stopped
so you will be able to see like the
docker container list it's not going to
show you because the container is in
stop status so again you have to do the
restart of the container to make sure
that the container is up and running
right so the attach one will help you to
go uh and attach your standard input and
output with your container with a
running container but when the moment
you comes out you will be able to uh
disconnect or shut down the container
and again I had to restart that
container over here in this one then
again of course we have like Docker
weight
then container is there so this will
basically put the container into a block
status because all the requests and all
will be not sent to the container and it
will be blocked and uh if you really
want to like uh consume or if you want
to make use of the container you will
not be able to do that so Docker weight
command will kind of block the container
and you know if you again uh try to come
out of that so
again the request to The Container will
be
resumed and they will be able to use on
that part so Docker wheat command is
being used to block the container and
then we have Docker attached to attach
to the container the standard input and
output and lastly is the docker kill
which is going to kill the container and
you will be able to see that the
container is in simply killed status so
it's uh you know it's not like a stopped
over here if it's gracefully stopped
then the exit code definitely will be
zero but in this case since it's being
killed the particular status is 137 over
here in this one so that's a non-zero
status code so that's how where you will
be able to manage the containers on
different aspects let's go back the main
container so these were some of the
commands which was there from the docker
continuous perspective let's talk about
some of the commands from the share
command perspective so how we can share
the docker images so we have a typical
mechanism which is there using which we
can actually interact with the docker
Hub or a Docker registry so uh if you
really want to pull a Docker image so in
that case you can go for the docker pull
command if we want to push an image to
the registry so you can use for the
docker push command if you want to
execute a command inside a particular
running container or you want to execute
a command or S create or something like
that so you can go for the docker as a
command and with that the execution will
be done and you will be able to get a
response so that will make sure that yes
whatever the execution you are trying to
do so that will be done over here in
this mechanism now let's see that how we
can do a pull and push activity to a
Docker image so first of all the pull
command is pretty simple all you need to
do is that you need to say that okay I
want to pull a Docker image if let's say
that our Docker image is already present
then it's just going to say that it's
already up to date it's already
available there but if in some cases you
feel that the docker image is not
available and you want to pull it so all
you can do is that you can give that a
Docker image name and that you can give
the tag also but in this case we are
using the tag as in latest
so that's the reason why even if you
provide or you don't provide it doesn't
really matter so I'm going to give like
a Docker pull a MySQL over here and the
moment I hit on this one what will
happen that it says this a particular
Docker image is not available into my
system it's going to download it and
once it's going to be downloaded you
will be able to see that into your local
system so Docker images is something
which is going to help you with the
commands and it will give you the uh
particular ways that how the docker
image is available so I can run like
Docker images over here to see that what
are the different Docker images which is
available in this manner now if you
really want to move any of the docker
image then of course you have to do a
Docker login and you will be doing the
uh you know the connectivity to the
docker Hub and then establish the uh
connectivity and all so what you need to
do is that you can run the docker login
command we will also see this command in
the next particular slide but right now
we are just doing it because we want to
showcase that how the push really works
so uh you can actually give your
username that what username you are
holding up so the password does not
shows any activity but yes still it will
let you know that whether the login is
successful or not so in this case it
says login is successful so all you need
to do is let's say like I got this
Custom Image and I want to push it to
the docker home so you need to rename or
you need to do a tag over here because
the same if you run
Docker push custom
image
column version one so if you do that it
will not be pushed into these uh
particular Docker Hub so it's going to
throw error over here that the access is
denied so all you need to do is that you
need to do a tag here
so you have to rename it in such a way
that you should be able to uh use uh add
your username over there because Docker
Hub will identify that if it's there
with your username then only it will be
pushed otherwise it will be simply
rejected over there so that's what you
need to do you need to rename your
Docker image the same Docker image which
you have you have to append your
username of the docker Hub and once that
is done so all you need to do is that
you need to just run like docker
then push is there
then this image name and then version 1.
so the moment you do this what will
happen that it will start transferring
those Docker image to the docker Hub you
see this uh specifies that yes this is
going to the docker Hub docker.io where
my username is there and this is the
Custom Image which is available there so
if I refresh my page on the docker Hub I
will be able to see like this public
repo which is available there but again
this one just requires a credentials and
the login which we did in just a last
command where we ran the docker login
command to do the login to the portal
and after that only we are able to do
the setup and the modifications over
here so that's how you do the login and
you are able to connect on that now
these are some of the pull and push
operations but let's say that if you got
a container which is up and running like
this is uh right now we are not getting
any container which is available over
here so you can say like Docker run
hyphen D httpd like I'm just running a
dummy container over here so that I will
be able to show you this command now if
you for some reason want to go inside
the container and want to see some
processing and want to see some content
so you can run the command called Docker
is it hyphen ID and there you can give
the container ID this is nothing but the
full format of the container ID which is
given to you so you can use this one
also or you can use the shortened also
shortened value also you can use and in
the last we can say like bash the moment
you do this you will be going you will
be going inside the container you see
that this is the host name which is
being shown as the container ID you can
do a like psyphen EF command and it will
throw an error that DPS command is not
available because you are running this
command into the docker into the docker
container so using these kind of
commands which is not available in
container will help you to identify that
yes you are not on the host machine you
are in fact into the docker container so
that's the reason why you can do it
because for you the parts and all
everything is same the terminal is also
same so you may not be able to
differentiate in that part so that's the
reason why it's preferred that we use
that commands which is going to fail but
that's okay it will give you that okay
this command is failing because I am
into the docker container right now so
that's what we are performing over here
right and you can come out of that but
still the docker container is going to
be still up and running over here in
this case so the container is not going
to be down over here in this one so
that's how we manage the container we
can go inside and we can check some
informations and get some files there
so these are some of the commands where
we can interact now in order to log into
the docker Hub because before doing the
docker push you have to login uh you
have to get a credentials and the
connectivity with Docker Hub so you can
run the docker login command where you
can see that how the execution can be
done now using the docker login command
you can interact uh with the particular
Docker Hub you can set up the
credentials once the credentials is
being set up so what you can do is that
you can go ahead and say that okay the
configurations is done and we can go
ahead and we can perform the execution
so Docker login is the command where you
can give the credentials Docker info is
the command using which you can get the
information about the docker tool so
whatever the docker info is available
there so those things you can actually
get it over here in the case of the
docker info so Docker info is going to
give you complete information that what
exactly you have the information
available over here on this one
and then we have the docker history so
using the docker history you can see
history of the docker image that what
are exactly uh the mechanisms or what
are the different image IDs are
available there so this is basically
giving you the complete history on how
the docker image is prepared all the
commands is available there and that
will help you to eventually go and
understand that how the images can be
going on and you can manage on that part
so uh these are some of the couple of
options which is available there where
we can see that how the executions can
be done and we can go ahead on that
particular part so Docker history is the
command again which will help you to see
that how you can go for the particular
mechanism and you can listen on on the
things over there so Docker login was
already done previously when we tried to
push the image but again if you try to
run Docker login you will be able to do
that now Docker info is something which
will give some additional information
about the docker and all the
informations about the docker will be
given back to you over here in this one
so the complete Clarity uh then let's
say that you got a Docker image called
uh httpd now you want to check the
history of this image that how this
image is prepared what are the different
layers which is available how many
layers are there how much size is being
modified in each and every layer so you
can see that we got a layer where we
have a particular changes of 7.838 MB
but pretty much the uh from the complete
size of this httpd The major portion is
actually of the uh this one the basic
operating system which is being
implemented and then of course there are
some packages which got deployed due to
which this uh 60 MB and 7 MB size is
coming up there so this actually gives
you the complete layer wise mechanism
that what are the informations which is
available that how the modifications
actually got modified and got changed
over here in this one so this is the
additional uh help which you can get
from the docker to manage the containers
here so these are the ways where you can
interact with the docker Hub if you
really want to create a volumes for the
containers so you can go for the docker
volume create command so Docker volume
create command is there using which you
can create the docker volumes and you
can pretty much do that uh setup so
Docker volume create is the command
which is available there for that
particular part it will create a volume
for you now let's see that how we can
actually work with the volume so you can
run the command called Docker volume LS
to find out at what are the different
volume switches available there now if
you really want to create a volume over
here in this one so you can say like
Docker volume create and you can go for
the volume name here now the moment you
do that what will happen that a volume
will be created and you can validate it
with the help of Docker volume LS
command that whether the volume is being
created or not over here so this is the
way that how you can manage the volumes
onto the docker host
now let's talk about some of the docker
spam commands here now Docker spam
commands is there to set up a particular
mechanism where you can see that how you
can perform a setup you can actually go
ahead and see that how the setup and
modifications can be done
so Docker spam is there where we can see
that how the execution needs to be done
so you can go for the command called
Docker spam init command this will
actually go for an initialization of a
Docker swamp cluster and once the docker
spam cluster is initialized then you can
pretty much go there and perform
whatever the executions you want to
perform you can do that and you can
execute as such over there so Docker
swam in it is a command using which you
can actually see that how the execution
needs to be done and how we can perform
the uh you know mechanism and the
changes over here so Docker Sam init
hyphen iPhone advice ADR address is
there which helps you to see that how
the executions can be done
now joins you have the docker swamp join
command which is available there now if
you go for Docker spam join command so
uh you can have a particular host so
docus swam join command is there which
you can run on the Node machine and in
that case what will happen that node
will be a part of the docker spam
cluster and then if you want to leave uh
from the swam cluster then you can go
for Docker swim leave command and it
will be simply removed from the darker
swamp cluster there some of the couple
of
finish slides or we can remove from the
docker swamp cluster here so docker
spam is already being initialized or
it's been created when the moment you do
the installation of Docker so Docker
spam in it is the command using which
you can analyze analyze a Docker
container now once you analyze over here
so initial after the initialization you
got a command you which you want to add
it like you want to run it onto the
Dockers fam cluster
it will act like a worker node so that
you will be able to deploy the
containers on top of that also if you
want to join it as a manager so you can
also get the join token again so it will
give you the proper command now if you
run like Docker node LS so you will be
able to see like what are the different
nodes which is available there and for
any reason if you're not comfortable
with that and you want to reinitialize
your Docker spam cluster so all you can
do is that you can say like Docker swamp
a leave hyphen f and you will be left
out of the docker spam and now if you
want to run like knock Docker node LS
you will not be able to get that outward
because it's saying that there is no uh
particular swap manager which is
available so you can run Docker spam in
it or you can run the join command to
connect to that specific node but we
have already left it and nothing is
available over here so that's a way that
how you can manage like what are the
different Docker spam commands we can
typically run to manage the docker spam
cluster now let's talk about the docker
compost camera months now Docker compose
commands is actually there using which
we can create multiple containers it's
not only one container which can we can
create we can create go for multiple
creations of the containers with the
help of Docker compose file so dock
compose hyphen version is the U command
which we can give you the version of
Docker compose which is being used so
here we use a particular yml format file
which will help us to create the
multiple containers we Define the
services that what are the different
containers we want to create here and
then with the help of a Docker hyphen
compose dot vimal file we can create or
we can have that particular Docker
compose file set up over here in this
case so Docker hyphen compose dot yml
file is used to do the setup and the
modification so that you will be able to
have the modifications done and you will
be able to find out a particular
multiple containers gets created all
together so Docker compose is there is
the way that how you can do the setup
how you are going to run the docker
compose file so in that case you're
going to run the docker hyphen compose
up command which will help you to set up
like how the compose can be done and
once you are done with the docker hyphen
compose up command so using this the
compose will be done and the containers
will be up and running so these are the
commands using which we can manage
multiple containers using the docker
compose one so let's try to run some
commands related to Docker compose so
Docker iPhone compose iPhone version
will let you know that what exactly
version of Docker compose is installed
into your system so you can run typical
apt command to do the installation so
Docker hyphen composes the package name
which you can install so that the
executables and the package can be
installed into your system onto the
server now all we need to do is that we
need to create a Docker compose file so
what we need to do is that we need to
create a Docker hyphen compose dot yml
file is the default name of the yml file
which you want to create here that's a
standard name so once you create this
all you need to do is like you need to
put some values attributes the very
first one is the version value that what
version of this particular attribute you
want to create here let's say that I
want to create a version two so I will
configure here that yes the version 2 is
what I require I want to set up over
here in this one now again if I want to
configure that what are the different
Services we want to learn because
Services is the main thing which we will
be running on this machine on the server
here so that's what we require so then
we need to put for a particular service
called Web so web service we want to
search and we want to run and then again
we need to take care about the spacing
because that's a very important part
and we need to provide the values of the
image and uh all these configurations
here so that you should be able to set
up like how the uh images and how the
docker images can be built up over here
so uh web is the service which we want
to deploy and then of course the image
is the attribute which is there for the
web that yes engine is the one which we
are trying to build up now you can give
the ports configuration also so you can
say like ports
and inside that you can see like what
port mapping you want to perform so you
can say like
um 8080 so or sorry 80 is what you can
dash for the engineers because it runs
on 80 Port but now I have to run like 84
I want to attach that Port over here in
this one so I can specify that 80 Port
should be available in this one so I'm
going to save this one here and I'm
going to run a command called Docker
iPhone compose up now this will run the
containers in the interactive mode but
if you feel that I want to run in the
detach mode so you can say like hyphen D
also which will of course run these ones
into the detached mode and it will be
executed so it will process your Docker
compose file and since you have used a
standard name called hyphen Docker
hyphen compose dot priml so it will
automatically pick that Docker details
and start creating the container
as angular is a front-end framework
we'll be using a web server called nginx
to serve the application HTML files
after this session you will understand
three things first what is Docker and
what is it used for you will understand
the basics of Docker and what problem
Docker fixes
next you'll learn how to write a Docker
file which you can think of as the
recipe Docker uses to build your image
in this session we'll be using a
multi-stage Docker file to build our
application
third you will learn how to use an nginx
server to serve an angular application
inside a Docker container
to follow through the examples in this
session you should have an angular
application ready and the latest version
of Docker installed on your Workstation
you can install Docker by following
instructions on docker.com
first let's look at what Docker is
Docker is the most popular
containerization technology and it has
quickly become the de facto standard
when talking about containers
containerizing an application means
packaging it and all its dependencies
into a single easily transportable and
isolated container
this container will run in exactly the
same fashion regardless of the computer
it is run on
by providing this layer of consistency
Docker fixes the traditional but it
works on my machine problem
instead of Distributing just our
application we're Distributing a full
runtime environment along with our
application
well not exactly correct it might help
you to think of a Docker container as a
lightweight virtual machine inside your
computer
your computer can run multiple Docker
containers at the same time stopping and
starting them individually as required
a common problem in software delivery is
dependency management
when one application is run on multiple
development machines and multiple server
environments a small difference in the
version of an external Library can
change the functionality of your
application making it behave differently
on different environments
the beauty of Docker is that if you
build your application into a container
image and transfer the same container
image to your colleague's computer you
can be sure that the application will
function identically on both computers
this is because the container includes
all dependencies for the application
inside it
on the other hand a Docker container
should not have any dependencies to the
host it's running on apart from Docker
itself
[Music]
it's important for you to understand
what images and containers are and
what's the difference between them
for the purposes of this session you can
think of a Docker image as something
that holds a file system and some
metadata in it
the metadata includes things like the
name or tag of the image and
instructions for Docker like what
command to run by default when the image
is started
while the official Docker documentation
is a bit vague about the meaning of
names and tags you should learn the
basics of it
every Docker image has at least one tag
and in fact the same Docker image can
have multiple tags pointing to it
every image tag has a name part and a
version part separated by a colon
for example the nginx image has multiple
tags representing different versions and
different flavors of the image
there's a special version tag called
latest which points the latest version
of the image
all Docker commands default to using the
latest version tag if no other version
tag is explicitly defined
when you build an image on your local
machine or for example a build server
you can upload the image into a Docker
registry
when you configure other machines to use
the same Docker registry you can
download the same image to as many other
machines as you like
there is an official Docker registry
called the docker Hub and the model is
the same as for example GitHub you can
store public images free of charge under
your account but you will need to pay to
host private images
having your images public is great for
hosting open source projects but for any
commercial proprietary applications you
will most likely want to opt for hosting
the images privately
if you don't want to use a third-party
service you can always run your own
Docker registry
my advice is that unless you're planning
on hosting a very large number of images
it's absolutely worth the cost to spend
a few dollars per month on a hosted
Duggar registry such as Docker hub
once your application ecosystem starts
to build around Docker the registry
becomes a critical piece of
infrastructure and keeping it up and
running can prove to be a substantial
piece of work
when you build or download a Docker
image to a computer and run it it
becomes a running container
if you don't tell Docker otherwise
Docker will run a pre-specified command
inside the container
as far as this command is concerned it
is running inside a Linux server and it
has access to other commands and
resources that are available inside the
same container
you can think of a container as a
Sandbox where processes running inside
the container do not have access to the
host system or other containers unless
you explicitly specify otherwise
note that the complete container runs
around a single process created by the
initial command used to start the
container
if this process terminates for any
reason the container will stop
an example for our container would be
the nginx server process
when you start the container the nginx
server process starts in it
and when the nginx server process dies
or is killed the container will stop as
well
it is possible to run multiple processes
inside a single container by for example
using an init system to spawn the
processes you need but for the purposes
of this session we'll use the concept of
just a single process per container
the basic workflow with Docker is as
follows
first you will build a Docker image
either on your workstation or on a
continuous integration server
second you will push the docker image
into a Docker registry to make it
available for other computers
third you will most likely want to run
the image on a server that is accessible
via the Internet so clients can access
whatever service software is running
inside your container
most often this is an HTTP server and in
this session we'll be using the nginx
HTTP server
in software delivery environments that
Embrace continuous delivery this
workflow is often part of a fully
automated pipeline
a pipeline like this can trigger from
committing a piece of code into a
version control system and thus
automatically deploy a new version of
your application into a testing or even
production environment whenever you
change your application code
as we discussed earlier a container is
isolated from the machine it's running
on as well as any other containers
running on the same machine unless you
explicitly Define connections between
them
this is out of the scope of this session
but I recommend you spend some time
after the session to learn about Docker
networking and links between containers
in this session we'll be running a
single container that can connect to the
internet to the host machine and expose
a single TCP port for the nginx server
process
building Docker images is done with
Docker files
you can think of a Docker file as a text
file containing a recipe to tell Docker
how to build an image
the basic function of a Docker file is
to be a list of instructions that
incrementally manipulate an existing
Docker image
the word existing here is important
while you can theoretically build a
Docker image from scratch in most use
cases you will actually be using another
Docker image as the base for your new
image
many Frameworks and open source projects
publish their own Docker images which
can be used by developers to build
images on top of
in this session we'll be using a new
Docker feature called multi-stage builds
this feature is available from Docker
version 17.05 forward and it allows you
to use one base image for building our
application and another base image for
serving it
this works brilliantly for our use case
while angular is often used as part of
the full mean stack in this session
we're only concentrating on angular
I'm assuming you know angular and how
the angular command line interface works
and that you have a package.json in your
application directory
if you're not quite there yet check out
the excellent getting started guide on
the angular website
please note that the functionality of
angular varies wildly between major
versions
however the fundamental idea behind
containerizing an angular application is
the same
our Docker file is going to have two
stages
first we'll need to build our angular
application using the NG build command
this will output the packaged
application in the disk directory under
our application route
in the second stage we will place the
files from the disk directory into a
directory that's served by a web server
and finally run the web server itself
this is where the multi-stage build
feature comes in handy as you remember a
Docker container only has the software
available which we've explicitly added
into it
this means that for the NG build command
to work we need to have the angular
command line interface installed inside
our Docker container
this is something we wouldn't want on
our final Docker image because it's just
extra weight and on the other hand we
wouldn't want the web server inside the
image we're building our application in
because the angular CLI is distributed
as a node package we can use the
official node Docker image as the base
of our first Builder stage
because angular is a front-end framework
it doesn't come with a server-side
component other than the test server
shipped with the angular CLI
unfortunately this test server isn't fit
to be run on production so we'll want to
use something else to serve the angular
HTML files
in this example we will use a robust web
server called nginx to serve the angular
application
please note you could just as well be
using a server application like Apache
or Lighty instead I'm not going to be
diving into the specifics of configuring
nginx nginx does come with good
documentation but more importantly it
also comes with an official Docker image
we can use the official nginx Docker
image as the base for our angular
application image
the good thing is that this makes our
Docker file very short and simple let's
have a look
this is the docker file and it's divided
in the two stages
the way the multi-stage build Works in
Docker is that we're actually creating
multiple Docker images with a single
Docker file but we're only keeping the
last image we've defined
this allows us to build the application
first and then copy the resulting build
artifacts into the next Docker image
the idea behind this is that our final
Docker image won't have all the build
time dependencies in it which makes the
resulting image nice and small
in the first stage we'll use the from
directive to instruct Docker that we
want to use the Note 8 image as our base
image
we're also using the S Builder keyword
so we'll be able to reference this stage
in our second stage later
the next directive is the copy directive
this command is for copying files and
directories from our local machine to
the docker image
we're giving this directive two
arguments the location of our
application code directory and the
target path within the docker container
in this example our application code
sits in test app and we want to place it
in the directory test app inside the
container
the next directive is work there
this tells Docker that all following
commands should be run within the
specified directory which in this
example is the application directory
within the container
the next directives are the Run
directives
as you might guess these directives
instruct Docker to run commands inside
the container
first we're running npm install to
install alt node packages defined in the
package.json file of our application
then we're running NG build to package
our angular application into this
directory
now we'll get to the second stage which
is copying the contents of the disk
directory into a directory that's served
by default by nginx
we're using the from directive again
this tells Docker to start building
another image
we're defining the from image to be an
image called nginx
which is the official nginx Docker image
this image is pre-configured to run the
nginx server and serve HTML files that
are stored in a predefined directory
next we'll use the copy directive
but this time with the from argument
this tells Docker to not look for the
source path in our workstation but
instead the previous stage in this
Docker file
essentially we're copying the contents
of the disk directory in the previous
Docker image into a directory called
user share nginx HTML inside our new
image
the nginx docker image will then serve
the contents of this directory
finally we're using a directive called
expose to tell Docker that the port 80
has a server running in this image
the port 80 is the default HTTP Port as
you might know
note that the exposed directive alone
isn't enough to actually expose the port
when the image is run
you can merely consider this as a
documentation feature
and will still need to explicitly map
Port 80 from the container into a port
on the host machine
this enables you to connect to the
mapped port on the host machine which
will turn forward the connection into
the container
to follow this session for your angular
application create a file with these
contents and place it in The Parent
Directory of your application code
remember to customize the paths in the
file accordingly
make sure the name of the file is Docker
file written as one word with a capital
D
now that you've learned what the docker
file is and what one looks like for an
angular application let's learn how to
use it to containerize an example
angular application
for this we'll be using the docker
command line interface
after you've installed Docker on your
workstation you can use the docker
command line interface to build and run
images
to find the command line shell on a Mac
click on the spotlight search icon on
the top right corner of your screen and
right terminal followed by enter
while the docker command line provides
many features in this session we'll
concentrate on two the docker build and
the docker run commands first let's
navigate to the directory with our
Docker file
the command we'll be using is Docker
build
we'll be giving the command the minus t
argument which defines the tag for the
image that will be built
for this we'll be using the tag test app
the second argument for the docker build
command is the build context
this is where Docker expects to find the
docker file
we'll use a single dot to denote the
current directory and press enter to
start building
you can see Docker runs through every
directive in the docker file as a step
outputting the directive and the output
of running it
finally you'll see that Docker has
successfully built the image and tagged
it as test app latest
as you might remember from before since
we didn't Define the version tag the
attack latest is used automatically
great work
now let's run the image we just built
for this we'll be using the docker run
command
we'll be giving the docker run command a
few arguments
first minus I and minus t to denote that
we want to run the image in an
interactive terminal that is running the
container attached to our current
terminal session so we can see the
output and interact with the process
we'll also be using the minus P argument
to map the port 8080 on our workstation
to Port 80 inside the container
as you remember the nginx server is
listening on Port 80. with this mapping
anyone connecting to port 8080 on our
workstation will get forwarded to Port
80 inside the container
finally the last argument is Attack of
the image we want to run
we're using test app colon latest now
that the container is running let's use
our web browser to navigate to port 8080
on our local machine
and we can see our angular application
being served well done you can see that
nginx by default outputs all the
requests in the standard output of the
container
after you've finished testing press Ctrl
C in your terminal to terminate the
nginx process and thus kill the
container you were running and that's
how you dockerize an angular application
before you go three takeaways
one using Docker makes it easy to ensure
that your application runs consistently
on different workstations and servers
secondly angular is a front-end
framework angular needs a separate web
server software for serving it
nginx is a fast and robust choice
finally using multi-stage Docker files
enables you to streamline your build
process and create slim Docker images if
you are looking to get certified in
devops and become a Docker X but
hurry up and Android now why exactly the
docker networking is important let's
talk about some scenario over here so my
application is something which totally
works perfectly fine into my system and
there is no issues on there I ran it
perfectly fine and I got the response
I'm able to interact everything I'm able
to do but the same application does not
works on the other person's system now
what could be the issue because it's
working on one machine and it's not
working on another machine now the
application does not work on another
system because there is a difference in
the computer systems maybe one system is
having a different machine and another
system is having a different machine and
you may be having some of the
differences maybe in the term of
Hardware maybe in the terms of software
so the differences are possible in two
different machines so how we can get a
solution that we don't get this kind of
problems and once the container is
prepared it should be able to run it
across different systems so Docker
networking is the ideal solution for
this uh problem over here with Docker
networking the application works totally
fine and it's something which can work
on any system you don't have any
dependency that it will run only on a
specific machine or a particular system
so what exactly is in Docker networking
all about so Docker networking enables a
user to link a Docker container to as
many as networks you know he or she
requires so you can actually connect a
Docker container to a specific Network
the docker networks are used to provide
complete isolation to the docker
containers so you can have it like okay
I want to run some couple of containers
in a different network and another
containers I want to run it into a
different networks so you can do that
isolations and the whole purpose of
doing that isolation is so that we will
be able to have a proper networking and
proper executions and all that stuff so
Docker networking is a very important
concept when we talk about or when we
come up with the interaction on having
the docker containers isolated so it
resolves most of the problems okay so
um you can have like a multiple
containers in running in same network
you can have a different uh particular
networks also there so a specific
Network
have its own attributes and then you can
use those attributes to connect to that
containers and to access that containers
but the whole idea is that the
containers are something which is uh
specifically running in a set of servers
or in a set of uh platform so whenever
I'm trying to look forward to that
particular container it has to be there
into a swim network if you don't run it
into your custom Network then whatever
the default or network present it will
be hosted in that Network there so how
does the docker networking actually
works so you got a Decor file so using
the docker file you got up a custom
Docker image and from this Docker image
you have to actually go there and you
know put up like the docker container so
you can also store this Docker image to
the docker Hub so that the same Docker
image can be shared across different
users and they can also pull down their
Docker image and they can have the
container running in their systems so
for the collaborating the docker image
you require the algorithm and the
container is the ultimate thing or
object which we need to create out of
the docker image when we go with the
particular process when the whole
process goes on over there so what
happens that the instead of the docker
Hub you can use your own private
repositories also that's something which
totally relies on to you that how
exactly those things works on so a
Docker file creates a Docker image using
the build command a Docker image
contains all the project source code all
the dependencies uh using this Docker
image you can actually run the source
code in order to get a Docker container
the moment you initialize or running
instance of Docker image you will be
able to get a Docker container you can
actually use the docker hub for
collaborating or for sharing the docker
image so you can store the docker image
on Docker Hub and then different people
can actually go ahead and download or
pull this Docker image onto their system
and they can also run their specific
containers so what are the different
advantages we get with the docker
networking verify first one is the rapid
deployment portability better efficiency
a faster configuration scalability and
the security over here so these are some
of the advantages which we get when we
talk about the docker networking here
now let's talk about the container
Network model here so what is the
network model we are looking forward
from the containers perspective now this
is a kind of a architecture here now in
this one let's see that how it actually
works on now you have a Docker engine
which is a main component which takes up
all the things and all the important
components are actually integrated with
each other now the network sandbox is
the one in which you will be having your
container running up and running now
every network is having its own
attributes the particular different
Gateway IPS the IP addresses the range
of IP address the cidr blocks also
everything can be different to for two
different networks so the whole point is
that you will be defining okay if you
don't Define that I want to run this
particular container into a specific
Network then definitely that's going to
run into default Network which is which
gets created the moment you do the
installation of Dockers but anyhow any
network is required for running a Docker
container so there are different network
drivers which is available there so if
you feel that you want to create a
network so you have to choose a
particular networking or the
connectivity over there and it really
helps us to understand how the
configurations and how the different
modifications and those changes can be
really done here so a lot of changes are
actually going on and uh you know
performing there and this really helps
us to understand that how the things are
being done as such over there so network
is something which is very important
from the container's perspective so the
network sandbox is in kind of a isolated
sandbox that holds the all the network
configurations of the containers so you
will be requiring at least one Docker
container network if you want to run a
container inside in that particular
sandbox so sandbox is created when a
user requires to generate an endpoint on
the network so that's where the sandbox
gets created it can have several
endpoints in a network as it represents
a container's network configurations
like IP address Mac address DNS name so
every container will be having its own
unique IP address right so that can be a
particular endpoint to connect to that
container and it can have it like
different uh endpoints and different
different networks so that's always the
possibilities which is available there
now the endpoints establishes the
connectivity for the container services
with other services also so the moment
you work on the uh particular endpoints
so that can be used for having okay
there is a service running in a
container so how you can do that so if
you in order to connect that you require
these endpoints so that you can
establish the connectivity with the
container services whatever running
inside these Docker containers
right now the network is something which
provides the specific underlining
Network attributes like the networking
support to the docker containers because
these uh Network sandbox uh boxes will
definitely have a requirement of the
network so how you're going to get that
that you will be only able to get when
you go okay I will be looking forward
for the connectivity and I can go for
the communications and I can give the
hardware because although it's a OS
level virtualization but still you
require some interaction with the
hardware from the networking perspective
and then Docker engine is uh the base
engine which is installed on the host
machine and it's actually the main
component which makes sure that the
docker containers and the corresponding
Docker services are always up and
running and he uh using the network
drivers you can actually create multiple
networks onto your system and you can
have those uh configurations now the
network architecture is something which
helps us to provide the connectivity
among the endpoints that probably
belongs to the same network and uh it's
something which is a kind of isolated
Network so different different networks
can be isolated uh networks and we can
have the endpoints to them so uh there
are different ways in which we can
establish the connectivity and this is a
complete Network architecture which is
there so you have a network drivers you
have the networks you have the network
sandboxes in which internally the docker
containers are running and then these uh
Network sandboxes are having their
respective endpoints there to access
them to establish the connectivity now
let's talk about the network drivers now
there are five type of network drivers
which is available first one is the
bridge
host none overlay and Mac VLAN so these
are the different uh particular network
drivers which is available over here uh
first one is a bridge it's a private
default Network created on the host if
you go there if you go to the
installation of a Docker this is the
default driver which gets installed onto
the system onto the host the containers
linked to this network will have an
internal IP address through which we
they can actually communicate with each
other on the same machine also you can
establish the connectivity you can
connect to the containers using that
private IP address but outside the
server you will not be able to have the
communication but internally you can
connect to the containers and containers
can also interact with each other they
can also communicate with each other
with this one uh the docker Daemon
creates the uh Docker 0 so if you run
the ipcon config or if config on the
system you will be able to see a Docker
0 kind of a ethernet Bridge Network
which is created using which you can see
that okay this is the network which is
created by default it's a default
installation or default configuration
which automatically happens the moment
you set up a Docker instance so you
don't have to go into the configurations
or in the setup of there it will be
already available on that particular
installation board there and the next
one is the host one so it's a public
network so when we go with the public
network it actually uses the host IP
address and a TCP port in order to
interact with the service running inside
the docker container so if you are
looking forward that I want to reach out
or I want to connect to the container
but I want to use an uh IP address which
can be accessible from outside world
probably some other server should be
able to resolve that in that situation
the host Network driver comes into the
picture so it effectively disables the
network isolation between the docker
host and the docker containers which
means that using this driver a user can
actually run multiple containers you
know it's it's not possible for the user
to run multiple containers on the same
host because of the fact that you are
using the host IP address now when we
were talking about the bridge connection
at that moment of time we were getting
the private IP addresses which was
totally isolated and totally unique so
there were actually if you create a
using the bridge connection multiple
containers so you will see that if there
are five three containers everyone will
be having a different private IP address
but the moment you go for the host one
so in that situation the IP address is
not going to change so you get some
restrictions okay because the
combination is of IP address and Port so
iprs and 8080 can happen only once it
cannot happen multiple times so that's a
limitation that you cannot run multiple
containers on the same host due to this
restriction and the next one is the none
so in this network driver the docker
containers will neither have any access
to the external network or it will not
be able to communicate with the other
containers also total isolation and
total private containers are being done
when we use this driver so if we these
options are usually used when you want
to disable so this is not doing nothing
but disabling the total networking
functionality for a specific container
that's what it happening over here next
one is the overlay so this one is uh
utilized for creating an internal
private Network to the docker nodes in
the docker spam cluster so overlay does
not comes up in case of a standalone
Docker installation the moment you go
with the particular Docker swam cluster
at that moment of time the overlay type
of network is created so Docker uh swam
is an orchestration tool so that
requires an overlay kind of uh driver
which can you know help them to achieve
the networking in case of cluster
because in case of Docker spam you have
a master node manager node worker nodes
different different machines are there
so in that situation overlay is the main
component or is the main type of driver
which is used and then we have the Mac
VLAN so so it simplifies the
communication processes between their
containers this network assigns a MAC
address to the docker containers with
this Mac address the docker you know
servers will route the network traffic
to a router so it's it's not on the
basis of IP address it's actually on the
basis of Mac address so on the specific
Mac address the network the docker
server will redirect the network to a
specific router
right it's usable when a user wants to
directly connect the containers to the
physical Network rather than the docker
host so that's where this communication
works on so it's a direct connectivity
between the host Network and the docker
container so there is no virtualization
which is happening over here so uh it's
something which is you know giving a
direct access to the networks but it's
definitely on the basis of Mac address
which on which the communication is
happening now let's talk about the
networking implementation and we'll see
that how we can go for a particular
Bridge Network implementation there so
let's go back to the virtual machine so
I am connected to a particular system
here so I can check that if the docker
is available to me and I can just run
the docker version so it says that okay
the client and the server component both
is available there now we are going to
see that what are the different uh
containers which is uh what are the
different Docker containers Network
which is available there so I'm going to
run a command called Docker Network LS
now the moment you do it over here you
will see that none host and Bridge these
are the different containers Network
which is available there and you can see
their Network IDs the names the drivers
the drivers is also very important
because that's what we are going to use
here so it's a very good one here so the
default one you can see that the bridge
is available and uh it's a default one
whenever you create any container by
default whether you give the container
attribute or not it's something which
gets created into the uh terms or in the
basis of the container only in the
default Network only so how we can get
that we can always get that part by you
know having the understanding that what
network we are using so I'm going to run
a couple of containers or small
container and we can see that how
exactly the containers can be executed
and can be done over the am
right so Alexi so I'm going to run a
command
docker
run hyphen d i t
I have an iPhone name
Alpine one
Alpine
image is what I am going to use
and yes
so it pulls down the Alpine image that's
what it happened and the same thing I'm
going to run but I this time I'm going
to run uh alpine2 because I want to
create two containers
right so with this one what will happen
that I will be having a like two
containers available here so I can say
like Docker PS so I can see the two
containers are running one is Alpine and
one is Alpine two right so now what
happens that we will see that how the
connectivity happens and what are the
network details so what I'm going to do
is that first of all I'm going to do an
uh inspect over there onto the uh Bridge
connectivity so that I can get the
detailed information so
um this is the name of the particular
ones of the network so this is the
network which is available and this is
the default one so if I scroll up you
can see that this is the name of the
bridge connectivity and even if you run
the docker Network LS so uh you will be
able to see that the ID also so ID also
if you compare so e65 that's the one
which is available now this one is
something which is using a driver called
bridge and here you can see the subnet
information now if you scroll down you
can can see that two containers are
available here so I did not provide any
kind of network details but uh
definitely what happens that the two
containers which I deployed are
basically running inside this network
okay and you can see that uh the bridge
name Docker 0 is the internet or network
which gets created uh which during the
installation of Docker gets uh
configured now
you can see that this is the IP address
this is the IP address here this is the
MAC address and this is also a MAC
address here so uh the IP address you
can see this is a
3.0.3 in the last and this is dot two
over here so you can see that it
increase the sequence is there so Alpine
one was having an
ip.172.17.0.2 and alpine2 is having an
incremented IP address like a different
one as in three so you can see that
these different IP addresses are there
and uh represents if you want to access
them if you want to connect on them you
will be able to have the connectivity
accordingly so even if you go inside
these containers you can actually see
that what exactly uh the connectivity
you want to establish or the
connectivity you want to perform here
right so this is the way that how the
network usually works and uh the
networks in which you are using over
here is the bridge ones which definitely
helps us to understand that how the
configurations can be done now so uh
what we are going to do over here is
that we will try to see that how exactly
uh we can uh go ahead and uh we can
connect on there and we can also see
that if we are able to communicate with
each other because probably uh if I run
like Ping command over here and I'll put
the containers like two
and I'm able to reach out there is no
problem in inching out to these IPS
because these are the private IP
addresses and I will be able to reach
out to them without any problem and
without any concerns so from the current
uh connectivity perspective there is no
problem as such and I have a connection
available over here but let's see uh
we'll go uh back to the containers we
will see that how exactly we can talk
about we can reach out to the one
container with each other so I want to
connect Alpine one with n Pine two so
how we can do that
so I'm going to use like Docker l
Alpine one
okay now I got a terminal so what I can
do is that I can go for the IPA and this
show so with this one what will happen
that I will be able to know the IP
address so dot 2 is the one which is
available over here okay so this is the
IP address now I can uh simply do the uh
small ping command so I can say like
Ping google.com so that will show me
that if I have an internet connectivity
so that is great so I got a connectivity
with the internet uh over there so I
will be able to reach out any websites
or any kind of that order now I have
another IP so dot three is remember we
were using three different IP so I can
actually do a ping also here so ping
hyphen C5 5 request will put and three
will do here now we are trying to reach
out to the Alpine 2 over there so
although we are connecting to the Alpine
one what we have the connectivity with
alpine two also so from Alpine one you
can connect to the Alpine two containers
so that shows that since both of them
are running in that same container so
they are having the automatic uh
connectivity and you can easily reach
out to them and without any issues you
can do that and similarly the same thing
you can do it uh in case of uh alpine2
also so you get it connected to the
Alpine 2 and uh you can just follow the
same thing so you can connect it with
the if you go to the alpine2 you can
connect to the Alpine one also so if you
come out the containers I will be always
up and running and you will be able to
have the mechanisms there so the whole
idea about these things is that it will
be basically helping you to understand
that how the executions can be done and
how we can perform the activities as
such over here so this is the way that
how we deal with the networkings and how
the containers gets created into a
network and then they can interact with
each Docker but before we jump into that
I want you to hit the Subscribe button
so you get notified about new content as
it gets made available and If you hit
the notification button that
notification will then pop up on your
desktop as a video is published from
Simply learn in addition if you have any
questions on the topic please post them
in the comments below we read them and
we do reply to them as often as we can
so with that said let's jump into
kubernetes versus Docker so let's go
through a couple of scenarios let's do
one for kubernetes and then one for
Docker and we can actually go through
and understand what the problem specific
companies have actually had and how they
able to use the two different tools to
solve them so our first one is with Bose
and Bose had a large catalog of products
that kept growing and their
infrastructure had to change so the way
that they looked at that was actually
establishing two primary goals to be
able to allow their product groups to be
able to easier more easily catch up to
the scale of their business so after
going through a number of solutions they
ended up coming up with a solution of
having kubernetes running their iot
platform as a service inside of Amazon's
AWS cloud service and what you'll see
with both these products is they're very
Cloud friendly but here we have um Bose
and kubernetes working together with AWS
to be able to scale up and meet the
demands of their product catalog and so
the result is that we were able to
increase the number of non-production
deployments significantly by taking the
number of services from being large
bulky surfaces down to small micro
Services being able to handle as many as
1250 plus deployments every year an
incredible amount of time and value has
been opened through the use of
kubernetes now let's have a look at
Docker and see what a similar problem
that people would have so the problem is
with PayPal and PayPal and processes
something in the region of over 200
payments per second across all of their
products and PayPal doesn't just have
PayPal they have Braintree and venmo so
the challenge that PayPal was really
being given is that they had different
architectures which resulted in
different maintenance cycles and
different deployment times and an
overall complexity from having a
decade-old architecture with PayPal
through to a modern architecture with
venmo through the use of Docker PayPal
was able to unify the application
delivery and be able to centralize the
management of all of the containers with
one existing group the net net result is
that PayPal was able to migrate over 700
applications into Docker Enterprise
which consists of over 200 000
containers this ultimately opened up a
50 increase in availability for being
able to add in additional time for
building testing and deploying of
applications just a huge win for PayPal
now let's dig into kubernetes and Docker
and so kubernetes is an open source
platform and it's designed for being
able to maintain a large number of
containers and what you're going to find
is that your argument for kubernetes
versus Docker isn't a real argument it's
kubernetes and Docker working together
so kubernetes is able to manage the
infrastructure of a containerized
environment and Docker is the number one
container management solution and so
with document you're able to automate
the deployment of your applications
being able to keep them in a very
lightweight environment and being able
to create a nice consistent experience
so that your developers are working in
the same containers that are then also
pushed out to production so with Docker
you're able to manage multiple
containers running on same Hardware much
more efficiently than you are with a VM
environment the productivity around
Docker is extremely high you're able to
keep your applications very isolated the
configuration for dark here is really
quick and easy you can be up and running
in minutes with Docker once you have it
installed and running on your
development machine or inside of your
devops environment so if we look at the
deployment between the two and the
differences kubernetes is really
designed for a combination of PODS and
services in its deployment whereas with
Docker it's around about deploying
services in containers so the difference
um here is that kubernetes is going to
manage the entire environment and then
and that environment consisting of PODS
and inside of a pod you're going to have
all of your containers that you're
working on and those containers that can
control the surfaces that actually power
the applications that are being deployed
kubernetes is by default an auto scaling
solution it has it turned on and is
always available whereas a Docker does
not and and that's not surprising
because Docker is a tool for building
out Solutions whereas kubernetes is
about managing your infrastructure
kubernetes is going to run health checks
on the liveness and Readiness of your
entire environment so not just one
container but tens of thousands of
containers whereas Docker is going to
limit the health check to the services
that it's managing within its own
containers now I'm not going to kid you
kubernetes is quite hard to set up is if
of the tools that you're going to be
using in your devops environment it's
it's not an easy setup for you to use
and for this reason you want to really
take advantage of the surfaces within
Azure and other similar cloud of
environments where they actually will do
the setup for you Docker in contrast is
really easy to set up you can as I
mentioned earlier you can be up and
running in a few minutes as you would
expect the fault tolerance within
kubernetes is very high and this is by
Design because the architecture of
kubernetes is built on the same
architecture that Google uses for
managing its entire Cloud infrastructure
in contrast Docker has lower fault
tolerance but that's because it's just
managing the the services within its own
containers what you'll find is that most
public Cloud providers will provide
support for both kubernetes and Docker
here we've highlighted Microsoft Azure
because they were very quick to jump on
and support kubernetes but the realities
is that today Google Amazon and many
other provider is us having first level
support for kubernetes it's just become
extremely popular in a very very short
time frame the company is using both
kubernetes and Docker is vast and every
single day there are more and more
companies using it and you should be
able to look and see whether or not you
can add your own company to this list
did you know the famous Global
researcher Gartner predicts by 2023 more
than 50 percent of the companies will be
adopting Docker containers however a
serverless container like Docker will
have a rise in the revenue from a small
base of
465.8 million in the year 2020 to 944
million in the year 2024. in this video
we will be doing a side-by-side
comparison of the docker containers and
the virtual machines but before that
let's have a look at the objectives of
this lesson in this session you will
have a good understanding of what a
virtual machine is and what a Docker is
followed by that we will have a major
difference between Docker and the
virtual machines also we will discuss
some of the key differences between a
Docker and virtual machines and finally
a use case on what made BBC to use
docker
so the first one what exactly is a
virtual machine it is an isolated
Computing environment that enables a
person to use an operating system via a
physical machine virtual machines
provide the functionality of a physical
computer as you can see on my screen
there is a virtual machine and this is
how a typical virtual machine looks like
now let's look at what a Docker is
Docker is an operating system level
virtualization software platform that
enables software developers and it
administrators to create deploy and run
applications in a Docker container with
all their dependencies and a Docker
container is a lightweight software
package that includes all the
dependencies like Frameworks libraries
and many more which are essentially
required to execute an application
this is the architecture of Docker tool
now let's move on to our next main topic
for the discussion that is Docker versus
virtual machine when it comes to
comparing the two we could say the
docker containers have much more
potential than virtual machines as you
can see on the left and right hand side
both the images look similar now let's
define the layers of virtual machine
from the bottom up let's begin with the
infrastructure infrastructure could be a
computer system a laptop or a virtual
private server such as an Amazon ec2
instance or Azure virtual machine or gcp
virtual machine instance on the top of
the infrastructure runs the operating
system and operating system is a
software that manages hardware and
software resources for the computer
programs on your laptop this will be
likely Windows Mac OS or Linux flavor
operating systems such as Ubuntu next
comes the hypervisor a hypervisor is a
firmware that builds and runs virtual
machines there are two types of
hypervisors type 1 hypervisors are
hyperkit for Mac OS hyper-v for Windows
and kvn for Linux operating systems and
type 2 hypervises are virtualbox and
VMware then we have the guest operating
systems consider an example where you
want to run two apps on your server in
total isolation this process would
require two guest operating systems
which will be controlled by a hypervisor
virtual machines come with many
dependencies where each guest operating
system will at least occupy 512 MB of
your RAM this is worse because each
guest operating system needs its own CPU
and memory resources eventually this
will be expensive then on top of that
each guest operating system requires its
own dependencies such as binaries and
libraries since every application has
different dependencies it requires its
own set of libraries finally it's the
application this layer consists of the
source code for the application you have
built this was all about the virtual
machine on a server here's what the same
setup looks like when you're using
Docker containers here you'll notice
that there are a lot fewer layers Docker
doesn't require a bunch of massive guest
operating systems let me break it down
from the bottom up again infrastructure
and host operating systems are the same
as we discussed for the virtual machine
coming into the third layer instead of a
hypervisor Docker uses Docker engine
Docker engine or Docker is a client
server application that builds and
executes containers using Docker
component
next we have our dependencies just like
on the virtual machines here
dependencies are built into a template
called Docker images as you can see on
the screen each application is still
isolated and occupies less space now
let's have a look at the significant
differences between Docker and virtual
machine let's start with the operating
system first Docker is a container based
model where containers are software
packages used for executing an
application on any operating system
on the other side virtual machine is a
container based model it utilizes user
space along with the kernel space of the
operating system a kernel space is where
the core of the operating system runs
and provides its services to the user a
user cannot modify the space and user
space is the portion of the system
memory in which the user's processes run
Docker containers share operating system
kernels with other applications and
result in a higher server efficiency
hence Docker provides the most
substantial default isolation
capabilities among the other
configuration tools on the other hand
virtual machines did not share the
operating system also it does not
provide isolation in the host kernel in
Docker multiple workloads can run on a
single operating system but in virtual
machine each workload needs a complete
operating system or a hypervisor the
next significant difference between the
two is the performance in the case of
Docker they use the same operating
system without any additional software
like hypervisor in case of virtual
machines performance issues are a major
problem it can be due to the several
reasons like CPU constraints memory
allocation Network latency and many more
running multiple virtual machines lead
to an unstable performance Docker
containers can start up quickly and
result in less boot up time whereas
virtual machines do not start quickly
and lead to a poor performance now let's
talk about the next difference which is
portability with Docker containers a
developer can build an application and
store it into a Docker image image later
he or she can run it across any host
environment but when it comes to Virtual
machines it has portability issues
virtual machines do not have a central
Hub like Docker dust and it requires
more memory space to store data Docker
containers are smaller than virtual
machines due to which the process of
transferring files on the host file
system is easier on the other hand
dependency on the host operating system
and Hardware makes virtual machines less
portable while transferring files
virtual machines should have a copy of
the operating system and its
dependencies due to which image size is
increased and becomes a tedious process
to share data
the boot of time between the both is
very different the application in Docker
containers start without any delay since
the operating system is already up and
running on the other hand virtual
machines take much longer time than it
takes for a container to run
applications these containers were
basically designed to save time in the
deployment process of an application
whereas in Virtual machines to deploy a
single application virtual machine needs
to start an entire operating system
which would result in a full board
process so those are the major
differences between Docker and a virtual
machine now let's discuss the minor
differences between them
Docker is not always in a running state
it stops when the stop command is
executed whereas the virtual machines
are always running in the background
which will result in huge RAM
consumption talking about snapshots
Docker has a lot of snapshots a snapshot
is an image that you can upload on a
private repository to access it on
another host but virtual machines do not
consist of any snapshots in Docker
images can be version controlled just
like git they have local registry called
Docker Hub where uses store and
distribute container images on the other
hand virtual machine does not have a
central Hub they are not version
controlled
Docker can run multiple containers on a
system users can connect multiple
containers using user-defined networks
and shared volumes multiple containers
can be accessed on the same machine and
share the operating system kernel with
other containers each container is
isolated in user space Also Docker
containers occupy less space than
virtual machines and start instantly
virtual machine can run only a limited
number of virtual machines on a system
since each virtual machine requires a
certain amount of CPU RAM memory and
other resources your physical systems
will have less space multiple containers
can be started at a time on the docker
engine the isolation environment allows
a user to run and deploy several
containers simultaneously on a given
host virtual machines can run only a
limited number of virtual machines on a
system since each virtual machine
requires certain amount of CPU RAM
memory and other resources your physical
systems will have less space now that I
have told you the differences between
Docker containers and virtual machines
let me show you a real life case study
of how PCC uses Docker BCC news is a
British News Channel over 500 developers
working across the globe BCC news
delivers broadcasts in almost 30
different languages and with over 80 000
Daily News in English alone the news
channel ran more than 26 000 jobs with
more than 10 continuous Integrations
with sequential scheduling the company
had issues with identifying a way to
unify the coding process and monitor the
continuous integration consistently also
the existing jobs took up to more time
that is up to 60 Minutes to schedule and
perform its task
Docker allowed BCC news to eliminate job
wait times and run jobs in parallel it
also gave the users the ability to work
in a more flexible CI environment where
entire core processes were unified and
stored in a single place however Docker
succeeded in spreading up the whole
continuous integration process so let's
go through and you're going to be asked
to explain what the architecture of
Docker is and Docker really is the most
popular containerization environment so
Docker uses a client server architecture
and the docker client is a service which
runs in a command line and then the
docker Daemon which is run as a rest API
within the command line will accept the
requests and interacts with the
operating system in order to build the
docker images and run the docker
containers and then the docker image is
a template of instructions which is used
to create containers the docker
container is an executable package of
applications and its dependencies to
together and then finally the docker
registry is a service to host and
distribute Docker images among other
users so you'll also be asked to provide
what are the advantages of Docker over
virtual machine and and this is
something that comes up very
consistently in fact and you may want to
even extend it as having what are the
differences between having a dedicated
machine a virtual machine and a Docker
or docker-like environment and really
the the arguments for Docker are just
absolutely fantastic you know first of
all Docker does contain an occupy docket
containers occupy significantly less
space than a virtual machine or a
dedicated machine the boot up time on
Docker is significantly faster than a VM
containers have a much better
performance as they are hosted in a
single Docker image Docker is highly
efficient and very easy to scale
particularly when you start working with
kubernetes easily portable across
multiple platforms and then finally for
space allocation Docker data volumes can
be shared in re use among multiple
containers the argument against virtual
machines is significant and particularly
if you're going into an older
environment where a company is still
using actual dedicated hardware and
haven't moved to a cloud or cloud-like
environment your Arguments for Docker
are going to be very very persuasive be
very clear on what the advantages are
for Docker over a virtual machine
because you want to be able to
succinctly share them with your team and
this is something that's important when
you're going through the interview
process but also equally important
particularly if you're working with a
company that's transitioning or going
through a digital transformation where
they aren't used to working with tools
like Docker you need to be able to
effectively share with that team what
the benefits are so how do we share
Docker containers with different nodes
and in this instance what you want to be
able to do is Leverage The Power of
Docker swarm so darker swarm is a tool
which allows the it administrators and
developers to create and manage clusters
of swarm nodes within the docker
platform on and there are two elements
to the node there's the manager node and
then there's the the worker node the
manager node as you'd assume manages the
entire infrastructure and the working
node is actually the work of the agent
as it gets executed so what are the
commands to create a Docker Swan and so
here we have an example of what a
manager node would look like and once
you've created a swarm on your manager
node you can now add working nodes to
that swarm and again when you're
stepping through this process be very
precise in the execution part that needs
to be taken to be able to effectively
create a swarm so start with the manager
node and then you create a worker node
and then finally when a node is
initializes a manager node it can
immediately create a token and that
token is used for the worker nodes and
associating the IP address with the
worker nodes question 17 how to run
multiple containers using a single
service it is possible to run multiple
containers as a single service by using
Docker compose and and Docker compose
will actually run each of the services
in isolation so that they can interact
with each other the language used to
write out the compose files that allow
you to run the service is called yaml
and yaml stands for yet another markup
language so what is the use of a Docker
file so a Docker file actually is used
for creating Docker images using the
build command so let's go through and
show on the screen what that would look
like and this would be an opportunity
where if you're actually in a technical
interview you could potentially even ask
hey can I draw on a whiteboard and show
you what the architecture for using the
build command would look like and what
the process would look like and again
when you're going through an interview
process as someone who interviews a lot
of people one of the things I really
like is when an interview candidate does
something that's slightly different and
in this instance this is a great example
of where you can stand up to the
Whiteboard and actually show what can
actually be done through actually
creating images on the Whiteboard very
quickly little square boxes where you
can actually show the flow for creating
a build environment as in architect this
should be something that you're
comfortable doing and by doing it in the
interview and suddenly you want to ask
permission before you actually do it but
doing this in the interview really helps
demonstrate your comfortable feelings of
working with these kind of architecture
drawings so back to the question of
creating a Docker file so we go through
and we have a Docker file that actually
then goes ahead and creates the docker
image which then in turns creates the
docker container and then we are able to
push that out up to a Docker Hub and
then share that Docker Hub with
everybody else as part of the docker
registry with the whole network so what
are the differences between Docker image
and Docker containers so let's go
through the docker image so the docker
images are templates of a Docker
container an image is built using a
Docker file and it stores that Docker
file in a Docker repository or a docker
up and you can use Docker Hub as an
example and the image layer is a
read-only file system the docker
container is a collection of the runtime
instances of a Docker image and the
containers are created using Docker
images and they are stored in the docker
Daemon and every container is a layer is
a read write file system so you can't
replace the information you can only
append to it so while you can actually
use yaml for writing your so a question
you can be asked is instead of yaml what
can be an alternate file to build Docker
compose so yaml is the one that is the
default but you can also use Json so if
you are comfortable working with Json
and my that is something that should be
get comfortable with is you want to be
able to use that to name your files and
it's a frame reference Json is a logical
way of being able to do value paired
matching using a JavaScript like syntax
so you're going to be asked to how to
create a Docker container so let's go
through what that would look and we'll
break it down task by task so the task
is going to be create a MySQL Docker
container so to do that you want to be
able to build a Docker image or pull
from an existing Docker image from a
Docker repository or Hub and then you
want to be able to then use Docker to
create a new container which has my SQL
from the existing Docker image
simultaneously the layer of read write
file system is also created on top of
that image now below at the bottom of
the screen we have what the commands
lines look for that so what is the
difference between a registry and a
repository so let's go through that so
for the docker registry and repository
for the registry we have Docker registry
Is An Open Source server-side service
used for hosting and distributing Docker
images whereas in contrast for
repositories collection of multiple
versions of a Docker image in a registry
a user can distinguish between Docker
images with their tag names and then
finally on the the registry Docker also
has its own default registry called
Docker hub for the repository it is a
collection of multiple versions of
Docker images it is stored in a Docker
registry and it has two tabs a public
and private registry so you can actually
create your own Enterprise registry so
you're going to be asked you know what
are the cloud platforms that support
Docker really you know lists them all
and we have listed here Amazon web
services Microsoft Azure Google Cloud
Rackspace but you could add in their IBM
bluemix you could put in Red Hat really
any of the cloud service providers out
there today do support Docker it's just
become an industry standard so what is
the purpose of Expose and publish
commands in Docker so if we go through
expose is an instruction used in Docker
file whereas publish is used in Docker
run command for expose it is used to
expose ports within a Docker Network
whereas with Publishers can be used as
side of a Docker environment for expose
it is a documenting instruction used at
the time of building an image and
running a container whereas with
published is used as to map a host port
to a running container port for expose
is the command used in Docker whereas
for publish we use the command Dash p
for when we're doing our command line
used in Docker and examples of these are
exposed 8080 or with Docker we would put
in all for publish we would do the
example Docker run Dash Dash p and then
0.0.0.80 a colon 80 as our command line
before we end up this course listen to
what the Learners say about our courses
and their own words finishing the course
led to personal recognition and
door for even more artificial
intelligence
I'm 61 years old in last year's compiled
skill was simple as post-operation
program and cyber security I'm happy to
tell you that I was able to clear and
pass my cissp and ccsp certification
exams on the first attempt after taking
the course we decided to make a course
in devops from Simply learning is very
beneficial for both our careers yeah I
got a promotion after the course and I
go to hire with a new company at first
I'd like to learn from YouTube and then
apply for your job foreign
because of its tremendously High waiting
to be rated as one of the best Education
portal online worldwide is truly
promising I switched to a new world as a
digital marketing manager with this my
skill upgraded my salary increase and my
self-confidence boost alongside numerous
other possible outcomes so here we wrap
up Docker crash course if you like this
video consider subscribing to Simply
learn to stay updated with devops
technology thanks for watching
staying ahead in your career requires
continuous learning and upskilling
whether you're a student aiming to learn
today's top skills or a working
professional looking to advance your
career we've got you covered explore our
impressive catalog of certification
programs in Cutting Edge domains
including data science cloud computing
cyber security AI machine learning or
digital marketing designed in
collaboration with leading universities
and top corporations and delivered by
industry experts choose any of our
programs and set yourself on the path to
Career Success click the link in the
description to know more
hi there if you like this video
subscribe to the simply learned YouTube
channel and click here to watch similar
videos turn it up and get certified
click here
thank you