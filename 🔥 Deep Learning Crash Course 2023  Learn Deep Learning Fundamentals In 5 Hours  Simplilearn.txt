hi everyone I am mayank and welcome to
this amazing deep learning crash course
by simply n but before we begin if you
enjoy watching these type of videos and
find them interesting subscribe to our
YouTube channel as we bring you the best
videos daily also hit the Bell icon to
never miss any update from Simply learn
so let's get started we will be briefing
you with a detail introduction to deep
learning that covers the data source
like what is neural network after that
we will see deep learning engineer road
map and top deep learning projects up
ahead we will walk you through the
concepts like AI versus ml versus DL
which is artificial intelligence versus
machine learning versus deep learning
moving forward we will see pytorch
versus tensorflow versus caras and many
more after that we will see what is g
CNN RNN and deep learning and
computational graph in deep learning
speaking of interviews we have covered
you along with the most frequently Asked
deep learning interview questions to
help you crack the toughest interviews
before we move on to what is deep
learning if you are an aspiring AIML
engineer then there is no better time to
train yourself in the exciting field of
AI if you're looking for a course that
covers everything from the fundamentals
to Advanced Techniques then accelerate
your career in AIML with our
comprehensive post-graduate program in
Ai and machine learning boost your
career with this a IML course delivered
in collaboration with P University and
IBM learn in demand skills such as
machine learning deep learning NLP
computer vision reinforcement learning
generative AI prompt engineering chat
GPT and many more you will receive a
prestigious certificate and ask me
anything session by IBM with five
Capstone in different domain using real
data set you will gain practical
experience master classes by PD faculty
and IBM expert ensure top know education
simply learn job assist help you get
notice by Leading in companies this
programs covers statistics python
supervised and unsupervised learning NRP
neural network T flow and many more
skills admission to this postgraduate
program in AIML course requires 2 plus
years of work experience preferred a
bachelor degree with an average of 50%
or higher marks basic understanding of
programming Concepts and Mathematics so
enroll now and unlock exciting AIML
opportunities the course link is in the
description box below so without any
further Ado over to our training expert
ever wondered how Google Translates an
entire web page to a different language
in a matter of seconds or your phone
gallery group's images based on their
location all of this is a product of
deep learning but what exactly is deep
learning deep learning is a subset of
machine learning which in turn is a
subset of artificial intelligence
artificial intelligence is a technique
that enables a machine to mimic human
behavior machine machine learning is a
technique to achieve AI through
algorithms trained with data and finally
deep learning is a type of machine
learning inspired by the structure of
the human brain in terms of deep
learning this structure is called an
artificial neural network let's
understand deep learning better and how
it's different from machine learning say
we create a machine that could
differentiate between tomatoes and
cherries if done using machine learning
we'd have to tell the machine the
features based on which the too can be
differentiated these features could be
the size and the type of stem on them
with deep learning on the other hand the
features are picked out by the neural
network without human intervention of
course that kind of Independence comes
at the cost of having a much higher
volume of data to train our machine now
let's dive into the working of neural
networks here we have three students
each of them write down the digit nine
on a piece of paper notably they don't
all write it identically the human brain
can easily recognize the digits but what
if a computer had to recognize them
that's where deep learning comes in
here's a neural network trained to
identify handwritten digits each number
is present as an image of 28 * 28 pixels
now that amounts to a total of 784
pixels neurons the core entity of a
neural network is where the information
processing takes place each of the 784
pixels is fed to a neuron in the first
layer of our neural network this forms
the input layer on the other end we have
the output layer with each neuron
representing a digit with the hidden
layers existing between them the
information is transferred from one
layer to another over connecting
channels each of these has a value
attached to it and hence is called a
weighted Channel all neurons have a
unique number associated with it called
bias
this bias is added to the weighted sum
of inputs reaching the neuron which is
then applied to a function known as the
activation function the result of the
activation function determines if the
neuron gets activated every activated
neuron passes on information to the
following layers this continues up till
the second last layer the one neuron
activated in the output layer
corresponds to the input digit the
weights and bias are continuously
adjusted to produce a well-trained
network so where is deep learning
applied in Customer Support when most
people converse with customer support
agents the conversation seems so real
they don't even realize that it's
actually a bot on the other side in
medical care neural networks detect
cancer cells and analyze MRI images to
give detailed results self-driving cars
what seemed like science fiction is now
a reality Apple Tesla and Nissan are
only a few of the companies working on
self-driving cars so deep learning has a
vast scope but it too faces some
limitations the first as we discussed
earlier is data while deep learning is
the most efficient way to deal with
unstructured data a neural network
requires a massive volume of data to
train let's assume we always have access
to the necessary amount of data
processing this is not within the
capability of every machine and that
brings us to our second limitation
computational power training a neural
network requires graphical processing
units which have thousands of cores as
compared to CPUs and gpus are of course
more expensive and finally we come down
to training time deep neural networks
take hours or even months to train the
time increases with the amount of data
and number of layers in the network so
so here's a short quiz for you arrange
the following statements in order to
describe the working of a neural network
a the bias is added B the weighted sum
of the inputs is calculated C specific
neuron is activated D the result is fed
to an activation function leave your
answers in the comment section below
three of you stand a chance to win
Amazon vouchers so hurry some of the
popular deep learning Frameworks include
tensor flow pytorch Caris deep learning
4J Cafe and Microsoft cognitive toolkit
considering the future predictions for
deep learning and AI we seem to have
only scratched the surface in fact Horus
technology is working on a device for
the blind that uses deep learning with
computer vision to describe the world to
the users replicating the human mind at
the entirety may be not just an episode
of Science Fiction for too long the
future is indeed full of surprises and
that is deep learning for you in short
deep learning
tutorial by simply learn my name is
Richard kersner with the simply learn
team and today we're going over the Deep
learning tutorial what is deep learning
deep learning is a type of machine
learning that works on the basis of
functionalities of human brain it trains
machines to work on vast volumes of
structured and unstructured data deep
learning uses the concept of neural
networks that is derived from the
structure of a human brain and if you've
ever seen images of a human brain
Network you have dendrites inputs you
have cell which is a nucleus is your
nodes you have synapses which create
weights and an axon which create an
output now this is a very simplified
version of the human brain and there are
certain sets of cells that are strung
together very similar to how today's
neural networks perform but they still
have a long ways to go and the human
brain is still significantly more
complex with uh hundreds of different
kinds of cells and different
interactions we don't see yet so what is
deep learning artificial intelligence uh
so you have your AI is a method of
building smart machines that are capable
to think like human and mimic their
actions be careful with that definition
we're a long way from a human cyborg
coming in and taking over when we talk
about this we're usually talking about
automating uh single kind of instances
or things going on how do we automate a
new process how do we automate a small
robot to do something how do we get a
drone to fly out when it loses contact
to turn around and come back in the
direction it came from those are very
simple processes and significantly lower
than the scale of like what human
thinking does now a subcategory of
artificial intelligence is machine
learning machine learning is an
application of AI that allows machines
to automatically learn and improve with
experience there's a lot of tools out
there to use with machine learning and
you'll see linear regression models and
things like that they're much more
common when dealing with straight uh
numbers a linear regression model and
there's other models that just uh do
basic math functions quite well uh but
then we get into one of the
subcategories deep learning deep
learning is a subfield of machine
learning that is used to extract
patterns from data using Neal networks
and again we're talking about
complicated patterns uh when we talk
about image processing and things like
that they're not as straightforward as
the numbers in stock exchange uh so you
start looking at another way to solve
these problems and figure out is that a
raccoon in the picture or something much
more complicated if getting your
learning started is half the battle what
if you could do that for free visit
skillup by simply learn click on the
link in the description to know more
deep learning performance so when you
have we talk about performance of deep
learning and the amount of data uh the
performance goes up the more data you
have the higher the performance when we
talk about a lot of machine learning
platforms they kind of peek out at a
lower level so again you can think of
this as having thousands of pictures of
raccoons like I said before versus the
numbers in a stock exchange which are
very uh rigid and very clear there you
have a close and an open in the stock
exchange kind of thing where when you
have an image of a raccoon the color
shading all kinds of things go into
trying to figure out is it a
raccoon so when we look at at what is a
neural network we really look into kind
of a nice uh image of it today's neural
networks usually have a layer of inputs
and you'll see here we have input one
two and three with X1 X2
X3 uh so you have your input layer you
have your hidden layers this might have
multiple layers depending on what you're
working on uh then you have your out
output layer which in this case we have
two outputs y1 and Y2 and you can also
see the connectivity here uh so
everything in the first row connects
with everything in the second row in the
hidden layer and if you had another
hidden layer everything in the first
hidden layer would connect to the second
hidden layer and then everything in the
second hidden layer would connect to
each of the outputs and then they'd Cal
make calculations based on that and this
is interesting because there's so many
different aspects of neural networks
nowadays that are changing this basic
configuration
they find that if you skip a hidden
layer or two with your input going
through that that actually changes the
results and uh works better in some
cases there's also convolutional neural
networks which look at windows and
adding up the numbers in them there's a
lot of complexity when we start getting
into the different aspects of what you
can do and how you build neural networks
and again it's very very much in an
infant stage so this basic diagram does
a great job of capturing what it looks
like now the input layer is responsible
to accept the inputs in various formats
the hidden layers again there's that s
could be multiple layers is responsible
for extracting features and hidden
patterns from the data and you know a
hidden patterns and features isimportant
we want to look at features usually talk
about features as your input you have
input one as one feature input two is
another feature if you're looking at the
iris data it might be the width and
length of the pedal each one of those
would be a feature you have these nodes
which generate a number that doesn't
really have a specific
representation but it becomes uh a way
of looking at it of the adding the
features together and creating an
importance value there and the output
layer produces the desired output after
completing the entire processing of the
data what is a perceptron a perceptron
is a binary classification algorithm
proposed by Frank Rosen blot and you'll
see here we have our constants come in
in our inputs uh we have a constant one
for the bias the bias is important uh
you can go back to ukian Geometry where
you have a line uh y = mx + b b being
the Y intercept that's what that
constant is is there needs to be some
kind of adjustment that basically is the
Y
intercept and you have your inputs you
have your weights you have your weighted
sum your activation function and an
output of Z or one and so here we have
we'll go ahead and walk through these we
have X1 X2 X3 are the inputs W not W1 W2
W3 are the weights weights are values
that determine the strength of the
connection between two
neurons and if we go back uh to the
slide let me just flash back here to
this slide you can see how each one of
these nodes has multiple inputs so when
youit look at the hidden layer of the
outputs they have multiple inputs so
each one of these nodes has these inputs
they might be the original features
coming in they might be the layer of
nodes before so we have your X1 X2 and
X3 are the input into your node your
weights are the weighted values that
determine the strength of the connection
between two
neurons the input neurons are multiplied
with the weight and a bias is added
there's that one which is weighted the Y
intercept in ukan
Geometry to give the resulted weight
sum the activation function applies a
step to check if the output of the
waiting function is greater than zero or
not there's a lot of ways to do an
activation function but this is the most
common or the most uh not common but the
most easy to see way of doing an
activation on here so we look at here we
go another walking back through this
diagram and taking a closer look at it
uh we have the predicted output is
compared with the actual output so once
you go through the step and it adds
everything together in there and these
are your weights are multiplied they're
just multiples so you have feature of X1
* weight of one plus feature of X2 *
weight of two so on plus the weight
times the bias and we go ahead and
compute all those all the way through
the node comes out and we have uh the
actual output and then we go ahead and
have a predicted output what do we think
it's going to be and this is on each
node so keep in mind we're zero in on
the Node this isn't the whole process
uh because this actually goes through
all the noes but there's there's another
step in here when we get to that part
the error in the network is estimated
using the cost function and back
propagation technique is used to improve
the
performance so when we look at the uh uh
in the back propagation algorithm the
cost function is minimalized by changing
weights and biases in the network and
you can see here we have our input layer
uh we have our hidden layers and we have
our output layer and so you could think
of this as we have our actual output we
predict what it's going to be in this
case we have two outputs we might
predict that it's either uh nothing
there or a raccoon so one of them comes
up and says I don't see any kind of
animal and the other one says this is a
raccoon so it's either yes or no and if
it gets it wrong it says that's wrong
and it sends that error back and that
error goes to the first set of Weights
which then go to the hidden layers and
their set of Weights which goes back to
B1 the other layer uh and their set of
weights and it adjusts those weights as
it goes backwards and it says hey this
is the error on the output we kind of
adjust it a little bit uh the part that
we don't adjust in the first set of
Weights we say there's still an error so
we send it to the second set of weights
and so forth what happens is as we
adjust these weights each one of these
nodes starts creating kind of a uh
category or something to look for in
whatever image or data we have going in
and so for making a better predictions a
number of epics are executed where the
error is determined by the cost function
now epic is a important thing to keep
track of epics is if you have a large uh
or any data set and let's say you split
out your test date and your training
data set and you're running your data
through how many times do you have to
run all of that data through before you
start uh getting something that is
usable till the error goes down to you
can't adjust the error anymore it's the
lowest error you can get uh so each time
you go through a full set of data before
you repeat and go through the same data
that's called an epic the error is
backward propagated until a sufficiently
small error is
achieved and again that's what we're
talking about we're trying to minimize
the error so we get to a point where
that error really isn't changing anymore
uh you just have the same error coming
out and there's other ways to wait that
error too the cost function or l cost
function measures the accuracy of the
network the cost functions tries to
penalize the network when it makes
errors and you can see here we have a
formula for the cost function uh Cal 12
of the Y predicted minus the Y the
actual y uh squared and of course the
squared value removes the sign because
we don't know whether it's plus or minus
when you look at an error and then the C
this is your cost function how much is
it going to cost how much of an error do
we really have that we're sending back
and so we have with cost function we can
look at this uh we can look at it as a
loss and the Epic again that's every
time we go through the data that's an
epic how many epic runs are we going to
go and so we have a nice graph here that
shows like a very high learning rate low
learning rate different data is going to
change depending on what you're working
on they put the yellow as a good
learning rate uh because it has a nice
slope to it and you think yeah the more
epics I go in the lower the loss so that
means they going in a good direction
there uh and as he curves down it gets
to the the best answer has the lowest
loss on
there so uh looking at types of neural
networks uh we have the Gan we have the
dbn the rbfn the auto encod the RNN the
lstm this is a flash of some of the main
ones that are out there there are now so
many variations that there's variations
on the variations and so just quickly
jumping into the these uh you can see we
have a number of them here listed these
are just like a flash of some of the
more common ones like the Gan General
adversarial network uh where you have
two different models competing against
each other until they find which one can
beat the other one kind think of a chess
game where you keep flipping who's in
charge uh and then we might look at the
um
dbn which is a deep belief
Network and they're used to recognize
clusters and generate image video
sequence quences generally and we have
the RBF uh Network or
rbfn uh which is your radial basis
function Network which uses a different
set of U activation functions um to
figure out which is the right weights uh
Auto encoder the auto encoder is a
little different than the other ones in
that uh it looks for an error but the
error is based in finding data that
groups together
uh so it's a way of sorting the data out
without knowing the answer that's what
an auto encoder does uh RNN RNN has a
couple different definitions uh most of
the time you now see it as a recurrent
neural network where the output part of
the output goes back into the input uh
so they call it this what's recurrent uh
there's also another RNN which deals
with uh learning step by step as opposed
to over a set of data and waiting it as
you go
and there's the uh
lstm or long short-term memory recurrent
neural network it's also an RNN uh which
deals with how do you sort something
like a sentence out uh where the last
word depends on what the first word was
and so it slowly Waits things as it goes
through to figure out what's being
said again these are just a quick Flash
and even as I was describing them you
should have been like well why don't you
hook an auto encoder into a gan and then
run that with the dbn well there are
tools to go ahead and mix and match
these things so you will actually see uh
when you start doing layers you might
have the layers of one type of neural
network feed into the next neural
network and that's actually pretty
common and we talk about deep learning
libraries there's a lot of different
things out there um but the big ones
that they usually talk about and most of
these are dealing with larger datas like
tensor flow Cor cross uh cross and
tensor flow play nice with each other
and cross kind of is is usually
integrated with tensor flow uh Cafe Theo
pytorch uh DL forj these are all
different deep learning libraries and
there's many more out there there's a s
kit uh has a deep learning library in it
under python um Scala has one that
they've been building on there uh so
these aren't the only ones tensorflow is
probably the most uh robust one at this
time but that's not to say the other
ones aren't catching up and there's not
all kinds of other stuff out there
Basics and starting with the basics
we'll start with what is deep learning
so deep learning often touted as an
attempt to replicate human brain
learning operates through intricate
mathematical functions enabling
computers to perform tasks similar to
humans for instance it underpins
groundbreaking Technologies like
driverless cars as well as voice
activated assistants such as C and
Amazon Alexa in practice deep learning
empowers computers to learn directly
from various data formats such as images
text or audio through this process
computer model achieve remarkable
accuracy occasionally surpassing human
level performance it's not just about
mimicking human cognition it's about
harnessing the immense computational
power to expect vast amounts of data and
extract meaningful patterns the backbone
of deep learning consist of neural
networks interconnected nodes that
process data through layers if each
layer refining the representation of the
data training these networks involve
adjusting parameters to minimize errors
and improve accuracy a process
resembling the human learning experience
Albert on a vastly accelerated scale
while data and computational challenges
exist the field continues to evolve
research and Innovation are driving deep
learning forward opening doors to
improved image recognition natural
language understanding medical
Diagnostics and more the promise of
unsupervised learning and the
integration of Deep deep learning into
diverse Industries paints an exciting
future for this transformative
technology impacting the way we interact
with machines and pushing the boundaries
of what's possible so this was all about
the basics and about the Deep learning
now we'll see why we should learn deep
learning so it's a game changer in
various aspects offering
state-of-the-art performance scalability
with data reduced feature engineering
and transferability across task firstly
deep learning shines where traditional
machine learning algorithms struggle
speech recognition image classification
and object detection are areas with deep
learning algorithms like CNN
convolutional neural networks and reant
neural networks RNN Excel due to their
specialized architectures pushing the
boundaries of words achievable moreover
as data becomes abundant deep learning's
performance continues to improve unlike
classical machine learning algorithms
which plate in performance with more
data deep learning algorithms thrive on
it offering a scalable Sol solution for
the ever expanding data sets another
significant Advantage is the reduced
need for intricate feature engineering
traditional methods often require
complex manual feature extraction in
deep learning the emphasis is on
learning directly from the data reducing
the burden of feature engineering and
making it more accessible for beginners
additionally deep learning models are
transferable pre-train networks such as
vgg16 rest Nets and mobile net can be
leveraged as feature extraction tools
for related task this accelerates model
training and enhances performance
especially when faced with limited data
and computational resources in a sense
deep learning empowers you with cutting
it skills enabling you to tackle complex
problems scale with data and Achieve
impressive results without the need for
extensive feature engineering its
transferability across task provide a
strategic Advantage making it an
invaluable skill for modern Ai and
machine learning practitioners so these
were the points that show us why we
should learn deep learning and before
moving on to the ra and before moving on
to the road map to become a deep
learning engine we have a course
offering for you that is if you are one
of the aspiring deep learning
enthusiasts looking for online training
or a professional who I assits to switch
careers in deep learning by learning
from the experts then try giving a short
to celtech a ml boot camp that is in
collaboration with simply learn and the
link to this boot camp is mentioned in
the description box below that will
navigate you to the Boot Camp Page where
you can find a complete overiew of the
program being offered now moving on to
the road map to become an deep learning
engineer so learning deep learning as a
beginner involves several essential
steps each building upon the previous to
develop a comprehensive understanding
and practical skills in this field so
the first is learn and master python so
python is the foundation for many deep
learning libraries and Frameworks start
by mastering python syntax data
structures and basic programming
Concepts it it's essential to be
comfortable with python as you will use
it extensively for coding deep learning
models and moving on to the next step
that is mathematics for deep learning
develop a solid understanding of the
mathematical Concepts behind deep
learning this includes linear algebra
that includes vectors matrices
operations then we have calculus in that
we have differentiation integration and
then we have statistics that includes
probability and distributions these
concepts are vital to grasp the inner
workings of neural networks and then we
have neural networks the third step that
is dive into the core of deep learning
understanding neural networks explore
topics like lose functions that is how
the model measures its performance
activation functions that is which add
nonlinearity to the network then we have
weight initialization that is critical
for model convergence and the vanishing
or exploding gradient problem that is a
challenge in deep networks then we we
have the next step that is architectures
so familiarize yourself with various
neural network architectures start with
feed forward neural networks which form
the basis for many models progress to
Auto encoders convolutional neural
networks CNN for image task recen neural
networks RNN for sequential data
Transformers for NLP task SES networks
for similarity comparison generative at
veral networks GN for generating data
and and explore evolving architectures
like neat neuro evolution of augmenting
topologies and now moving on to the next
step that is learning tools and
Frameworks so learn to work with popular
deep learning tools and Frameworks
familiarize yourself with tensor flow
pytorch caras and ml flow these tools
simplify model development and
management so after this step you should
learn the model optimization so
understand techniques for model
optimization this includes the
distillation that is knowledge transfer
from large models to smaller ones
quantization reducing model size without
significant L loss of performance and
neural architecture search that is
automated methods to find Optimal
Network architectures then you should
have Hands-On projects to practice Z
that is apply what you have learned by
working on Hands-On projects start with
simple task and gradually tackle more
complex problems implement the different
architectures you have learned use the
Frameworks and experiment with model
optimization techniques by following
this structured approach you will build
a strong foundation in deep learning
enabling you to create train and
optimize neural networks for various
tasks the combination of theoretical
understanding and practical experience
gained from Hands-On projects will equip
you to explore Advanced topics and
contribute to the exciting field of deep
learning and the field of deep learning
is rapidly evolving so stay updated with
the latest research papers new
techniques and emerging Technologies
engage with the AI community Through
forums blogs and conferences and
consider joining AI focused online
communities and attend conferences like
neural PS cvpr or icml and there you
have it the Deep learning engineer road
map remember this journey requires
dedication continuous learning and a
passion for solving complex problems
whether you are aiming to work in
Industry research or both this road map
will set you on the right path talk
about our first deep learning project we
have image classification using the 10
data set now this data set was created
by the Canadian Institute for advanced
research the cifar 10 data set contains
6,32 cross 32 color images in 10
different classes the 10 different
classes are airplanes cars Birds cats
deer and others which you can see on the
screen there are 6,000 images of each
class now there are 50,000 training
images and 10,000 test images you can
build a convolutional neural network
model using the Caris library to
classify each image into a category so
it is more of an image recognition kind
of a project that is recommended for
beginners who are new to deep learning
the next project in our list is brain
tumor detection such projects are
heavily used in the healthcare
Industries for detecting diseases before
they occur brain tumors are the
consequence of abnormal growths and
uncontrolled cell division in the brain
they can lead to death if they are not
detected early and accurately brain
tumors can be classified into two types
benign and
malignant deep learning can help
Radiologists in tumor Diagnostics
without invasive measures a deep
learning algorithm that has achieved
significant results in image
segmentation and classification is the
convolutional neural network it can be
used to detect a tumor through brain
using magnetic resonance imaging or MRI
image pixels are First Fed as input to
the CNN soft Max's fully connected
layers are used to classify the images
the accuracy of the convolutional neural
network can be obtained with the radial
basis function classifier next we have
Anna chatbot now this chatbot is the
world's first open-source conversation
platform which comes with a graphical
chart flow designer and chat simulator
it's supported channels of web chat
Android iOS and Facebook Messenger the
Anna chatboard is available to answer
your questions 24 hours a day 365 days a
year Anna is free for personal and
commercial use with the Anna Studio
server simulator and SDK which is
software development kit your
development time is cut from days to
hours using Anna you can create chat
Bots to suit your needs chat Bots play
an important role in customer support
for an e-commerce website you can seeare
realtime order and sipping updates
personalized product recommendations and
targeted offers within the conversation
similarly automobile Brands showrooms
and service centers can use a chatbot
for lead generation scheduling test
drives and roadside
assistance you can check the GitHub
repository that's on the screen to know
more about working on this project the
next project we have in deep learning is
image captioning now image captioning is
the process of generating textual
description of an image it uses both
methods from computer vision to
understand the content of the image and
a language model from the field of
natural language processing to turn the
understanding of the image into words in
the right order Microsoft has built its
own caption Bo
where you can upload an image or the URL
of an image and it will display the
textual description of the image on the
screen you can see we have the picture
of Alon musk and the caption bot has
generated a description that says I
think it's Alon musk wearing a shoot and
a tie and he seems it ends with an
emoticon another such application that
suggests a perfect caption and best
hashtags for a picture is caption AI
automatic image caption generation
software can be built using current
neural networks and long shortterm
memory networks or
lstms now we have image colorization as
our next project image colorization has
seen significant advancements using deep
learning image colorization is the
process of taking an input of a
grayscale image and then producing an
output of a colored image or a colorized
image colorization is a highly
undetermined problem that requires
mapping a real valued luminance image to
a three-dimensional colored value one
that has not a unique Sol ution chroman
is an example of a picture colorization
Model A generative network is framed in
an adversarial model that learns to
colorize by incorporating perceptual and
semantic understanding of color and
class distributions the model is trained
using a fully supervised strategy if you
want to implement chroman check the
GitHub link that's on the screen below
next in our list of projects we have
open nmt machine translation open nmt is
an open source ecosystem for neural
machine translation and neural sequence
learning it started in December 2016 by
the Harvard NLP group and C Tran the
project has since been used in several
research and Industry applications
neural machine translation or nmt is a
new methodology for machine translation
that has led to significant improvements
particularly in terms of human
evaluation compared to rule-based and
statistical machine translation systems
open and empty provides implementations
in two popular deep learning Frameworks
P torch and tensorflow please feel free
to refer to the below GitHub link to
learn more about neural machine
translation now we have music generation
using deep learning it is possible for a
machine to learn the notes structures
and patterns of Music and start
producing music automatically music 21
is a python toolkit used for computer
rated musicology it allows us to teach
the fundamentals of Music Theory
generate music examples and study music
the toolkit provides a simple interface
to acquire the music notation of MIDI
files which stands for musical
instrument digital interface using webet
architecture and long shortterm memory
networks you can generate music without
human intervention Amper music mubert
and juk deck produce smart music powered
by Deep learning
algorithms then we have Alpha go now
alphago was created by Google Deep Mind
and is the first computer program to
defeat a professional human go player
Alpha go algorithm uses a combination of
machine learning and Tre search
techniques combined with extensive
training both from human and computer
play it uses Monte Carlo tree search
Guided by a value Network and a policy
Network both implemented using deep
neural network technology alphao was
initially trained to mimic human play by
attempting to match the moves of expert
players from recorded historical games
it was done using a database of around
30 million moves once it had reached a
certain degree of proficiency by being
set to play large number of games
against other instances of itself using
reinforcement learning to improve its
play next in our list of projects we
have deep dream deep dream is a computer
vision program which uses a
convolutional neural network to find and
enhance patterns in images bya
algorithmic paradia it then creates a
dream like helicogenic
appearance in the overprocessed images
deep dream is an experiment that
visualizes the patterns learned by a
neural
network similar to when a child watches
clouds and interprets and tries to
interpret random shapes deep dream over
interprets and enhances the patterns it
sees in an image do check the given
GitHub link to install dependencies
listed in the notebook and play with the
code locally then we have deep voice
deep voice was developed by BYU which is
an AI system that can clone an
individual's voice the train model takes
just 3 seconds to replicate the output
of a person's voice deep voice 3 is a
fully convolutional attention based
neural text to speech system that
converts text to spectrograms or other
acostic parameters to be used with an
audio waveform synthesis method below
you can find the GitHub link for pytorch
implementation of convolutional networks
based text to speech synthesis
models now we have ibbm Watson IBM
Watson helps run machine learning models
anywhere across any Cloud using IBM
Watson machine learning you can build
analytical models and neural networks
trained with your own data that you can
deploy for use in
applications with its open extensible
model operation Watson machine learning
helps businesses simplify and harness AI
at scale across any Cloud it is used in
healthcare teaching assistant chatbot as
well as for weather forecasting and
finally in our list of deep learning
projects we have the YOLO realtime
object detection you only look once or
YOLO is a state-of-the-art real-time
object detection system it frames object
detection in images as a regression
problem to spatially separated bounding
boxes and Associated class
probabilities using this approach a
single neural network divides the image
into regions and predicts bounding boxes
and probabilities for each region the
neural network predicts bounding boxes
and class probabilities directly from
full images in one evaluation
the base YOLO model processes images in
real time at 45 frames per second you
can use the Coco data set and tensorflow
library to train and test the model to
learn more about YOLO object detection
check the GitHub link that's shown on
the screen
below human versus artificial
intelligence humans are amazing let's
just face it we're amazing creatures
we're all over the planet we're
exploring every niit and Nook we've gone
to the the moon uh we've got into outer
space we're just amazing creatures we're
able to use the available information to
make decisions to communicate with other
people identify patterns and data
remember what people have said adapt to
new situations so let's take a look at
this so so you can get a picture you're
a human being so you know what it's like
to be human let's take a look at
artificial intelligence versus the human
artificial intelligence develops
computer systems that can accomplish
tasks that require human intelligence
so we're looking at this one of the
things that computers can do is they can
provide more accurate results this is
very important recently I did a project
on cancer where it's identifying
markers and as a human being you look at
that and you might be uh looking at all
the different images and the data that
comes off of them and say I like this
person so I want to give them a very
good um Outlook and the next person you
might not like so you want to give them
a bad Outlook well with artificial
intelligence you're going to get a
consistent prediction of what's going to
come out interacts with humans using
their natural language we've seen that
as probably the biggest development
feature right now that's in the
commercial Market that everybody gets to
use as we saw with the example of Alexa
they learn from their mistakes and adapt
to new environments so we see this
slowly coming in more and more and they
learn from the data and automate
repetitive learning repetitive learning
has a lot to do with the neural netor
works you have to program thousands upon
thousands of pictures in there and it's
all automated so as today's computers
evolved it's very quick and easy and
affordable to do this what is machine
learning and deep learning all about
imagine this say you had some time to
waste not that any of us really have a
lot of time anymore to just waste in
today's world and you're sitting by the
road and you have a whole lot of and a
whole lot of time passes by here's a few
hours and suddenly you wonder how many
cars buses trucks and so on passed by in
the six hours now chances are you're not
going to sit by the road for 6 hours and
count buses cars and trucks unless
you're working for the city and you're
trying to do City Planning and you want
to know hey do we need to add a new
truck route maybe we need a Bicycle Link
we have a lot of bicyclists here that
kind of thing so maybe City Planning
would be great for this machine learning
well the way machine Learning Works is
we have labeled data with features
okay so you have a truck or a car a
motorcycle a bus or a bicycle and each
one of those are labeled it comes in and
based on those labels and comparing
those features it gives you an answer
it's a bicycle it's a truck it's a
motorcycle this look a little bit more
in depth on this in the model here it
actually the features we're looking at
would be like the tires someone sits
there and figures out what a tire looks
like takes a lot of work if you try to
try to figure the difference between a
car tire a bicycle tire a motorcycle
tire uh so in the machine learning field
this could take a long time if you're
going to do each
individual aspect of a car and try to
get a result on there and that's what
they did do that was a a very this is
still used on smaller amounts of data
where you figure out what those features
are and then you label them deep
learning so with deep learning one of
our Solutions is to take a very large
unlabeled data set and we put that into
a training model using artificial neural
networks and then that goes into the
neural network itself and we create a
neural network and you'll see um the
arrows are actually kind of backward but
uh which actually is a nice point
because when we train the neural network
we put the bicycle in and then it comes
back and says if it said truck it comes
back and says well you need to change
that to bicycle and then it changes all
those weights going backward they call
it back propagation and let it know it's
a bicycle and that's how it learns
once you've trained the neural network
you then put the new data in and they
call this testing the model so you need
to have some data you've kept off to the
side where you know the answer to and
you take that and you provide the
required output and you say okay is this
is this neural network working correctly
did it identify a bike as a bike a truck
is a truck a motorcycle as a motorcycle
let's just take a little closer look at
that determining what objects are
present in the data so how does deep
learning do this and here we have the
image of the bike it's 28 by by 28
pixels that's a lot of information there
um could you imagine trying to guess
that this is a bicycle image by looking
at each one of those pixels and trying
to figure out what's around it uh and we
actually do that as human beings it's
pretty amazing we know what a bicycle is
and even though it comes in is all this
information and what this looks like is
the image comes in it converts it into a
bunch of different nodes in this case
there's a lot more than what they show
here and it goes through these different
layers and outcomes and says okay this
is a b
bicycle a lot of times they call this
the magic Black Box why because as we
watch it go across here all these
weights and all the math behind this and
it's not it's a little complicated on
the math side you really don't need to
know that when you're programming or
doing working with the Deep learning but
it's like magic you you don't know you
really can't figure out what's going to
come out by looking what's in each one
of those dots and each one of those
lines are firing and what's going in
between them so we like to call it the
magic box uh so that's where deep
learning comes in
and in the end it comes up and you have
this whole neural notwork it comes up
and it says okay we fire all these
different pixels and we connects all
these different dots and gives them
different weights and it says okay this
is a bicycle and that's how we determine
what the object is present in the data
with deep learning machine learning
we're going to take a step into machine
learning here and you'll see how these
fit together in a minute the system is
able to make predictions or take
decisions based on past data that's very
important for machine learning
is that we're looking at stuff and based
on what's been there before we're
creating a decision on there we're
creating something out of there we're
coloring a beach ball we're telling you
what the weather is in Chicago what's
nice about machine learning is a very
powerful processing capability it's
quick and accurate outcomes so you get
results right away once you program the
system the results are very fast and the
decisions and predictions are better
they're more accurate they're consistent
you can analyze very large amounts of
data some of these data things that
they're analyzing now are pedabytes and
terabytes of data it would take hundreds
of people hundreds of years to go
through some of this data and do the
same thing that the machine learning can
do in a very short period of time and
it's inexpensive compared to hiring
hundreds of people so it becomes a very
affordable way to move into the future
is to apply the machine learning to
whatever businesses you're working on
and deep Learning Systems think and
learn like humans using artificial
neural networks again it's like a magic
box performance improves with more data
so the more data the Deep learning gets
the more it gives you better results
it's scalability so you can scale it up
you can scale it down you can increase
what you're looking at currently you
know we're limited by the amount of
computer processing power as to how big
that can get but that envelope
continually gets pushed every day on
what it can do problem solved in an end
to end method so instead of having to
break it apart and you have the first
piece coming in and you identify tires
and the second piece is identifying uh
labeling handlebars and then you bring
that together that if it has handlebars
and tires it's a bicycle and if it has
something that looks like a large Square
it's probably a truck the neural
networks does this all in one network
you don't really know what's going on in
all those weights and all those little
bubbles uh but it does it pretty much in
one package that's why the neural
network systems are so big nowadays and
coming into their own best features are
selected by the system and it this is
important they kind of put it it's on a
bullet on the side here it's a subset of
machine learning this is important when
we talk about deep learning it is a form
of machine learning there's lots of
other forms of machine learning data
analysis but this is the newest and
biggest thing that they apply to a lot
of different packages and they use all
the other machine learning tools
available to work with it and it's very
fast to test um you put in your
information you then have your group of
uh test and then you held some aside you
see how does it do it's very quick to
test it and see what's going on with
your deep learning and your neural
network are they really all that
different H AI versus machine learning
versus deep learning concepts of AI so
we have concepts of II you'll see
natural language processing uh machine
learning an approach to create
artificial intelligence so it's one of
the subsets of artificial intelligence
knowledge representation automated
reasoning computer vision robotics
machine learning versus AI versus deep
learning or Ai and machine learning and
deep
learning so when we look at this we have
ai with machine learning and deep
learning and so we're going to put them
all together we find out that AI is a
big picture we have a collection of
books that goes through some deep
learning the Digital Data is analyzed
text mining comes through the particular
book you're looking for maybe it's a
genre book is identified and in this
case uh we have a robot that goes and
gives a book to the patron I have yet to
be at a library that has a robot bring
me a book but that will be cool when it
happens uh so we look at some of the
pieces here this information goes into
uh as far as this example the
translation of the handwritten printed
data to digital form
that's pretty hard to do that's pretty
hard to go in there and translate
hundreds and hundreds of books and
understand what they're trying to say if
you've never read them so in this case
we use the Deep learning because you can
already use examples where they've
already classified a lot of books and
then they can compare those texts and
say oh okay this is a book on automotive
repair this is a book on robotic
building the Digital Data is in analyzed
then we have more text mining using
machine learning so maybe we'd use a
different program to do a basic classify
uh what you're looking for and say oh
you're looking for auto repair and
computers so you're looking for
automated cars once it's identified then
of course it brings you the
book so here's a nice summation of what
we were just talking about AI with
machine learning and deep learning deep
learning is a subset of machine learning
which is a subset of artificial
intelligence so you can look at
artificial intelligence as a big picture
how does this compare to The Human
Experience in either uh doing the same
thing as a human we do or it does it
better than us and machine learning
which has a lot of tools uh is something
that learns from data past experiences
it's programmed it's uh comes in there
and it says hey we already had these
five things happen the sixth one should
be about the same and then uh then
there's a lot of tools in machine
learning but deep learning then is a
very specific tool in machine learning
it's the artificial neural network which
handles large amounts of data and is
able to take huge pools of experiences
pictures and ideas and bring them
together real life
examples artificial intelligence news
generation very common nowadays as it
goes through there and finds the news
articles or generates the news based
upon the news feeds or the backend
coming in and says okay let's give you
the actual news based on this there's
all the different things Amazon Echo
they have a number of different Prime
music on there of course there's also
the Google command and there's also
Cortana there's tons of smart home
devices now where we can ask it to turn
the TV on or play music for us that's
all artificial intelligence from front
to back you're having a human experience
with these computers and these objects
that are connected to the process
machine learning uh spam detection very
common machine learning doesn't really
have the human interaction part so this
is the part where it goes and says okay
that's a Spam that's not a Spam and it
puts it in your spam folder search
engine result refining uh another
example of of machine learning whereas
it looks at your different results and
it Go and it uh is able to categorize
them as far as this had the most hits
this is the least viewed this has five
stars um you know however they want to
wait it uh all exam good examples of
machine learning and then the Deep
learning uh deep learning another
example is as you have like a exit sign
in this case is translating it into
French sorti I hope I said that right um
neural network has been programmed with
all these different words and images and
so it's able to look at the exit in the
middle and it goes okay we want to know
what that is in French and it's able to
push that out in French French and learn
how to do
that and then we have chatbots um I
remember when Microsoft first had their
little paperclip um boy that was like a
long time ago that came up and you would
type in there and chat with it these are
growing you know it's nice to just be
able to ask a question and it comes up
and gives you the answer and instead of
it being were you're just doing a search
on certain words it's now able to start
linking those words together and form a
sentence in that chat box types of AI
and machine
learning types of artificial
intelligence this in the next few slides
are really important so one of the types
of artificial intelligence is reactive
machines systems that only react they
don't form memories they don't have past
experiences they have something that
happens to them and they react to it my
washing machine is one of those if I put
a ton of clothes in it and they had all
clumped on one side it automatically
adds a weight to reciter it so that my
washing machine is actually a reactive
machine working with whatever the load
is and keeps it nice and so when it
spins it doesn't go thumping against the
side limited memory another form of
artificial intelligence systems look
into the past information is added over
a period of time and information is
shortlived when we're talking about this
and you look at like a neural network
that's been programed to identify cars
it doesn't remember all those pictures
it has no me memory as far as the
hundreds of pictures you process through
it all it has is this is the pattern I
use to identify cars as the final output
for that neural network we looked at so
when they talk about limited memory this
is what they're talking about they're
talking about I've created this based on
all these things but I'm not going to
remember any one
specifically theory of Mind systems
being able to understand human emotions
and how they affect decision-making to
adjust their behaviors according to
their human understanding this is
important because this is our pagemark
this is how we know whether it is an
artificial intelligence or not is it
interacting with humans in a way that we
can understand uh without that
interaction is just an object uh so we
talk about theory of mind we really
understand how it interfaces that whole
if you're in web development user
experience would be the term I would put
in there so a theory of mind would be
user experience how's the whole UI
connected together and one of the final
things is as we get into artificial
intelligence is system being aware of
themselves understanding their internal
States and predicting other people's
feelings and act appropriately so as
artificial intelligence continues to
progress uh we see ones are trying to
understand well what makes people happy
how would they increase our happiness uh
how would they keep themselves from
breaking down if something's broken
inside they have that self-awareness to
be able to fix it and just based on all
that information predicting which action
would work the best what would help
people uh if I know that you're having a
cup of coffee first thing in the morning
is what makes you happy as a robot I
might make you a cup of coffee every
morning at the same time uh to help your
life and help you grow that'd be the
self-awareness is being able to know all
those different things types of machine
learning and like I said on the last
slide this is very important this is
very important if you decide to go in
and get certified in machine learning or
know more about it these are the three
primary types of machine learning the
first first one is supervised learning
systems are able to predict future
outcome based on past data requires both
an input and an output to be given to
the model for it to be trained so in
this case we're looking at anything
where you have 100 images of a
bicycle and those 100 images you know
are bicycle so it's they're preset
someone already looked at all 100 images
and said these are pictures of bicycles
and so the computer learns from those
and then it's given another picture and
maybe the the next picture is a bicycle
and it says oh that resembles all these
other bicycles so it's a bicycle and the
next one's a car and it says it's not a
bicycle that would be supervised
learning because we had to train it we
had to supervise it unsupervised
learning systems are able to identify
hidden patterns from the input data
provided by making the data more
readable and organized the patterns
similarities or anomalies become more
evident uh you'll heard the term cluster
how do you cluster things together some
of these things go together some of of
these don't this is unsupervised where
can look at an image and start pulling
the different pieces of the image out
because they aren't the same the human
all the parts of the human are not the
same as a fuzzy tree behind them because
it's slightly out of focus which is not
the same as the beach ball it's
unsupervised because we never told it
what a beach ball was we never told it
what the human was and we never told it
that those were trees if you're looking
for a course that covers everything from
the fundamentals to Advanced Techniques
then accelerate your career in AIML with
a comprehensive a post-graduate program
in Ai and machine learning so enroll now
and unlock exciting AIML opportunities
the course link is in the description
box below now let's move on to the
differences between tensorflow kasas and
pyo the first difference that we'll be
looking at is called level of API there
are two main types of apis a lowlevel
API and a high level API API stands for
application programming
interface a lowlevel application
programming interface is generally more
detailed and allows you to have more
detailed control to manipulate functions
within them on how to use and Implement
them while a high level API is more
generic and simple and provides more
functionality with one command
statements than a lower level API high
level interfaces are comparatively
easier to learn and to implement the
models using them they allow you to
write code in a shorter amount of time
and to be less involved with the details
in this case tensor flow is a high and
lowlevel API pure tensor flow is a
lowlevel API while tensor flow wrapped
in caras is a high level
API kasas in itself is a high level API
which uses multiple low-level apis as a
back end and simplifies the operation of
these low-level apis py torch is a
low-level API the next criteria that
we'll be looking at is speed tens of
flow is is very fast and is used for
high performances caras is slower as it
works on top of tensor flow not only
does it have to wait for tens of flow to
finish implementation it then starts its
own
implementation meanwhile pytorch works
at the same speed as tensor flow as both
of them are both low-level apis now
kasas is a rapper class for tensor flow
and has added abstraction
functionalities on top of tensorflow
which make it slower than tensorflow and
py toch in computer ation speed both
tensor flow and py torch are almost
equal and in development speed kasas is
faster as it has built-in
functionalities which can significantly
reduce your development time the next
difference is on the
architecture tensor flow is not very
easy to use and even though it provides
caras as a framework that makes it work
easier tensorflow still has a very
complex architecture which is hard to
use meanwhile Keras has a simpler
architecture and is easier to use it
provides a high level of abstraction
which makes implementation of programs
in kasas significantly easier pych on
the other hand also has a complex
architecture and the readability is less
when compared to kasas tens oflow uses
computational graphs which makes it very
complex and hard to interpret but it has
amazing computational ability across
platforms py is a little hard for
beginners but is really good for
computer vision and deep learning
purposes data sets and debugging tensor
flow works with large data sets due to
its high execution speed and debugging
is really hard intensive flow due to its
complex nature meanwhile kasas only
works with very small data sets as its
speed of execution is low programs do
not require frequent debugging in kasas
as they are relatively simpler and py
torch can manage high level tasks in
higher Dimension data sets and is easier
to debug than both kiras and tens oflow
next we'll be looking at ease of
development as we said before tensorflow
works with many hard Concepts such as
computational graphs and tensors which
means that writing code in tens oflow is
very hard it is generally used by people
when they are doing research work and
really need very specific
functionalities kasas on the other hand
provides a high level of abstraction
which makes it very easy to use it is
best for people who are just starting
out with python and machine learning py
is easier than tens oflow but is still
comparatively hard than caras it is not
very easy to learn for beginners but is
significantly more powerful than just
plain caras ease of deployment tensor
flow is very easy to deploy as it uses
tensor flow serving tensorflow serving
is a flexible high performance serving
system for machine learning models is
designed for production environments
tensorflow serving makes it easy to
deploy new algorithms and experiments
while keeping the same server
architecture and apis tensorflow serving
provides outof thee boox integration
with tensorflow models but can be easily
extended to serve other types of models
and data in kasas model deployment can
be done with either tensorflow serving
or flask which makes it relatively easy
but not as easy as you as it would be
with tensor flow and py pyo uses py
mobile which makes deployment easy but
again for tensorflow deployment is way
easier as tensorflow serving can update
your machine learning back end on the
fly without the user even realizing
there's a growing need to execute ml
models on edge devices to reduce latency
preserve privacy and enable new
interactive use cases in the past
Engineers used to train models
separately they would then then go
through a multi-step error prone and
often complex process to train the
models for execution on a mobile device
the mobile run time was often
significantly different from the
operations available during training
leading to inconsistent developer and
eventually user experience all of these
frictions have been removed by py Mobile
by allowing a seamless process to go
from training to deployment by staying
entirely within the py toch ecosystem it
provides an endtoend workflow that
simplifies the research to production
environment for mobile devices in
addition it paves the way for privacy
preserving features via Federated
learning
techniques at the end of the day the
question that really matters is which
framework should you use Keras tensor
flow or py
to now tensor flow has implemented
various levels of abstraction to make
implementation of deep learning and
neural network n Works easy this has
also made debugging easier kasas is
simple and easy but not as fast as tens
oflow it is more user friendly than any
other deep learning API however and is
easier to learn for beginners pyo on the
other hand is the preferred deep
learning API for teachers but it is not
as widely used in production as tensor
flow is it is faster but it has lower
GPU
utilization at the end of the day the
framework that we would suggest that you
use is tensor
flow why while py torch may have been
the preferred deep learning library for
researchers denslow is much more widely
used in day-to-day production pych is
ease of use combined with the default
eager execution mode for easier
debugging predestines it to be used for
fast hacky Solutions and smaller scale
models but tensor flows extensions for
deployment on both servers and mobile
devices combined with with the lack of
python overhead makes it the preferred
option for companies that work with deep
learning models in addition the tens
flow board visualization features offers
a nice way of showing the inner workings
of your model to say your
customers meanwhile between tensor flow
and caras the main difference isn't in
performance tensor flow is a bit faster
due to less overhead but also the level
of control you would like kasas is much
easier to start with than plain tensor
flow but if you want to do something
with kasas that doesn't come out of the
box it'll be harder to implement that
tens of flow on the other hand allows
you to create any arbitrary
computational graph providing much more
flexibility so if you're doing more
research type of work tensor flow is the
sure route to go due to the flexibility
that it provides this so what are
generative adversarial networks
generative adversarial networks or Gans
introduced in 2014 by ianj good fellow
and co-authors became very popular in
the field of machine learning Gan is an
unsupervised learning task in machine
learning it consists of two models that
automatically discover and learn the
patterns in input data the two models
called generator and discriminator
compete with each other to analyze
capture and copy the variations within a
data set Gans can be used to generate
new examples that possibly could have
been drawn from the original data
set in the image below you can see that
there is a database that has real 100
rupee notes the generator which is
basically a neural network generates
fake 100 rupees notes the discriminator
network will identify if the notes are
real or fake let us now understand in
brief about what is a generator a
generator in Gans is a neural network
that creates fake data to be trained on
the discriminator it learns to generate
plausible data the generated instances
become negative training examples for
the discriminator it takes a fixed
length random Vector carrying noise as
input and generates a sample now the
main aim of the generator is to make the
discriminator classify its output as
real the portion of the Gan that trains
the generator
includes a noisy input Vector the
generator Network which transforms the
random input into a data instance a
discriminator network which classifies
the generator data and a generator loss
which penalizes the generator for
failing to do the discriminator the back
propagation method is used to adjust
each weight in the right direction by
calculating the weight's impact on the
output the back propagation method is
used to optain gradients and these
gradients can help change the generator
weights now let us understand in brief
what a discriminator is a discriminator
is a neural network model that
identifies real data from the fake data
generated by the generator the
discriminator training data comes from
two
sources the real data instances such as
real pictures of birds humans currency
notes Etc are used by the discriminator
as positive samples during the training
the fake data instances created by the
generator are used as negative examples
during the training process while
training the discriminator it connects
with two loss functions during
discriminator training the discriminator
ignores the generator law and just uses
the discriminator law in the process of
training the discriminator the
discriminator classifies both real data
and fake data from the generator the
discriminator law penalizes the
discriminator from misclassifying a real
data instance as fake or a fake data
instance as real now moving ahead let's
understand how Gans work now Gans
consists of two networks a generator
which is represented as G of X and A
discriminator which is represented as D
ofx they both play an adversarial game
where the generator tries to fool the
discriminator by generating data similar
to those in the training set the
discriminator tries not to be fooled by
identifying fake data from the real data
they both work simultaneously to learn
and train complex data like audio video
or image files now you are aware that
Gans consists of two networks a
generator G ofx and discriminator D ofx
now the generator Network takes a sample
and generates a fake sample of data the
generator is trained to increase the
probability of the discriminator network
to make mistakes on the other hand the
discriminator network decides whether
the data is generated or taken from the
real sample using a binary
classification problem with the help of
a sigmoid function that gives the output
in the range 0 and 1 here is an example
of a generative adversarial Network
trying to identify if the 100 rupe notes
are real or fake so first a noise vector
or the input Vector is fed to the
generator Network the generator creates
fake 100 rupe notes the real images of
100 rupe notes stored in a database are
passed to the discriminator along with
the fake nodes the discriminator then
identifies the notes and class
classifies them as real or fake we train
the model calculate the loss function at
the end of the discriminator network and
back propagate the loss into both
discriminator and Generator now the
mathematical equation of training again
can be represented as you can see here
now this is the
equation and these are the parameters
here G represents generator D represents
the discriminator now P dat of X is the
probability distribution of real data P
of Z is the distribution of Generator X
is the sample of probability data of X
Zed is the sample size from P of z d of
X is the discriminator network and G of
Zed is the generator Network now the
discriminator focuses to maximize the
objective function such that D of X is
close to 1 and Z of Z is close to zero
it simply means that the discriminator
should identify all the images from the
training set as real that is one and all
the generated Imes just as fake that is
zero the generator wants to minimize the
objective function such that D of Z of Z
is 1 this means that the generator tries
to generate images that are classified
as real that is one by the discriminator
network next let's see the steps for
training a neural network so we have to
first Define the problem and collect the
data then we'll choose the architecture
of Gan now depending on your problem
choose how your Gan should look like
then we need to train the discriminator
in real data that will help us predict
them as real for n number of times next
you need to generate fake inputs for the
generator after that you need to train
the discriminator on fake data to
predict the generator data is fake
finally train the generator on the
output of
discriminator with the discriminator
predictions available train the
generator to fool the discriminator let
us now look at the different types of
Gans so first we have vanilla Gans
now vanilla Gans have minmax
optimization formula that we saw earlier
where the discriminator is a binary
classifier and is using sigmoid cross
entropy loss during
optimization in vanilla Gans the
generator and the discriminator are
simple multi-layer percept droms the
algorithm tries to optimize the
mathematical equation using stochastic
gradient descent up next we have deep
convolutional Gans or DC Gans now DC
Gans support convolutional neuron
networks instead of vanilla neural
networks at both discriminator and
Generator they are more stable and
generate higher quality images the
generator is a set of convolutional
layers with fractional strided
convolutions or transpose convolutions
so it unsampled the input image at every
convolutional layer the discriminator is
a set of convolutional layers with
strided convolutions so it down samples
the input image at every convolutional
layer moving ahead the third type you
have is conditional Gans or C Gans when
Ila Gans can be extended into
conditional models by using an extra
label information to generate better
results in C Gan an additional parameter
called Y is added to the generator for
generating the corresponding data labels
are fed as input to the discriminator to
help distinguish the real data from fake
data generated finally we have super
resolution Gans now Sr Gans use deep
neural networks along with adversarial
neural network to produce higher
resolution images super resolution Gans
generate a photo realistic high
resolution image when given a low
resolution image let's look at some of
the important applications of
Gans so with the help of DC Gans you can
train images of cartoon characters for
generating faces of Anime characters and
Pokemon characters as well next Gans can
be used on the images of humans to
generate realistic faces the faces that
you see on your screens have been
generated using Gans and do not exist in
reality third application we have is is
ganss can be used to build realistic
images from textual descriptions of
objects like birds humans and other
animals we input a sentence and generate
multiple images fitting the description
here is an example of a text to image
translation using Gans for a bird with a
black head yellow body and a short beak
the final application we have is
creating 3D objects so Gans can generate
3D models using 2D pictures of objects
from multiple perspectives Gans are very
popular in the gaming industry Gans can
help automate the task of creating 3D
characters and backgrounds to give them
a realistic feeli do you know how deep
learning recognizes the objects in an
image and really this particular neural
network is how image recognition works
it's very Central one of the biggest
building blocks for image recognition it
does it using convolution neural network
and we over here we have the basic
picture of a u hummingbird pixels of an
image fed as input you have your input
layer coming in so it takes that graphic
and puts it into the input layer you
have all your hidden layers and then you
have your output layer and your output
layer one of those is going to light up
and say oh it's a bird we're going to go
into depth we're going to actually go
back and forth on this a number of times
today so if you're not catching all the
image um don't worry we're going to get
into the details so we have our input
layer accepts the pixels of the image as
input in the form of arrays and you can
see up here where they've actually um
labeled each block of the bird in
different arrays so we'll dive into deep
as to how that looks like and how those
matrixes are set up your hidden layer
carry out feature extraction by
performing certain calculations and
manipulation so this is the part that
kind of reorganizes that picture
multiple ways until we get some data
that's easy to read for the neural
network this layer uses a matrix filter
and performs convolution operation to
detect patterns in the image and if you
remember that convolution means to coil
or to twist so we're going to twist the
data around in alter it and use that
operation to dedi a new pattern there
are multiple hidden layers like
convolution layer real U is how that is
pronounced and that's the rectified
linear unit that has to do with the
activation function that's used pooling
layer also uses multiple filters to
detect edges corners eyes feathers beak
Etc and just like the term says pooling
is pulling information together and
we'll look into that a lot closer here
so if you're if it's a little confusing
now we'll dig in deep and try to get you
uh squared away with that and then
finally there is a fully connected layer
that identifies the object in the image
so we have these different layers coming
through in the hidden layers and they
come into the final area and that's
where we have say one node or one neural
network entity that lights up that says
it's a bird what's in it for you we're
going to cover an introduction to the
CNN what is convolution neural network
how CNN recognizes images we're going to
dig deeper into that and really look at
the individual layers in the
convolutional neural network and finally
we do a use case implementation using
the CNN we'll begin our introduction to
the CNN by introducing the pioneer of
convolutional neural network Yan leun he
was the director of Facebook AI research
group built the first convolutional
neural network called lenette in
1988 so these have been around for a
while and have had a chance to mature
over the years it was used for character
recognition tasks like reading zip code
digits imagine processing mail and
automating that process CNN is a feed
forward neural network that is generally
used to analyze visual images by
producing data with a grid-like topology
a CNN is also known as a convet and very
key to this is we are looking at images
that was what this was designed for and
you'll see the different layers as we
dig in Mirror some of the other some of
them are actually now used since we're
using uh tensorflow and carass in our
code later on you'll see that some of
those layers appear in a lot of your
other neural networ work Frameworks uh
but in this case this is very Central to
processing images and doing so in a
variety that captures multiple images
and really drills down into their
different features in this example here
you see flowers of two varieties Orchid
and a rose I think the Orchid is much
more dainty and beautiful and the rose
smells quite beautiful I have a couple
rose bushes in my yard uh they go into
the input layer that data is in sent to
all the different nodes in the next
layer one of the Hidden layers based on
its different weights and its setup then
comes out and gives those a new value
those values then are multiplied by
their weights and go to the next hidden
layer and so on and then you have the
output layer and one of those notes
comes out and says it's an orchid and
the other one comes out and says it's a
rose depending on how was well it was
trained what separates the CNN or the
convolutional neural network from other
neural networks is a convolutional
operation forms a basis of any
convolutional neural network in a CNN
every image is represented in the form
of array a pixel values so here we have
a real image of the digit 8 uh that then
gets put onto its pixel values
represented the form of an array in this
case you have a two-dimensional array
and then you can see in the Final End
form we transform the digit 8 into its
representational form of pixels of zeros
and one where the ones represent in this
case the black part of the eight and the
zeros represent the white background to
understand the convolution neural
network or how that convolutional
operation Works we're going to take a
side step and look at Matrix in this
case we're going to simplify it we're
going to take two matrices A and B of
one dimension now kind of separate this
from your thinking as we learned that
you want to focus just on the Matrix
aspect of this and then we'll bring that
back together and see what that looks
like when we put the pieces for the
convolutional operation here we've set
up two arrays we have uh in this case
there a single Dimension Matrix and we
have a = 5
37597 and we have b = 1 1 2 3 so in the
convolution as it comes in there it's
going to look at these two and we're
going to start by doing multiplying them
a * B and so we multiply the arrays
element wise and we get 5
66 where five is the 5 * 1 6 is 3 * 2
and then the other 6 is 2 * 3 and since
the two arrays aren't the same size
they're not the same setup we're going
to just truncate the first one and we're
going to look at the second array
multiplied just by the first three
elements of the first array now that's
going to be a little confusing remember
a computer gets to repeat these
processes hundreds of times so we're not
going to just forget those other numbers
later on we'll see we'll bring those
back in and then we have the sum of the
product in this case 5 + 6 plus 6 equals
17 so in our a * B our very first digit
in that Matrix of a * B is 17 and if you
remember I said we're not going to
forget the other digits so we now have
325 we move one set over and we take 325
and we multiply that times B and you'll
see that 3 * 1 is 3 2 * 2 is 4 and so on
and so on WE sum it up so now we have
the second digit of our a * B product in
The Matrix and we continue on with that
same thing so on and so on so then we
would go from uh 375 to 759 to 597 this
short Matrix that we have for a we've
now covered all the different entities
in a that match three different levels
of B now in a little bit we're going to
cover where we use this math at this
multiplying of matrixes and how that
works uh but it's important to
understand that we're going through the
Matrix and multiplying the different
parts to it to match the smaller Matrix
with the larger Matrix I know a lot of
people get lost at is you know what's
going on here with these matrixes uh oh
scary math not really that scary when
you break it down we're looking at a
section of a and we're comparing it to B
so when you break that down your mind
like that you realize okay so I'm I'm
just taking these two matrixes and
comparing them and I'm bringing the
value down into one Matrix a * B we're
reducing that information in a way that
will help the computer see different
aspects let's go ahead and flip over
again back to our images here we are
back to our images talking about going
to the most basic two-dimensional image
you can get to consider the following
two images the image for the symbol back
slash when you press the back slash the
above image is processed and you can see
there for the image for the forward
slash is the opposite so we click the
forward slash button that flips uh very
basic we have four pixels going in can't
get any more basic than that here we
have a little bit more complicated
picture we take a real image of a smiley
face um then we represent that in the
form of black and white pixels so if
this was an image in the computer it's
black and white and like we saw before
we convert this into the zeros and one
so with the other one would have just
been a matrix of just four dots now we
have a significantly larger image coming
coming in so don't worry we're going to
bring this all together here in just a
little bit layers in convolutional
neural network when we're looking at
this we have our convolution layer and
that really is the central aspect of
processing images in the convolutional
neural network that's why we have it and
then that's going to be feeding in and
you have your reu layer which is you
know as we talked about the rectified
linear unit we'll talk about that a
little bit later the reu is an how it
Act is how that layer is activated is
the math behind it what makes the
neurons fire you'll see that in a lot of
other neural networks when you're using
it just by itself it's for processing
smaller amounts of data where you use
the atom activation feature for large
data coming in now because we're
processing small amounts of data in each
image the reu layer works great you have
your pooling layer that's where you're
pulling the data together pooling is a
neural network term it's very commonly
used I like to use a term reduce so if
you're coming from the map and reduce
side you'll see that we're mapping all
this data through all these networks and
then we're going to reduce it we're
going to pull it together and then
finally we have the fully connected
layer that's where our output's going to
come out so we have started to look at
matrixes we've started to look at the
convolutional layer and where it fits in
and everything we've taken a look at
images so we're going to focus more on
the convolution layer since this is a
convolutional neural network a
convolution layer has a number of
filters and perform convolution
operation every image is considered as a
matrix of pixel values consider the
following 5x5 image whose pixel values
are only zero and one now obviously when
we're dealing with color there's all
kinds of things that come in on color
processing but we want to keep it simple
and just keep it black and white and so
we have our image pixels uh so we're
sliding the filter Matrix over the image
and Computing the dot product to detect
the patterns and right here you're going
to ask where does this filter come from
this is a bit confusing because the
filter is going to be derived uh later
on we build the filters when we program
or train our model so you don't need to
worry what the filter actually is what
you do need to understand how a
convolution layer works is what is the
filter doing filter and you'll have many
filters you don't have just one filter
you'll have lots of filters that are
going to look for different aspects and
so the filter might be looking for just
edges it might be looking for different
parts we'll cover that a little bit more
detail in a minute right now we're just
focusing on how the Filter Works as a
matrix remember earlier we talked about
multiplying matrixes together and here
we have our two-dimensional Matrix and
you can see we take the filter and we
multiply it in the upper left image and
you can see right here 1 * 1 1 * 0 1 * 1
we multiply those all together then sum
them and we end up with a convolved
feature of four we're going to take that
and sliding the filter Matrix over the
image and Computing the dot product to
detect patterns so we're just going to
slide this over we're going to predict
the first one and slide it over one
notch predict the second one and so on
and so on all the way through until we
have a new Matrix and this Matrix which
is the same size as a filter has reduced
the image and whatever filter whatever
that's filtering out it's going to be
looking at just those features reduced
down to a smaller uh Matrix so once the
feature maps are extracted the next step
is to move them to the reu layer so the
realu layer The Next Step first is going
to perform an element one operation so
each of those Maps coming in if there's
negative pixels so it sets all the
negative pixels to zero um and you can
see this nice graph where it just zeros
out the negatives and then you have a
value that goes from zero up to whatever
value is coming out of the Matrix this
introduces nonlinearity to the network
uh so up until now we have a we say
linearity we're talking about the fact
that the feature has a value so it's a
linear feature this feature um came up
and it has let's say the feature is the
edge of the beak you know it's like or
the backs slash that we saw um you'll
look at that and say okay this feature
has a value from -10 to 10 in this case
um if it was one it' say yeah this might
be a beak it might not might be an edge
right there a minus five means no we're
not even going to look at it to zero and
so we end up with an output and the
output takes all these feature all these
filtered features remember we're not
just running one filter on this we're
running a number of filters on this
image and so we end up with an rectified
feature map that is looking at just the
features coming through and how they
weigh in from our filters so here we
have an input of a looks like a twocan
bird very exotic looking real image is
scanned in multiple convolution and the
ru layers for locating features and you
can see up here is turned it into a
black and white image and in this case
we're looking in the upper right hand
corner for a feature and that box scans
over a lot of times it doesn't scan one
pixel at a time a lot of times it will
Skip by two or three or four pixels uh
to speed up the process that's one of
the ways you can compensate if you don't
have enough resources on your
computation for large images and it's
not just one filter slowly goes across
the image uh you have multiple filters
have been programmed in there so you're
looking at a lot of different filters
going over the different aspects of the
image and just sliding across there and
forming a new Matrix one more aspect to
note about the reu layer is we're not
just having one Ru coming in uh so not
only do we have multiple features going
through but we're generating multiple
relu layers for locating the features
that's very important to note you know
so we have a quite a bundle we have
multiple filters multiple railu uh which
brings us to the next step forward
propagation now we're going to look at
the pooling layer the rectified feature
map now goes through a pooling layer
pooling is a down sampling operation
that reduces the dimensionality of the
feature map that's all we're trying to
do we're trying to take a huge amount of
information and reduce it down to a
single answer this is a specific kind of
bird this is an iris this is a rose so
you have a rectified feature map and you
see here we have a rectified feature map
coming in um we set the max pooling with
a 2 x two filters and a stride of Two
And if you remember correctly I talked
about not going one pixel at a time uh
well that's where the stride comes in we
end up with a 2X two pulled feature map
but instead of moving one over each time
and looking at every possible
combination we skip a we skip a few
there we go by two we skip every other
pixel and we just do every other one um
and this reduces our rectified feature
map which as you can see over here 16x
16 to a 4x4 so we're continually trying
to filter and reduce our data so that we
can get to something we can manage and
over here you see that we have the Max
uh 34 1 and two and in the max pooling
we're looking for the max value a little
bit different than what we were looking
at before so coming from the rectified
feature we're now finding the max value
and then we're pulling those features
together so instead of think of this as
image of the map think of this as how
valuable is a feature in that area how
much of a feature value do we have and
we just want to find the best or the
maximum feature for that area they might
have that one piece of the filter of the
beak said oh I see a one in this beak in
this image and then it skips over and
says I see a three in this image and
says oh this one is rated as a four we
don't want to sum it to together cuz
then you know you might have like five
ones and I'll say ah five but you might
have uh four zeros and one 10 and that
tin says well this is definitely a beak
where the ones will say probably not a
beak a little strange analogy since
we're looking at a bird but you can see
how that pulled feature map comes down
and we're just looking for the max value
in each one of those matrixes pooling
layer uses different filters to identify
different parts of the image like edges
corners body feathers eyes beak Etc um I
know I focus mainly on the beak but
obviously uh each feature could be each
a different part of the bird coming in
so let's take a look at what that looks
like structure of a convolution neural
network so far this is where we're at
right now we have our input image coming
in and then we use our filters and
there's multiple filters on there that
are being developed to kind of twist and
change that data and so we multiply the
matrixes we take that little filter
maybe it's a 2 by two we multiply it by
each piece of the image and if we step
two then it's every other piece of the
image that generates multiple
convolution layers so we have a number
of convolution layers we have um set up
in there is looking at that data we then
take those convolution layers we run
them through the reu setup and then once
we've done through the reu setup and we
have multiple reu going on multiple
layers that are reu then we're going to
take those multiple layers and we're
going to be pooling them so now we have
the pooling layers or multiple poolings
going on up until this point we're
dealing with uh some sometimes it's
multiple Dimensions you can have three
dimensions some strange data setups that
aren't doing images but looking at other
things they can have four five six seven
dimensions uh so right now we're looking
at 2D image Dimensions coming in into
the pooling layer so the next step is we
want to reduce those Dimensions or
flatten them so flattening flattening is
a process of converting all of the
resultant two-dimensional arrays from
pulled feature map into a single long
continuous linear Vector so over here
you see where we have a pulled feature
map maybe that's the bird wing and it
has values 6847 and we want to just
flatten this out and turn it into 6847
or a single linear vector and we find
out that not only do we do each of the
pulled feature Maps we do all of them
into one long linear Vector so now we've
gone through our convolutional neural
network part and we have the input layer
into the next setup all we've done is
taken all those different pooling layers
and we flattened them out and combined
them into a single linear vector ctor
going in so after we've done the
flattening we have just a quick recap
because we've covered so much so it's
important to go back and take a look at
each of the steps we've gone through the
structure of the network so far is we
have our convolution where we twist it
and we filter it and multiply the
matrixes we end up with our
convolutional layer which uses the reu
to figure out the values going out into
the pooling and you have numerous
convolution layers that then create
numerous pooling layers pulling that
data together which is the max value
which one we want to send forward we
want to send the best value and then
we're going to take all of that from
each of the pooling layers and we're
going to flatten it and we're going to
combine them into a single input going
into the final layer once you get to
that step you might be looking at that
going boy that looks like the normal
inut to most neural network and you're
correct it is so once we have the
flattened Matrix from the pooling layer
that becomes our input so the pooling
layer is fed as an input to the fully
connected layer to classify the image
and so you can see as our flattened
Matrix comes in in this case we have the
pixels from the flatten Matrix fed as an
input back to our twocan or whatever
that kind of bird that is um I need one
of these to identify what kind of bird
that is it comes into our Ford
propagation network uh and that will
then have the different weights coming
down across and then finally it selects
that that's a bird and that it's not a
dog or a cat in this case even though is
not labeled the final layer there in red
is our output layer our final output
layer that says bird cat or dog so quick
recap of everything we've covered so far
we have our input image which is twisted
and multiply the filters are multiplied
times the uh matri the two matrixes
multiplied all the filters to create our
convolution layer our convolution layers
there's multiple layers in there because
it's all building multiple layers off
the different filters then goes through
the reu as say activation and that
creates our pooling and so once we get
into the pooling layer we then and the
pooling look for who's the best with
some max value coming in from our
convolution and then we take that layer
and we flatten it and then it goes into
a fully connected layer our fully
connected neural network and then to the
output and here we can see the entire
process how the CNN recognizes a bird
this is kind of nice cuz it's showing
the little pixels and where they're
going you can see the filter is
generating this convolution network and
that filter shows up in the bottom part
of the convolution network and then
based on that it uses the relo for the
pooling the pooling then find out which
one's the best and so on all the way to
the fully connected layer at the end or
the classification and the output layer
so that'd be a classification neural
network at the end so we covered a lot
of theory up till now and you can
imagine each one of these steps has to
be broken down in code so putting that
together can be a little complicated not
that each step of the process is overly
complicated but because we have so many
steps uh we have 1 2 3 four five
different steps going on here with
substeps in there we're going to break
that down and walk through that in code
so in our use case implementation using
the CNN we'll be using the cfar 10 data
set from Canadian Institute for advanced
research for classifying images across
10 categories Unfortunately they don't
let me know whether it's going to be a
toucan or some other kind of bird but we
do get to find out whether it can
categorize between a ship a frog deer
bird airplane automobile cat dog horse
truck so that's a lot of fun and if
you're looking anything in the news at
all of our automated cars and everything
else you can see where this kind of
processing is so important in today's
world and very Cutting Edge as far as
what's coming out in the commercial
deployment I mean this is really cool
stuff we're starting to see this just
about everywhere in Industry uh so a
great time to be playing with this and
figuring it all out let's go ahead and
dive into the code and see what that
looks like when we're actually writing
our script before we go on let's do uh
one more quick look at what we have here
let's just take a look at data batch one
keys and remember in Jupiter notebook I
can get by with not doing the print
statement if I put a variable down there
it'll just display the variable and you
can see under data batch one for the
keys since this is a dictionary we have
the batch one label data and file names
uh so you can actually see how it's
broken up in our data set so for the
next step or step four as we're calling
it uh we want to display the image using
Matt plot Library there's many ways to
display the images you could even uh
well those other ways to drill into it
but map plot library is really good for
this and we'll also look at our first
reshape uh setup or shaping the data so
you can have a little glimpse into what
that means uh so we're going to start by
importing our map plot and of course
since I am doing jupyter notebook I need
to do the map plot inline command so it
shows up on my page so here we go we're
going to import matplot library. pip
plot is PLT and if you remember map plot
Library the P plot is like a canvas so
we paint stuff onto and there's my
percentage sign map plot library in line
so it's going to show up in my notebook
and then of course we're going to import
numpy as NP for our numbers python array
setup and let's go ahead and set u x
equals to data batch one so this will
pull in all the data going into the x
value and then because this is just a
long stream of binary data uh we need to
go a little bit of reshaping so in here
we have to go ahead and reshape the data
we have 10,000 images okay that looks
correct and this is kind of an
interesting thing it took me a little
bit to I had to go research this myself
to figure out what's going on with this
data and what it is is it's a 32x 32
picture and let me do this let me go
ahead and do a drawing pad on here uh so
we have 32 bits by 32 bits and it's in
color so there's three bits of color now
I don't know why the data is
particularly like this it probably has
to do with how they originally encoded
it but most pictures put the three
afterward so what we're doing here is
we're going to take uh the shape we're
going to take the data which is just a
long stream of information and we're
going to break it up into 10,000 pieces
and those 10,000 pieces then are broken
into three pieces each and those three
pieces then are 32x 32 you could look at
this like an oldfashioned projector
where they have the red screen or the
red projector the blue projector and the
green projector and they add them all
together and each one of them those is a
32x 32 bit so that's probably how this
was originally formatted with in that
kind of Ideal things have changed so
we're going to transpose it and we're
going to take the three which was here
and we're going to put it at the end so
the first part is reshaping the data
from a single line of bit data or
whatever format it is into 10,000x 3x
32x 32 and then we're going to transpose
the color factor to the last place so
it's the image then the 32x 32 in the
middle middle that's this part right
here and then finally we're going to
take this uh which is three bits of data
and put it at the end so it's more like
we do we process images now and then as
type this is really important that we're
going to use an integer 8 you can come
in here and you'll see a lot of these
they'll try to do this with a float or a
float 64 what you got to remember though
is a float uses a lot of memory so once
you switch this into uh something that's
not integer 8 which is goes up to 128
you are just going to the the amount of
ram I me just put that in here is going
to go way up the amount of ram that it
loads uh so you want to go ahead and use
this you can try the other ones and see
what happens if you have a lot of RAM on
your computer but for this exercise this
will work just fine and let's go ahead
and take that and run this so now our X
variable is all loaded and it has all
the images in it from the batch one data
batch one and just to show we were
talking about with the as type on there
if we go ahead and take x0 and just look
for its max value let me go ahead and
run that uh you'll see it doesn't oops I
said 128 it's 255 uh you'll see it
doesn't go over 255 because it's an
basically an asky character is what
we're keeping that down to we're keeping
those values down so they're only 255 0
to 255 versus a float value which would
bring this up um exponentially in size
and since we're using the map plot
Library we can do um oops that's not
what I wanted since we're using the map
plot Library we can take our canvas and
just do a PLT do IM for image show and
let's just take a look at what x0 looks
like and it comes in I'm not sure what
that is but you can see it's a very low
grade image uh broken down to the
minimal pixels on there and if we did
the same thing oh let's do uh let's see
what one looks like hopefully it's a
little easier to see run on there not
enter let's hit the run on that uh and
we can see this is probably a semi truck
that's a good guess on there and I can
just go back up here instead of typing
the same line in over and over and we'll
look at three uh that looks like a dump
truck unloading uh and so on you can do
any of the 10,000 images we can just
jump to 55 uh looks like some kind of
animal looking at us there probably a
dog and just for fun let's do just one
more uh uh run on there and we can see a
nice car for image number four uh so you
can see we past through all the
different images and it's very easy to
look at them and they've been reshaped
to fit our View and what the uh map plot
Library uses for its format so the next
step is we're going to start creating
some helper functions we'll start by a
one hot encoder to help us we're
processing the data remember that your
labels they can't just be words they
have to switch it and we use the one hot
encoder to do that and then we'll also
create a uh class uh CFR helper so it's
going to have a knit and a setup for the
images and then finally we'll go ahead
and run that code so you can see what
that looks like and then we get into the
fun part where we're actually going to
start creating our model our actual
neural network model so let's start by
creating our one hot encoder we're going
to create our own here uh and it's going
to return an out and we'll have our
Vector coming in and our values equal 10
what this means is that we have the 10
values the 10 possible labels and
remember we don't look at the labels as
a number cuz a car isn't one more than a
horse and that'd be just kind of bizarre
to have horse equals zero car equals 1
plane equals 2 cat equals 3 so a cat
plus a car equals what uh so instead we
create a numpy array of zeros and
there's going to be 10 values so we have
10 different values in there so you have
uh zero or one one means it's a cat zero
means it's not a cat um in the next line
it might be that uh one means it's a car
zero means it's not a car so instead of
having one output with a value of 0 to
10 you have 10 outputs with the values
of 0 to one that's what the one hot
encoder is doing here and we're going to
utilize this in code in just a minute so
let's go ahead and take a look at the
next helpers we have a few of these
helper functions we're going to build
and when you're working with a very
complicated python project dividing it
up into separate definitions and classes
is very important otherwise it just
becomes really ungainly to work with so
let's go ahead and put in our next
helper uh which is a class and this is a
lot in this class so we we'll break it
down here let's just start uh so we can
put a space right in there there we go
now this a little bit more readable add
a second space so we're going to create
our class the cipher Helper and we'll
start by initializing it now there's a
lot going on in here so let's start with
the uh nit part uh self. I equals z
that'll come in in a little bit we'll
come back to that in the lower part we
want to initialize our training batches
so when we went through this there was
like a meta batch we don't need the meta
batch but we do need the data batch one
two three four five and we do not want
the testing batch in here this is just
the self all train batches so we're
going to come make an array of of all
those different images and then of
course we left the test batch out so we
have our self. test batch uh we're going
to initialize the training images and
the training labels and also the test
images and the test labels so these are
just this is just to initialize these
variables in here then we create another
definition down here and this is going
to set up images let's just take a look
and see what's going on in there now we
could have all just put this as part of
the uh init part uh since this is all
just helper stuff but breaking it up
again makes it easier to read it also
makes it easier when we start executing
the different pieces to see what's going
on so that way we have a nice print
statement to say hey we're now running
this and this is what's going on in here
we're going to set up these
self-training images at this point and
that's going to go to a numpy array
vstack and in there we're going to load
up uh in this case the data for D and S
all trained batches again that points
right up to here so we're going to go
through each one of these uh files or
each one of these data sets because
they're not a file anymore we've brought
them in data batch one points to the
actual data and so our self-training
images is going to stack them all into
our into a numpy array and then it's
always nice to get the training length
and that's just a total number of uh
self-training images in there and then
we're going to take the selft trining
images look me switch marker colors
because I am getting a little too much
on the markers up here oops there we go
bring down our marker change so we can
see it a little better and at this point
this should look familiar where did we
see this well when we wanted to uh uh
look at this above and we want to look
at the images in the matplot library we
had to reshape it so we're doing the
same thing here we're taking our
self-training images and uh based on the
training length total number of images
because we stacked them all together so
now it's just one large file of images
we're going to take and look at it as
our our three video cameras that are
each displaying uh 32x 32 we're going to
switch that around so that now we have
um each of our images that stays the
same place and then we have our 32 by 32
and then by our three our last our three
different values for the color and of
course we want to go ahead and uh they
run this where you say divide by 255
that was from earlier it just brings the
data into 0 to one that's what this is
doing so we're turning this into a 0:
one array which is uh all the pictures
32x 32x 3 and then we're going to take
the self-training labels and we're going
to pump those through our one hot
encoder we just made and we're going to
stack them together and uh again we're
converting this into an array that goes
from uh instead of having horse equals 1
dog equals 2 and then horse plus dog
would equal three which would be C
no it's going to be uh you know an array
of 10 where each one is zero to one then
we want to go ahead and set up our test
images and labels and uh when we're
doing this you're going to see it's the
same thing we just did with the rest of
it let me just change colors right here
this is no different than what we were
doing up here with our training Set uh
we're going to stack the different uh
images uh we're going to get the length
of them so we know how many images are
in there uh you certainly could add them
by hand but it's nice to let the
computer do it especially if it ever
changes on the other in and you're using
other data and again we reshape them and
transpose them and we also do the one
hot encoder same thing we just did on
our training images so now our test
images are in the same format so now we
have a definition which sets up all our
images in there and then the next step
is to go ahead and batch them or next
batch and let's do another breakout here
for batches because this is really
important to understand T to throw me
for a little Loop when I'm working with
tensor flow or carass or a lot of these
we have our data coming in if you
remember we had like 10,000 photos let
me just put 10,000 down here we don't
want to run all 10,000 at once so we
want to break this up into batch sizes
and you also remember that we had the
number of photos in this case uh length
of test or whatever number is in there
uh we also have 32 by 32 by three so
when we're looking at the batch size we
want to change this from 10,000 to um a
batch of in this case I think we're
going to do batches of a 100 so we want
to look at just 100 the first 100 of the
photos and if you remember we set selfi
equal to zero uh so what we're looking
at here is we're going to create X we're
going to get the next batch from the
very initialized we've already
initialized it for zero so we're going
to look at X from zero to batch size
which we set to 100 so just the first
100 images and then we're going to
reshape that into uh and this is
important important to let the data know
that we're looking at 100x 32x 32x 3 now
we've already formatted it to the 32x
32x 3 this just sets everything up
correctly so that X has the data in
there in the correct order and the
correct shape and then the Y just like
the X uh is our labels so our training
labels again they go from zero to batch
size in this case they do selfi plus
batch size cuz the selfi is going to
keep changing and then finally we
increment the selfi CU we have zero so
so the next time we call it we're going
to get the next batch size and so
basically we have X and Y X being the
photograph data coming in and Y being
the label and that of course is labeled
through one hot encoder so if you
remember correctly if it was say horse
is equal to zero it would be um one for
the zero position since this is the
horse and then everything else would be
zero in here me just put lines through
there there we go there's our array hard
to see that array so let's go ahead and
take that and uh we're going to finish
loading it since this is our class and
now we're armed with all this um uh our
setup over here let's go ahead and load
that up and so we're going to create a
variable CH with the CFR helper in it
and then we're going to do ch. setup
images uh now we could have just put all
the setup images under the init but by
breaking this up into two parts it makes
it much more readable and um also if
you're doing other work there's reasons
to do that as far as the setup let's go
ahead and run that and you can see where
it says uh setting up training images
and labels setting up test images and
that's one of the reasons we broke it up
is so that if you're testing this out
you can actually have print statements
in there telling you what's going on
which is really nice uh they did a good
job with this setup I like the way that
it was broken up in the back and then
one quick note you want to remember that
batch to set up the next batch so we
have to run uh batch equals CH next
batch of 100 because we're going to use
the 100 size uh but we'll come back to
that we're going to use that just
remember that that's part part of our
code we're going to be using in a minute
from the definition we just made so now
we're ready to create our model first
thing we want to do is we want to import
our tensor flow as TF I'll just go ahead
and run that so it's loaded up and you
can see we got a a warning here uh
that's because they're making some
changes it's always growing and they're
going to be depreciating one of the uh
values from float 64 to float type or
it's treated as an NP float 64 uh
nothing to really worry about cuz this
doesn't even affect what we're working
on cuz we've set all of our stuff to a
200 55 value or 0 to one and do keep in
mind that 0 to one value that we
converted to 255 is still a float value
uh but it'll easily work with either the
uh numpy float 64 or the numpy dtype
float it doesn't matter which one it
goes through so the depreciation would
not affect our code as we have it and in
our tensor flow uh we'll go ahead let me
just increase the size in there just a
moment so you can get better view of the
um what we're typing in uh we're going
to set a couple placeholders here and so
we have we're going to set x equals TF
placeholder TF float 32 we just talked
about the float 64 versus the numpy
float we're actually just going to keep
this at float 32 more than a significant
number of decimals for what we're
working with and since it's a
placeholder we're going to set the shape
equal to and we've set it equal to none
because at this point we're just holding
the place on there we'll be setting up
as we run the batches that's what the
first value is and then 32x 32x 3 that's
what we reshaped our data to fit in and
then we have our y true equals
placeholder TF float 32 and the shape
equals none comma 10 10 is the 10
different labels we have so it's an
array of 10 and then let's create one
more placeholder we'll call this a hold
prob or hold probability and we're going
to use this we don't have to have a
shape or anything for this this
placeholder is for what we call Dropout
if you remember from our Theory before
we drop out so many nodes is looking at
or the different values going through
which helps decrease bias so we need to
go ahead and put a a placeholder for
that also and we'll run this so it's all
loaded up in there so we have our three
different placeholders and since we're
in tensor flow when you use caras it
does some of this automatically but
we're in tensorflow direct sits on
tensor flow we're going to go ahead and
create some more helper functions we're
going to create something to help us
initialize the weights initialize our
bias if you remember that each uh layer
has to have a bias going in we're going
to go ahead and work on our our
conversional 2D our Max pool so we have
our pooling layer our convolutional
layer and then our normal full layer so
we're going to go ahead and put those
all into definitions and let's see what
that looks like in code and you can also
grab some of these helper functions from
the MN the uh nist setup let me just put
that in there if you're under the tensor
flow so a lot of these are already in
there but we're going to go ahead and do
our own and we're going to create our uh
a knit weights and one of the reasons
we're doing this is so that you can
actually start thinking about what's
going on in the back end so even though
there's ways to do this with an
automation sometimes these have to be
tweaked and you have to put in your own
setup in here uh now we're not going to
be doing that we're just going to
recreate them for our code and let's
take a look at this we have our weights
and so what comes in is going to be the
shape and what comes out is going to be
uh random numbers so we're going to go
ahead and just knit some random numbers
based on the shape with a standard
deviation of 0.1 kind of a fun way to do
that and then the TF variable uh in nit
random distribution so we're just
creating a random distribution on there
that's all that is for the weights now
you might change that you might have a a
higher standard deviation in some cases
you actually load preset weights that's
pretty rare usually you're testing that
against another model or something like
that and you want to see how those
weights configure with each other uh now
remember we have our bias so we need to
go ahead and initialize the bias with a
constant uh in this case we're using 0
one a lot of times the bias is just put
in as one and then you have your weights
to add on to that uh but we're going to
set this as say 0.1 uh so we want to
return a convolutional 2d in this case a
neural network this is uh would be a
layer on here what's going on with the
con 2D is we're taking our data coming
in uh we're going to filter it strides
if you remember correctly strides came
from here's our image and then we only
look at this picture here and then maybe
we have a stride of one so we look at
this picture here and we continue to
look at the different filters going on
there the other thing this does is that
we have our data coming in as
32 by
32 by three and we want to change this
so that it's just this is three
dimensions and it's going to reformat
this as just two Dimensions so it's
going to take this number here and
combine it with the 32x 32 so this is a
very important layer here because it's
reducing our data down using different
means and it connects down I'm just
going to jump down one here uh it goes
with the convolutional layer so you have
your your kind of your pre- formatting
and the setup and then you have your
actual convolution layer that goes
through on there and you can see here we
have a knit weights by the shape a knit
bias shape of three because we have the
three different uh here's our three
again and then we return the tfnn Rel
you with the convention 2D so this
convolutional uh has this feeding into
it right there it's using that as part
of it and of course the input is the XY
plus b the bias so that's quite a
mouthful but these two are the are the
keys here to creating the convolutional
layers there the convolutional 2D coming
in and then the convolutional layer
which then steps through and creates all
those filters we saw then of course we
have our pooling uh so after each time
we run it through the convectional layer
we want to pull the data uh if you
remember correctly on the on the pool
side and let me just get rid of all my
marks it's getting a little crazy there
and in fact let's go ahead and jump back
to that slide let's just take a look at
that slide over here uh so we have our
image coming in we create our
convolutional layer with all the filters
remember the filters go um you know the
filters coming in here and it looks at
these four boxes and then if it's a step
let's say step two it then goes to these
four boxes and then the next step and so
on uh so we have our convolutional layer
that we generate or convolutional layers
they use the uh reu function um there's
other functions out there for this
though the reu is the uh most the one
that works the best at least so far I'm
sure that will change then we have our
pooling now if you remember correctly
the pooling was XX uh so if we had the
filter coming in and they did the
multiplication on there and we have a
one and maybe a two here and another one
here and a three here three is the max
and so out of all of these you then
create an array that would be three and
if the max is over here two or whatever
it is that's what goes into the pooling
of what's going on in our pooling uh so
again we're reducing that data down
we're reducing it down as small as we
can and then finally we're going to
flatten it out into a single array and
that goes into our fully connected layer
and you can see that here in the code
right here we're going to create our
normal full layer um so at some point
we're going to take from our pooling
layer this will go into some kind of
flattening process and then that will be
fed into the full the different layers
going in down here um and so we have our
input size you'll see our input layer
get shape which is just going to get the
shape for whatever is coming in uh and
then input size initial weights is also
based on uh the input layer coming in
and the input size down here is based on
the input layer shape so we're just
going to already use the shape and
already have our size coming in and of
course uh you have to make sure you init
the bias always put your bias on there
and we'll do that based on the size so
this will return tf. matmo input layer
w+b this is just a normal full layer
that's what this means right down here
that's what we're going to return so
that was a lot of steps we went through
let's go ahead and run that so those are
all loaded in there and let's go ahead
and uh create the layers let's see what
that looks like now that we've done all
the heavy lifting and everything uh we
get to do all the easy part let's go
ahead and create our layers we'll create
a convolution layer one and two two
different convolutional layers and then
we'll take that and we'll flatten that
out create a a reshape pooling in there
for our reshape and then we'll have our
full uh layer at the end so let's start
by creating our first uh convolutional
layer then we come in here and let me
just run that real quick and I want you
to notice on here the three and the 32
this is important because coming into
this convolutional layer we have three
different channels and 32 pixels each uh
so that has to be in there the four and
four you can play with this is your
filter size so if you remember you have
a filter and you have your image and the
filter slowly steps over and filters out
this image depending on what your step
is for this particular setup 4 four is
just fine that should work pretty good
for what we're doing and for the size of
the image and then of course at the end
once you have your convolutional layer
set up you also need to pull it and
you'll see that the pooling is
automatically set up so that it would
see the different shape based on what's
coming in so here we have Max Two by 2x
two and we put in the convolutional one
that we just created the convolutional
layer we just created goes right back
into it and that right up here as you
can see is the X it's coming in from
here so it knows to look at the first
model and set the the data accordingly
set that up so it matches and we went
ahead and ran this already I think I ran
let me go and run it again and if we're
going to do one layer let's go ahead and
do a second layer down here and it's uh
we'll call it convo 2 it's also a
convolutional layer on this and you'll
see that we're feeding convolutional one
in the pooling so it goes from
convolutional one into convolutional one
pooling from convolutional one pooling
into convolutional 2 and then from
convolutional 2 into convolutional 2
pooling and we'll go ahead and take this
and run this so these variables are all
loaded into memory and for our flatten
layer uh let's go ahead and we'll do uh
since we have 64 coming out of here and
we have a 4x4 going in let's do 8X 8 by
64 so let's do
4,096 this is going to be the flat layer
so that's how many bits are coming
through on the flat layer and we'll
reshape this so we'll reshape our convo
2 pooling and that will feed into here
the convo 2 pooling and then we're going
to set it up as a single layer that's
4,096 in size that's what that means
there we'll go ahead and run this so
we've now created this variable the
convo 2 flat and then we have our first
full layer this is the final uh neural
network where we have the flat layer
going in and we're going to again use
the uh reu for our uh setup on there on
a neural network for evaluation and
you'll notice that we're going to create
our first full layer our normal full
layer that's our definition so we
created that that's creating the normal
full layer and our input for the data
comes right here from the this goes
right into it uh the convo to flat so
this tells it how big the data is and
we're going to have it come out it's
going to have uh 1024 that's how big the
layer is coming out we'll go ahead and
run this so now we have our full layer
one and with the full layer one we want
to also Define the full one Dropout to
go with that so our full layer one comes
in uh keep probability equals whole
probability remember we created that
earlier and the full layer one is what's
coming into it and this is going
backwards and training the data we're
not training every weight we're only
training a percentage of them each time
which helps get rid of the bias so let
me go ahead and run that and uh finally
we'll go ahead and create a y predict
which is going to equal the normal full
one Dropout and 10 because we have 10
labels in there now in this neural
network we could have added additional
layers that would be another option to
play with you can also play with instead
of 1024 you can use other numbers for
the way that sets up and what's coming
out going into the next one we're only
going to do just the one layer and the
one layer Dropout and you can see if we
did another layer it'd be really easy
just to feed in the full one Dropout
into full layer two and then full Layer
Two Dropout would have full Layer Two
feed into it and then you'd switch that
here for the Y prediction for right now
this is great this particular data set
is tried and true and we know that this
will work on it and if we just type in y
predict and we run that uh we'll see
that this is a tensor object uh shaped
question mark 10 dtype 32 a quick way to
double check what we're working on so
now we've got all of our uh we've done a
setup all the way to the Y predict which
we just did uh we want to go ahead and
apply the loss function and make sure
that's set up in there uh create the
optimizer and then uh trainer optim
Optimizer and create a variable to
initialize all the global TF variables
so before we dive into the um loss
function let me point out one quick
thing or just kind of a rehap over a
couple things and that is when we're
playing with this these setups um we
pointed out up here we can change the 44
and use different numbers there they
change your outcome so depending on what
numbers you use here will have a huge
impact on how well your model fits and
that's the same here the 1024 also this
is also another number that if you
continue to raise that number you'll get
um possibly a better fit you might
overfit and if you lower that number
you'll use less resources and generally
you want to use this in um the
exponential growth an exponential being
2 4 8 16 and in this case the next one
down would be 512 you can use any number
there but those would be the ideal
numbers uh when you look at this data so
the next step in all this is we need to
also create uh a way of tracking how
good our model is and we're going to
call this a loss function and so we're
going to create a cross entropy loss
function and so before we discuss
exactly what that is let's take a look
and see what we're feeding it uh we're
going to feed it our labels and we have
our true labels and our prediction
labels uh so coming in here is where're
the two different uh variables we're
sending in or the two different
probability distributions is one that we
know is true and what we think it's
going to be now this function right here
when they talk about cross entropy uh in
information Theory the cross entropy
between two probability distributions
over the same underlying set of events
measures the average number of bits
needed to identify an event drawn from
the set that's a mouthful uh really
we're just looking at the amount of
error in here how many of these are
correct and how many of these um are
incorrect so how much of it matches and
we're going to look at that we're just
going to look at the average that's what
the mean the reduced to the mean means
here so we're looking at the average
error on this and so the next step is
we're going to take the error we want to
know our cross entropy or our loss
function how much loss we have that's
going to be part of how we train the
model so when you know what the loss is
and we're training it you feed that back
into the back propagation setup and so
we want to go ahead and optimize that
here's our Optimizer we're going to
create the optimizer using an atom
Optimizer remember there's a lot of
different ways of optimizing the data
atoms the most popular used uh so our
Optimizer is going to equal the TF train
atom Optimizer if you don't remember
what the learning rate is let me just
pop this back into here here's our
learning rate when you have your weights
you have all your weights in your
different nodes that are coming out
here's our node coming out um and it has
all its weights and then the error is
being prop sent back through in reverse
on our neural network so we take this
error and we adjust these weights based
on the different formulas in this case
the atom formula is what we're using we
don't want to just adjust them
completely we don't want to change this
weight so it exactly fits the data
coming through because if we made that
kind of adjustment it's going to be
biased to whatever the last data we sent
through is instead we're going to
multiply that by 0.001 and make a very
small shift in this weight so our Delta
W is only 0.001 of the actual Delta W of
the full change we're going to compute
from the atom and then we want to go
ahead and train it so our training or
set up a training uh uh variable or
function and this is going to equal our
Optimizer minimize cross entropy and we
make sure we go ahead and run this so
it's loaded in there and then we're
almost ready to train our model but
before we do that we need to create one
more um variable in here and we're going
to create a variable to initialize all
the global TF variables and when we look
at this um the TF Global variable
initializer this is a tensor flow um
object it goes through there and it
looks at all our different setup that we
have going under our tensor flow and
then initial Iz es those variables uh so
it's kind of like a magic one because
it's all hidden in the back end of
tensor flow all you need to know about
this is that you have to have the
initialization on there which is an
operation um and you have to run that
once you have your setup going so we'll
go ahead and run this piece of code and
then we're going to go ahead and train
our data so let me run this so it's
loaded up there and so now we're going
to go ahead and run the model by
creating a graph session graph session
is a tensorflow term so you'll see that
coming up it's one of the things that
throws me because I always think of
graphic and Spark and graph as just
general graphing uh but they talk about
a graph session so we're going to go
ahead and run the model and let's go
ahead and walk through this uh what's
going on here and let's paste this data
in here and here we go so we're going to
start off with the with the TF session
as sess so that's our actual TF session
we've created uh so we're right here
with the TF uh session our session we're
creating we're going to run TF Global
variable initializer so right off the
bat we're initializing our variables
here uh and then we have for I and range
500 so what's going on here remember 500
we're going to break the data up and
we're going to batch it in at 500 points
each we've created our session run so
we're going to do with TF session as
session right here we've created our
variable session uh and then we're going
to run and we're going to go ahead and
initialize it so we have our TF Global
variables initializer that we cre Creed
um that initializes our our session in
here the next thing we're going to do is
we're going to go for I in range of 500
batch equals ch. nextt batch so if you
remember correctly this is loading up um
100 pictures at a time and uh this is
going to Loop through that 500 times so
we are literally doing uh what is that
uh 500 time 100 is uh 50,000 so that's
50,000 pictures we're going to process
right there in the first process is
we're going to do a session run we're
going to take our train we created our
train variable or Optimizer in there
we're going to feed it the dictionary uh
we had our feed dictionary that created
and we have x equals batch 0o coming in
y true batch one hold the probability
05 and then just so that we can keep
track of what's going on we're going to
every uh 100 steps we're going to run a
print So currently on step format C
accuracy is um and we're going to look
at match equals tf. equal TF argument y
prediction 1 tf. AR Max y true comma 1
so we're going to look at this is how
many matches it has and here our ACC uh
all we're doing here is we're going to
take the matches how many matches they
have it creates it generates a chart
we're going to convert that to float
that's what the TF cast does and then we
just want to know the average we just
want to know the average of the um
accuracy and then we'll go ahead and
print that out uh print session run
accuracy feed dictionary so it takes all
this and it prints out our accuracy on
there so let's go ahead and take this
oops screens there let's go ahead and
take this and let's run it and this is
going to take a little bit to run uh so
let's see what happens on my old laptop
and we'll see here that we have our
current uh we're currently on Step Zero
it takes a little bit to get through the
accuracy and this will take just a
moment to run we can see that on our
Step Zero it has an accuracy of 0.1 or
0128 um and as it's running we'll go
ahead you don't need to watch it run all
the way but uh this accuracy is going to
change a little bit up and down so we've
actually lost some accuracy during our
step
two um but we'll see how that comes out
let's come back after we run it all the
way through and see how the different
steps come out I was actually reading
that backwards uh the way this works is
the closer we get to one the more
accuracy we have uh so you can see here
we've gone from a 0.1 to a 39 um and
we'll go ahead and pause this and come
back and see what happens when we're
done with the uh full run all right now
that we've uh prepared the meal got it
in the oven and pulled out my finished
dish here if you've ever watched uh any
of the old cooking shows let's discuss a
little bit about this accuracy going on
here and how do you interpret that we've
done a couple things first we've defined
accuracy um the reason I got it
backwards before is you have uh loss or
accuracy and with loss you'll get a
graph that looks like this it goes oops
that's an ass by the way there we go you
get a graph that curves down like this
and with accuracy you get a graph that
curves up this is how good it's doing
now in this case uh one is supposed to
be really good accuracy that mean it
gets close to one but it never crosses
one so if you have an accuracy of one
that is phenomenal um in fact that's
pretty much un you know unheard of and
the same thing with loss if you have a
loss of zero that's also unheard of the
zero is actually on this this axis right
here as we go in there so how do we
interpret that because you know if I was
looking at this and I go oh 05 51 that's
uh 51% you're doing 50/50 no this is not
percentage let me just put that in there
it is not percentage uh this is
logarithmic what that means is that 0. 2
is twice as good as 0.1 and uh when we
see 0.4 that's twice as good as 0. 2
real way to convert this into a
percentage you really can't say this is
is a direct percentage conversion what
you can do though is in your head if we
were to give this a percentage uh we
might look at this as uh 50% or just
guessing equals 0.1 and if 50% roughly
equals 0.1 that's where we started up
here at the top remember at the top here
here's our 1028 the accuracy at 50% then
75% is about 02 and so on and so on
don't quote those numbers because it
doesn't work that way they say that if
you have
95
that's pretty much saying 100% And if
you have uh anywhere between you'd have
to go look this up let me go and remove
all my drawings there uh so the magic
number is 0. five we really want to be
over a 0. five in this whole thing and
we have uh both 0504 remember this is
accuracy if we were looking at loss then
we'd be looking the other way but 0.0
you know instead of how high it is we
want how low it is uh but with accuracy
being over a 0.5 is pretty valid that
means is pretty solid and if you get to
a 0.95 then it's a direct correlation
that's what we're looking for here in
these numbers you can see we finished
with this model at 0
5135 so still good um and if we look at
uh when they ran this in the other end
remember there's a lot of Randomness
that goes into it when we see the
weights uh they got 0. 5251 so a little
better than ours but that's fine you'll
find your own uh comes up a little bit
better or worse depending on uh just
that Randomness and so we've gone
through the whole model we've created we
train the model and we've also gone
through on every uh 100th run to test
the model to see how accurate it
is what's in it for you we will start
with of course of fundamentals what is a
neural network and popular neural
networks it's important to know the
framework we're in and what we're going
to be looking at specifically then we'll
touch on why a recurrent neural network
what is a recurrent neural network and
how does an RNN work uh one of the big
things about rnns is what they call the
vanishing and exploding gradient problem
so we'll look at that and then we're
going to be using a use case uh study is
going to be in carass on tensor flow
carass is a python module for doing
neural networks in deep learning and uh
in there there's the what they call long
shortterm memory lstm and then we'll use
the use case to implement our lstm on
the carass so when you see that lstm
that is basically the RNN Network and we
we'll get into that the use case is
always my favorite part before we dive
into any of this we're going to take a
look at what is an RNN or an
introduction to the RNN do you know how
Google's autocomplete feature predicts
the rest of the words a user is typing I
love that autocomplete feature as I'm
typing away saves me a lot of time I can
just kind of Hit the enter key and it
autofills everything and I don't have to
type as much well first there's a
collection of large volumes of most
frequently occurring consecutive words
uh this is fed into a recurrent neural
network analyses the data by finding the
sequence of words occurring frequently
and builds a model to predict the next
word in the sentence and then Google
what is the best food to eat in L I'm
guessing you're going to say l Mexico no
it's going to be Las Vegas uh so the
Google search will take a look at that
and say hey the most common auto
complete is going to be Vegas in there
and usually gives you three or four
different choices so it's a very
powerful tool it saves us a lot of time
especially when we're doing a Google
search sech or even in Microsoft Words
has a some people get very mad at it it
autofills with the wrong stuff uh but
you know you're typing away and it helps
you autofill I have that in a lot of my
different packages as just a standard
feature that we are all used to now so
before we dive into the RNN and getting
Into the Depths let's go ahead and talk
about what is a neural network neural
networks used in deep learning consist
of different layers connected to each
other and work on the structure and
functions of a human brain you're going
to see that thread human in human brain
and human thinking throughout deep
learning the only way we can evaluate an
artificial intelligence or anything like
that is to compare it to human function
very important note on there and it
learns from a huge volumes of data and
it uses complex algorithm to train a
neural net so in here we have image
pixels of two different breeds of dog uh
one looks like a nice floppy eared lab
and one a German Shepherd you know both
wonderful breeds of animals that image
then goes into an in inut layer uh that
input layer might be formatted at some
point because you have to let it know
like you know different pictures are
going to be different sizes and
different color content then it'll feed
into hidden layers so each of those
pixels or each point of data goes in and
then um splits into the hidden layer
which then goes into another hidden
layer which then goes to an output layer
RNN there's some changes in there which
we're going to get into so it's not just
a straightforward propagation of data
like we've covered in many other
tutorials and finally you have an output
layer and the output layer has two
outputs it has one that lights up if
it's a German shepherd and another that
lights up if it's a labador so
identifies the dog's breed certain
networks do not require memorizing the
past output so our forward propagation
is just that it goes forward it doesn't
have to rememorize stuff and you can see
there that's not actually me in the
picture uh dressed up in my
suit I haven't worn a suit in years so
as we're looking at this we're going to
change it up a little bit before we
cover that let's talk about popular
neural network n works first there's the
feed forward neural network used in
general regression and classification
problems and we have the convolution
neural network used for image
recognition deep neural network used for
acoustic modeling deep belief Network
used for cancer detection and recurrent
neural network used for speech
recognition now taken a lot of these and
mixed them around a little bit so just
because it's used for one thing doesn't
mean it can't be used for other modeling
but generally this is where the field is
and this is how those models are
generally being used right now so we
talk about a feed forward neural network
in a feed forward neural network
information flows only in the forward
direction from the input nodes through
the hidden layers if any and to the
output nodes there are no Cycles or
Loops in the network and so you can see
here we have our input layer I was
talking about how it just goes straight
forward into the hidden layers so each
one of those connects and then connects
to the next hidden layer connects to the
output layer and of course we have a
nice simplified version where it has a
predicted output and they refer to the
input as X a lot of times and the output
as y decisions are based on current
input no memory about the past no future
scope why recurrent neural network
issues in feed forward neural network so
one of the biggest issues is because it
doesn't have a scope of memory or time a
feed forward neural network doesn't know
how to handle sequential data uh it only
considers only the current input so if
you have a series of things and because
three points back affects what's
happening now and what your output
affects what's happening that's very
important so whatever I put as an output
is going to affect the next one um a
feed forward doesn't look at any of that
it just looks at this is what's coming
in and it cannot memorize previous
inputs so it doesn't have that list of
inputs coming in solution to feed
forward neural network you'll see here
where it says recurrent neural network
and we have our X on the bottom going to
H going to Y that's your feed forward uh
but right in the middle it has a value C
so it's a whole another process so it's
memorizing what's going on in the hidden
layers and the hidden layers as they
produce data feed into the next one so
your hidden layer might have an output
that goes off to Y uh but that output
goes back into the next prediction
coming in what this does is this allows
it to handle sequential data it
considers the current input and also the
previously received inputs and if we're
going to look at General drawings and um
Solutions we should also look at
applications of the RNN image captioning
RNN is used to caption an image by
analyzing the activities present in it a
dog catching a ball in midair uh that's
very tough I mean you know we have a lot
of stuff that analyzes images of a dog
and the image of a ball but it's able to
add one more feature in there that's
actually catching the ball in midair
time series prediction any time series
problem like predicting the prices of
stocks in a particular month can be
solved using RNN and we'll dive into
that in our use case and actually take a
look at some stock one of the things you
should know about analyzing stock today
is that it is very difficult and if
you're analyzing the whole stock the
stock market at the New York Stock
Exchange in the US produces somewhere in
the neighborhood if you count all the
individual trades and fluctuations by
the second um it's like three terabytes
a day of data so we're only look at one
stock just analizing one stock is really
tricky in here we'll give you a little
jump on that so that's exciting but
don't expect to get rich off of it
immediately another application of the
RNN is natural language processing text
Mining and sentiment analysis can be
carried out using RNN for natural
language processing and you can see
right here the term natural language
processing when you stream those three
words together is very different than I
if I said processing language natural
leave so the time series is very
important when we're analyzing
sentiments it can change the whole value
of a sentence just by switching the
words around or if you're just counting
the words you might get one sentiment
where if you actually look at the order
they're in you get a completely
different sentiment when it rains look
for rainbows when it's dark look for
stars both of these these are positive
sentiments and they're based upon the
order of which the sentence is going in
machine translation given an input in
one language RNN can be used to
translate the input into a different
languages as output I myself very
linguistically challenged but if you
study languages and you're good with
languages you know right away that if
you're speaking English you would say
big cat and if you're speaking Spanish
you would say cat big so that
translation is really important to get
the right order to get uh all kinds of
parts of speech that are important to
know by the order of the words here this
person is speaking in English and
getting translated and you can see here
a person is speaking in English in this
little diagram I guess that's denoted by
the flags I have a flag I own it no um
but they're speaking in English and it's
getting translated into Chinese Italian
French German and Spanish languages some
of the tools coming out are just so cool
so somebody like myself who's very
linguistically challenged I I can now
travel into Worlds I would never think
of because I can have something
translate my English back and forth
readily and I'm not stuck with a
communication gap so let's dive into
what is a recurrent neural network
recurrent neural network works on the
principle of saving the output of a
layer and feeding this back to the input
in order to predict the output of the
layer sounds a little confusing when we
start breaking it down it'll make more
sense and usually we have a propagation
forward neural network with the input
layers the hidden layers the output
layer with the recur current neural
network we turn that on its side so here
it is and now our X comes up from the
bottom into the hidden layers into Y and
they usually draw very simplified X to H
with c as a loop a to Y where a B and C
are the perimeters a lot of times you'll
see this kind of drawing in here digging
closer and closer into the H and how it
works going from left to right you'll
see that the C goes in and then the X
goes in so the x is going Upward Bound
and C is going to the right a is is
going out and C is also going out that's
where gets a little confusing so here we
have xn uh CN and then we have y out and
C out and C is based on HT minus one so
our value is based on the Y and the H
value or connected to each other they're
not necessarily the same value because H
can be its own thing and usually we draw
this or we represent it as a function h
of T equals a function of C where H of T
minus one that's the last H output and x
a t going in so it's the last output of
H combined with the new input of x uh
where HT is the new state FC is a
function with the parameter C that's a
common way of denoting it uh HT minus
one is the Old State coming out and then
X of T is an input Vector at time of
Step T well we need to cover types of
recurrent neural networks and so the
first one is the most common one which
is a one: one single output one: one
neural network is usually known as a
vanilla neural network used for regular
machine learning problems why because
vanilla is usually considered kind of a
just a real basic flavor but because
it's a very basic a lot of times they'll
call it the vanilla neural network uh
which is not the common term but it is
you know like kind of a slang term
people will know what you're talking
about usually if you say that then we
run one to Min so you have a single
input and you might have a multiple
outputs in this case uh image captioning
as we looked at earlier where we have
not just looking at it as a dog but a
dog catching a ball in the air and then
you have many to1 Network takes in a
sequence of inputs examples sentiment
analysis where a given sentence can be
classified as expressing positive or
negative sentiments and we looked at
that as we were discussing if it rains
look for a rainbow so positive sentiment
where rain might be a negative sentiment
if you were just adding up the words in
there and then of course if you're going
to do a one to one many to one one to
many there's many to many networks takes
in a sequence of inputs and generates a
sequence of outputs example machine
translation so we have a lengthy
sentence coming in in English and then
going out in all the different languages
uh you know just a wonderful tool very
complicated set of computations you know
if you're a translator you realize just
how difficult it is to translate into
different languages one of the biggest
things you need to understand when we're
working with this neural network is
what's called The Vanishing gradient
problem while training an RNN your slope
can be either too small or very large
and this makes training diff ult when
the slope is too small the problem is
known as Vanishing gradient and you'll
see here they have a nice U image loss
of information through time so if you're
pushing not enough information forward
that information is lost and then when
you go to train it you start losing the
third word in the sentence or something
like that or it doesn't quite follow the
full logic of what you're working on
exploding gradient problem Oh this is
one that runs into everybody when you're
working with this particular neural
network when the slope tends to grow
exponentially instead of decaying this
problem is called exploding gradient
issues in gradient problem long tring
time poor performance bad accuracy and
I'll add one more in there uh your
computer if you're on a lower-end
computer testing out a model will lock
up and give you the memory error
explaining gradient problem consider the
following two examples to understand
what should be the next word in the
sequence the person who took my bike and
blank a thief the students who got into
engineering ING with blank from Asia and
you can see in here we have our x value
going in we have the previous value
going forward and then you back
propagate the error like you do with any
neural network and as we're looking for
that missing word maybe we'll have the
person took my bike and blank was a
thief and the student who got into
engineering with a blank were from Asia
consider the following example the
person who took the bike so we'll go
back to the person who took the bike was
blank a thief in order to understand
what would be the next word in the
sequence the RNN must memorize the
previous context whether the subject was
singular noun or a plural noun so was a
thief is singular the student who got
into engineering well in order to
understand what would be the next word
in the sequence the RNN must memorize
the previous context whether the subject
was singular noun or a plural noun and
so you can see here the students who got
into engineering with blank were from
Asia it might be sometimes difficult for
the eror to back propagate to the
beginning of the sequence to ICT what
should be the output so when you run
into the gradient problem we need a
solution the solution to the gradient
problem first we're going to look at
exploding gradient where we have three
different solutions depending on what's
going on one is identity initialization
so the first thing we want to do is see
if we can find a way to minimize the
identities coming in instead of having
it identify everything just the
important information we're looking at
next is to truncate the back propagation
so instead of having uh whatever
information it's sending to the next
series we can truncate what it's sending
we can lower that particular uh set of
layers make those smaller and finally is
a gradient clipping so when we're
training it we can clip what that
gradient looks like and narrow the
training model that we're using when you
have a Vanishing gradient the OPA
problem uh we can take a look at weight
initialization very similar to the
identity but we're going to add more
weights in there so it can identify
different aspects of what's coming in
better choose using the right activation
function that's huge so we might be
activating based on one thing and we
need to limit that we haven't talked too
much about activation functions so we'll
look at that just minimally uh there's a
lot of choices out there and then
finally there's long shortterm memory
networks the
lstms and we can make adjustments to
that so just like we can clip the
gradient as it comes out we can also um
expand on that we can increase the
memory Network the size of it so it
handles more information and one of the
most common problems in today's uh setup
is what they call longterm dependencies
suppose we try to predict the last word
in the text the clouds are in the and
you probably said sky here we do not
need any further context it's pretty
clear that the last word is going to be
Sky suppose we try to predict the last
word in the text I have been staying in
Spain for the last 10 years I can speak
fluent maybe you said Portuguese or
French no you probably said Spanish the
word we predict will depend on the
previous few words in context here we
need the context of Spain to predict the
last word in the text it's possible that
the gap between the relevant information
and the point where it is needed to
become very large
lstms help us solve this problem so the
lstm SS are a special kind of recurrent
neural network capable of learning
long-term dependencies remembering
information for long periods of time is
their default Behavior All recurrent
neural networks have the form of a chain
of repeating modules of neural network
connections in standard rnns this
repeating module will have a very simple
structure such as a single tangent H
layer lstm s's also have a chain-like
structure but the repeating module has a
different structure instead of having a
single neural network layer there are
four interacting layers communicating in
a very special way lstms are a special
kind of recurrent neural network capable
of learning long-term dependencies
remembering information for long periods
of time is their default Behavior LST
tms's also have a chain-like structure
but the repeating module has a different
structure instead of having a single
neural network layer there are four
interacting layers communicating in a
very special way as you can see the
deeper we dig into this the more
complicated the graphs kit in here I
want you to note that you have X of T
minus one coming in you have x a t
coming in and you have X of t + one and
you have H of T minus one and H of T
coming in and H of t plus one going out
and of course on the other side is the
output a um in the middle we have our
tangent H but it occurs in two different
places so not only when we're Computing
the x of t + one are we getting the
tangent H from X of T but we're also
getting that value coming in from the X
of T minus one so the short of it is as
you look at these layers not only does
it does the propagate through the first
layer goes into the second layer back
into itself but it's also going into the
third layer so now we're kind of
stacking those up and this can get very
complicated as you grow that in size it
also grows in memory too and in the
amount of resources it takes uh but it's
a very powerful tool to help us address
the problem of complicated long
sequential information coming in like we
were just looking at in the sentence and
when we're looking at our long shortterm
memory network uh there's three steps of
processing assing in the lstms that we
look at the first one is we want to
forget irrelevant parts of the previous
state you know a lot of times like you
know is as in a unless we're trying to
look at whether it's a plural noun or
not they don't really play a huge part
in the language so we want to get rid of
them then selectively update cell State
values so we only want to update the
cell State values that reflect what
we're working on and finally we want to
put only output certain parts of the
cell state so whatever is coming out we
want to limit what's going out too and
let's dig a little deeper into this
let's just see what this really looks
like uh so step one decides how much of
the past it should remember first step
in the lstm is to decide which
information to be omitted in from the
cell in that particular time step it is
decided by the sigmoid function it looks
at the previous state h of T minus one
and the current input xit T and computes
the function so you can see over here we
have a function of T equals the sigmoid
function of the weight of f the H at T
minus one and then X at T Plus of course
you have a bias in there with any of
your neural network so we have a bias
function so F of equals forget gate
decides which information to delete that
is not important from the previous time
step considering an L STM is fed with
the following inputs from the previous
and present time step Alice is good in
physics JN on the other hand is good in
chemistry so previous output JN plays
football well he told me yesterday over
the phone that he had served as a
captain of his college football team
that's our current input so as we look
at this the first step is the forget
gate realizes there might be a change in
context after en counting the First full
stop Compares with the current input
sentence of xit so we're looking at that
full stop and then Compares it with the
input of the new sentence the next
sentence talks about John so the
information on Alice is deleted okay
that's important to know so we have this
input coming in and if we're going to
continue on with John then that's going
to be the primary information we're
looking at the position of the subject
is vacated in is assigned to JN and so
in this one we've seen that we've weeded
out a whole bunch of information and
we're only passing information on JN
since that's now the new topic so step
two is in to decide how much should this
unit add to the current state in the
second layer there are two parts one is
a sigmoid function and the other is a
tangent H in the sigmoid function it
decides which values to let through zero
or one tangent H function gives the
weightage to the values which are passed
deciding their level of importance
minus1 to 1 and you can see the two
formulas that come up uh the I of T
equals the sigmoid of the weight of I a
to t minus1 x of t plus the bias of I
and the C of T equals the tangent of H
of the weight of C of H of T minus one X
of t plus the bias of C so our I of T
equals the input gate determines which
information to let through based on its
significance in the current time step if
this seems a little complicated don't
worry because a lot of the programming
is already done when we get to the case
study
understanding though that this is part
of the program is important when you're
trying to figure out these what to set
your settings at you should also note
when you're looking at this it should
have some semblance to your forward
propagation neural networks where we
have a value assigned to a weight plus a
bias very important steps than any of
the neural network layers whether we're
propagating into them the information
from one to the next or we're just doing
a straightforward neural network
propagation let's take a quick look at
this what it looks like from the human
standpoint um as I step out in my suit
again consider the current input at xft
John plays football well he told me
yesterday over the phone that he had
served as a captain of his college
football team that's our input input
gate analyses the important information
John plays football and he was a captain
of his college team is important he told
me over the phone yesterday is less
important hence it is forgotten this
process of adding some new information
can be done via the input gate now this
example is is as a human form and we'll
look at training this stuff in just a
minute uh but as a human being if I
wanted to get this information from a
conversation maybe it's a Google Voice
listening in on you or something like
that um how do we weed out the
information that he was talking to me on
the phone yesterday well I don't want to
memorize that he talked to me on the
phone yesterday or maybe that is
important but in this case it's not I
want to know that he was the captain of
the football team I want to know that he
served I want to know that John plays
football and he was a captain of the
college football team those are the two
things that I want to take away as a
human being again we measure a lot of
this from the human Viewpoint and that's
also how we try to train them so we can
understand these neural networks finally
we get to step three decides what part
of the current cell State makes it to
the output the third step is to decide
what will be our output first we run a
sigmoid layer which decides what parts
of the cell State make it to the output
then we put the cell State through the
tangent H to push the values to be
between minus1 and one and multiply it
by the output of the sigmoid gate so
when we talk about the output of T we
set that equal to the sigmoid of the
weight of zero of the H of T minus one
you back One Step in Time by the x of t
plus of course the bias the H of T
equals the out of ttimes the tangent of
the tangent h of c a t so our o t equals
the output gate allows the past in
information to impact the output in the
current time step let's consider the
example to predicting the next word in
the sentence John played tremendously
well against the opponent and won for
his team for his contributions Brave
blank was awarded player of the match
there could be a lot of choices for the
empty space current input Brave is an
adjective adjectives describe a noun JN
could be the best output after Brave
thumbs up for JN awarded player of the
match and if you were to pull just the
nouns out of the sentence team doesn't
look right because that's not really the
subject we're talking about
contributions you know Brave
contributions or Brave team Brave player
Brave match um so you look at this and
you can start to train this the this
neural network so it starts looking at
and goes oh no JN is what we're talking
about so brave is an adjective Jon's
going to be the best output and we give
John a big thumbs up and then of course
we jump into my favorite part the case
study use case implementation of lstm
let's predict the prices of stocks using
the lstm network based on the stock
price data between 2012 2016 we're going
to try to predict the stock prices of
2017 and this will be a narrow set of
data we're not going to do the whole
stock market it turns out that the New
York Stock Exchange generates roughly
three terabytes of data per day that's
all the different trades up and down of
all the different stocks going on and
each individual one uh second to second
or Nano second to nanc uh but we're
going to limit that to just some very
basic fundamental information so don't
think you're going to get rich off this
today but at least you can give an a you
can give a step forward in how to start
processing something like stock prices a
very valid use for machine learning in
today's markets use case implementation
of
lstm let's dive in we're going to import
our libraries we're going to import the
training set and uh get the scaling
going uh now if you watch any of our
other tutorials a lot of these pieces
just start to look very familiar cuz
it's very similar setup uh but let's
take a look at that and um just a
reminder we're going to be using
Anaconda the Jupiter notebook so here I
have my anaconda Navigator when we go
under environments I've actually set up
a caros python 36 I'm in Python 36 and
uh nice thing about Anaconda especially
the newer version remember a year ago
messing with anaconda in different
versions of python and different
environments um Anaconda now has a nice
interface um and I have this installed
both on a Ubuntu Linux machine and on on
window so it works fine on there and you
can go in here and open a terminal
window and then in here once you're in
the terminal window this is where you're
going to start uh installing using pip
to install your different modules and
everything now we've already
pre-installed them so we don't need to
do that in here uh but if you don't have
them installed on your particular
environment you'll need to do that and
of course you don't need to use the
anaconda or the Jupiter you can use
whatever favorite python ID you like I'm
just a big fan of this because it keeps
all my stuff separate you can see on
this machine I have spefic specifically
installed one for carass since we're
going to be working with carass under
tensor flow when we go back to home I've
gone up here to application and that's
the environment I've loaded on here and
then we'll click on the launch Jupiter
notebook now I've already in my Jupiter
notebook um have set up a lot of stuff
so that we're ready to go kind of like
uh Martha Stewarts in the old cooking
shows we want to make sure we have all
our tools for you so you're not waiting
for them to load and uh if we go up here
to where it says new you can see where
you can um create create a new Python 3
that's what we did here underneath the
setup so it already has all the modules
installed on it and I'm actually renamed
this so if you go under file you can
rename it we I'm calling it RNN stock
and let's just take a look at start
diving into the code let's get into the
exciting part now we've looked at the
tool and of course you might be using a
different tool which is fine uh let's
start putting that code in there and
seeing what those Imports and uploading
everything looks like now first half is
kind of boring when we hit the rum
button because we're going to be
importing numpy as NP that's uh uh the
number python which is your numpy array
and the mat plot Library CU we're going
to do some plotting at the end and our
pandas for our data set our pandas is PD
and when I hit run uh it really doesn't
do anything except for load those
modules just a quick note let me just do
a quick uh draw here oops shift alt
there we go you'll notice when we're
doing this setup if I was to divide this
up oops I'm going to actually um let's
overlap these here we
go
uh this first part that we're going to
do
is our
data prep a lot of prepping
involved um in fact depending on what
your system is since we're using carass
I put an overlap here uh but you'll find
that almost maybe even half of the code
we do is all about the data prep and the
reason I overlap this with uh carass me
just put that down because that's what
we're working in uh is because carass
has like their own preset stuff so it's
already pre-built in which is really
nice so there's a couple Steps A lot of
times that are in the carass setup uh
we'll take a look at that to see what
comes up in our code as we go through
and look at stock and then the last part
is to evaluate and if you're working
with um shareholders or uh you know
classroom whatever it is you're working
with uh the evaluate is the next biggest
piece um so the actual code here crossed
is a little bit more but when you're
working with uh some of the other
packages you might have like three lines
that might be it all your stuff is in
your pre-processing in your data since
carass has is is Cutting Edge and you
load the individual layers you'll see
that there's a few more lines here and
cross is a little bit more robust and
then you spend a lot of times uh like I
said with the evaluate you want to have
something you present to everybody else
to say hey this is what I did this is
what it looks like so let's go through
those steps this is like a kind of just
general overview and let's just take a
look and see what the next set of code
looks like and in here we have a data
set train and it's going to be read
using the PD or pandas read CSV and it's
a Google stock price train.csv and so
under this we have training set equals
data set train. iocation and we've kind
of sorted out part of that so what's
going on here let's just take a look at
let's let's look at the actual file and
see what's going on there now if we look
at this uh ignore all the extra files on
this um I already have a train and a
test set where it's sorted out this is
important to notice because a lot of
times we do that as part of the
pre-processing of the data we take 20%
of the data out so we can test it and
then we train the rest of it that's what
we use to create our neural network that
way we can find out how good it is uh
but let's go ahead and just take a look
and see what that looks like as far as
the file itself and I went ahead and
just opened this up in a basic word pad
text editor just so we can take a look
at it certainly you can open up an Excel
or any other kind of spreadsheet um and
we note that this is a comma separated
variables we have a date uh open high
low close volume this is the standard
stuff that we import into our stock one
the most basic set of information you
can look at in stock it's all free to
download um in this case we downloaded
it from uh Google that's why we call it
the Google stock price um and it
specifically is Google this is the
Google stock values from uh as you can
see here we started off at 13
20102 so when we look at this first
setup up here uh we have a data set
train equals pdor CSV and if you noticed
on the original frame um let me just go
back there they had it set to home
Ubuntu downloads Google stock price
train I went ahead and change that
because we're in the same file where I'm
running the code so I've saved this
particular python code and I don't need
to go through any special paths or have
the full path on there and then of
course we want to take out um certain
values in here and you're going to
notice that we're using um our data set
and we're now in pandas uh so pandas
basically it looks like a spreadsheet um
and in this case we're going to do I
location which is going to get specific
locations the first value is going to
show us that we're pulling all the rows
in the data and the second one is we're
only going to look at columns one and
two and if you remember here from our
data as we switch back on over columns
we always start with zero which is the
the datee and we're going to be looking
at open and high which would be one and
two we'll just label that right there so
you can see now when you go back and do
this you certainly can extrapolate and
do this on all the columns um but for
the example let's just limit a little
bit here so that we can focus on just
some key aspects of
stock and then we'll go up here and run
the code and uh again I said the first
half is very boring whenever you hit the
Run button doesn't do anything cuz we're
still just loading the data and setting
it up now that we've loaded our data we
want to go ahead and scale it we want to
do what they call feature scaling and in
here we're going to pull it up from the
sklearn or the SK kit pre-processing
import min max scaler and when you look
at this you got to remember that um
biases in our data we want to get rid of
that so if you have something that's
like a really high value U let's just
draw a quick graph and I have something
here like the maybe the stock has a
value One stock has a value of 100 and
another stock has a value of
five um you start to get a bias between
different stocks and so when we do this
we go ahead and say okay 100's going to
be the Max and five is going to be the
Min and then everything else goes and
then we change this so we just squish it
down I like the word squish so it's
between one and zero so 100 equals 1 or
1 = 100 and 0 = 5 and you can just
multiply it's usually just a simple
multiplication we're using uh
multiplication so it's going to be uh
minus5 and then 100 divided or 95
divided by 1 so or whatever value is is
divided by 95 and uh once we've actually
created our scale we've toing it's going
to be from 0 to one we want to take our
training set and we're going to create a
training set scaled and we're going to
use our scaler SC and we're going to fit
we're going to fit and transform the
training Set uh so we can now use the SC
this this particular object we'll use it
later on our testing set because
remember we have to also scale that when
we go to test our uh model and see how
it works and we'll go ahead and click on
the run again uh it's not going to have
any output yet because we're just
setting up all the
variables okay so we pasted the data in
here and we're going to create the data
structure with the 60 time steps and
output first note we're running 60 time
steps
and that is where this value here also
comes in so the first thing we do is we
create our X train and Y train variables
and we set them to an empty python array
very important to remember what kind of
array we're in and what we're working
with and then we're going to come in
here we're going to go for I in range 60
to 1258 there's our 60 60 time steps and
the reason we want to do this is as
we're adding the data in there there's
nothing below the 60 so if we're going
to use 60 time steps uh we have have to
start at 60 because it includes
everything underneath of it otherwise
you'll get a pointer error and then
we're going to take our X train and
we're going to append training set
scaled this is a scaled value between
zero and one and then as I is equal to
60 this value is going to be um 60 - 60
is 0 so this actually is 0 to I so it's
going to be 0 60 1 to 61 let me just
circle this part right here 1 to 61 uh 2
to 62 and so on and so on and if you
remember I said 0 to 60 that's incorrect
because it does not count remember
starts at zero so this is a count of 60
so it's actually 59 important to
remember that as we're looking at this
and then the second part of this that
we're looking at so if you remember
correctly here we go we go from uh 0 to
59 of I and then we have a comma a zero
right here and so finally we're just
going to look at the open value now I
know we did put it in there for one 1 to
two um if you remember correctly it
doesn't count the second one so it's
just the open value we're looking at
just open um and then finally we have y
train. append training set I to zero and
if you remember correctly I to or I
comma Zer if you remember correctly this
is 0 to 59 so there's 60 values in it uh
so when we do I down here this is number
60 so we're going to do this is we're
creating an array and we have 0 to 59
and over here we have number 60 which is
going into the Y train it's being
appended on there and then this just
goes all the way up so this is down here
is a 0 to 59 and we'll call it 60 since
that's the value over here and it goes
all the way up to 12 58 that's where
this value here comes in that's the
length of the data we're loading so
we've loaded two arrays we've loaded one
array that has uh which is filled with
arrays from 0 to 59 and we loaded one
array which is just the value and what
we're looking at you want to think about
this as a Time sequence uh here's my
open open open open open open what's the
next one in the series so we're looking
at the Google stock and each time it
opens we want to know what the next one
uh 0 through 59 what's 60 1 through 60
what's 61 2 through 62 what's 62 and so
on and so on going up and then once
we've loaded those in our for Loop we go
ahead and take XT train and YT train
equals np. array XT tr. NP array YT
train we're just converting this back
into a numpy array that way we can use
all the cool tools that we get with
numpy array including reshaping so if we
take a look and see what's going on here
we're going to take our X
train we're going to reshape it wow what
the heck does reshape mean uh that means
we have an array if you remember
correctly um so many numbers by
60 that's how wide it is
and so we're when you when you do xtrain
do shape that gets one of the shapes and
you get um XT train. shape of one gets
the other shape and we're just making
sure the data is formatted correctly and
so you use this to pull the fact that
it's 60 by um in this case where's that
value 60 by
1199 1258 minus
60199 and we're making sure that that is
shaped correctly so the data is grouped
into uh 11 99 by 60 different arrays and
then the one on the end just means at
the end because this when you're dealing
with shapes and numpy they look at this
as layers and so the in layer needs to
be one value that's like the leaf of a
tree where this is the branch and then
it branches out some more um and then
you get the Leaf np. reshape comes from
and using the existing shapes to form it
we'll go ahead and run this piece of
code again there's no real output and
then we'll import our different carass
modules that we need so from carass
Models we're going to import the
sequential model dealing with sequential
data we have our dense layers we have
actually three layers we're going to
bring in our dense our lstm which is
what we're focusing on and our Dropout
and we'll discuss these three layers
more in just a moment but you do need
the with the lstm you do need the
Dropout and then the final layer will be
the dents but let's go ahead and run
this and that'll bring Port our modules
and you'll see we get an error on here
and if you read it closer it's not
actually an error it's a warning what
does this warning mean these things come
up all the time when you're working with
such Cutting Edge modules or completely
being updated all the time we're not
going to worry too much about the
warning all it's saying is that the
h5py module which is part of carass is
going to be updated at some point and uh
if you're running new stuff on carass
and you start updating your carass
system you better make sure that your H5
Pi is updated too otherwise you're going
to have an error later on and you can
can actually just run an update on the
H5 Pi now if you wanted to not a big
deal we're not going to worry about that
today and I said we were going to jump
in and start looking at what those
layers mean I meant that and uh we're
going to start off with initializing the
RNN and then we'll start adding those
layers in and you'll see that we have
the lstm and then the Dropout lstm then
Dropout lstm then Dropout what the heck
is that doing so let's explore that
we'll start by initializing the RNN
regressor equals sequential because
we're using the sequential model and
we'll run that and load that up and then
we're going to start adding our lstm
layer and some Dropout
regularization and right there should be
the Q Dropout regularization and if we
go back here and remember our exploding
gradient well that's what we're talking
about the uh Dropout drops out
unnecessary data so we're not just
shifting huge amounts of data through um
the network so and so we go in here
let's just go ahead and uh add this in
I'll go ahead and run this and we had
three of them so let me go and put all
three of them in and then we can go back
over them there's the second one and
let's put one more in let let's put that
in and we'll go and put h two more in I
meant to I said one more in but it's
actually two more in and then let's add
one more after that and as you can see
each time I run these they don't
actually have an output so let's take a
closer look and see what's going on here
so we're going to add our first lstm
layer in here we're going to have units
50 the units is the positive integer and
it's the dimensionality of the output
space this is what's going out into the
next layer so we might have 60 coming in
but we have 50 going out we have a
return sequence because it is a sequence
data so we want to keep that true and
then you have to tell it what shape it's
in well we already know the shape by
just going in here and looking at XT
train shape so input shape equals the
xtrain shape of one comma 1 makes it
really easy you don't have to remember
all the numbers that put in 60 or
whatever else is in there you just let
it tell the regressor what model to use
and so we follow our STM with a Dropout
layer now understanding the Dropout
layer is kind of exciting because one of
the things that happens is we can over
Trin our Network that means that our
neural network will memorize such
specific data that it has trouble
predicting anything that's not in that
specific realm to fix for that each time
we run through the training mode we're
going to take 02 or 20% of our neurons
and just turn them off so we're only
going to train on the other ones and
it's going to be random that way each
time we pass through this we don't
overtrain these nodes come back in in
the next training cycle we randomly pick
a different 20 and finally they see a
big difference is we go from the first
to the second and third and fourth the
first thing is we don't have to input
the shape because the shape's already
the output units is 50 here this Auto
The Next Step automatically knows this
layer is putting out 50 and because it's
the next layer it automatically sits set
says oh 50 is coming out from our last
layer it's coming out you know goes into
the regressor and of course we have our
drop drop out and that's what's coming
into this one and so on and so on and so
the next three layers we don't have to
let it know what the shape is it
automatically understands that and we're
going to keep the units the same we're
still going to do 50 units it's still a
sequence coming through 50 units and a
sequence now the next piece of code is
what brings it all together let's go
ahead and take a look at that and we
come in here we put the output layer the
dense layer and if you remember up here
we had the three layers we had uh lstm
Dropout and dents uh d just says we're
going to bring this all down into one
output instead of putting out a sequence
we just know want to know the answer at
this point and let's go ahead and run
that and so in here you notice all we're
doing is setting things up one step at a
time so far we've brought in our uh way
up here we brought in our data we bought
in our different modules we formatted
the data for training it we' set it up
you know we have our YX train and our y
train we have our source of data and the
answers we're we know so far that we're
going to put in there we re shaped that
we've come in and built our carass we've
imported our different layers and we
have in here if you look we have what uh
five total layers now carass is a little
different than a lot of other systems
because a lot of other systems put this
all in one line and do it automatic but
they don't give you the options of how
those layers interface and they don't
give you the options of how the data
comes in carass is Cutting Edge for this
reason so even though there's a lot of
extra steps in building the model this
has a huge impact on the output and what
we can do with this these new models
from carass so we brought in our dense
we have our full model put together our
regressor so we need to go ahead and
compile it and then we're going to go
ahead and fit the data we're going to
compile the pieces so they all come
together and then we're going to run our
training data on there and actually
recreate our regressor so it's ready to
be used so let's go ahead and compile
that and I can go ahe and run that and
uh if you've been looking at any of our
other tutorials on neural networks
you'll see we're going to use the
optimizer atom Adam is optimized for Big
Data there's a couple other optimizers
out there uh beyond the scope of this
tutorial but certainly Adam will work
pretty good for this and loss equals
mean squared value so when we're
training it this is what we want to base
the loss on how bad is our error or
we're going to use the mean squared
value for our error and the atom
Optimizer for its differential equations
you don't have to know the math behind
them but certainly it helps to know what
they're doing and where they fit into
the bigger models and then finally we're
going to do our fit fitting the RN into
the training set we have the regressor
do fit xtrain y train epics and batch
size so we know where this is this is
our data coming in for the X train our y
train is the answer we're looking for of
our data our sequential input epex is
how many times we're going to go over
the whole data set we created a whole
data set of XT train so this is each
each of those rows which includes a Time
sequence of 60 and badge size another
one of those things where carass really
shines is if you were pulling this say
from a large file instead of trying to
load it all into RAM it can now pick
smaller batches up and load those
indirectly we're not worried about
pulling them off a file today this isn't
big enough to uh cause the computer too
much of a problem to run not too
straining on the resources but as we run
this you can imagine what would happen
if I was doing a lot more than just one
column in one set of stock in this case
Google stock imagine if I was doing this
across all the stocks and I had instead
of just the open I had open close high
low and you can actually find yourself
with about 13 different variables times
60 cuz it's a Time sequence suddenly you
find yourself with a gig of memory
you're loading into your RAM which will
just completely you know if it's just if
you're not on multiple computers or
cluster you're going to start running
into resource problems but for this we
don't have to worry about that so let's
go ahead and run this and this will
actually take a little bit on my
computer it's an older laptop and give
it a second to kick in there there we go
all right so we have epic so this is
going to tell me it's running the first
first run through all the data and as
it's going through it's batching them in
32 pieces so 32 uh lines each time and
there's 1,98 I think I said 1199 earlier
but it's $198 I was off by one and each
one of these is 13 seconds so you can
imagine this is roughly 20 to 30 minutes
runtime on this computer like I said
it's an older laptop running at uh 0.9
GHz on a dual processor and that's fine
what we'll do is I'll go ahead and stop
go get a drink of coffee and come back
and let's see what happens at the end
and where the takes us and like any good
cooking show I've kind of gotten my
latte I also had some other stuff
running in the background so you'll see
these numbers jumped up to like 19
seconds 15 seconds which you can scroll
through and you can see we've run it
through 100 steps or 100 epics so the
question is what does all this mean one
of the first things you'll notice is
that our loss can is over here it kind
of stopped at 0.0014 but you can see it
kind of goes down until we hit about 014
three times in a row so we guessed our
epic pretty close since our loss has
remain the same on there so to find out
what we're looking at we're going to go
ahead and load up our test data the test
data that we didn't process yet and real
stock price data set test iocation this
is the same thing we did when we prepped
the data in the first place so let's go
ahead and go through this code and you
can see we've labeled it part three
making the predictions and visualizing
the results so the first thing we need
to do is go ahead and read the data in
from our test CSV you see I've changed
the path on it for my computer and uh
then we'll call it the real stock price
and again we're doing just the one
column here and the values from ication
so it's all the rows and just the values
from these that one location that's the
open Stock open and let's go ahead and
run that so that's loaded in there and
then let's go ahead and uh create we
have our inputs we're going to create
inputs here and this should all look
familiar this is the same thing we did
before we're going to take our data set
total we're going to do a little Panda
concap from the data State train now
remember the end of the data set train
is part of the data going in and let's
just visualize that just a little bit
here's our train data let me just put TR
for train and it went up to this value
here but each one of these values
generated a bunch of columns it was 60
across and this value here equals this
one and this value here equals this one
and this value here equals this one and
so we need these top 60 to go into our
new data so to find out we're looking at
we're going to go ahead and load up our
our test data the test data that we
didn't process yet and real stock price
data set test iocation this is the same
thing we did when we prep the data in
the first place so let's go ahead and go
through this code and we can see we've
labeled it part three making the
predictions and visualizing the results
so the first thing we need to do is go
ahead and read the data in from our test
CSV and you see I've changed the path on
it for my computer and uh then we'll
call it the real stock price and again
we're doing just the one column here and
the values from I location so it's all
the rows and just the values from these
that one location that's the open Stock
open and let's go ahead and run that so
that's loaded in there and then let's go
ahead and uh create we have our inputs
we're going to create inputs here and
this should all look familiar this is
the same thing we did before we're going
to take our data set total we're going
to do a little Panda concap from the
data State train now remember the end of
the data set train is part of the data
going in let's just visualize that just
a little bit here's our train data let
me just put TR for train and it went up
to this value here but each one of these
values generated a bunch of columns so
it was 60 across and this value here
equals this one and this value here
equals this one and this value here
equals this one and so we need these top
60 to go into our new data CU that's
part of the next data or it's actually
the top 59 so that's what this first
setup is over here is we're going in
we're doing the real stock price and
we're going to just just take the data
set test and we're going to load that in
and then the real stock price is our
data test. test location so we're just
looking at that first uh column the open
price and then our data set total we're
going to take pandas and we're going to
concat and we're going to take our data
set train for the open and our data site
test open and this is one way you can
reference these columns we've referenced
them a couple different ways we've
referenced them up here with the one two
but we know it's labeled as a panda set
is open so pandas is great that way lots
of Versatility there and we'll go ahead
and go back up here and run this there
we go and uh you'll notice this is the
same as what we did before we have our
open data set we're pended our two
different or concatenated our two data
sets together we have our inputs equals
data set total length data set total
minus length of data set minus test
minus 60 values so we're going to run
this over all of them and you'll see why
this works because normally when you're
running your test set versus your
training set you run them completely
separate but when we graph this you'll
see that we're just going to be we'll be
looking at the part that we didn't train
it with to see how well it graphs and we
have our inputs equals inputs. reshapes
or reshaping like we did before we're
Transforming Our inputs so if you
remember from the transform between zero
and one and finally we want to go ahead
and take our X test and we're going to
create that X test and for I in range 60
to 80 so here's our X test and we're
appending our inputs I to 60 which
remember is 0 to 59 and I comma 0 on the
other side so that's just the First
Column which is our open column and uh
once again we take our X test we convert
it to a numpy array we do the same
reshape we did before and uh then we get
down to the final two lines and here we
have something new right here on these
last two lines let me just highlight
those or or mark them predicted stock
price equals regressor do predicts X
test so we're predicting all the stock
including both the training and the
testing model here and then we want to
take this prediction and we want to
inverse the transform so remember we put
them between zero and one well that's
not going to mean very much to me to
look at a at a float number between Z
one I want the dollar amounts I want to
know what the cash value is and we'll go
ahead and run this and you'll see it
runs much quicker than the training
that's what's so wonderful about these
neural networks once you put them
together it takes just a second to run
the same neural network that took us
what a half hour to traine and plot the
data we're going to plot what we think
it's going to be and we're going to plot
it against the real data what what the
Google stock actually did so let's go
ahead and take a look at that in code
and let's uh pull this code up so we
have our PLT that's our uh oh if you
remember from the very beginning let me
just go back up to the top we have our
matplot library. pyplot as PLT that's
where that comes in and we come down
here we're going to plot let me get my
drawing thing out again we're going to
go ahead and PLT is basically kind of
like an object it's one of the things
that always threw me when I'm doing
graphs in Python because I always think
you have to create an object and then it
loads that class in there well in this
case PLT is like a canvas you're putting
stuff on so if you've done HTML L 5
you'll have the canvas object this is
the canvas so we're going to plot the
real stock price that's what it actually
is and we're going to give that color
red so it's going to be in bright red
we're going to label it real Google
stock price and then we're going to do
our predicted stock and we're going to
do it in blue and it's going to be
labeled predicted and we'll give it a
title because it's always nice to give a
title to your H graph especially if
you're going to present this to somebody
you know to your shareholders in the
office and uh the xlabel is going to be
time because it's a Time series
and we didn't actually put the actual
date and times on here but that's fine
we just know they're incremented by time
and then of course the Y label is the
actual stock price pt. Legend tells us
to build the legend on here so that the
color red and and real Google stock
price show up on there and then the plot
shows us that actual graph so let's go
ahead and run this and see what that
looks like and you can see here we have
a nice graph and let's talk just a
little bit about this graph before we
wrap it up here's our Legend I was
telling you about that's why we have the
Legend to showed the prices we have our
title and everything and you'll notice
on the bottom we have a Time sequence we
didn't put the actual time in here now
we could have we could have gone ahead
and um plotted the X since we know what
the the dates are and plotted this to
dates but we also know this only the
last piece of data that we're looking at
so last piece of data which ends
somewhere probably around here on the
graph I think it's like about 20% of the
data probably less than that we have the
Google price and the Google price has
this little up jump and then down and
you'll see that the actual Google
instead of a a turn down here just
didn't go up as high and didn't low go
down so our prediction has the same
pattern but the overall value is pretty
far off as far as um stock but then
again we're only looking at one column
we're only looking at the open price
we're not looking at how many volumes
were traded like I was pointing out
earlier we talk about stock just right
off the bat there's six columns there's
open high low close volume then there's
weather uh I mean volume shares then
there's the adjusted open adjusted High
adjusted low adjusted close they have a
special formula to predict exactly what
it would really be worth based on the
value of the stock and then from there
there's all kinds of other stuff you can
put in here so we're only looking at one
small aspect the opening price of the
stock and as you can see here we did a
pretty good job this curve follows the
curve pretty well it has like a you know
little jumps on it bends they don't
quite match up so this Bend here does
quite match up with that bin there but
it's pretty darn close we have the basic
shape of it and the prediction isn't too
far off and you can imagine that as we
add more data in and look at different
aspects in the specific domain of stock
we should be able to get a better
representation each time we drill in
deeper of course this took a half hour
for my program my computer to train so
you can imagine that if I was running it
across all those different variables
might take a little bit longer to train
the data not so good for doing a quick
tutorial like this if you're looking for
a course that covers everything from the
fundamentals to Advanced Techniques then
accelerate your career in AIML with our
comprehensive postgraduate program in Ai
and machine learning so enroll now and
unlock exciting AIML opportunities the
course link is in the description box
below graphs so computational graphs are
really the heart and soul of neural
networks uh we talk about a
computational graph they are a visual
representation of expressing and
evaluating mathematical equations the
nodes and data flow in a graph
correspond to mathematical operations
and variables you'll hear a lot uh some
of the terms you might hear are node and
Edge The Edge being the data flow in
this case um it could also represent an
actual value they have um oh I think in
spark they have a graph x which works
just on Computing edges there's all
kinds of stuff that has evolved from
computational graphs we're focusing just
on carass and on neural networks so
we're not going to go into great detail
on everything a computational graph does
it is a core component of a neural
network is what's important to know on
this so carass offers a python
userfriendly front- end while
maintaining a strong computation Power
by using a low-level API like tensorflow
pie torch Etc which use computational
grass as a back end so one this allows
for abstraction of complex problems
while specifying control flow
if you've ever looked at some of the
backend or the original versions of
tensorflow uh it's really a nightmare
you have all these different settings
you have to put in there and create uh
it's a lot of a lot of backend
programming this is like the old
computers when you had to uh tell it how
to dispose of a variable and how to
properly reallocate the memory for use
all that is covered nowadays in our
higher level programming well this is
the same thing with carass is it covers
a lot of this stuff and does things for
you that you would could spend hours on
just trying to figure
out it's useful for calculating
derivatives by using back propagation
we're definitely not going to teach a
class on derivatives uh in this little
video but understanding uh a derivative
is the rate of change so if you have a
particular function you're using in your
neural network a lot of them is just
simple uh uh y = mx plus b um your ukian
geometry where you just have a simple
slope times the intercept and they get
very complicated they have the inverse
tangent function for Activation as
opposed to just a linear idian model and
you can think about this as you have
your data coming in and you have to
alter it somehow well you alter it going
down to get an answer you end up with an
error and that error goes back up and
you have to have that back propagation
with the derivative you want to know how
it changed so that you can figure out
how to adjust it for the
error a lot of that's hidden so so you
don't even have to worry about it with
carass and in today's carass it'll even
if you create your own um uh formula for
computing an answer it will
automatically compute the back prop the
the derivative for you in a lot of
cases it's easier to implement
distributed computation uh so cross is
really nice way to package it and get it
off on different computers and share it
and it allows parallelism which means
that two operations can run
simultaneously so as we start developing
these backends it can do all all kinds
of cool things and utilize multiple
cores gpus on a computer uh to get that
parallel processing
up what are neural networks well like I
said there already uh we talked about in
computational edges you have a node and
you have a connection or your Edge so
neural networks are algorithms fashioned
after the human brain which contain
multiple layers each layer contains a
node called a neuron which performs a
mathematical operation
they break down complex problems into
simple
operations so one an input layer takes
in our data and pre-processes it when we
talk about pre-processing when you're
dealing with neural networks uh you
usually have to pre-process your data so
that it's between minus one and one or
zero and one um into some kind of value
that's usable that occurs before it gets
to the neural network uh in fact 80% of
data science is usually impr prepping
that data and getting it ready for
different
models two you have hidden layer
performs a nonlinear transformation of
input now it can do a hidden a linear
transformation it can use just a basic
um ukian geometry and you could think of
a node adding all the different
connections coming in uh so each
connection would have a weight and then
it would add to that weight plus an
intercept um in the note itself so you
can actually use ukian geometry but a
lot of these get really complicated they
have all these different formulas
and they're really cool to look at but
when you start looking at them look at
how they work uh you really don't need
to know the high math behind it um to
figure them out and figure out what
they're doing which is really cool that
means a lot of people can use this
without having to go get a PhD in
mathematics number three the output
layer takes the results from hidden
layer transform them and gives a final
output so sequential models uh so whates
makes this a sequential model sequential
models are linear stacks of layers where
one layer leads to the next it is simple
and easy to implement and you just have
to make sure that the previous layer is
the input to the next layer so uh you
have used for plain stack of layers
where each layer has one input and one
output tensor and this is what tensor
flow is named after is um each one of
these layers is like a tensor each each
node is a tensor and then the layer is
also considered a tensor of
values and it's used for simple
classifier declassify models you can
it's also used for regression models too
so it's not just about uh this is
something this is a teapot this is a cat
this is a dog um it's also used for
generating um uh regret the actual
values you know this is worth $10 that's
worth $30 uh the weather's going to be
90 out or whatever it is so you can use
it for both classifier and declassify uh
models and one more note when we talk
about sequential models the term
sequential is used a lot and it's used
in different areas and different
notations when you're in data science so
when we talk about time series we'll
talk about sequential that is something
very different uh sequential in this
case means it goes from the input to
layer one to Layer Two to the output so
it's very directional it's important to
note this because if you have a
sequential model can you have a
non-sequential model and the answer is
yes
uh if you master the basics of a
sequential model you can just as easily
have another model that shares layers um
you can have another model where you
have an input coming in and it splits
and then you have one set that's doing
one set of uh nodes maybe they're doing
a yes no kind of node where it's either
putting out a zero or a one a classifier
and the other one might be regression
it's just processing numbers and then
you recombine them for the output um
that's what they call a cross uh the
cross API
so there's a lot of different
availabilities in here and all kinds of
cool things you can do as far as
encoding and decoding and all kinds of
things and you can share layers and
things like that we're just focusing on
the basic cross model with the
sequential
model so let's dive into the meat of the
matter let's do a and do a demo on here
uh today's demo in this demo we'll be
performing flower classification using
sequential model and carass and we'll
use our model to classify between five
different types of
flowers now for this demo and you can do
this demo on whatever platform you want
or whatever um user interface for
developing python um I'm actually using
anaconda and then I'm using Jupiter
notebooks to develop in and if you're
not familiar with this um you can go
under environment once you've created
environment you can come in here to open
a terminal window and if you don't have
the different modules in here you can do
youra install whatever module it is um
just happened that this particular setup
didn't have a caborn in it which I
already installed uh so here's our
anaconda and then I'm going to go
back and start up my jupyter
notebook where I already created a uh
new uh python project Python 3 I'm in
Python 3.8 on this particular one um
sequential model for flowers so lots of
fun there uh so we're going to jump
right into this the first thing is to
make sure you have all your modules
installed so if you don't have uh numpy
pandas map plot library and Seaborn and
the carass um and sklearn or S kit it's
not actually sklearn you'll need to go
ahead and install all of those now
having done this for years and having
switched environments and doing
different things um I get all my imports
done and then we just run it and if we
get an error we know we have to go back
and install something um right off the
bat though we have numpy pandas matplot
Library Seaborn these are built on top
of each other pandas the data frame and
built on top of numpy the uh um data
array and then we bring in our SK learn
or scikit this is the S kit setup SCI uh
kit even though you use sklearn to bring
it in it's a Psy kit and then our carass
we have our pre-processing the images
image data generator um our model this
is our basic model or sequential
model uh and then we bring in from caros
layers uh import dents um
optimizers these optimizers a lot of
them already come in these are your
different optimizers and it's almost a
lot of this is so automatic now um atom
is the a lot of times the default
because you're dealing with a large data
uh and then we get our SGD which is uh
smaller data does better on smaller
pieces of data and I'm not going to go
into all of these uh different
optimizers we didn't even use these in
the actual demo you just have to be
aware that they are different optimizers
and the Digger the more you dig into
these models um you'll hit a point where
you do need to play with these a little
bit but for the most part leave it at
the default when you're first starting
out and we're we're doing just the
sequential you'll see here layers
dense and then if we come down a little
bit more uh when they put this together
and they're running the dense layers
you'll also see they have Dropout they
have flatten they have activation uh
they have the uh convolutional layer 2D
Max pooling 2D batch
normalization what are all these layers
uh and when we get to the model we're
going to talk about them uh a lot of
times when you're just starting you can
just uh uh import cross. layers and then
you have your Dropout your flatten uh
your convolutional uh neural network 2D
and we'll we'll cover what these do in
the actual example when we get down
there uh what I want you to take from
here though is you need to run your
Imports um and load your different
aspects of this and of course your
tensor flow TF because this is all built
on tensor
flow and then finally uh import random
as RN just for random generation and
then we get down here we have our uh
CV2 that is your um open image or your
open CV they call it for processing
images that's what the CV2
is uh we have our
tqdm the tqdm is for um is a progress
bar just a fancy way um of adding when
you're running a process you can view
the bar going across in the Jupiter uh
setup not really necessary but it's kind
of fun to have um we want to be able to
shuffle some files uh again these are
all different things pill is another um
image processor it goes with the CV2 a
lot of times you'll see both of those
and so we run those we got to bring them
all
in and the next thing is to set up our
directories and so we come into the
directories there's an important thing
to note on here other than we're looking
at a a lot of flowers which is
fun uh is we get down here we have our
directory archive flowers that just
happens to be where the different uh
files for different flowers are put in
we're denoting an X and a z and the x is
the data of the image and the Z is the
tag for it what kind of flower is this
uh and the image size is really
important because we have to resize
everything if you have a neural network
and if you remember from our neural n
Works uh let me flip back to that
slide we look at this slide we have two
input nodes here uh with an image you
have an input node depending on how you
set it up for each pixel and that pixel
has three different color schemes
usually in it sometimes four so if you
have a picture that's 150 by
150 uh you multiply 150 * 150 * 3 that's
how many nodes input layers coming in I
mean so this is a mass passive input a
lot of times you think oh yeah it's just
a a small amount of data or something
like that uh no it's a full image coming
in then you have your hidden layers A
lot of times they match what the image
size is coming in so each one of those
is also just as big and then we get down
to just a single output so that's kind
of a a thing to note in here what's
going on behind the scenes and of course
each one of these layers has a lot of
processes and stuff going
on and then we have our our different uh
directories on here let me go and run
that so I'm just setting the directories
that's all this is um archive flowers
Daisy sunflower tulip dandelion Rose uh
just our different directories that
we're going to be looking
at uh and then we want to go ahead and
we're need to assign labels remember we
defined x and
z so we're just going to create a uh uh
definition here um and the first thing
is uh return flower type
okay just returns it what kind of flower
it is I guess assign label to it uh but
we're going to go ahead and make our
train data and when you look at this
there's a couple things to take away
from here uh the first one is we're just
appending right onto our numpy array the
image we're going to let numpy handle
all that different aspects as far as 150
by 150 by 3 uh we just it right into the
numpy which makes it really easy we
don't have to do anything funky on the
processing and we want to leave it like
that and I'm going to talk about that in
a minute uh and then of course we have
to have the string a pin the label on
there and I want you to notice right
here uh we're going to read the image
in and then we're going to size it and
this is important because we're just
changing this to 150 by 150 we're
resizing the image so it's uniform every
image comes in identical to the other
ones uh this is something that's so
important is um when you're resizing or
reformatting your data you really have
to be aware of what's going on with
images it's not a big deal because with
an image you just resize it so it looks
squishy or spread out or stretched um
the neural network picks up on that and
it doesn't really change how it
processes
it so let's go ahead and run that uh and
now we've got our definition set up on
there and then we want to go ahead and
make our
uh training data uh so make the train
data uh daisy flower daisy directory uh
print length of X so here we go let's go
and run that and we're just loading up
the flower daisy uh so this is going all
in there and it's setting um it's adding
it in to the our setup on there to our x
and z setup and we see we have
769 um and then then of course you can
see this nice bar here this is the bar
going across is that little added uh
code in there that just makes it really
cool for doing demos uh not necessarily
when you're building your own model or
something like that but if you're going
to display this to other people adding
that little what was it called
um tqdm I can never remember that uh but
the tqdm module in there is really nice
and we'll go ahead and do sunflowers and
of course you could have just uh created
an array of these um but this has an
interesting problem that's going to come
up and I want to show you something it
doesn't matter how good the people in
the back are or how good you are at
programming errors are going to come up
and you got to figure out how to handle
them uh and so when we get all the way
down
to the um where is it dandelion here's
our dandelion directory we're going to
build
um Jupiter has some cool things it does
which makes this really easy to deal
with but at the same time you would want
to go back in there depending on how
many times you rerun this how many times
you pull this so when you're finding
errors uh I'm going in here there's a
couple things you can do and we're just
going to um oh it wasn't there it is
there's our error I knew there was an
error this processed
1,62 out of
1065 now I can do a couple things one I
can go back into our definition and I
can just put in here try and so if it
has a bad conversion because this is
where the errors coming from uh just
skip it that's one way to do it um when
you're doing a lot of work in data
science and you look at something like
this where you're losing three points of
uh data at the end you just say okay I
lost three points who cares um or you
can go in there and try to delete it um
it really doesn't matter for this
particular demo and so we're just going
to leave that error right alone and skip
over because it's already added all the
other files in there and this is
wonderful thing about Jupiter notebook
is that I can just continue on there and
the x and z which we're creating is
still uh running and we'll just go right
into the next flower row so all these
flowers are in there um that's just a
cool thing about Jupiter
notebook uh and then we can go ahead and
just take a quick look and
see what we're dealing with and this is
of course really when you're dealing
with other people and showing them stuff
this is just kind of fun where we can
display it on the plot Library here and
we're just going to go through and um
let's see what we got here uh looks like
we're going to do like five of each of
them I think is that how they set this
up um plot Library 5 by two okay oh I
see how they did it okay so two each so
we have 5 by two set up on our axes and
we're just going to go in and look at a
couple of these
flowers it's always a good thing to look
at some of your data uh no matter what
you're doing we've reformatted this to
150 by 150 you can see how it really
blurs this one up here on the Tulip that
is that resize to 150 by 150 um and
these are what's actually going in these
are all 150 by 150 images you can check
the dimensions on the side and you can
see uh just a quick sampling of the
flowers we're actually going to process
on here and again like I said at the
beginning most of your work in data
science is reprocess
processing this different uh information
so we need to go ahead and take our
labels uh and run a label encoder on
there and then we're just going to Le is
a label encoder one of the things we
imported and then we always use the fit
um to categorical y comma 5 uh X here's
our array um X so if you look at this
here's our fit we're going to transform
Z that's our Z array we created
um and then we have Y which equals that
and then we go ahead and do uh to
categorical we want five different
categories and then we create our x uh
inpay of x x = x over
255 so what's going on here there's two
different Transformations one we've
turned our categories into 0 1 2 3 4 5
as the output and we have taken our X
array and remember the X array is three
values of your different
colors this is so important to
understand when we do this across a
numpy array this takes every one of
those three colors so we have 150 by 150
pixels out of those 150 by 150 pixels
they each have three um color arrays and
those color arrays ra range from 0 to
250 so when we take the xal X over
255 I'm sorry range from 0 to 255 this
converts all those pixels to a number
between zero and one and you really want
to do that when you're working with
neural networks uh now if you do a
linear regression model um it doesn't
affect it as much and so you don't have
to do that conversion if you're doing
straight numbers but when you're running
neural networks if you don't do this
you're going to create a huge bias and
that means they'll do really good on
predicting one or two things and they'll
just totally die on a lot of other
predictions so now we have our um X and
Y values uh X being the data in y being
our no one
output and with any good setup we want
to divide this data into our training so
we have X train uh we have our X test
this is the data we're not going to
program the model with and of course
your y train corresponds to your X train
and your y test corresponds to your X
test the outputs and this is uh when we
do the train test split this was from
the S kit sklearn we imported train test
split and we're just going to go ahead
and do the test size at about a quarter
of the data 0.25 and of course random is
always good this is such a good tool I
mean certainly you can do your own
division um you know you could just take
the first you know 0.25 of the data or
whatever do the link of the data not
real hard to do but this is randomized
so that if you're running this test a
few times you can kind of get an idea
whether it's going to work or
not sometimes what I will do is um I'll
just split the data into three parts
and then I'll test it on two with one
being the U or train it on two of those
parts with one being the test and I
rotate it so I come up with three
different answers which is a good way of
finding out just how good your model is
uh but for setting up let's stick with
the X train X test and the SK learn
package and then we're going to go ahead
and uh do a random
seed uh now a lot of times the cross
actually does this automatically but
we're going to go ahead and set it up on
here and you can see we do it an NP
random seed um from 42 and we get a nice
RN number um and then we do TF random we
set the seed so you can set your
Randomness at the beginning of your
tensor flow and that's what the tf.
random. set
is so that's a lot of prep um all this
prep and then we finally get to the
exciting part um this is where you
probably spend once you have the data
prepped and you have your pipeline going
and you have everything set up on there
this is the part that's exciting is
building these
models and so we look at this model one
we're going to designate it sequential
um they have the API which is a cross
the cross tensorflow API versus
sequential sequential means we're going
one layer to the next so we're not going
to split the layer and bring it back
together it looks almost the same with
the exception of um bringing it back
together so it's not a huge step to go
from this to an API
and the first thing we're going to look
at is um our convolutional neural
network in 2D uh so what's going on here
there's a lot of stuff that's going on
here um the default for well let's start
with the beginning what is a
convolutional 2d
Network well convolutional 2D Network
creates a number of small windows and
those small Windows float over the
picture and each one of them is their
own neural network and it's basically
becomes like a um a categorization and
then it looks at that and it says oh if
we add these numbers up a certain way uh
we can find out whether this is the
right flower based on this this little
window floating around which looks at
different things and we have filters 32
so this is actually creating 32 Windows
is what that's
doing and the kernel size is 5x5 so
we're looking at a 5x5 Square remember
it's 150 by by 150 so this narrows it
down to a 5x5 it's a 2d so it has your
XY coordinates um and we look at this
5x5 remember each one of these is is
actually looking at 5x5
by3 uh so we're actually looking at 15
by 15 different um
pixels and padding is just um uh usually
I just ignore that activation by default
is railu we went ahead and put the railu
in
there there's a lot of different
activations railu is for your smaller uh
when you remember I mentioned atom when
you have a lot of data data used in atom
kind of activation or using atom
processing we're using the railu here uh
it kind of gives you a yes or no but it
it doesn't give you a full yes or no it
has a um a zero and then it kind of
shoots off at an angle very common it's
the most common wand and then of course
here's our input shape 150 by 150 by
three
pixels and then we have to pull it so
whenever you have a two convolutional 2D
um uh layer we have to bring this back
together and pull this into uh neural
network and then we're going to go ahead
and repeat
this uh so we're going to add another
Network here one of the cool things if
you look at this is that it as it comes
in it just kind of automatically assumes
you're going down to the next layer and
so we we have another convolutional null
network uh 2D here's our Max pooling
again we're going to do that again Max
pooling uh and we're just going to
filter on down now one of the things
they did on this one is they changed the
kernel size they changed the number of
filters and so each one of these steps
kind of looks at the data a little bit
differently and that's kind of cool
because then you get a little added
filtering on there this is where you
start playing with the model you might
be looking at a convolutional no network
which is great for image
classifications um we get down to here
one of the things we see is flatten so
we had a we just flatten it remember
this is 150 by 150 by 3 well and
actually the pool size changes so it's
actually smaller than that flatten just
puts that into a 1D array uh so instead
of being you know a tensor of this
really complexity with the the pixels
and everything it's just flat and then
the
DSE is just another activation on there
um by default it is probably railu as
far as its
activation and then oh yeah here we go
in sequential they actually added the
activation as railu so this just because
this is sequential this activation is
attached to the dents uh and there's a
lot of different activations but Ru is
the most common one and then we also see
a soft Max uh soft Max is similar but it
has its own kind of variation and one of
the cool things you know what let me
bring this up because if we if you you
don't know about these activations this
doesn't make
sense and I just did a quick Google
search on images of tensorflow
activations um I should probably look at
which website this is but this is the
output of the values uh so as your X as
it adds in all those uh weighted X
values going into the node it's going to
activate it a certain way and that's a
sigmoid activation and you can see it
goes between zero and one and has a nice
curve there this also shows the
derivatives um and if we come down the
seven popular activation functions
nonlinear activations there's a lot of
different options on this let me see if
I can find
the oop let me see I can find the
specific to
railu so this is a leaky
railo and you can see instead of it just
being zero and then a value between uh
going up it has a little leaky there
otherwise your railu loses some nodes
they just become inactive um but you can
see there's a lot of different options
here here's a good one right here with
The Rao you can see the railo function
on the upper on the upper left here and
then the Leaky raayo over here on the
right which is very commonly used
also one of the things I use with
processing um language is the S is the
exponential one or the tangent H the
hyperbolic tangent because they have
that nice uh funky curve that comes in
that um has a whole different meaning
and captures word ed better again these
are very specific to domain and you can
spend a lot of time playing with
different models for our basic model uh
we'll stick to the railu and the softmax
on here and we'll go a and run and build
this
model so now that we've had fun playing
with all these different models that we
can add in there uh we need to go ahead
and have a batch size on here uh 128
epics
10 this means that we're going to send
128 uh rows of data or flowers at a time
to be processed and the Epic 10 that's
how many times we're going to Loop
through all the data reduce um the
values and verbose verbose equals 1
means that we're going to show what's
going on um value monitor what we're
monitoring we'll see that as we actually
train the model this is what's what's
going to come out of there if you set
the verbos equal to zero um you don't
have to watch it train the model
although it is kind of nice to actually
know what's going on
sometimes and since we're still working
on uh bringing the data in here's our
batch site here's our epics we need to
go ahead and create a data generator uh
this is our image data
generator and it has all the different
settings in here almost all of these are
defaults uh so if you're looking at this
going oh my gosh this is confusing most
of the time you can actually just ignore
most of this um vertical flip so you can
randomly flip pictures you can randomly
horizontally flip them um you can shift
the picture around this kind of helps
gives you multiple data off of them uh
zooming rotation there's all kinds of
different things you can do with images
most of these we're just going to leave
as false we don't really need to do all
that um um setup because we already have
a huge amount of data if you're short
data you can start flipping like a
horizontal picture and it will generate
it's like doubling your data almost um
so so the upside is you double your data
the downside is that if you already have
a bias in your data you already have um
5,000 sunflowers and only two roses
that's a huge bias it's also going to
double that bias uh that is the downside
of
that and so we have our model comp model
compile and this you're going to see in
all the carass we're going to take this
model here we're going to take all this
information as far as how we want it to
go and we're going to compile it this
actually builds the model and so we're
going to run that and I want you to
notice uh learning
rate very important this is the default
001 um there's there you really don't
this is how slowly it adjusts to find
the right answer and the more data you
have you might actually make this a
smaller number um with larger with you
have a very small sample of data you
might go even larger than that and then
we're going to look at the loss
categorically categorical cross entropy
most commonly used and this is uh how
how much it improves the model is
improving is what this number means or
yeah that's that's important on there
and then the accuracy we want to know
just how good our model is on the
accuracy and then uh one of the cool
things to do is if you're in a group of
people who are studying the model if
you're in shareholders you don't want to
do this is you can run the model summary
I do this by default and you can see the
different layers that you built into
this model just a quick summary on there
so we went ahead and we're going to go
ahead and create a
um we'll call it history but we want to
do a model fit
generator and so what this history is
doing is this is tracking what's going
on as while it fits the
model now there's a lot of new setups in
here where they just use fit and then
you put the generator in here
um we're going to leave it like this
even though the new default um is a
little different on that doesn't really
matter it does the same thing and we'll
go ahead and just run
that and you can see while it's running
right here uh we're going through the
epics we have one of 10 now we're going
through six of 25 here's our loss we're
printing that out so you can see how
it's improving and our accuracy the
accuracy gets better and better and this
is six out of of 25 this is going to
take a couple minutes to process uh
because we are training 150 by 150 by 3
pixels across uh six layers or eight
layers whatever it
was that is a huge amount of processing
so this will take a few minutes to
process this is when we talk about the
hardware and the problems that come up
in data science and why it's only now
just exploding being able to do neural
networks this is why this process takes
a long time
now you should have seen a jump on the
screen here because I did uh uh pause
the recorder to let this go ahead and
run all the way through its epics let's
go ahead and take a look and see what
these epics are and um if you set the
verbos to zero instead of one it won't
show what's going on in the behind the
scenes as it's training it so when we
look at this epic 10 epics we went
through all the data 10 times uh if I
remember correctly there's roughly a gig
of data there so that's a lot of data
the first thing you're going to notice
is the 270 seconds um that's how much
each of those epics took to run and so
if you divide 60 in there you roughly
get about 5 minutes worth of each epic
so if I have 10 epics that's 50 minutes
almost an hour of
runtime that's a big deal when we talk
about processing uh in on this
particular computer um I actually have
what is it uh uh eight cores with 16
dedicated threads so it runs like a 16
Cor computer it alternates the threads
going in and it still takes it 5 minutes
for each one of these epics so you start
to see that if you have a lot of data
this is going to be a problem if you
have a number of models you want to Fe
find out how good the models are doing
and what model to use and so each of
those models could take all night to run
in fact I have a model I'm running now
that takes over uh takes about a day and
a half to test each model um it takes
four days to do with the whole data uh
so what I do is I actually take a small
piece of the
data test it out to find out uh get an
idea of of how the different setups are
going to do and then I increase that
size of the data and then increase it
again and I can just take that that
curve and kind of say okay if U the data
is doing this then I need to add in more
dense layers or whatever uh so you can
do a small chunks of data then figure
out what it cost to do a large set of
data and what kind of model you
want the loss is we see here it
continues to go down uh this is the
error this is how much errors is in
there it really isn't a um userfriendly
number other than the more it Trends
down the better and so if you continue
to see the loss going down eventually
you'll get to the point where it stops
going down and it goes up and down and
kind of waivers a little bit that point
you know you've run too many epics
you're you're starting to get a bias in
there and it's not going to give you a
good model fit the accuracy just turns
this into something that uh we can use
and so the accuracy is what percentage
of guess is in this case it's
categorical so this is a percentage of
guesses are correct um value loss is
similar you know it's a minus a value
loss and then you have the value
accuracy and you'll see the value
accuracy is pretty similar to the
accuracy um just rounds it off basically
and so a lot of times you come down here
and you go okay we're doing 0.5
6 7 and that is 70% accuracy or in this
case
68 59% accuracy that's a very usable
number and it's very important to have
if you're identifying uh flowers that's
probably good enough if you can get
within a close distance and knowing what
flower you're you're identifying uh if
you're trying to figure out whether
someone's going to die from a heart
attack or not might want to rethink it a
little bit or uh rekey how you're
building your model so if I'm working
with a uh uh a group of um clients um
shareholders ERS in a company or
something like that you don't really
want to show them this um you don't want
to show them hey you know this is what's
going on with the accuracy these are
just numbers and so we want to go and
put the finishing touches just like when
you are building a house and you put in
the frame and the trim on the house it's
nice to have something a nice view of
what's going on and so we'll go ahead
and do a uh pie plot and we'll just plot
the history of the loss uh the history
of the value
loss over here
um epics train and test and so we're
just going to compute these this is
really important uh and what I want you
to notice right here is when we get to
about oh five epics a little more than
five six epics you see a cross over here
and it starts Crossing as far as the um
uh value loss and what's going on here
is you have the loss in your actual
model and your actual data and you had
the value loss where it's testing it
against the the test data the te the
data wasn't used to program your model
wasn't used to train your model on and
so when we see this crossing over this
is where the bias is coming in this is
becoming overfitted and so when you put
these two together uh right around five
and six you start to see how it does
this this switch over here and that's
really where you need to stop right
around five yeah six um it's always hard
to guess because at this point the
model's kind of a black box
uh see but you know that right around
here if you're saving your model after
each run you want to use the one that's
right around five epics because that's
the one that's going to have the least
amount of bias so this is really
important as far as guessing what's
going on with your model and its
accuracy and when to stop uh it also is
you know I don't show people this mess
up here um I show somebody this kind of
model and I say this is where the
training and the testing comes in on
this model uh it just makes it easier to
see and people can understand what's
going
on
so that completes our demo and you can
see we did what we were set out to do we
took our flowers and we're able to
classify them uh within about you 68 70%
accuracy whether it's going to be a
dollia sunflower cherry blossom Rose um
a lot of other things you can do with
your output as far as a different tables
to see where the errors are coming from
and what problems are coming
up and we're going to take a look at
image classification using carass and
the basic setup we'll actually look at
two different demos on
here uh what's in it for you today what
is image
classification Intel image
classification data creating neural
networks with carass and the vgg16
model what is image
classification the process of image
classification refers to assigning
classes to an entire image images can be
classified based on different categories
like whether it is a nighttime or
daytime shot what the image represents
Etc you can see here we have uh
mountains looking for mountains we'll
actually be doing some uh uh pictures of
scenery and stuff like that in deep
learning we perform image classification
by using neural networks to extract
features from images and classifies them
based on these
features and you can see here where it
says like what computers see and this is
oh yeah we see mostly Forest maybe a
little bit of mountains because the way
the image is um and this is really where
one of the areas that neural networks
really shines um if you try to run this
stuff through uh more like a linear
regression model you'll still get
results uh but the results kind of miss
a lot of things as they as the neural
networks get better and better at what
they do with different tools we have out
there uh so Intel image class
classification
data the data being used is the Intel
image classification data set which
consists of images of six types of land
areas and so we have Forest building
glaciers and mountains sea and Street uh
and you can see here is's a couple of
the images out of there as a setup in
the in the um uh Intel image
classification data that they
use and then we're going to go into
creating a neural networks with
carass the convolutional neural network
that we are creating from scratch looks
uh as showing
below and you see here we have our input
layer
um they have a listed Max pulling uh so
you have as you're coming in with a
input layer and this the input layer is
actually um before this but the first
layer that it's going to go into is
going to be a convolutional neural
network uh then you have a Max pooling
that pulls those the the convolutional
neural networks returns uh in this case
they have two of those that is very
standard with convolutional neural
networks uh one of the ones that I was
looking at earlier that was standard
being used by um I want one of the
larger companies I can't remember which
one for doing a large amount of
identification had two convolutional
neural networks each with their Max
pooling and then about 17 dense layers
after it we're not going to do that
heavy duty of a of a code but we'll get
you head in the right direction and that
gives you idea of what you're actually
going to be looking at when you look at
the flattened part and then the dense
we're talking like 17 dense layers
afterwards uh I find that a lot of the
stuff I've been working on I end up
maxing it out right around nine dense
layers it really depends on what you
have going in and what you're working
with and the vgg16
model uh vgg16 is a pre-trained CNN
model which is used for image
classification it is trained on a large
large varied data set and fine-tune to
fit image classification data sets with
ease and you can see down here we have
the input coming in uh the convolutional
neural network 1: one 1:2 and then
pooling and then we do two to one 2:2
convolutional neural network then
pooling 3:2 and you can see there's just
this huge layering of convolutional
neural networks and in this case they
have five such layers going in and then
three dents going out or uh more now
when they took this setup this actually
won an award uh back in 2019 for this
particular
setup uh and it does it does really good
except that again um we only show the
three dense layers here and as you find
out depending on your data going in and
what you have set up uh that really
isn't enough on one of these setups and
I'm going to show you why we restricted
it because it does take up a lot of
processing power and some of these
things so let's go ahead and roll up our
sleeves and we're going to look at both
the setups we're going to start with the
um the first
classification um and then we'll go into
the vgg16 and show you how that's set up
now I'm going to be using anaconda and
let me flip over to my anaconda so you
can see what that looks like now I'm
running in the Anaconda here uh you'll
see that I've set up a main python uh 38
I always put that in there this is where
I'm doing like most of my kind of
playing around uh this is done in Python
version 3.8 we're not going to dig too
much into versions uh at this point you
should already have carass installed on
there usually carass takes a number of
extra
steps and then our usual um uh setup is
the numpy the pandas uh your SK your s
kit which is going to be the sklearn
your caborn and I'll I'll show you those
in just a minute um and then I'm just
going to be in the Jupiter lab where
I've created a new um notebook in here
and let's flip on over there to my blank
notebook now I'm there's a couple cool
things to note in here is that um one I
used the the um Anaconda Jupiter
notebook setup because it keeps
everything separate uh except for carass
uh carass is actually running separately
in the back I believe it's a a c program
uh what's nice about that is that it
utilizes the multiprocessors on the
computer and I'll mention that just in a
little bit when we actually get down to
running the
code and when we look in here uh a
couple things to note is here's our uh
um oops I thought I grabbed the other
drawing thing uh but here's our numpy
and our pandas right here and our
operating system this is our s kit we
always import it as sklearn for the
classification report uh we're going to
be using well usually import like
Seaborn brings in all of your pip plot
Library
also kind of nice to throw that in there
I can't remember if we're actually using
caborn if they just the people in the
back just threw that together um and
then we have the SK learn shuffle for
shuffling data here's our map plot
library that the Seaborn is pretty much
built on um CV2 if you're not familiar
with that that is our image um module
for importing the image and then of
course we have our tens or flow down
here which is what we're really working
with and then the last thing is just for
visual effect while we're running this
um if you're doing a demo and you're
working with uh the partners or the
shareholders uh this TQ DM is really
kind of cool it's an extensible progress
bar for Python and I I'll show you that
too remember data science is not I mean
you know most of this code when I'm
looking through this code I'm not going
to show half of this stuff to the
shareholders or anybody I'm working with
they don't really care about pandas and
all that we do because we want to
understand how it
works uh so we need to go ahead and
import those different um uh setup on
there and then the next thing is we're
going to go ahead and set up our
classes uh now we remember if we had
Mountain Street Glacier building sea and
Forest those were the different images
that we have coming
in and we're going to go ahead and just
do class name labels and we're going to
kind of match that class name of I for I
class name uh equals the class names so
our labels are going to match the names
up
here uh and then we have the number of
uh
classes and print the class names and
the labels and we'll go ahead and set
the image size this is important that we
resize everything because if you're
remember with neural
networks they take one size data coming
in and so when you're working with
images you really want to make sure they
all resized to the same uh setup it
might squish them it might stretch them
that generally does not cause a problem
in these uh and some of the other tricks
you can do with if you if you need more
data um and this is one that's used
regularly we're not going to do it in
here is you can also take these images
and not only resize them but you can til
them one way or the other crop parts of
them um so they process slightly
differently and it'll actually increase
your accuracy of some of these
predictions
uh and so you can see here we have
Mountain equals z that's what this class
name label is Street equals 1 Glacier
equals 2 buildings equals 3 C4 Forest
equals
5 now we did this as an enumerator so
each one is 0 through five uh a lot of
times we do this instead as um uh uh 01
0 1 0 1 so you have five outputs and
each one's a zero or a one coming out so
the next next thing we really want to do
is we want to go ahead and load the data
up and just put a label in there loading
data just just so you know what we're
doing I'm going to put in the loading
data down here uh make sure it's well
labeled uh and we'll create a definition
for this and this is all part of your
preprocessing at this point you could
replace this with all kinds of different
things depending on what you're working
on and if you once you download you can
go download this data set uh send a not
to the simply learn team here in YouTube
um and they'll be happy to direct you in
the right direction and make sure you
get this path here um so you have it
right whatever wherever you saved it a
lot of times I'll just abbreviate the
path or put it as a sub thing and just
get rid of the directory um but again
double check your paths um we're going
to separate this into a segment for
training and a segment for testing and
that's actually how it is in the folder
let me just show you what that looks
like
so when I have my uh lengthy path here
where I keep all my programming simply
learned this particular setup we're
working on image classification and
image classification clearly you
probably wouldn't have that lengthy of a
list and when we go in here uh you'll
see sequence train sequence test they've
already split this up this is what we're
going to train the data in and again you
can see buildings Forest Glacier
Mountain Sea Street uh and if we double
click let's go under Forest you can see
all these different Forest uh images and
and there's a lot of variety here I mean
we have wintertime we have
summertime um so it's kind of
interesting you know here's like a
Fallen Tree versus um a road going down
the middle that's really hard to train
and if you look at the
buildings A lot of these buildings
you're looking up a skyscraper we're
looking down the
setup here's some trees with one I want
to highlight this one it has trees in it
uh let me just open that up so you can
see a little
closer the reason I want to highlight
this is I want you to think about this
we have trees growing is this the city
or a
forest um so this kind of imagery makes
it really hard for a classifier and if
you start looking at these you'll see a
lot of these images do have trees and
other things in the foreground weird
angles really a hard thing for a
computer to sort out and figure out
whether it's going to be a forest or a
um
city and so in our loading of data uh
one we have to have the path the
directory we're going to come in here we
have our images and our labels so we're
going to load the images in one section
the labels in another
um and if you look through here it just
goes through the different folders uh in
fact let me do this let
me there we go uh as we look at this
we're just going to Loop through the
three the six different folders that
have the different Landscapes and then
we're going to go through and pull each
file
out and each label uh so we set the
label we set the folder for file and
list uh here's our image path join the
paths this is all kind of n General
stuff um so I'm kind of skipping through
it really
quick and here's our image setup uh if
you remember we're talking about the
images we have our CV2 reader so reads
the the uh image in uh it's going to go
ahead and take the image and convert it
to from blue green red to red green
green blue this is a CV2 thing um almost
all the time it Imports it and instead
of importing it as a standard that's
used just about everywhere it Imports it
with the BGR versus RGB um RGB is pretty
much a standard in here you have to
remember that was CV2 uh and then we're
going to go ahead and resize it this is
the important part right here we've set
it we've decided what the size is and we
want to make sure all the images have
the same size on
them and then we just take our images
and we're just going to pin the image
pin the label um and then the images
it's going to turn into a numpy array
this just makes it easier to process and
manipulate and then the labels is also a
numpy array and then we just return the
output to pend images and labels and we
return the output down
here
so we've loaded these all into memory uh
we haven't talked to much there'd be a
different setup in there because there
is ways to feed the files directly into
your cross model uh but we want to go
ahead and just load them all it's
really for today's processing and that
what our computers can handle that's not
a big deal and then we go ahead and set
the uh train images train labels test
images test labels and that's going to
be returned in our output app pinned and
you can see here we did um uh images and
labels set up in there and it just loads
them in there so we'll have these four
different categories let me just go
ahead and run
that
uh so now we've gone ahead and loaded
everything on
there and then if you remember from
before uh we imported just go back up
there Shuffle here we go here's our
sklearn utilities import Shuffle and so
we want to take these labels and shuffle
them around a little bit um just mix
them up so it's not having the same if
you run the same process over and over
uh then you might run into some problems
on
there and just real quick let's go ahead
and do uh um a plot so we can just you
know we we've looked at them as far as
from outside of our code we pulled up
the files and I showed you what that was
going on we can go and just display them
here too and tell you when you're
working with different
people this should be highlighted right
here um this thing is like when I'm
working on code and I'm looking at this
data and I'm trying to figure out what
I'm doing I skip this process the second
I get into a meeting and I'm showing
what's going on to other people I skip
everything we just did so and go right
to here where we want to go ahead and
display some images and take a look at
it and in this display um I've taken
them and I've resized the images to 20
by
20 that's pretty small uh so we're going
to lose just a massive amount of detail
and you can see here these nice
pixelated images um I might even just
stick with the folder showing them what
images we're
processing uh again this is you got to
be a little careful this maybe resizing
it was a bad idea um in fact let me try
it without resizing it and see what
happens oops so I took out the image
size and then we put this straight in
here one of the things again this is
um put the D there we go one of the
things again that we want to
know whenever we're working on these
things uh is the
CV2 there are so many different uh image
classification setups it's really a
powerful package when you're doing
images but you do need to switch it
around so that it works with the PIP
plot and so make sure you take your
numpy array and change it to a u integer
8 format uh because it comes in as a
float otherwise you'll get some weird
images down there um and so this is just
basically we split up our we've created
a plot we went ahead and did the plot 20
by 20 um or the plot figure size is 20
by 20 um and then we're doing 25 so a
5x5 subplot um nothing really going on
here too exciting but you can see here
where we get the images and really when
you're showing people what's going on
this is what they want to see uh so you
skip over all the code and you have your
meeting you say okay here's our images
of the
building um don't get caught up in how
much work you do get caught up in what
they want to see so if you want to work
in data science that's really important
to know
and this is where we're going to start
uh having fun uh here's our model this
is where it gets exciting when you're
digging into these models and you have
here uh let me
get there we
go when you have here if you look here
here's our convolutional neural network
uh
2D and uh 2D is an image you have two
different dimensions XY and even though
there's three colors it's still
considered 2D if you're running a video
you'd be convolution old neural network
3D if you're doing a series going across
um a Time series it might be
1D and on these you need to go ahead and
have your convolutional Nal network if
you look here there's a lot of really
cool settings going on to dig into um we
have our input shape so everything's
been set to 150 by 150 uh and it has of
course three different color schemes in
it that's important to notice um
activation default is railu uh this is
small amounts of data being processed on
a bunch of little um neural
networks and right here is the 32 that's
how many of these convolutional null
networks are being strung up on here and
then the
3X3 uh when it's doing its steps it's
actually looking at uh a little 3x3
Square on each image
and so that's what's going on here and
with convolutional noral networks the
window floats across and adds up all
these numbers going across on this data
and then eventually it comes up with 30
in this case 32 different feature
options uh that it's looking for and of
course you can change that 32 you can
change the 3X3 so you might have a
larger setup and you know if you're
going across
150 um by 150 that's a lot of steps so
we might run this as uh 15x 15 uh
there's all kinds of different things
you can do here we're just putting this
together again that would be something
you would play with to find out which
ones are going to work better on this
setup um and there's a lot of play
involved that's really where it becomes
an art form is guessing at what that's
going to be the second part I mentioned
earlier and I I can only begin to
highlight this um when you get to these
dense layers one is the activation is a
r they use a Ru and a softmax here um
it's a whole whole uh setup just
explaining why these are different um
and how they're different because
there's also an exponential there's a
tangent in fact uh there's just a ton of
these and you can build your own custom
activations depending on what you're
doing a lot of different things go into
these activations uh there are two or
three major thoughts on these
activations and Ru and soft Max or uh U
well Ru uh you're really looking at just
the number you're adding all the numbers
together and you're looking at ukan
geometry um ax plus b X2 plus
cx3 plus
bias with softmax this belongs to the
party of um it's activated or it's not
except it's they call it softmax because
when you get to the to zero instead of
it just being zero uh it's actually
slightly a little bit less than zero so
that when it trains it doesn't get lost
um there's a whole series of these
activations another activation is the
tangent um where it just drops off and
you have like a very narrow area where
you have from minus one to one or
exponential which is 0 to one so there's
a lot of different ways to do the
activation again we can do that'd be a
whole separate lesson on here we're
looking at the convolutional neural
network um and we're doing the two pools
this is so common you'll see two two
convolutional n networks stacked on top
of each other each with its own Max pull
underneath and let's go ahead and run
that so we built our model there and we
need to go ahead and
um compile the model so let's go ahead
and do
that uh we are going to use the atom uh
Optimizer the bigger the data the Adam
fits better on there there's some other
Optimizer but I think Adam is a default
um I don't really play with the
optimizer too much that's like the if
once you get a model that works really
good you might try some different
optimizers uh but atom's usually the
most and then we're looking at
loss pretty standard we want to minimize
our Lo we want
to maximize the loss of error and then
we're going to look at accuracy um
everybody likes say accuracy I'm G to
tell you right now
I start talking to people and like okay
what's what's the loss on this and that
and as a data science yeah I want to
know how the Lo what what's going on
with that and we'll show you why in a
minute but everybody wants to see
accuracy they want to know how accurate
this is uh and then we're going to run
the fit and I wanted to do this just so
I can show you even though we're in uh
python setup in here where Jupiter
notebook is using only a single
processor I'm going to bring over my
little CPU Tool uh this is eight cores
on 16 dedicated threats so it shows up
as 16
processors and actually I got to run
this and then move it over so we're
going to run this and hopefully it
doesn't destroy my
mic uh and as it comes in you can see
it's starting to do go through the epics
we said I set it for five epics and then
this is really nice because carass uses
all the different uh threads available
so it does a really good job of doing
that uh this is going to take a while if
you look at here it's um uh ETA 2
minutes and 25 seconds 24 seconds so
this is roughly 2 and a half minutes per
epic uh and we're doing five epics so
this is going to be done in roughly 15
minutes I don't know about you but I
don't think you want to sit here for 15
minutes watching The Green bars go
across so we'll go ahead and let that
run and
there we go uh there was our 15 minutes
it's actually less than that uh because
I did when I went in here realized that
uh uh where was
it here we go here's our model compile
here's our model flit uh fit and here's
our epics uh so I did four epics so a
little bit better more like 10 to 11
minutes instead of uh uh doing the full
uh 15 and when we look at this here here
our model we did talked about the
compiler uh here's our history we're
going to um history equals the model fit
we'll go and do that in just a
minute and what we're looking at is we
have our epics um here's our validation
split so as we train it uh we're
weighing the accuracy versus you kind of
pull some data off to the side uh while
you're training it and the reason we do
that is that um you don't want to
overfit
and we'll look at that chart in just a
minute uh here's batch
size this is just how many images you're
sending through at a time the larger the
batch it actually increases the
processing speed um and there's reasons
to go up or down on the batch size
because of the U the the smaller of the
batch there's a certain point where um
you get too large of a batch and it's
trying to fit everything at once uh so I
128's kind of big um depends on the
computer you're on what it can handle
and then of course we have our train
images and our train labels going in
telling it what we're going to train
on and then we look at our four epics
here uh here's our accuracy we want the
accuracy to go up and we get all the way
up to uh
83 or
83% now this is actual percentage based
pretty much and we can see over here our
loss we want our loss to go down really
fluctuates uh 55 1.2
7748 uh so we have a lot of things going
on there let's go ahead and graph
those turn that off and our our team in
the back did a wonderful job of putting
together um this basic plot setup um
here's our subplot coming in we're going
to be looking at um
uh from the history we're going to send
it the accuracy and the value
accuracy labels and set up on there um
and we're going to also look at loss and
value loss so you can see what this
looks like what's really interesting
about this setup and let me let me just
go ahead and show you because uh without
actually seeing the plots it doesn't
make a whole lot of sense uh it's just
basic plotting of uh of the data using
the pi plot library and I want you to
look at this this is really interesting
um when I ran this the first time I had
very different
results um and they they vary greatly
and you can see here our accuracy
continues to
climb um and there's a crossover
here put it in here right here is our
crossover and I point that out because
as we get to the right of that crossover
where our accuracy um and we're like oh
yeah I got 8% we're starting to get an
overfit here that's what this this
switch over means um as our value U as a
training set versus the value um
accuracy stays the same and so that this
is the one we're actually really want to
be aware of and where it
crosses is kind of where you want to
stop at um and we can see that also with
the train loss versus the value loss
right here we did one Epic and look how
it just flat lines right there with our
loss so really one Epic is probably
enough and you're going to say wow okay
0.8% um certainly if I was working with
the shareholders um telling them that it
has an 80% accuracy isn't quite true and
and we'll look at that a little deeper
it really comes out here that the
accuracy of our actual values is closer
to 0 41% right here
um even after running it this number of
times and so you really want to stop
right here at that crossover one Epic
would have been enough um so the DAT is
a little overfitted on this when we do
four
epics and uh oops there we are
okay my drawing won't go away um let me
see if I can get there we
go uh for some reason I've killed my
drawing ability on my
recorder all right took a couple extra
clicks uh so let's go ahead and take a
look at our actual test loss um so you
see where a cross is over that's where
I'm looking at that's where we start
overfitting the
model and this is where if uh we were
going to go back and continually upgrade
the model we would start taking a look
at the images and start rotating them uh
we might start playing with the
convolutional neural network instead of
doing the 3X3 window um we might expand
that or you know find different things
that might make a big difference as far
as the way it processes these things um
so let's go ahead and take a click at
our uh our test loss now remember we had
our training data now we're going to
look at our test images and our test
labels for our test loss here and this
is just model evaluate uh just like we
did fit up here where was it um one more
model fit with our training data going
in and now we're going to evaluate it on
the and and this data has not been
touched yet so this model's never seen
this data this is on uh completely new
information as far as the model is
concerned of course we already know what
it is from the labels we
have and this is what I was talking
about here's the actual accuracy right
here 0
48 uh or
4847 so this 49% of the time guess is
what the image
is uh and I mean really that's a bottom
dollar uh does this work for what you're
needing does 49% work do we need to
upgrade the model more um in some cases
this might be uh oh what was I doing I
was working on uh stock
evaluations and we were looking at what
stocks were the top
performers well if I get that 50%
correct on top perform
performers uh I'm good with that um
that's actually pretty good for stock
evaluation in fact the number I had for
stock was more like U um 30 something
perent as far as being a top performer
stock much harder to predict um but at
that point you're like well I'm you'll
make money off of that so again this
number right here depends a lot on the
domain you're working
on and then we want to go ahead and
bring this home a little bit more uh as
far as looking at the different setup in
here and one of the uh from sklearn if
you remember actually let's go back to
the top uh we had the classification
report and this came in from our sklearn
or S kit setup and that's right here you
can see it right here on the
um see there we go uh classification
report right here uh sklearn metrics
import classification report that's what
we're going to look at next
a lot of this stuff uh depends on who
you're working with so when we start
looking at um
Precision you know this is like for each
value I can't remember what one one one
was probably mountains so if 44% is not
good enough if if you're doing like um
you're in the medical department and
you're doing cancer is ITA is this
cancerous or not and I'm only 44%
accurate not a good deal you know I
would not go with that um so it depends
on what you're working with on the
different labels and what they're used
for Facebook you know 44% I'm guessing
the right person I would hope it does a
little bit better than that um but
here's our main accuracy this is what
almost everybody looks at they say oh
48% that's what's important um again it
depends on what domain you're in and
what you're working
with and now we're going to do the same
model somehow I got my there it goes I
thought I was going to get stuck in
there again uh this time we're going to
be using the
vgg16 and remember this one has uh all
those layers going into it so it's
basically a bunch of convolutional n
networks getting smaller and smaller on
here uh and so we need to go ahead and
um import all our different stuff from
carass uh we're importing the main one
is the v g 16 setup on there just aim
that there we go
um there's kind of a pre-processing
images um applications pre-process input
this is all part of the VG g16 setup on
there uh and once we have all those we
need to go ahead and create our
model and we're just going to create a
vgg16 model in here um inputs model
inputs outputs model inputs I'm not
going to spend as much time as they did
on the other one uh we're going to go
through it really quickly one of the
first things I would do is if you
remember in carass you can Trea treat a
model like you would a
layer um and so at this point I would
probably add a lot of dense layers on
after the vgg16 model and create a new
model with all those things in there and
we'll go ahead and uh run this uh
because here's our model coming in and
our setup and it'll take it just a
moment to compile that what's funny
about this is I'm I'm waiting for it to
download the um package since I pre- ran
this um it takes it a couple minutes to
download the vgg16 model into here um
and so we want to go ahead and train
features for the model we're going to
predict the we're going to predict the
train images and we're going to test
features on the predict test images on
here and then I told you I was going to
create another model too and the people
in the back uh did not disappoint me
they went ahead and did just that and
this is really an important part um this
is where stopping for I told you I was
going to go through this really quick so
here's our
uh we we have our model
two um coming in and we we've created a
model up here with the vgg16 model
equals model inputs model inputs and so
we have our
vgg16 this has already been pre-
programmed uh and then we come down here
and I want you to notice on this um
right here layer model 2 layers minus 4
to 1 x layer X um we're basically taking
this model and we're adding stuff onto
it and so um we've taken we've just
basically duplicated this model we could
have done the same thing by using model
up here as a layer um we could have had
the the input go to this model and then
have that go down here so we've added on
this whole setup this whole block of
code from 13 to 17 has been added on to
our vgg16 model and we have a new model
uh with the layer input and X down here
let's go ahead and run that and compile
it and that was a lot to go through
right there uh when you're building
these models this is the part that gets
so
complicated that you get stuck playing
in and yet it's so fun uh it's like a
puzzle how can I Loop these models
together and in this case you can see
right here that the layers uh we're just
copying layers over and adding each
layer in um this is one way to build a
new model and we'll go ahead and run
that like I said the other way is you
can actually use the model as a layer I
had a little trouble playing with it uh
sometimes when you're using the Straight
model over you you run into issues
um it seems like it's going to work and
then you mess up on uh the input and the
output layers there's all kinds of
things that come
up let's go ahead and do the new model
we're going to compile it uh again
here's our metrics accuracy sparse
categorical loss uh pretty
straightforward just like we did before
you got to compile the
model and just like before we're going
to take our create a history uh the
history is going to be uh new model fit
train 128
and uh just like before if you remember
when we started running this stuff we're
going to have to go ahead and it's going
to light up our uh setup on here and
this is going to take a little bit to
get us all set up uh it's not going to
just happen in in a couple minutes so
let me go ahead and pause the video and
run it and then we'll talk about what
happened okay now when I ran that these
actually took about six minutes each um
so it's a good thing I put it on hold we
did four epics uh I actually had to stop
it is at 10 and switch it to four cuz I
didn't want to wait an
hour and you can see here our
accuracy um and our loss numbers going
down and just at a
glance it actually per armed if you look
at the
accuracy.
2658 um so our accuracy is going down or
you know
26% um 34% 35% and you can see here at
some point it just kind of kicks a
bucket again this is
overfitting that's always an issue when
you're running on uh programming these
different neural
networks and then we're going to go
ahead and plot the accuracy um history
we built that nice little sub routine up
above so we might as well use it and you
can see it right
here um there's that crossover
again and if you look at this look how
the how the um uh the red shifts up how
the uh our loss functions and everything
crosses over we're overfitting after one
Epic um we're
clearly not helping the problem or doing
better um we're just going to it's just
going to Baseline this one actually
shows with the training versus the
loss um value loss maybe second epic so
on here we're now talking more between
the first and the SE second epic and and
that also shows kind of here so
somewhere in here it starts over fitting
and right about now you should be saying
uh-oh uh something went wrong there I
thought that um when we went up here and
ran this look at this we have the
accuracy up here is hitting that
48% and we're down here
um you look at the score down here that
looks closer to 20% not nearly anywhere
in the ballpark of what we're looking
for
and we'll go ahead and run it through
the uh the actual test features here and
and there it is um we actually run this
on the Unseen data and
everything8 or
18% um I don't know about you but I
wouldn't want you know at 18% this did a
lot worse than the other one I thought
this is supposed to be the super model
the model that beats all models the
vgg16 that won the awards and everything
well the reason is is that um one we're
not pre-processing the data uh so it
needs to be more there needs to be more
um as far as like rotating the data at
you know 45 degree angle taking partials
of it so you can create a lot more data
to go through here um and that would
actually greatly change the outcome on
here and then we went up here we only
added a couple dense layers uh we added
a couple convolutional neural networks
this huge pre-train setup is looking for
a lot more information coming in as far
as how it's going to train and so uh
this is one of those things where I
thought it would have done better and I
had to go back and research it and look
at it and say why didn't this work why
am I getting only uh 18% here instead of
44% or better and that would be wise it
doesn't have enough training data coming
in uh and again you can make your own
training data so it's not that we'll
have a shortage of data it's that that
some of that has to be switched around
and moved around a little bit and this
is interesting right here too if you
look at the
Precision we're getting it on number two
and yet we had zero on everything else
so for some reason is not
seeing uh the different variables in
here so it'd be something else to look
in and try to find track down um and
that probably has to do with the input
but you can see right here we have a
really good solid Point 48 up here uh
and that's where I'd really go with is
starting with this model and then we
look at this model and find out why are
these numbers not coming up better is it
the data coming in um where's the setup
on there and that is the art of data
science right there is finding out which
models work better and why if you're
looking for a course that covers
everything from the fundamentals to
Advanced Techniques then accelerate your
career in AIML with a comprehensive
postgraduate program in Ai and machine
learning so enroll now and unlock
exciting AIML opportunity is the course
link is in the description box below
keep learning interview questions and
we're going to go from the very basics
of neural networks and deep learning
into some of the more commonly used
models so you can have an understanding
of what kind of questions are going to
come up and what you need to know in
interview questions we'll start with a
very general concept of what is deep
learning this is where we take large
volumes of data in this case on cats and
dogs or whatever A lot of times you use
um a training setup to train your model
remember it's kind of like a magic Black
Box going on there then we use that to
extract features or extract information
and in this case classify the image of a
cat and a dog so the primary takeaway
we're talking about deep learning is it
learns from large volumes of structured
and even unstructured data and uses
complex algorithms to train neural
network it also performs complex
operations to extract hidden patterns
and features and if we're going to
discuss deep learning in this very uh
simplified overview and we also have to
go over what is a neural network this is
a common image you'll see of a drawing
of a forward propagation neural network
and it's it's a human brain inspired
system which replicate the way humans
learn so this is inspired how our own
neurons in our brain fire but at a much
simplified level obviously it's not
ready to take over the human uh
population and and be our leader yet not
for many years it's very much in its
infant stage but it's inspired by how
our brains work um and they use a lot of
other Inspirations you can study brains
of moths and other animals that they've
used to figure out how to improve these
neural networks the most common one
consists of three layers of network and
this is generally how you view these
networks is you have an input you have a
hidden layer and an output and the
neural network is uh broken up into many
pieces but when we focus just on the
neural network it's always on the hidden
layers that we're making all the
adjustments and figuring out how to best
set up those hidden layers for their
functions to both train faster and to
function better when we look at this of
course we have our input hidden and
output each layer contains neurons
called as nodes perform various
operations and you can see here we have
the list of the nodes we have both our
input nodes and our output nodes and
then our hidden layer nodes and it's
used in deep learning algorithm like CNN
RNN GN Etc we'll address some of these
models a little closer at least the most
common models as we go down the list and
we study the Deep learning and the
neural network framework let's start
with what is a multi-layer patron or MLP
a lot of time is it referred to and
you'll see these abbreviations I'll be
honest I have to write them down on a
piece of paper and go through them cuz I
never remember what they all mean even
though I play with them all the time
what is a multi-layer patron well if you
look at the image on the right it's very
similar to what we just looked at you
have your input layer your hidden layer
and your output layer and that's exactly
what this is it has the same structure
of a single layer Perron with one or
more hidden layers except the input
layer each node in the other layers uses
a nonlinear activation function what
that means is your input layers your
data coming in and then your activation
function is based upon all those nodes
and weights being added together and
then it has the output MLP uses
supervised learning method called back
propagation for training the model very
key word there is back propagation
single layer Perron can classify only
linear separable classes with binary
output 01 but the MLP can classify
nonlinear classes so let's break this
down just a little bit the multi-layer
percept Seaton with an input layer and a
hidden layer and an output layer as you
see that it comes in there it has adds
up all the numbers and weights depending
on how your setup is that then goes to
the next layer that then goes to the
next hidden layer if you have multiple
hidden layers and finally to the output
layer the back propagation takes the
error that it sees so whatever the
output is it says hey this has an error
to it it's wrong and then sends that
error backwards from where it came from
and there's a lot of different functions
used to uh train this based on that
error and how that error goes backwards
in the notes uh so forward is you get
your answers backward is for training
you see this every day even my uh Google
pixel phone has this it they train the
neural network which takes a lot more
data to train than it does to use and
then they load up that neural network
into in this case I have a pixel 2 which
actually has a built-in neural network
for processing pictures and so it's just
the forward propagation I use when it
processes my photos but when they were
training it you use a back propagation
to train it with the errors they had
we'll be coming back to different models
that are used for right now though
multi-layer Perron MLP put that down as
your vocabulary word and of course back
propagation what is data normalization
and why do we need it this is so
important we spend so much time in
normalizing our data and getting our
data clean and setting it up uh so we
talk about data there's a pre-processing
step to standardize the data so whatever
we have coming in we don't want it to be
a you know one gigabyte file here a 2
gigabyte picture here and a 3 kilobyte
text there even as a human I can't
process those all in the same group I
have to reformat them in some way that
Loops them together so that a
standardized format we use this uh data
normalization in pre-processing to
reduce and eliminate data redundancy a
lot of times the data comes in and you
end up with two of the same images or um
uh the same information in different
formats then we want to rescale values
to fit fit into a particular range for
achieving better convergence what this
means is with most neural networks they
form a bias we've seen this recently in
attacks on neural networks where they
light up one pixel or one piece of the
view and it skews the whole answer so
suddenly um because one pixel is really
bright uh it doesn't know what to do
well when we start rescaling it we put
all the values between say minus one and
one and we change them and refit them to
those values it helps get rid of that
bias helps fix for some of those
problems and then finally we restructure
the data and improve the Integrity we
want to make sure that we're not missing
values or we don't have partial data
coming in one way to look at this is uh
bad data in bad data out so we want
clean data in and you want good answers
coming out one of the most basic models
used is a boltzman machine so let's
address what is a boltzman machine and
if you know we just did the MLP
multi-layer prron so now we're going to
come into almost a simple lii version of
that and in this we have our visible
input layer and we have our hidden layer
the Bol spin machines are almost always
shallow they're usually just two layer
neural Nets that make stochastic
decisions whether a neuron should be on
or off true false yes no first layer is
a visible layer and second layer is the
hidden layer nodes are connected to each
other across layers but no two nodes of
the same layer are connected hence it is
also known as restricted boltzman
machine now that we've covered a basic
MLP or multi-layer Perron and we've gone
over the boltzman machine also known as
the restricted boltzman machine let's
talk a little bit about activation
formulas and this is a huge topic that
can get really complicated but it also
is automated so it's very simple so you
have both a complicated and a simple at
the same time so what is the role of
activation functions in a neural network
activation function decides whether a
neuron should be fired or not that's the
most basic one and that actually changes
a little bit because it's either either
whether it's fired or not in this case
activation function or what value should
come out when it's fired but in these
models we're looking at just the boltman
restricted layers so this is what causes
them to fire either they don't or they
do it's a yes or no true false All or
Nothing it accepts the weighted sum of
the inputs the bias as input to any
activation function so whatever our
activation function is it needs to have
the sum of the weights times the input
so each input if you remember on that
model and let's just go back to that
model real quick and then you always
have to add a bias and you can look at
the bias if you remember from your ukian
geometry you draw a straight line
formula for that line has a y-coordinate
at the end it's always um CX plus M or
something like that where m is where it
crosses the Y coordinates if you're
doing a straight line with these weights
it's very similar but a lot of times we
just add it in as its own weight we take
it as a node of a one value coming in
and then we compute its new weight and
that's how we compute that that bias
just like we compute all the other
weights coming in the node which gets
fires depends on the Y value and then we
have a step function and the step
function this is where remember I said
it's going to get complicated and simple
all at the same time we have a lot of
different step functions we have the
sigmoid function we have just a standard
step function we have the um rayu it's
pronounced like Ray the ray of from the
Sun and L like a name so Ru function and
we have the tangent H function and if
you look at these they all have
something similar they all either force
it to be um one value or the other they
force it to be in the case of the first
three is zero or one and in the last one
it's either a minus one or one and you
can easily convert that to a zero one
yes no true false and on this one of the
most common ones is the step function
itself because there is no middle value
there is no um uh discrepancy that says
well I'm not quite sure but as you get
into different models probably the most
commonly used used to be the sigmoid was
most commonly used but I see the ru used
more often really depending on what
you're doing you just have to play with
these and find out which one works best
depending on the data and your output
the reason to have a non Z1 answer or
something kind of in the middle is when
you're looking at this and it's coming
out you can actually process that middle
ground as part of the answer into
another neural network so it might be
that the ru function says hey this is
only a 6 not a one and uh even though
the one is what's going in into the next
neural network or the next hidden layer
as an input the 6 value might also be
going in there to let you know hey this
is not a straight up one or straight up
zero is someplace in the middle this is
a little uncertain what's coming out
here so it's a very powerful tool but in
the basic neural network you usually
just use the step function it's yes or
no let's take a um a big step back and
take a kind of an overview the next
function is what is a cost function that
we're going to cover this is so
important because this is your end
result that you're going to do over and
over again and use to decide whether the
model is working or not whether you need
to try a different step function whether
you need to try a different activation
whether you need to try a fully
different model used uh so what is the
cost function cost function is a measure
to evaluate how good your model's
performance is it is also referred as
loss or error used to compute the error
of the output layer during back
propagation there's our back propagation
where we're training our model that's
one of our key words mean squared error
is an example of a popular cost function
and so here we have the cost function C
equals half of Yus y predicted um and
then you square that so the first thing
is um you know real quick if you haven't
done statistics this is not a percentage
it's not a percentage of how accurate it
is it's just a measurement of the error
and we take that error for training it
and we push that error backwards through
the neural network and we use that
through the different training functions
depending on what model you're using to
to train the neural network so when you
deploy the network you're usually done
training it cuz it takes a lot of
computational force to train it um this
is a very simple model and so you deploy
the trained one uh but we want to know
how your error is and so how do we do
that well you split your data part of
your data is for training and part of
your data is for testing and then we can
also test the error on there so it's
very important and then we're going to
go one more step on this we got to look
at both the local and the global setup
it might work great to test your data on
what you have on your computer but
that's different than in the field so
when we're talking about all these
different tests and the error test as
far as your loss you don't you want to
make sure that you're in a closed
environment when you do initial testing
but you also want to open that up and
make sure you follow up with the testing
on the larger scale of data because it
will change it might not fit the larger
scale there might be something in there
in the way you brought the data in
specifically or the data group you used
or um any of those could cause an error
so it's very important to remember that
we're looking at both the local and the
global cont context of our error and
just one other side note on a lot of the
newer models of neural networks by
comparing the error we get on the data
our training data with a portion of the
test data we can actually figure out how
good the model is whether it's
overfitted or not we'll go into that a
little bit more as we go into some of
the different models so we have our
output we're able to um figure out the
error on it based on the Square means
usually although there's other uh
functions used so we want to talk about
what is gradient descent another
vocabulary word gradient descent is an
optimation algorithm to minimize the
cost function or to minimize the error
aim is to find the local or Global
Minima of a function determine the
direction the model should take to
reduce the error so as we're looking at
this we have our uh squared error that
we just figured out the co based on the
cost function it says how bad is my
model fitting the data I just put
through it and then we want to reduce
that error so how do you figure out what
direction to do that in well it could be
that you're looking at just that line of
that line of data coming in so that
would be a local Minima we want to know
the error of that particular setup
coming in and then you have your Global
your Global Minima we want to minimize
it based on the overall data we're
putting through it and with this we can
figure out the global minimum cost we
want to take all those local minimum
costs of each piece of data coming in
and figure out the global one how are we
going to adjust this model to fit all
the data we don't want it to be biased
just on three or four lines of data
coming in we want it to kind of
extrapolate a general answer for all the
data coming in and this of course uh we
mentioned it briefly about back
propagation this is where really comes
in handy is training our model neural
network technique to minimize the cost
function helps to improve the
performance of the network back
propagates the error and updates the
weights to reduce the error so as you
can see here is a very nice depiction of
a back propagation we have our U
predicted y coming out and then we have
since it's a a training set we already
know the answer and the answer comes
back and based on case of the square
means was one of the functions we looked
at uh one of the activation functions
based on cost function that cost
function then depending on what you
choose for your back propagation method
and there's a number of them will change
the weights it will change the weight
going to each of one of those nodes in
the hidden layer and then based upon the
error that's still being carried back
it'll change the weights going to the
next hidden layer and then it computes
an error level on that and sends that
back up and you're going to say well if
it computes the error into the first
hidden layer and fixes it why would it
stop there well remember we don't want
to create a biased neural network so we
only make small adjustments on these
weights we don't make a big adjustment
that changes everything right off the
bat so no matter how far back you go
you're always going to have a small
amount of error and that's still going
to continue to go all the way back up
the hidden layers for right now focus on
the back propagation is taking that
error and moving it backwards on the the
neural network to change the weights and
help program it so that it'll have the
correct answers so far we've been
talking about forward propagation Nal
networks everything goes forwards goes
left to right uh but let's let's take a
little detep tour and let's see what is
the difference between a feed forward
neural network and a recurrent neural
network now this is in the function not
when we're training it using the back
propagation so you've got new
information coming in and you want to
get the answer and there's a couple
different networks out there and we want
to know we have a feed forward neural
network and we have a new uh vocabulary
term recurrent neural network a feed
forward neural network signals travel in
one direction from input to Output no
feedback loops considers only the
current input cannot memorize previous
inputs one example of one of these feed
forward neural networks and we've
covered a number of them but one of the
ones this has a big highlight nowadays
is the CNN a convolutional neural
network tensorflow the one put out by
Google is probably most known for their
CNN where they the information goes
forward it uh first takes a picture
splits it apart goes through the
individual pixels on the picture so it
picks up a different reading then
calculates based on that goes into a
regular feed forward neural network and
then gives your categorization on there
now we're not covering the CNN today but
we do have a video out that you can look
up on YouTube put out by simply learn
the convolutional neural network
wonderful tutorial check that out and
learn a lot more about the convolutional
neural network but you do need to know
that the CNN is is a forward propagation
neural network only so it's only moving
in One Direction so we want to look at a
recurrent neural network signals travel
in both directions making it a looped
Network considers the current input
along with the previous received inputs
for generating the output of a layer has
the ability to memorize past data due to
its internal memory and you can see they
have a nice uh image here we have our um
input and for some reason they always do
the recurrent neural network um in
Reverse from bottom up in the images
kind of a standard although I'm not sure
why your X goes into your hidden layer
and your hidden layer the answer for
part of the answer from that it
generates feeds back into the hidden
layer so now you have an input of both X
and part of the Hidden layer and then
that feeds into your output now if we go
back to the forward let me just go back
a slide and we're looking at uh our
forward propagation Network one of the
tricks you can do to use just a forward
propagation network is if you're in a
what they call a Time sequence that's a
good uh term to remember or a Time
series meaning that it's sequential data
each term comes after the other you can
trick this by creating your input nodes
as with the history so if you know that
uh you have values one five and seven
going in and you know what the output is
from one what those outputs are you can
expand the input to include the history
input that's one of the ways to trick a
forward propagation Network into looking
at that but when you doal with the
recurrent neural network you let the
hidden layer do that for you it's sends
that data and reprocesses it back into
itself what are some of the applications
of recurrent neural network the RNN can
be used for sentiment analysis and text
mining getting up early in the morning
is good for health it's a positive
sentiment one of the catches you really
want to look at this when you're looking
at the language is that I could switch
this around and totally negate the
meaning of what I'm doing so it no
longer be positive so when you're
looking at a sentence knowing the order
of the words is as important as the
meaning of the words you can't just
count how many good words there are
versus bad words to get positive
sentiment you now have to know what
they're addressing and there's lots of
other different uses uh kids are playing
football or soccer as we call it in the
US RN can help you caption an image So
based on previous information coming in
it refeeds that back in and you have a
uh image Setter and then time series
problems like predicting the prices of
stocks in a month or quarter or sale of
products can be solved using an RNN and
this is a really good example you you
have whatever your stocks were doing
earlier this month will have a huge
effect on what they're doing today if
you're investing so having an RNN Model
A recurrent neural network feeding into
itself what was happening previously
allows it to take that model and program
in that whole series without having to
put in the whole a month at a time of
data you only put in one day at a time
but if you keep them in order it will
look back and say oh this because of
what happened yesterday I need some
information from that and I'm going to
use that to help predict today and so on
and so on we're going to go back to our
activation functions remember I told you
Ru is one of the most common functions
used uh so let's talk a little bit more
about Ru and also softmax softmax is an
activation function that generates the
output between 0o and one it divides
each output such that the total sum of
the outputs is equal to one it is often
used in the output layers softmax L of
the N equals e to L on the N over the
absolute value of e to the L so what
does this function mean I mean what is
actually going on here so we have our
output outut noes and our output nodes
are giving us let's say they gave us
1.2.9 and4 as a human being I look at
that and I say well the greatest value
is 1.2 so whatever category that is if
you have three different categories
maybe you're not just doing if it's a
cat or it's a dog or um oh let's say
it's a cow we had cats and dogs earlier
why the cats and dogs are hanging out
with a cow I don't know but we have a
value and it might say 1.2 is a cat 0.9
is a dog and 04 is a cow uh for some
reason it SS that there's a chance of it
being any one of these three items and
that's how it comes out of the output
layer well as a human I can look at 1.2
and say this is definitely what it is
it's definitely a cat or whatever it is
uh maybe it's looking at different kinds
of cars might be a better whether it's a
car truck or motorcycle maybe that'd be
a better example well from a computer
standpoint that may be a little
confusing because they're just numbers
waving at us and so with the soft Max we
want all those numbers to always add up
to one so when I add three numbers
together I want the final output to be
one on there and so it goes through this
formula changes each of these numbers in
this case it changes them to
4634 and2 they all add up to one and
that's a lot easier to register CU it's
very set it's a set output it's never
going to be more than one it's never
going to be less than zero and so you
can see here that there's probably a
pretty high chance that it's the first
one so you're a human being we have no
problem knowing that but this output can
then also go into say another input so
it might be an automated car that's
picking up images and it says that image
in front of us is probably a big truck
we should deal with it like it's a big
truck it's probably not a motorcycle um
or whatever those categories are that's
the softmax part of it but now we have
the railu well what where's the railu
coming from well the railu is what's
generating the 1.2 and the 0.9 and the
point4 and so if you remember our Ru
stands for rectified linear unit and is
the most widely used activation function
we looked at a number of different
activation functions including tangent H
the step function and remember I said
the step function is really used if
that's what your actual output is
because then you know it's a zero or one
but The Rao if you have that as your
output you know have a discrepancy in
there and if that's going into another
neural network or another process having
that discrepancy is really important and
it gives an output of x if x is positive
and zero otherwise so it says my x value
is going to be somewhere between zero or
one and then the uh usually unless it's
really uncertain the output's usually a
one or zero and then you have that
little piece of uncertainty there that
you can send forward to another Network
or you can look at to know that there's
uncertainty involved and is often used
in the hidden layers this is what's
coming out of the HD layers into the
output layer usually or as we reference
the convolution neural network the CNN
you'd have to go to another video to
review the railu is the most common used
for convolutional part of that Network
it has a bunch of little pieces that are
very simplified looking at all the
different images or different sections
of the map and and uh the ru works
really good for that like I said there's
other formulas used but that this is the
most common one and you'll see that in
the hidden layers going maybe between
one layer and the next layer so just a
quick recap we have our soft Max which
means that if you have uh numerous
categories only one of them is going to
be picked but you also want to have some
value attached to it how well it picked
it and you put that between zero one so
it's very uh standardized so we have our
soft Max we looked at that let's go back
one we looked at that here where it
transforms in numbers and then we have
our Ru function which takes the
information in the summation and puts it
between a zero and a one where it's
either clearly a zero or depending on
how confident our model is it'll go
between the zero and one value what are
hyperparameters well this is a great
interview question hyperparameters when
you are doing neural networks this is
what you're playing with most of the
time once you've gotten the data
formatted correctly a hyperparameter is
a parameter whose value is set before
the learning process begins begins
determines how a network is trained and
the structure of the network this
includes things like the number of
hidden units how many hidden layers are
you going to have and how many nodes in
each layer learning rate learning rate
is usually multiplied once you figured
out the error and how much you want to
change the weights we talked about or I
mentioned it early just briefly you
don't want to just make a huge change
otherwise you're going to have a biased
model so you only take little
incremental changes and that's what the
learning rate is is those small
incremental changes epic how many times
are you going to go through all the data
in your training set so one Epic is one
trip through all the data and there's a
lot of other things depending on which
model you're working with and which
programming script you're working with
like the python SK learn package will
have it slightly different than say
Google's tensorflow package which will
be a little bit different than the spark
machine learning package so these are
just some examples of the
hyperparameters and so you see in here
we have a nice image of our data coming
in and we train our model then we do a
comparison to see how good our model is
and then we go back and we say hey this
this model is pretty good but it's
biased so then we send it back and we
change our hyper parameters to see if we
can get an unbiased model or we can have
a better prediction on it that matches
our data closer what will happen if
learning rate is set too low or too high
we have a nice couple graphs here we
have one over here says the learning
rate set too low you can see that it
slowly Works its way down the curve and
on the right you can see a learning rate
set too high it's just bouncing back and
forth when your learning rate is is too
low that's what we studied at two slides
ear asked what the learning rate was
training of the model will progress very
slowly as we are making very tiny
updates to the weights we'll take many
updates before reaching the minimum
point so I just mentioned epic going
through all the data you might have to
go through all the data a thousand times
instead of 500 times for it to train
learning rate too high causes
undesirable Divergent Behavior to the
loss function due to drastic updates and
weights at times it may fail to converge
or even diverge so so if you have your
learning rate set too high and it's
training too quickly maybe you'll get
lucky and it trains after one epic run
but a lot of times it might never be
able to train because the weights are
changing too fast they they flip back
and forth too easy and you see down here
we've introduced uh two new terms
converge and diverge a converge means
that our model has reached a point where
it's able to give a fairly good answer
for all the data we put in all those
weights have adjusted and it's minimized
the error diverg means that the data is
so chaotic that it can never manage to
to train to that data the data is just
too chaotic for it to train so we have
two new words there are converge and
diverge are important to know also what
is Dropout and batch normalization
Dropout is a technique of dropping out
hidden and visible units of a network
randomly to prevent overfitting of data
it doubles the number of iterations
needed to converge the network so here
we have our standard neural network and
then after applying Dropout now it
doesn't mean we actually delete the note
node the node is still there and we're
still going to use that node what it
means is that we're only going to work
with a few of the nodes um a lot of
times I think the most common one right
now used is 20% uh so you'll drop out
20% of the nodes when you do your
training you reverse propagate your data
and then you'll randomly pick another 20
nodes the next time you go through an
epic data training so each time you go
through one Epic you will randomly pick
20 of those nodes not to not to mess
with and this allows for Less over of
the data so by randomly doing this you
create some I guess it just kind of pull
some nodes off to the side it says we're
going to handle the data later on so we
don't overfit batch normalization is the
technique to improve the performance and
stability of neural network the idea is
to normalize the inputs in every layer
so that they have mean output and
activation of zero and standard
deviation of one this question covers a
lot of different things which is great
it's a great uh interview question
because it pulls in that you have to
understand what the mean value is so
mean output activation of zero that
means our average activation is zero so
when you normalize it remember usually
we're going between minus one and one on
a lot of these it's a very standard
setup so you have to be very aware that
this is your mean output activation of
zero and then we have our standard
deviation of one so we want to keep our
error down to just a one value the
benefits of this doing a batch
normalization is it provides
regularization it trains faster Higher
Learning rates and weights are easy to
initialize what is the difference
between batch gradient descent and
stochastic gradient descent batch
gradient descent batch gradient computes
the gradient using the entire data set
it takes time to converge because the
volume of data is huge and weights
update slowly so you can look at the
batches a lot of times if you're using
big data batch the data in but you still
go through a full epic you still go
through all the data on there so bash
graded descent means you're going to use
it to fit all the data and look for a
convergence there stochastic gradient
descent stochastic gradient computes the
gradient using a single sample it
converges much faster than batch
gradient because it updates weight more
frequently explain overfitting and
underfitting and how to combat them
overfitting happens when a model learns
the details and noise in the training
data to the degree that it adversely
impacts the execution of the model on
the new information it is more likely to
occur with nonlinear models that have
more flexibility when learning a Target
function an example of this would be um
if you're looking at say cars and trucks
and motorcycles it might only recognize
trucks that have a certain box-like
shape it might not be able to notice a
flatbed truck unless it's only a
specific kind of flatbed truck or only
Ford trucks because that's what it saw
on the training set this means that your
model performs great on your train data
and great on maybe a small test amount
of data but when you go to use it in the
real world it leaves out a lot and start
is not very functional outside of your
small area your SL laboratory data
coming in underfitting doing the
opposite when you underfit your data
underfitting alludes to a model that is
neither well trained on training data
nor can generalize to new information
usually happens when there is less and
improper data to train a model has a
performance and accuracy so if you're
using underfitted data and you generate
a model and you distribute that in a
commercial Zone you'll have a lot of
people unhappy with you cuz it's not
going to give them very good answers so
we've explained overfitting and
underfitting so now we want to ask how
to combat them combating overfitting and
underfitting resampling the data to
estimate the model accuracy kfold cross
validation having a validation data set
to EV validate the model so when we do
the resampling we're randomly going to
be picking out data we'll run it a few
times to see how that works depending on
our random data and how we um sample the
data to generate our model and then we
want to go ahead and validate the data
set by having our training data and then
keeping some data on the side uh testing
data to validate it how are weights
initialized in a network initializing
all weights to zero all the weights are
set to zero this makes your model
similar to a linear model so if you have
linear data coming in doing a basic
setup like that might work all the
neurons in every layer perform the same
operation given the same output and
making the Deep net useless right there
is a key word it's going to be useless
if you initialize everything to zero at
that point be looking into some other uh
machine learning tools initializing all
weights randomly here are the weights
are assigned randomly by initializing
them very close to zero it gives better
accuracy to the model since every neuron
performs different computations and here
we have the weights are set randomly we
have our input layer the hidden layers
and the output layer and W equals NP
random random in layer size L layer size
L minus one this is the most commonly
used is to randomly generate your
weights what are the different layers in
CNN convolutional neural network first
is the convolutional layer that performs
a convolutional operation we have our
other video out if you want to explore
that more so you go into detail exactly
how the C the convolutional layer works
in the CNN as far as creating a number
of smaller uh picture windows that go
over the data uh the second step is has
a railu layer railu brings nonlinearity
to the network and converts all the
negative pixels to zero output is
rectified feature map so it goes into a
mapping feature there pooling layer
pooling is a down sampling operation
that reduces the dimensionality of the
feature map so we have all our railu
layer which is pulling all these little
Maps out of our convolutional layer is
taking that picture and little creating
little tiny neural networks to look at
different parts of the picture uh then
we need to pull it together and then
finally the fully connected layer so we
flatten our pulling layer out and we
have a fully connected layer recognizes
and classifies the objects in the image
and that's actually your forward
propagation reverse propagation training
model usually I mean there's a number of
different models out there of course
what is pooling in CN NN and how does it
work pooling used to reduce the spatial
dimensions of a CNN performs down
sampling operation to reduce the
dimensionality creates a pulled feature
map by sliding a filter Matrix over the
input Matrix I mentioned that briefly on
the previous slide um it's important to
know that you have if you can see here
they have a rectified feature map and so
each one of those colors like the yellow
color that might be one of the a smaller
little neural network using the ru
you'll look at it'll just kind of uh go
over the main picture and look at all
the different areas on the main picture
so you might step one two three four
spaces um and then you have another one
that's also looking at features and it
has a 2785 each one of those is a map so
it might be the first one might be a map
looking for cat ears and the second one
looking for human eyes when it does this
you then have this rectified feature map
looking at these different features and
the max pooling with a 2 x two filters
and a stride of two stride means instead
of skipping every pixel you're going to
go every two pixels you take the maximum
values and you can see over here when we
look at a pulled feature map one of the
features says hey I had a max value of
eight so somewhere in here we saw a
human eye labeled as eight pretty high
label and maybe seven was a human hand
and maybe four was cat whiskers or
something that we thought might be cat
whiskers four is kind of a low number in
this particular case compared to the
other ones so you have your full pulled
feature map you can see the process here
is we have our stepping we look for the
max value and then we create a pulled
feature map of the max value how does a
lstm network work that's long shortterm
memory so the first thing to know is
that an lstms are a special kind of
recurrent neural network capable of
learning long-term dependencies
remembering information for long periods
of time is their default Behavior we did
look at the RNN briefly talked about how
the hidden layer feeds back into itself
with the lstm has a much more
complicated feedback and you can see
here we have um the hidden layer of TUS
one and hidden layer that's what the H
stands for hidden layer of T and the
formula is going in as we can see here
we have the hidden layers we have T
minus one and then h of T where T stands
for time so this is a series remember
working with series and we want to
remember the past and you can see you
have your your input of T and that might
be a frame in a video as a frame comes
in they usually use in this one the
tangent H activation formula but you
also see that it goes through a couple
other formulas the Omega formula and so
when it combines these that thing goes
into the next layer your next hidden
layer that thing goes into the data
that's submitted to the next input so
you have your X of t + one so when you
have that coming in then you have your H
value that's coming forward from the
last process and depending on how many
of these um Omega structures you put in
there depends on how longterm the memory
gets so it's important to remember this
is more for your long-term recurrent
neural networks the three steps in an
LST M step one decides what to forget
and what to remember step two
selectively update cell State values So
based on what we want to remember and
forget we want to update those cell
values and then decides what part of the
current state make it to the output so
now we have to also have an output on
there what are Vanishing and exploding
gradients this is a great question that
affects all our neural networks while
training an RNN your slope can become
either too small or too large and this
makes the training difficult when the
slope is too small the problem is known
as Vanishing gradient so our slope we
have our change in X and our change in y
when the slope decreases gradually to a
very small value sometimes negative and
makes training difficult when the slope
tends to grow exponentially instead of
decaying this problem is called
exploding gradient the slope grows
exponentially you can see a nice graph
of that here issues in gradient problem
Long training time poor performance and
low accuracy what is the difference
between Epic bat and iteration in deep
learning Epic an epic represents one
iteration over the entire data set so
that's everything you're going to go
ahead and put into that training model
batch we cannot pass the entire data set
into the neural network at once so we
divide the data set into a number of
batches and then iteration if we have
10,000 images as data and a batch size
of 200 then the Epic should run 10,000
times over 200 so that means we have our
total number over the 200 equals 50
iteration so in each epic we're running
over all the data set we're going to
have 50 iterations and each of those
iterations includes a batch of 200
images in this case why tensorflow is
the most preferred library in deep
learning uh well first tensorflow
provides both C++ and python apis that
makes it easier to work on has a faster
compilation time than other deep
learning libraries like carass and torch
tensorflow supports both CPUs and gpus
Computing devices so right now
tensorflow is at the top of the market
CU it's so easy to use for both
programmer side and for Hardware side
and for the speed of getting something
up and running what do you mean by
tensor and tensor flow tensor is a
mathematical object represented as
arrays of higher Dimensions these arrays
of data with different dimensions and
ranks that are fed as input to the
neural network are called tensors and
you can see here we have a tensor of
Dimensions 5 comma 4 so it's a
two-dimensional tensor coming in um you
can look at an image like this that each
one of those pixels is a different value
if it's a black and white so might be Z
Z and ones and then each one represents
a black and white image in a color photo
you might um either find a different
value system or you might have a tensor
value that has the XY coordinates as we
see here plus the colors so you might
have three more different dimensions for
the three different images the red the
blue and the yellow coming in and even
as you go from one layer or one tensor
to the next these layers might change we
might flatten them might bring in
numerous in the case of the convergence
neural network we have all those small
smaller different mappings of features
that come in so each one of those layers
coming through is a tensor if it has
multiple Dimensions coming in and
weights attached to it what are the
programming elements in tensor flow well
we have our constants constants are
parameters whose value does not change
to define a constant we use tf. constant
command example a equal tf. constant 2.0
TF float 32 so it's a tensor float value
of 32 b equals TF constant 3.0 print AB
if we did a print of ab we'd have um t
do constant and then of course uh B is
that instance of it variables variables
allow us to add new trainable parameters
to graph to Define a variable we use tf.
variable command and initialize them
before running the graph in session
example W equal TF variable. 3 dtype TF
float 32 or b equal a TF variable minus
3 comma dtype float 32 placeholders
placeholders allow us to feed data to a
tensorflow model from outside a model it
permits a value to be assigned later to
define a a placeholder we use TF
placeholder command example AAL TF
placeholder B = A * 2 with the TF
session as sess result equals session
run B comma feed dictionary equals a 3.0
print result uh so we have a nice
example there a placeholder session a
session is run to evaluate the nodes
this is called as the tensor flow
runtime so for example you have AAL TF
constant 2.0 B = TF constant 4.0 C = A
plus b and at this point you'd go ahead
and create a session equals TF session
and then you could evaluate the tensor C
print session run C that would input c
as an input into your session what do
you understand by a computational graph
everything in tensorflow is based on
creating a computational graph it has a
network of nodes where each node
performs an operation nodes represent
mathematical operation and edges
represent tensors since data flows in a
form of a graph it is also called a data
flow graph and we have a nice a visual
of this graph or graphic image of a
computational graph and you can see here
we have our input nodes our add multiply
nodes and our multiply node at the end
and then we have the edges where the
data flows so we have from a going to C
A going to D you can see we have a two-
flowing uh four flowing explain
generative adversarial Network along
with an example suppose there's a wine
shop that purchases wine from dealers
which they will resell later so we have
our dealer point of the wine our shop
owner that then sells it for profit but
there are some malactor dealers who sell
fake wine in this case the shop owner
should be able to distinguish between
fake and authentic wine the forger will
try to different techniques to sell fake
wine and make sure certain techniques go
past the shop owner's check so here's
our forger fake wine shop owner the shop
owner would probably get some feedback
from the wine experts that some of the
wine is not original the owner would
have to improve how he determines
whether a wine is fake or authentic goal
of forger to create wines that are
indistinguishable from the authentic
ones goal of of shop owner to accurately
tell if the wine is real or not there
are two main components of generative
adversarial Network and we'll refer to
as a noise Vector coming in where we
have our forger who's going to generate
fake wine and then we have our real
authentic wine and of course our shop
owner who has to figure out whether it's
real or fake the generator is a CNN that
keeps producing images that are closer
in appearance to the real images while
the discriminator tries to determine the
difference between real and fake images
the ultimate aim aim is to make the
discriminator learn to identify real and
fake images what is an autoencoder the
network is trained to reconstruct its
inputs it is a neural network that has
three layers here the input neurons are
equal to the output neuron the Network's
Target outside is same as the input it
uses dimensionality reduction to
restructure the input input image comes
in we have our Latin space
representation and then it goes back out
reconstructing the image it works by
compressing the input to a lat Pat space
representation and then reconstructing
the output from this representation what
is bagging and boosting bagging and
boosting are Ensemble techniques where
the idea is to train multiple models
using the same learning algorithm and
then take a call so we have in here
where we're bagging we take a data set
and we split it we're going to have our
training data and our test data very
standard thing to do then we're going to
randomly select data into the bags and
train your model separately so we might
have bag one model one bag two model two
bag three model 3 so on in boosting the
empasis is to select the data points
which give wrong output in order to
improve the accuracy so in boosting we
have our data set again we split it to
test data and train data and we'll take
a bag one and we'll train the model data
points with wrong predictions then go
into Bag two and we then train that
model and repeat and with that we have
come to end of this amazing deep
learning crash course I hope you found
it valuable and entertaining please ask
any question about the topics covered in
this video in the comment section below
our experts will assist you in
addressing your problems thank you for
watching stay safe and keep learning
staying ahead in your career requires
continuous learning and upskilling
whether you're a student aiming to learn
today's top skills or a working
professional looking to advance your
career we've got you covered explore our
impressive catalog of certification
programs in cuttingedge domains
including data science cloud computing
cyber secur
AI machine learning or digital marketing
designed in collaboration with leading
universities and top corporations and
delivered by industry experts choose any
of our programs and set yourself on the
path to Career Success click the link in
the description to know
more hi there if you like this video
subscribe to the simply learn YouTube
channel and click here to watch similar
videos to nerd up and get certified
click
here