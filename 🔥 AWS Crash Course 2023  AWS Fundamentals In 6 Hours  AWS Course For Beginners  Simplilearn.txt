Netflix one of the world's largest
streaming platforms relies heavily on
AWS for its infrastructure certainly the
partnership between Netflix and AWS is a
notable example of how cloud computing
can transform how businesses operate and
scale many other well-known companies
and startups also use AWS for their
services on that note if you are looking
to become an AWS Cloud architect to
excel your career then have a look at
AWS Cloud architect Masters program by
simply learn with this certification
course you will learn architectural
principles scalability security and key
components like S3 and cloudformation
become expert in the tools like Amazon
ec2 AWS lambra Amazon dynamodb Amazon
cloud formation and much more our AWS
Cloud architect training is great for
beginners however AWS recommends at
least one year of relevant experience
before pursuing this program so hurry up
and try your hands on this amazing
course use discount coupon YT be15 to
get handsome discount find the course
Link in the description box listen to
what our Learners say about our courses
finishing the course like personal
recognition and open the door for even
more artificial intelligence projects
and it came with significant pay raise
money and hi I'm Phillip I'm 61 years
old and last year's compiled skill was
simpler than a postman program in cyber
security I'm happy to tell you that I
was able to clear and pass my cissp and
ccsp certification exams on the first
before we begin consider subscribing to
Simply learn never to miss any updates
from us over to our AWS experts if you
are looking to learn cloud computing
enroll into professional cloud computing
program in cloud computing and devops by
simply learn and gain hands-on
experience with industry projects
one-year experiences prefer to grab the
course find the course Link in the
description box
when we talk about networking in AWS
essentially we talk about how you can
isolate your resources in the cloud
that's something that is a key concern
or key requirement for all the
organizations that are moving their
workload to the cloud how do I do that
and you can do it using VPC the virtual
private Cloud we'll take a look at
virtual private Cloud as well then you
have something called elastic load
balancing where you can say I have many
applications and I want you to do a load
balancing for those applications and AWS
gives you out-of-the-box functionality a
managed service called elb elastic load
balancing which will do just that AWS
also has something called Route 53 this
is a fully managed highly available
Cloud DNS service and what it does is it
basically acts like a phone directory so
you put your IP address and and put your
website address and it converts that to
the IP address and then brings you to
your source which could be the S3 bucket
or your load balance or really anything
foreign
let's go to console and then take a look
at this thing right now I am in ec2 and
then I am at the load balancers if I
click on the load balancer I can have
two types of load balancer where I have
an application load balancer so think
about this if I am a retail website and
I have something like apparel I have
sports outdoors all of these sections
and I have a social section as well on
my website and I want load for all these
sections to be managed separately and
then what I can do I can come over here
and I can set up some Target groups and
then I can distribute my load for all
these sections of my website so
essentially I want to change the load of
my traffic and I want to divert that
traffic and I can do that all from here
and this again is available for HTTP and
https if you want to do a load balance
again a network Lan like a TCP or SSL
then I'm going to choose the classic
load balancer VPC or virtual private
Cloud again is a place where you can
isolate your resources in Cloud this is
one of the places where you can use a
VPN connection to create an encrypted
ipsec tunnel to talk between your data
center and your resources and clouds so
your VPC can talk to your resources in
your data center and you can do that by
using a customer Gateway at your end
your data center end then use a virtual
private Gateway at your Cloud end
there are firewalls that you can set up
which are called network access control
lists you have subnets which is a
logical grouping of your resources so
you would have resources who would talk
to the internet you would put that in
your public subnet by using internet
gateway attached to that or you can have
subnets that would not be talking to the
internet but would be talking internally
instead compute is one of the foundation
Services of AWS
in fact I was attending one of the
trainings at AWS and they told us that
73 percent of total revenue of AWS is
through ec2
that's a server in the cloud a compute
in the cloud so let's look at four
offerings among many that compute gives
us so the first one is light sale and
there are people who love to build their
servers have their databases put on the
firewall settings and then install a lot
of things but then on the other hand
there are people who simply want
something out of the box and they don't
want to provision a server to do
everything on end and they want it for a
monthly small price for all those use
cases we have something called light
sale which is one of the newest Services
launched by AWS and the plan starts at
about five dollars a month
and it involves the VM the storage the
data transfer you can access search it
from the console and you can have a
static IP or DNS management right from
the console and you don't need to worry
about it
we'll take a look at it in our demo
section then we have ec2 so this is
where you install any application that
you would want this is essentially the
compute in the cloud where they have
virtualized your services and then you
can access it anywhere from the world
using the internet they have multiple
pricing options on demand on reserve on
the spot so if you have your workload
which you know is going to run for a
very long time one year two years three
years you can go ahead pay up front
Reserve that thing and get 75 percent
off they have something called spot
option where all of the unused capacity
that AWS has for compute they put it on
auction and then you can put your price
over there and then you can probably get
close to ninety percent off and then it
is used for large workloads with huge
huge huge capacity
if you want to run microservices you
know that is a native support for Docker
in the cloud then ECS is the service
that does that you don't have to install
it you can operate and scale your own
cluster in the cloud and it's all a
managed Service open to you
now let's say you are focused only on
writing code you are a developer or you
want your team to focus only on writing
code you do not want to worry about the
infrastructure part of your application
then AWS Lambda is the service that is
the function as a service
so you don't have to worry about the
infrastructure at all AWS simply tells
you that you give me the code you tell
me how much memory you are going to need
and all I'm going to bill you for is the
time your function is going to take to
run in the cloud and I think it's an
amazing thing if you've heard about the
term serverless this is what serverless
is
[Music]
so we are going to take a look at the
demo of this service as well so let's go
back to console and see if we can take a
look at ec2 so here I am on the ec2
console and if you see here this is my
region and I am in Us East not in
Virginia region
now we are at ec2 dashboard and let's
look at how we can launch an instance
now before we start let's take a look at
the regions so I am in U.S East Northern
Virginia region and these are all the
regions that you can choose from and
this is where I'm going to launch my ec2
instance the first thing that I'm going
to do when I'm going to launch my
instance on the server in cloud is
choose an image which is known as Amazon
machine image Ami and this is nothing
but the software package so I'll go
ahead and I'll choose the default option
that I get over here and you can see
that it has the command line tools
python Ruby Pearl Java Docker PHP MySQL
so you could have anything that you want
and then I'll go ahead and I'll select
this
once I have selected this I'm going to
choose my instance type on hardware and
you can see that right now I have a T2
dot micro which simply is three-tier
eligible and the network performance is
kind of low to moderate but if I scroll
down you can see the network performance
and everything else goes way up and then
I can go and have a different purpose
compute memory fpga the graphics
anything that I want and this is the
variety of services you get with AWS
next step is to configure my install
details so I could have one or more
instances this is my networking so
essentially where do I want to put it do
I want to put it in my network or the
VPC that I have created or do you want
it in another VPC VPC is nothing but
your isolated Cloud your network inside
the cloud
apart from that I can put something
called Advanced details which is nothing
but boot up JavaScript so if you want
something to run when your ec2 instance
is provision when it starts this is
where you are going to put that so for
the purpose of the demo I'll keep it
where it is and move on to the next step
which is storage
so essentially your hard drive the root
volume is nothing it is the hard drive
where your OS is installed and then I
can add any more hard drives that I want
and then I can change the size of it if
I want it to be encrypted I can do that
as well so let's keep it simple and move
to the next step which is tags now tags
are used for identification purposes
let's say you have many teams there's a
Java team a dark net team and you want
to keep track of who is using all the
resources
so this is where you can add tags that
say name equals darknet team and this is
how you are going to identify who has
launched this thing
next up we have security groups these
are instant level firewalls so firewalls
for your server so typically if you want
to allow an SSH if you want to allow an
HTTP at https access this is where
you're going to go to add the firewall
rules and you can put the source as
anywhere which means anyone from the
internet can access this thing now I do
not want SSH access for everybody so I
just keep it HTTP and https
the SSH access will be essentially for
my people that I want and that's about
it I go and I'll say review and launch
and we can review all of the settings
that we have here over here and we will
simply launch and now it's going to ask
for my keypad this is now something that
is required of me to SSH into this
particular instance and I'll simply say
that yes I have access to this thing and
it will launch the instance
that's about it it's as simple as that
to launch your ec2 instance in cloud and
if I come on the dashboard I can see
that my instance is being provisioned
and I have all the information I need
over here then if I want more
information I can click over here and I
can get all of the information that I
want over here
moreover
I have these status checks so there are
two types of status checks that check
the health of the instance so it will
make sure that everything all of the AWS
resources that are required to keep this
resource up and running are running fine
and it will also see if I have something
that is not required in my instance it
is also there to make it up and running
now that our instance is up and running
I can click on it and I can connect
using the console so I don't have to use
a command prompt I can click on the
console itself and I can do whatever I
want
on the other hand if I take the IP
address and if I go here there it is
out of the box I see my WordPress site
up and running I don't have to do
anything it is right there for me to use
and I can just customize it any which
way that I want let's see if I can have
the SSH window open here and I can just
use the login credentials that I have
there it is I can simply log into this
and start playing around with it I can
also see all the metrics I can see the
networking I can take a snapshot I can
do pretty much whatever I want this is
something that if you have your own
virtual private server in the cloud with
all the resources that you need you can
run your small applications on it you
can run your WordPress site on it you
can do so many things there are so many
use cases that you can use light sail
for it is one of the most popular
compute services that was recently
launched let's look at Lambda so we come
to compute and then we click on Lambda
this essentially functions as a service
you can see that I already have
something that is already there so let's
create a Lambda function we'll take
something that is provided out of the
box for us you just click on this and
then we click on an SNS message so this
is like the messaging service of AWS
simple notification services
and we can enable trigger the SNS topic
is incident response and let's click on
it now you can see I can type my
function name so I'll simply say
my Lambda function
and the language that I'm choosing Edge
node.js I can also have C sharp Java
python
Etc and this is my simple code and then
I have environment variables I can use I
will create a new role and then I will
name my new role
my Lambda
SNS
role
I'll keep everything as it is and that I
would simply say go next then I'll
create a function now this function
would be using some memory and this
function will take some amount of time
to execute that amount of time is what
AWS is going to charge me for
I do not get charged for anything else
so here my Lambda function is ready and
if I click on test
then I can simply say
there it is SNS
say and test
and it should give me something that
says hello from SNS
let's look at this function at the
detail that we got if you see here that
is the duration so it ran for 2.84
milliseconds and then the billing
duration or the build duration is 100
milliseconds the resources configure so
I said that I needed 128 MB of ram to
run this thing and the maximum memory
that I actually used that this function
actually used was 90 megabytes so that
is all I need to do I need to upload my
code and then I need to say how much
memory I'm going to use and it can be up
to 512 MB there's only one more
limitation that says that our function
should not run for more than five
minutes essentially a micro service
would not necessarily run for five
minutes if it's running for more than
five minutes then it's probably the best
idea to break it down into several more
functions we get billed by milliseconds
and the best part is in the first year
you get something like 1 billion
milliseconds available in the first year
go and test this thing this is one of
the most interesting things that you
will find as a developer as an
organization you want your people to
focus on the code you know how easily
you can push your code to production to
the customers and you don't want to
worry about infrastructure so this is
one service that is really worth
exploring let's take a closer look at
light sale the newest addition to
compute now this is one of those things
which will give you a complete package
it starts at less than five dollars a
month so let's create an instance and
right now I am in the Mumbai region and
I can change my region so let's put it
in to Virginia
we can pick up our instance image if I
want only OS operating system I can do
that if I want apps and Os I can do that
as well so I'll keep it to Wordpress
I want to make a WordPress website on my
server I can add any script that I want
to fire at the bootstrap I can also have
the SSH keypad that I will use to log in
from the console for light sale and then
depending on how many resources I need I
can choose my instance plan and it goes
all the way from five dollars a month to
eighty dollars a month if I'm running
some really heavy duty application on it
but because I'm only running my
WordPress website I simply need the
basic one
then I am very good with 512 MB RAM one
CPU and one terabyte of data transfer
then I want to name my instance and I'll
keep it the way it is and here I can
change the quantity as well if I click
on it then that's about it that's about
all I need to do to have my instance up
and running right now it is in a pending
State now this is something that is
really good for the virtual private
server I don't have to worry about ec2
instance I don't have to worry about
security patches I don't have to worry
about the firewalls it's all available
to me in one small box so all I do is
come here I click a few things and my
instance is up and ready
let's talk about databases in AWS there
are a host of options to store your data
by using multiple database services in
AWS so quickly speaking you have RDS the
relational database service that
supports many engines then there's an
offering from AWS that is known as
Aurora
it is a fully managed MySQL compatible
relational database service then you
have a nosql service offered from AWS
that is a fully managed Service as well
it is called
dynamodb we are going to take a look at
that then you have redshift that is a
petabyte level data warehousing service
again it is a fully managed service then
you have elastic cache that supports in
memory cache two very popular engines
such as memcached and redis then you
have a very cool service called database
migration service that basically allows
you to migrate your database from Oracle
to Aurora you can use the service and
then there is no downtime at all when
you use this service
so let's take a look at RDS
dynamodb elastic cache and redshift so
once you're in AWS console you can see
that this is the RDS and it is a
Management Service
which essentially means that batch
upgrade operating systems installation
are all taken care of by AWS then what
we are responsible for is when we store
our data we have to provide permissions
to set up the firewall settings and to
allow access for everything then of
course we are responsible for setting up
the databases but the engine is provided
by AWS
then if I click on this thing let's say
for example I want to fire my SQL
instance and if I click on this thing
I'll just show you how the high
availability as well as the backup is
taken care of by AWS
so I'll select my server my Hardware
that's based on my workload and then I
can do something called multi-az
deployment when I do this it basically
asks if you want to have a master save
configuration a high availability
configuration and when I do that then
yes I do get copies of my database in
the multiple availability zones within
the region and these are synchronized
replicated copies so in the event of
failure I will automatically be switched
onto the other availability Zone that
has the same copy of my database
so let's quickly add the DB identifier
and the username and password so I'll
just call it MySQL for the sake of
Simplicity let me just copy and paste
all these
okay it doesn't like that okay so it
doesn't like my SQL database and let us
copy it all
over
and then let's go to the next step
foreign
now when we go to advanced settings I'll
show you that they have something called
backup and then this backup is
automatically provisioned by AWS and you
can see here that there is a maintenance
window as well if I want I can give a
window then this is where you should
take my backup and the same thing for
the maintenance now these are all the
things that are taken care of by AWS and
in a traditional environment we have to
do all these things ourselves so this is
the beauty of using AWS that we don't
have to worry about the patches we don't
have to worry about the backup we don't
have to worry about the high
availability it's just configured and
all we have to do is set some security
rules some firewall rules and some
accessibility rules for the users coming
back to the nosql offering of AWS so
this is Amazon dynamodb again it is a
very fast and flexible no SQL database
so for all of your iot platforms for
your mobile for your gaming this is the
AWS service that you are going to use
because it is a schema-less database
essentially where you can have a keypad
value and you don't have to worry about
whether I'm going to store one value or
do I have to store values in all columns
as well it deals in something called
read capacity unit and write capacity
unit for your read requirement and For
Your Right requirement and that's how it
works you don't have to worry about the
provisioning of resources it scales
automatically
redshift is a petabyte scale data
warehousing solution and again it is a
managed service so you don't have to
worry about you know the OS level or the
patches the same thing goes with elastic
cache as well it supports the very
popular memcache and redis engines and
it is a fully managed service so you
don't have to worry about management or
scaling of all of the memory in these
engines
now let's look at storage services that
AWS offers
now storage makes up a very big part of
the services that AWS offers in the
cloud they are heavily used by multiple
organizations
individuals consumers around the world
so let's go and look at a few things so
EBS is essentially your block storage
your hard drives they are low latency
and all of your web servers and your
database servers your analytics engines
they all use EBS elastic block storage
let's say you have a requirement to have
shared file storage that is where you
are going to use elastic file storage
and again it provides High fault
tolerance with consistent latencies and
you pay only for what you use so you do
not have to pre-provision it just in the
case of EBS you need to pre-provision it
then you have S3 a simple storage
service and one of the most popular
Services of AWS launched way back in
2006 it is secure it has 11 9's
durability and it scales past tens of
trillions of objects and it has many
many cases most importantly for storage
backup and Recovery tiered archiver
we're going to look at a demo of that
thing and it also acts like a data link
for big data analytics now if we have
something called cold data that we want
to keep for our audit records or some
archival records AWS has something
called Glacier that is very very
economical archival service the rates
start as low as like 4 cents a gigabyte
which is very very cheap you also have
something called storage Gateway where
you apply this virtual device to your
data center and then you can store all
data through the storage Gateway in the
cloud this is one of the very popular
services in the hybrid environment where
you want to keep some data in the cloud
and some data in your data center
there are a host of data transfer
services which help you move your data
to cloud and it can go from a small
amount of data that you can transfer
over the internet to exabytes of data
that you can transfer through truck to a
truck to AWS storage services so you
have something called data connect where
you connect directly to AWS Regional
data transfer you bypass the internet
and then your network is connected then
you have something called snowball which
is like a box where you migrate
petabytes of data in batches to the
cloud then you have something called
snowmobile where you migrate exabytes of
data in batches to the cloud and they
will simply send a truck to your data
center you plug it in the truck will
roll into AWS Data Center and then they
will put data anywhere you want
so let's also take a look at Kinesis
fire hose as well
so you have these Kinesis streams and
you have ingestion process streaming
data your iot data your mobile data your
Twitter feed the click stream data these
are the things you can use Kinesis to
store and process all of that data this
is where you would come in AWS Services
AWS console to request snowball
these are all the things that AWS gives
you to transfer your data again your
data is secure it's encrypted uh it's
addressed in transit and you can access
it anywhere using the internet let's
look at S3 in AWS Management console
S3 is a place which is used for data
leaks for any sort of data that you
would want to upload in Cloud I can very
easily go and I can create a bucket so
let's create a bucket
bucket is where you are going to store
your data alright so I have created
Albert simply learn lab bucket and if I
click on it I can do so many things over
here I can upload any data that I want
over here I can send the permissions of
who all can access this data by default
in AWS it's a shared responsibility
model so I can upload the data and I can
choose who can access this data in S3
there are so many things that I can do
with this bucket that is my folder in
the cloud S3 that is simple storage
service I can enable versioning so I can
keep all the versions of my objects over
here I can set up logging I can set up a
static website hosting we will have a
demo for this static website hosting as
well but we don't have to worry about
this killing the load balancing Etc it
just scales automatically
I did mention that it's a great tool for
disaster recovery and what you can do is
you can set up your backups your
snapshots in S3 you can put all your
data in S3 you can simply enable the
cross region replication so what happens
is that let's say you are in Us East 1
or Northern Virginia region and you want
to set the cross region replication to
Oregon or maybe somewhere in Europe you
can just come over here and you can
enable the cross region replications so
what happens is AWS will automatically
replicate your data from one region to
another of your choice you decide where
you want your data to be replicated AWS
will not do so on its own
there are so many types of things that
you can do you can tag everything you
can give permissions to all your people
there are so many types of permissions
that you can give there is an access
control list you can set up a bucket
policy it's also tightly integrated with
identity and access management so you
can very easily command who is able to
access your bucket apart from that there
are some really interesting things there
is something called life cycle now this
is a tiered storage so normally whenever
you get data typically that data is
going to be very hot so you would Define
the data or classify the data as a hot
data warm data or cold depending on how
frequently it is accessed then you would
normally have a tiered storage so S3 has
four classes that's a standard storage
and infrequent access storage reduced
redundancy store bridge where you would
put normally reproducible data and
archival storage that is Glacier so what
you normally do whenever you get some
data you would keep it in a standard and
you would know over a period of time you
would want to move that into an
infrequent storage like over here so I
will say that move this data into
infrequent storage which offers the same
security same durability but it's a lot
cheaper
okay and then after that I can also add
a data that I want to move into archive
which is Glacier after some time so we
can add it to Glacier after 60 days and
if I just click on next this is where I
will see it and then I can also delete
that object after whatever the amount of
time that I would want and I click on it
you see here that I have set up a
transition rule that says that for the
current version of the object anything
that I add to this S3 bucket I want to
transition all those objects
automatically to standard infrequent
access class
and then I will be charged lesser than
what I've been charged now and then I
also want to transition to Amazon
Glacier that is my archival service
after 60 days I do not want to delete
them ever so these are the things that
you can do with the S3 you also have a
lot of analytics tools which allow you
to see who is accessing your data what
data you have who is adding a lot of
data and these are all available to you
at a very economical rate
all these services are tightly
integrated with each other so you can
plug them in with an ec2 instance or IM
and you can get all the information at
your fingertips
so let's talk about static website
hosting on S3 and this is one of the
very very powerful features of S3 where
we can actually host a website and it's
static and some part dynamic as well so
you do not need a web server to host
your website I have this Bucket over
here AWS agile.n I click on it and you
can see that I have a couple of HTML
pages
error.html and
index.html and then my code my HTML CSS
files my image my video files are inside
the folder so I come over here I click
on the properties and you can see that
the bucket hosting is enabled so if I
click on it all I need is two documents
then whatever I want inside my website
if I simply click on this thing this is
what I get now these are the things that
I have inside my website my folder and
the beauty of it is that you do not need
any web servers you do not need any load
balancer you don't need any firewalls
you can simply give this URL and then
you can map it to your DNS the domain
name service and then anybody will type
your website name or your blog name and
then they are going to land on this page
and it's scalable it is load balanced
and you can scale it to any number of
people you can have millions of hits
coming in and this is one of the very
very very popular Services of S3 let's
look at AWS Marketplace AWS Marketplace
is an online store where you can buy
software that runs on Amazon web
services we've gone through a lot of
services and You Must Be Wondering Where
do I get help where do I buy some some
software that integrates with all these
Services AWS Marketplace is the buying
and selling platform where you can
either buy or sell software that
integrate with AWS Services they are
open source commercial these are for
businesses Enterprises and they're also
for developers and they are available in
a range of categories such as OS
security networking storage business
intelligence devops databases monitoring
logging
Etc
[Music]
let's take a look at AWS Marketplace
so once you come over here you can see
that you will have something called Ami
that is a machine image
and you can also have something for
software as a service
let's look at the devops
most of these are offered as a free
trial as well and if you want to see
just the free versions you can click on
the pricing plan and then you can see
that you get these images of software of
Amazon machine image the Ami
and you can just install it on your
Hardware that you provision from AWS and
that is how you start using these things
and as you can see here there are a lot
of developers tools infrastructure
business software
if you want to make some dashboards if
you want to run some analytics you can
do many of those things and more right
over here
so come over here explore the things you
want to do whether it's security
databases networking or anything else
you might be looking for the main
purpose of configuration management is
to quickly spin up new resources
whenever required you may need to
quickly spin up resources to upgrade to
new instance types
to replace a failed instance or
underperforming instance
for Disaster Recovery
for auto scaling
you may also need to deploy
configuration changes to a fleet of
existing and running instances
logging into each virtual machine and
updating the same configuration files
can take a ridiculous amount of time
so the deployment of the instances must
be automated and repeatable to make the
process easy and convenient AWS supports
a number of Technologies for configuring
and deploying Amazon ec2 instances and
other AWS infrastructure
user data
Amazon machine images or Amis
configuration management tools user data
is basically a command sequence that is
executed at the first launch of the
instance
when you launch an instance in Amazon
ec2 you have the option of passing user
data to the instance that can be used to
perform common automated configuration
tasks and even run scripts after the
instant starts
you can pass two types of user data to
Amazon ec2
shell scripts and Cloud init directives
you can also pass this data into the
launch wizard as plain text file for
launching instances via the command line
tools or as base64 encoded text for API
calls
the Ami provides the information
required to launch an instance which is
a virtual server in the cloud
you can specify an Ami when you launch
an instance and you can launch as many
instances from the Ami as you need
you can customize the instance that you
launched from a public Ami and then save
that configuration as a custom Ami for
your own use
instances that you launch from your Ami
can use all the customizations that
you've made
AWS also offers a variety of its own
configuration management tools and
services which allow you to configure
new instances using reusable templates
and update configuration dynamically in
response to change in this lesson we'll
review some of these tools such as AWS
cloud formation and AWS Ops works and
discuss strategies for combining these
Technologies into a comprehensive
configuration and deployment strategy
AWS cloud formation enables customers to
create and modify collections of AWS
resources using predefined reusable
infrastructure templates
the cloud formation templates provide
ec2 bootstrapping features to help
automate the initial installation and
configuration of the OS and application
stack templates can also be versioned
enabling customers to apply versioning
logic to their AWS infrastructure the
same way they do with software
AWS opsworks is a fully managed
application stack Management Service
that includes provisioning and
configuration of Amazon ec2 instances
Auto scaling application deployment and
application Health monitoring
it also provides the ability to
integrate with additional AWS services
such as elastic load balancing and
Amazon RDS instances
your organization may have decided that
it wants a base set of software included
on all instances launched within its
cloud
this may include homegrown utilities
in-house tools for using AWS services
and advanced software for Enterprise
scale activities such as monitoring and
intrusion detection among other
possibilities
in this circumstance consider using the
base Ami approach with this approach you
can pre-configure all of the software
your organization requires on an Amazon
ec2 instance and then create an Ami from
that instance
the new Ami would then become the amius
to create all new instances within the
organization
organizationally you could enforce use
of the base Ami in a couple of ways
create a custom toolset that becomes the
Gateway toolset for creating AWS
resources and that forces creation of
instances from the pool of custom Amis
create processes that scan a running
Amazon ec2 instances in your account and
terminate any instances that are not
using the standard Amis
third-party configuration tools such as
chef and puppet are used by many AWS
customers to start new Amazon ec2
instances in an automated and repeatable
manner the flexibility of these
environments enables the implementation
of a complete configuration management
system
this screen illustrates one way in which
configuration software can be integrated
within an AWS architecture
let's assume that you want to use
configuration software to initialize a
set of Amazon ec2 instances as web
servers the scenario breaks down as
follows
the administrator sets up a
configuration server this usually is a
standalone Amazon ec2 instance that will
contain a set of templates which
describe all of the applications files
and configuration that a server needs to
initialize itself for example a web
server might instruct an instance to
install Apache httpd configure apache's
httpd config file install all necessary
programming language environments such
as PHP Ruby and create a directory and
file structure for web server content
a configuration server will likely
contain a number of recipes for
configuring dozens of different types of
machines including MySQL server Nat
server Windows IIs server and so on
the administrator will create
configuration templates publish them to
the configuration server and check them
into a source control system for version
management and change tracking
the administrator will use custom
scripting cloud formation opsworks or
similar products to describe and create
a full AWS Cloud deployment environment
these tools may also specify what custom
applications will be hosted in these
environments on which instances they
should be hosted and how they ought to
be configured on each instance
for each Amazon ec2 instance that is
deployed the administrator will instruct
the instance on how to configure itself
as a configuration client of the
configuration server the administrator
will also Supply the name of the
templates to be downloaded and executed
on that machine
when an Amazon ec2 instance is deployed
using a deployment technology such as a
script or a cloud formation it will
configure itself as a configuration
client download the appropriate template
and execute it
various customers use different
combinations of the Technologies
discussed in this topic to manage their
Amis
the solution adopted by your
organization will depend on several
factors including your production
workflow and your organizational
standards
none of these techniques are mutually
exclusive they can all be used in tandem
to craft a strategy that's right for
your company in general placing software
in Amis is best reserved for
foundational slow changing components
that are truly required as standards
throughout your organization
to create an Amazon EBS backed Linux Ami
start from an instance you've launched
from an existing Amazon EVS backed Linux
Ami after you've customized the instance
to suit your needs create and register a
new Ami which you can use to launch new
instances with these customizations
ensure data Integrity by stopping the
instance before you create an Ami then
create the image when you create an
Amazon EBS backed Ami it's automatically
registered for you
Amazon ec2 Powers down the instance
before creating the Ami to ensure that
everything on the instance is stopped
and in a consistent State during the
creation process
some file systems such as xfs can freeze
and unfreeze activity making it safe to
create the image without rebooting the
instance
when you need to automate the creation
of an Ami you can use the AWS ec2 create
image command as displayed on the screen
during the Ami creation process Amazon
ec2 creates snapshots of your instances
root volume and any other EBS volumes
attached to your instance
if any volumes attached to the instance
are encrypted the new Ami will only
launch successfully on instances that
support Amazon EBS encryption
depending on the size of the volumes it
can take several minutes for the Ami
creation process to complete sometimes
up to 24 hours
you may find it more efficient to create
snapshots of your volumes prior to
creating your Ami this way only small
incremental snapshots need to be created
when the Ami is created and the process
completes quickly though the total time
for snapshot creation Remains the Same
after the process completes you have a
new Ami and snapshot created from the
root volume of the instance
when you launch an instance using the
new Ami a new EBS volume for its root
volume is created using the snapshot
both the Ami and the snapshot will
accrue charges to your account until you
delete them
you can copy an Ami within or across an
AWS region using the AWS Management
console the command line or the Amazon
ec2 API all of which support the copy
image action both Amazon EBS backed Amis
and instance store-backed Amis can be
copied you can't copy an encrypted Ami
between accounts instead if the
underlying snapshot and encryption key
have been shared with you you can copy
the snapshot to another account while
re-encrypting it with a key of your own
and then register this privately owned
snapshot as a new Ami
copying a source Ami results in an
identical but distinct Target Ami with
its own unique identifier
in the case of an Amazon EBS backed Ami
each of its backing snapshots is by
default copied to an identical but
distinct Target snapshot the exception
is when you choose to encrypt the
snapshot
The Source Ami can be changed or
de-registered with no effect on the
target Ami the reverse is also true
AWS does not copy launch permissions
user-defined tags or Amazon S3 bucket
permissions from The Source Ami to the
new Ami
after the copy operation is complete you
can apply launch permissions
user-defined tags and Amazon S3 bucket
permissions to the new Ami
the Ami creation process is different
for instance store-backed Amis
first launch an instance from an Ami
that's similar to the Ami that you'd
like to create you can connect to your
instance and customize it when the
instance is set up the way you want it
you can bundle it it takes several
minutes for the bundling process to
complete after the process completes you
have a bundle which consists of an image
manifest and files that contain a
template for the root volume
next you upload the bundle to your
Amazon S3 bucket and then register your
Ami
when you launch an instance using the
new Ami we create the root volume for
the instance using the bundle that you
uploaded to Amazon S3
the storage space used by the bundle in
Amazon S3 incurs charges to your account
until you delete it
you can convert an instant store-backed
Linux Ami that you own to an Amazon EBS
backed Linux Ami this involves creating
a bundle but the bundle is then
downloaded onto a new Amazon EBS volume
after that you need to snapshot the
volume and create your new Ami from the
snapshot
please note you cannot convert an
instant store-backed Windows Ami to an
Amazon EBS backed Windows Ami and you
cannot convert an Ami that you do not
own using a configuration server can
simplify many common administrative
tasks the most common example is
provisioning and de-provisioning user
access to Amazon ec2 instances
say that Joe an engineer has been given
access to multiple Amazon ec2 instances
across the fleet meaning Joe has a
private key that enables him to log in
with his unique user ID and perform
administrative tasks on these instances
but what happens when Joe leaves the
company without a configuration server
deprovisioning Joe's access to these
machines can be a manual nightmare using
a configuration server however a system
administrator can easily change Joe's
status in the system enroll this change
out to all instances in the fleet with a
few simple commands
one of the advantages of using a
configuration server such as Chef puppet
or ansible is that configuration is it
impotent resources that are created or
configured by a configuration server are
configured or created only once if
manual modifications are made to the
configuration of an instance the
configuration server detects them and
rolls them back
this ensures that the Integrity of the
instance isn't compromised by an
accidental change
most configuration software supports a
single machine or Standalone model which
means that it is typically capable of
managing one to hundreds of servers from
a central controller machine hence the
server orchestration turn
in a typical environment you can have a
single administrator machine on which
you create and edit the configuration
templates store them in the source
control system for example git and use
the configuration management tools to
orchestrate your infrastructure
we'll now review some typical failures
that might occur during the launch of
the instances
automated scripts or cloud formation
templates failed to create instances in
different regions
here you need to check the Ami IDs keep
in mind that in different regions you
always have different Ami IDs so you
need to make sure that you set the
correct ones in your configuration
another issue that you might come across
is when the instance is marked ready but
the configuration is not complete keep
in mind that the configuration Readiness
is separate from instance readiness you
need to take additional measures to
check Readiness of your configuration
for example your configuration scripts
can send an alert when the instance is
fully configured another common issue is
when the windows images take a long time
to boot from the Ami
the Microsoft system preparation or
sysprep tool simplifies the process of
duplicating a customized installation of
windows but it can't extend the time it
takes for an image to boot for instance
Readiness note that if you are using
cloud formation templates to deploy
infrastructure cloud formation provides
an object called weight condition that
can be used by user data scripts to
signal to cloud formation when
configuration has completed successfully
the cloud offers you lots of
possibilities but it also brings a lot
of questions about how to manage its
power and flexibility
how do you update servers that have
already been deployed into a production
environment
how do you consistently deploy an
infrastructure to multiple regions in
disparate geographical locations
how do you roll back a deployment that
didn't execute according to plan in
other words how do you reclaim the
resources that were already created
how do you test and debug a deployment
before rolling it out to production
how do you manage dependencies not only
on systems and Technologies but on
entire subsystems
the most commonly used Technologies for
automated repeatable deployments include
custom scripts and applications in which
you can use AWS command line interface
commands or AWS API to automate
deployments in a variety of languages
AWS cloud formation is a building block
service that enables customers to
provision and manage almost any AWS
resource via a json-based
domain-specific language
AWS cloud formation focuses on providing
foundational capabilities for the full
breadth of AWS without prescribing a
particular model for development and
operations
customers Define templates and use them
to provision and manage AWS resources
operating systems and application code
AWS Ops Works focuses on providing
highly productive and reliable devops
experiences for it administrators and
ops-minded Developers
to achieve this AWS Ops Works employs a
configuration management model based on
Concepts such as stacks and layers and
provides integrated experiences for Key
activities like deployment monitoring
Auto scaling and automation
compared to AWS Cloud Nation AWS
opsworks supports a narrower range of
application oriented AWS resource types
including Amazon ec2 instances Amazon
EBS volumes elastic ipts and Amazon
Cloud watch metrics
the cloud simplifies the migration of
systems by making it easier to create
parallel environments there is no need
to buy additional Hardware to stand up a
pilot version of your new environment
you can simply create the resources that
you need however you still need to
figure out how to ramp users off the
existing system and onto a new one a
task that is much more challenging than
a so-called Greenfield or initial
deployment
some common patterns used to bring new
systems online in a cloud environment
include blue or green deployment a
parallel or green system is deployed by
Auto scaling alongside an existing or
blue system
the traffic is gradually shifted onto
the green system we'll discuss this
model in depth a little later
red or black deployment
this is a variation on the blue or green
method used by Netflix in this model the
existing or the red system exists in
parallel to the new system
once the new system is fully initialized
and running the infrastructure is
considered to be in a red red state next
the traffic is cut off to the existing
system putting it in a black state
the system is now in red black mode
the old system is no longer receiving
requests and all new traffic is going to
the new system
the black system is kept up and running
while existing connections are completed
to ensure that the red system is
operating without incident
Engineers can bring the black system
online if required
dark launch used mainly for testing a
dark launch brings new features into a
system without enabling the new feature
in the user interface dark launches are
used to ensure that a new feature works
correctly in production and can handle
the load associated with a company's
existing user base
on this screen an example of a blue or
green deployment is displayed in this
model a parallel environment with its
own load balancer and auto scaling
configurations is brought up side by
side with the existing environment
a feature of Amazon Route 53 called
weighted routing can be used to begin
shifting users over from the existing or
blue environment to the new or green
environment Technologies such as
cloudwatch and cloudwatch logs can be
used to monitor the green environment if
problems are found in the new
environment weighted routing can be
deployed to shift users back to the
running blue servers
once the new green environment is fully
up and running without issues the blue
environment can gradually be shut down
due to the potential latency of DNS
records a full shutdown of the blue
environment can take anywhere between
one day and one week
as discussed earlier one of the
challenges faced while deploying in
cloud is implementing version upgrades
you can solve this problem with the help
of Auto scaling there are multiple
approaches
use the oldest launch configuration
termination policy to drain old
instances old instances will be replaced
with the newer ones launched using the
latest version of launch configuration
bring up two separate Auto scaling
groups and use Amazon Route 53 and
weighted routing to drive different
percentages of incoming traffic to the
two groups
you can start creating new instances and
use AWS Auto scaling attach instances to
bring new instances into group and use
the AWS Auto scaling detach instances
policy to remove existing ones
AWS cloud formation enables you to
create and provision AWS infrastructure
deployments predictably and repeatedly
it helps you leverage AWS products such
as Amazon elastic compute Cloud Amazon
elastic Block store
Amazon's simple notification service
elastic load balancing and auto scaling
to build a highly reliable highly
scalable cost-effective applications
without worrying about creating and
configuring the underlying AWS
infrastructure
AWS cloud formation enables you to use a
template file to create and delete a
collection of resources together as a
single unit or a stack
the two major Concepts in AWS cloud
formation are templates and stacks a
template is a specification of the AWS
resources to be provisioned a stack is a
collection of AWS resources that has
been created from a template you may
provision or create a stack numerous
times
when a stack is provisioned the AWS
resources specified by its template are
created any charges incurred from using
these services will start occurring as
they are created as part of the AWS
cloud formation stack
when a stack is deleted the resource is
associated with that stack are also
deleted the order of deletion is
determined by AWS cloud formation you do
not have to direct control over what
gets deleted when
first we'll be creating an Amazon S3
bucket for using the cloud formation
template
you need to decide in which region you
want to create the template for this
demonstration we'll create the template
in the mubai region
go to the cloud formation service under
the management tools section
click create new stack
on this page you get various options for
designing or selecting a template
we already have a Json code ready for
this template
so let's select the upload a template to
Amazon S3 option
click choose file to upload the file
from your system
select the file from your system to
upload it
it's a simple Json file that creates an
Amazon S3 bucket
click next
provide a stack name
click next
we'll keep all the values as default and
click next
on this page you can review the template
URL and the stack name
click create
under the events tab you can observe the
progression of the events related to
stat creation as you can see the status
is being shown as in progress
click the refresh icon to view the
progression of the steps in the stat
creation process
click the refresh icon again to refresh
The View
as you can see now stack has been
created
refresh the page again
now you can see the status also displays
that the stack has been created which
means that the stack is ready to use the
resources
click the resources tab to view the
bucket that has been created
and as you can see Amazon has provided a
physical ID for the bucket
let's now check out the bucket and see
it has been created correctly let's go
back to the console
click S3 under the storage and content
delivery section
as you can see our bucket is available
click the bucket name as you can see the
bucket is empty
click the properties button to view the
properties of the bucket
as you can see it has been created in
the mubai region
let's now go back to the Management
console and click cloud formation under
the management tools section
we'll now go and delete the stack once
the stack is deleted this bucket will be
automatically deleted
select the stack
click actions and select the delete
stack option
click the yes delete button to confirm
your action
under the events tab you can view all
the events related to the deletion
process
click the refresh icon to get the
updated view every time
as you can see the stack and the bucket
have been deleted
you have learned how to create an AWS
cloud formation template if you are
looking to become
architect to
Cloud architect Masters program by
simply learn with this certification
course you will learn architectural
principles scalability security and key
components like S3 and cloud formation
become expert in the tools like Amazon
ec2 AWS Lambda Amazon dynamodb Amazon
cloud formation and much more our AWS
Cloud architect training is great for
beginners however AWS recommends at
least one year of relevant experience
before pursuing this program so hurry up
and try your hands on this amazing
course use discount coupon YT be15 to
get handsome discount find the course
Link in the description box if you are
looking to learn cloud computing enroll
into professional cloud computing
program in cloud computing and devops by
simply learn and gain hands-on
experience with industry projects one
year experiences prefer to grab the
course find the course Link in the
description box
security is one of the key concerns for
all those who would want to come onto
the cloud it could be an Enterprise it
could be a startup it could be a small
business and they all worry about that
data how secure is the data so let's
spend some time talking about Security
on AWS security at AWS is job zero that
is what AWS talks about that is
everybody's responsibility and it starts
right from the infrastructure we spoke
about regions we spoke about
availability zones so availability zones
are the physical data centers and they
are designed to be fault tolerant they
are designed to be highly available so
they are always in a group of two or
more so that way if one goes down for
any reason we will still have the
services running from the second data
center AWS has something called a shared
security responsibility model
where they say that whatever happens to
the underlying infrastructure they are
responsible for securing that
infrastructure such as the regions the
availability zones The Edge location
they are responsible for that underlying
infrastructure the customers are
responsible for everything that they put
on top of that infrastructure
so the data the application the access
the firewall rules the customer is
responsible for configuring access to
anybody on that infrastructure
by default AWS provides all the tools
and they do not provide any access to
firewalls that we create
any user that we create or any object
that we upload to the cloud
by default it will be private so we have
to explicitly make it public for any
user
we have to give them permission to do
something
otherwise they will not be able to do
anything
whenever we add any data then it is
addressed we have to tell them who can
access it AWS gives you a whole set of
services to secure your data to secure
your applications to secure your
resources such as servers in the cloud
we will also talk a little bit about the
compliance that you need for your
application to be PCI DSS compliant or
if your application deals with the
healthcare industry and must be HIPAA
compliant and you need all those
documents that say that your
infrastructure is HIPAA compliant
there's a place to go and it's a shared
service so you go and you request
something
and you get it
take a look at all that AWS has to offer
when it comes to security
it all starts from the identity and
access management
at the heart of it you would have who is
accessing your system
and then what they are allowed to do in
that system
so we are talking about authentication
and we are talking about authorization
so AWS gives you so many options to
create users you can create rules you
can manage the credentials on your own
or you can have something called
identity Federation so you don't have to
create a login for everybody if you
already have their credentials in your
active directory
or if you want to have them connect with
Facebook ID or Gmail ID or LinkedIn ID
or Twitter ID they should be able to do
that this is known as identity
Federation and it allows you to do just
that they also have something called
roles where the credentials are managed
by AWS and they are rotated internally
so you don't have to worry about
managing credentials or storing them in
the servers or passing them to the Epi
request
they also have something called security
tokens which are nothing but temporary
credentials and you can allow somebody
to access your application or system for
a few minutes maybe 60 minutes maybe up
to a few hours and then these tokens
would automatically expire
they also have something called
multi-factor Authentication
you can have all your users use this
Hardware or software device to
authenticate themselves
you can have the MFA set up on their
phones and you'll be sure that anybody
who is logging in should be in
possession of that phone in order to log
in
similarly for your application they have
something called AWS Waf
that is a web application firewall and
in simply to protect your application
you can configure it to the best of your
choices
and it will simply protect you from any
unwanted traffic that you don't want to
hit your application in similar fashion
they have something called AWS Shield
that is the service to mitigate all DDOS
attacks it comes in two versions a
standard version and an advanced version
and the standard version is free of cost
so it will block most known DDOS and
other attacks
and you can configure it to protect your
Hardware or your resources from any
known attacks
let's say you know you have a lot of
resources in the cloud and you want to
monitor them continuously against any
security threat there's a service called
Amazon inspector where you simply
install an agent and this agent will
keep running we'll keep producing
reports for you on a monthly basis so
the bill is per agent per month and it
is really very economical when you
compare it with other services which are
either paid or open source and not
managed and there's something called AWS
certificate manager
that is a service that simply manages
your SSL and TLS certificate so you can
bring your own certificate and upload it
or you can buy that certificate from AWS
and in both ways you can simply manage
those certificates in one location
AWS gives you that capacity so you don't
have this hassle of managing those
services
now this is a very interesting AWS
artifact that we spoke about
so let's say you need a compliance
certificate that says that the
infrastructure that you're running on is
PCI certified because you are running an
application that takes credit card
information or does some credit card
transactions you could simply come here
and then you can request the artifact
that you want so you have something like
fedramp partner package you have ISO
certification and you can simply request
that certification and you are going to
get that certification over here
so these are the suite of services that
AWS has to offer for security
apart from that there are firewalls at
your instance level at your server level
firewalls at your subnet level there are
encryption Services you can ask AWS to
do their own encryption or you can run
your own encryption algorithms
there are several kinds of credentials
for accessing different services
and I like to think about it like this
there was once a time when people would
not go on the cloud because of security
and now people organizations Banks
government agencies they're moving
towards Cloud because their data their
Network
is much more secure than what they could
have on their own Data Center
let's see the steps to create an AWS
account
the first step is to open the AWS web
page
then click create a free account to
display the sign in or create an AWS
account page
in this page click I am a new user
enter your email ID or mobile number in
the email or mobile number box and then
click sign in using secure server the
AWS servers verify if an account already
exists with the entered email ID if yes
it displays an error message else
displays the login credentials page
in the login credentials page enter the
name
re-enter the previously entered email ID
and the password for your account
then click create account to display the
contact information page
this page gives you the option to create
a company account or a personal AWS
account
for this demonstration we will create
personal AWS account
then enter your full name
and address in the displayed fields
next enter the captcha characters from
the displayed security image
an important step is to open the AWS
agreement and read it thoroughly then
select the AWS customer agreement
checkbox
selecting the check box indicates that
you have read the AWS agreement and you
agree to it then click create account
and continue to display payment
information page
in this page enter the credit card
details and then click verify card and
continue to display the identity
verification page
before displaying the identity
verification page the AWS servers verify
the entered credit or debit card details
by performing a reversible transaction
of a small amount
this page displays your entered mobile
number and is used to verify and confirm
your identity
as you click call me now the AWS system
calls you on the displayed number and
your computer screen displays a
four-digit verification pin
enter the displayed four-digit pin in
your phone using the keypad
once the verification is successful the
page displays a message
identity verification complete simply
click continue to select your support
plan to display the support plan page
page displays and explains about the
three support plans basic developer and
business
for this demonstration we will select
the free or basic plan and then click
continue to display the welcome page of
the AWS services
congratulations you have successfully
created an AWS account somewhere far in
the country a scientist has a lab far in
the woods and at one day he stumbled on
a calculation that he had trouble
solving and he was very certain that his
computer can handle it and he asked his
computer to do the math or the
calculation for him and the computer
thought about it for a while then took
couple of hours in trying to process the
data and eventually at one point it gave
up and died due to low memory and this
old scientist is now clueless about how
he was going to finish the task and
complete his invention and let the whole
world know about it and that's when he
was introduced to this new friend AWS as
you see on the screen AWS was very handy
in replacing his old machines and it was
able to give him the service that he
wanted at that moment AWS has been
around for a while helping people with
their computer needs and now this new
scientist friend in need is welcomed by
the new and trendy ID technology AWS and
sure enough his new AWS friend welcomed
and walked him through the new way of
computing and this new Happy faced AWS
friend talked to this scientist about
AWS ec2 in specific and walked him
through about all the new Innovations in
ID and in Cloud that he had been missing
all these days so his new friend AWS
started the conversation about
explaining how there is no need for any
hardware units no managing Hardware or
provisioning in AWS and secondly it
explained increasing and decreasing the
capacity as per the demand is just a
click away and the best part here is
that there is no upfront commitment and
we only pay for what we have used just
like we pay electricity and water bills
and all this comes with complete control
from our side other words the whole key
for this infrastructure is with us and
as if this was not enough all this comes
with Enterprise grade security that's
beyond imagination and on-premises and
if this tool does not excite you then
this definitely would you can work from
anywhere in the world now this really
made the scientist to get excited
especially when he thought about working
from home working from home there is
nothing like it isn't it now this person
the scientist is not tied up to a
particular environment he can work from
home work on the Fly work from anywhere
still the data and everything else in
his ID environment is secure and safe
now let's move to the next level of
discussion let's talk about what is ec2
and some of the use cases of ec2 and
this use case we're going to talk about
are this architecture we're going to
talk about it notifies a group of users
about a new letter and the components
are resources this architecture would
need be first ec2 instance then SNS a
simple notification service and coupling
is ec2 and S3 service together that's
all it would require to get an
architecture that notifies a group of
users about a new newsletter and now I
think it would be a good time to talk
about what ec2 is AWS offers plenty of
services offered under different domains
like you know compute storage database
migration Network management tools Media
Services security business productivity
application integration a machine
learning game development and lot more
coming up out of which ec2 falls under
the compute capacity so what is ec2 ec2
is a web service which aims to make life
easier for Developers for providing
secure and resizable compute capacity in
the cloud with ec2 it is very easy to
scale up or scale down our
infrastructure based on the demand and
not only that this ec2 service can be
integrated well with almost all the
services in Amazon and out of all this
the best part could be we only pay for
what we use all right let's talk about
this use case a use case here is that a
successful business owner has a bunch of
users and a successful product that's
running and now he has developed few
more products that he thinks will be
very successful that he thinks his
customers are going to like now how can
he advertise his product to his new and
prospective customers or the solution
for this question can be addressed by
AWS in AWS we can use services like
simple notification service SNS and ec2
for compute and S3 for storage we can in
a way integrate them all and achieve
this business use case and that's what
I've got this business owner very chewed
up and now he wants to use the service
and he wants to get benefited from the
service he wants to advertise or he
wants to notify his users every time the
company creates a newsletter alright so
let's talk about about what it would
take to get this environment up and
running or what it would take to connect
the environment and put the applications
on top of it firstly we would require an
AWS account and then for compute
capacity we would require an ec2
instance and here's how we go about
doing it the first thing is to create an
Ami which is Amazon Mission image that's
really the softwares and the application
packages we would need to run our
application and the second is to choose
the hardware in here it's the instance
type depending on the workload we would
be choosing the hardware and depending
on the intents of the workload we will
be choosing the size of the hardware and
finally we would configure the instances
and how many instance do I want you know
which subnet do I want them in and
what's going to be the in a stop or
terminate a behavior of the instance and
do I want to update any patches when the
instance starts running all those piece
of information go in here when we
configure the instance and then the
first three steps is really about the OS
volume and the Basic Hardware now it's
time to add additional storage to the
ec2 instance that would be step four
here we add additional storage to the
ec2 instance and then tags we use tags
or we would configure tags to easily
identify an ec2 instance at a later
point you know we give it some
meaningful names so we can identify like
you know which team it belongs to which
billing department it belongs to what's
the purpose behind launching this
instance stuff like that in an
environment where we run 700 to 800 or
even more instances identifying an
instance and trying to understand you
know who owns the resource for what
purpose we created it could be an
full-time work so tagging comes to a
rescue at that time after tagging as
step 6 we would configure the firewall
which is also called Security Group for
the ec2 instance and this is where we
would allow or deny connection from
external world to this particular ec2
instance well it works both ways from
outside and from inside out this
firewall blocks the connection based on
port number and IP address and finally
as step 7 we review all the
configurations that you have done and we
make sure that the configurations is
what we wanted and finally click on
submit that's going to launch an ec2
instance all right this was just an
overview of how to create an ec2
instance now let's talk about each and
every step in detail so to begin with
let's talk about how to create an Ami
well the Ami is just a template a
template that's used to create a new
instance or an a new computer or a new
VM or a new machine based on the user
requirement the things that go into an
Ami are the software the operating
system the additional applications that
get installed in it stuff like that the
a Ami will also contain software
information you know information about
operating system information about
access permission information about
volumes they all compact in the Ami
again the Ami is of two types one is
predefined Amis are called Amazon
provided Amis the other one would be
custom Amis the Amis that we create and
if you're looking at a particular Ami
that you don't want to create but still
want to get it from Amazon there is a
place or a portal called Ami Marketplace
there we get like thousands of Amis in
there available for us to shop and use
them on a pay-as-you-go business model
and use them as pay as you go billing so
there you can search Ami that you're
looking for most probably you'll be able
to get it there now let's talk about
choosing the instance type the instance
type is basically the hardware
specification that's required for a
machine that we're trying to build and
the instant types is categorized into
five main families they are to begin
with it's computer optimized now compute
optimize gives us lots of compute power
a lot of processing power so if my
application is going to require a lot of
processing power I should be picking
compute optimized instance and the
second one is memory optimized now this
is very good for application that
require in-memory caching you know there
are some applications that performs well
with cash or through cash or the
application would create a lot of data
that it wants to keep in cash for
re-reading or for processing you know
for lengthy processing stuff like that
for those type of application this
memory optimized instance that comes
within memory cache is a very good use
case and the third one is the instant
that comes with the GPU otherwise called
GPU optimized GPU stands for graphical
process unit and this is very good for
application that deals with gaming this
is very good for application that's
going to require large graphical
requirements and storage optimized is
the fourth option just like the name
says this is a very good use case for
storage servers and the fifth family
type is general purpose just like the
name says it's for general purpose if
you're not particular about the family
then you generally would end up picking
the general purpose because here the
services are sort of equally balanced
you know you'll find a balance between
the virtual CPU and the memory and the
storage and the network performance it's
sort of balanced all the components all
the features that needs to go on a
computer are sort of balanced in general
purpose now these instant types are
fixed and they cannot be altered because
it's Hardware based we buy Hardware we
do not have much control on the hardware
that's being used well we have options
but we do not have control on the
hardware and these instant types are
divided into five main families they are
computer Optimum minimized memory
optimized GPU enabled storage optimized
and general purpose then as third thing
we have to configure the instance now
here is where I have a lot of options
about purchasing you know what type of
purchasing do I want to do do I want to
go for a spot instance do I want to go
for a reserved instance do I want to go
for an on-demand instance these are
different billing options available and
that's available under configure
instance not only that here's where I'm
going to put the ec2 instance do I want
an public ipaders assigned to it do I
want an IAM role attached to it IM role
is authentication what kind of
authentication am I going to provide and
the shutout Behavior the Sharon
behaviors include do I want to stop the
instance when the user shuts down the
machine from the desktop or do I want to
Simply terminate this instance when the
user shuts down the instance from the
desktop so those things go in here just
like the name says configure instance a
lot of instance configuration options
comes in here that's the third step and
not only that under the advanced details
or Advanced tab under configure instance
I can bootstrap the instance with some
scripts now bootstrap is nothing but the
scripts that you want to be run in the
instance before it actually comes online
let's say you're provisioning the
instance for somebody else you know
instead of you launching the instance
and then logging in and running some
commands and then handing it over to the
other person you can create bootstrap
shell scripts and you know paste it in a
console option available under configure
instance and Amazon is going to take
those commands run it on the instance
before it hands over to the user that
initially requested for that instance
now it could be a different user or just
you it sort of automates you know
software installation procedures in the
instance that we will be launching and
not only that there are multiple payment
options available under configure
instance the user can pick an instance
under normal price and that instance
would apply normal rates applied to it
and there are also options like reserved
instance where the user can pay for an
instance upfront before a year or before
months you know for a span of year or a
span of months and that way they can pay
less per hour for using that instance
not only that you can also go for spot
instance like bidding for those
instances whoever bids more they get the
instance for that particular time well
these instances are a lot cheaper than
on-demand instances and through bidding
and buying you can keep the instance as
long as your bid price doesn't exceed
the price that Amazon is proposing and
as the fourth step we will have to add
storage to the instance that we are
about to launch and here we have bunch
of storage options I can go for a
permanent storage which is free or I can
go for an external elastic block storage
also called EBS which is paid and it's a
permanent storage or else I can
integrate my ec2 instance with S3 for
its storage needs and the best part
about storage is a free subscription
users they get to use 30 gigabit of SSD
storage or magnetic storage for the
whole year in this page where we are
ready to add storage we will have to
mention or provide the size in gigabit
and the volume type is it going to be an
provisioned volume is it going to be an
general purpose volume is it going to be
a magnetic volume stuff like that there
are volume types and we also need to
give inputs about where the disk will be
mounted and whether this volume needs to
be encrypted or not so all these options
are all these inputs are received from
us under the adding storage section and
then the fifth option would be adding
tags like we this discuss some time back
tags are very helpful to identify a
machine in an environment where we have
700 or 1000 VMS running and security
groups are the actual firewall that sits
in front of ec2 instance and it protects
that ec2 instance from unintended
inbound and outbound traffic now here is
where I can fine tune the access to my
ec2 instance based on port numbers and
based on IP address from which it can be
accessed and finally we get to review
the whole changes or the whole
configurations that we have made to find
out whether they are intact with the
requirement and then click on submit
that's going to launch an ec2 instance
but hold on we're not done yet when
we're about to launch or before the
Amazon console actually launches the ec2
instance it's going to give us an option
to create a keypad remember I said it's
key pair you know key pair is two things
one is public and private the private
key is downloaded by the use user and is
kept with the user and the public key is
used by Amazon to confirm the identity
of the user so just go ahead and
download the private key and keep it for
yourself and this private key gets
downloaded as a DOT pem file it's a
format of the file and it gets
downloaded as dot pem file and our next
step is to access the ec2 instance and
because the instance that we have
launched in this example let's assume
it's Linux instance and that's going to
require a tool called putty to be able
to access it and this putty tool is
really needed when we are trying to
access a Linux instance from Windows
instance most of the time Windows
instance will have put the install in
them but in some rare cases they do not
come with putty in those cases we can go
ahead and download putty and putty
generator and we can start using it to
access the Linux instance now you might
ask well I understand put you what's
putty generator now the file that we
have downloaded is in dot pem format but
unfortunately putty does not accept dot
pem format as input you know it has to
be converted into a different format
called PPK and puttygen is a tool that
helps us to convert the dot pem file
into PPK file so as a quick way to do it
is download generator open it up click
on conversion and insert the dot pem key
that we have downloaded and save the
private key and this when we save the
private key it gets saved as a DOT PPK
type private key and when that's done
the next very step is to open putty and
try to log in and the way to log in is
to open putty put that IP address here
and then click on auth you know this is
where we would input the file that we
have created so click on op and then
click on browse and find the dot PPK
file that we have converted and stored
browse it and uploaded and then click
con open now that's going to open up a
login screen for us and the Amis comes
with a default username depending on the
Ami that we have picked the username
might differ in our case we have picked
an Ami for which the username is ec2
hyphen user and this is the default by
the way let's put the username ec2
hyphen user and hit enter and that's
going to open up the Linux instance
using CLI there are a few other things
that we can do with the terminal that
will explain it a little later alright
so we have successfully launched an ec2
instance and yeah give yourself a pat on
your back launching an instance was just
one part of the solution so let's
actually talk about how we can notify
our customers SNS or simple notification
service is a service or a product in
Amazon that helps us to notify customers
through email so navigate to SNS in your
account and create a topic and we're
going to use this topic for public
notification so let's make it public and
then add subscribers to it now these
subscribers these subscribers are the
people who you want to be notified about
the newsletter so we already have the
email database in there add them to the
subscribers list and then they will
start getting new newsletters as and
when we post them to the topic and as
Next Step create a bucket in S3 where
you can store content and in that bucket
create an event that triggers a
notification to simple notification
service so this is how it will be set up
and notification will be sent to our
subscribers anytime we put something in
our S3 bucket so S3 bucket is going to
create an event for the notification and
the notification is going to make sure
that it's delivered to your end Customer
because they're already subscribed to
the topic as subscribers and finally
let's connect the S3 e with ec2 so the
bucket and the AWS instance are in sync
so we put some content in the S3 bucket
our email system notifies our customers
and the customers can go online to a
website that's hosted in ec2 and because
S3 and ec2 are in sync the items that we
put in S3 will also show up in ec2 see
how this is all connected and it's
working together and once this is all
connected our subscribers will regularly
be informed anytime we put new content
in the S3 bucket and the same content
will be made available in ec2 instance
through the website so what are we going
to learn are we going to learn about
what is cloud storage in general and
then the types of storage available in
general in the cloud and how things were
before anybody adopted using S3 that's
something are we going to learn and then
we're going to immediately dive into
what is S3 and then the benefits of
fusing S3 over other storage and then
we're gonna do a couple of labs or
console walkthroughs about what is
object and how to add an object what is
bucket how to create a bucket stuff like
that and then we're going to talk about
how Amazon S3 generally works now it
comes with a lot of features it comes
with a lot of Promise how does this all
work how does Amazon able to keep up
with the promise we're going to talk
about that and then we will talk about
some features add-on features that comes
along with the Amazon S3 so what is
cloud storage in general cloud storage
is a service that provides web services
where we can store our data and not only
that the data can be easily accessed and
the data can be easily backed up
everything over the internet in chart if
you could store your data if you could
access the data if you could backup your
data everything you do through the
internet then that's a good definition
for cloud storage and additional
positions are in cloud storage we only
pay for what we use you know no
commitment no pre-proitioning is you
know pay as you go type subscription and
the best part is we pay on a monthly
basis you know we don't rent a hardware
for the year or we don't give commitment
for the whole year it's pay as you go
and pay on a monthly basis and these
Cloud storages are very reliable meaning
once you put the data in it it's never
going to get lost and these Cloud
storages are scalable assuming I have a
requirement to store 100 times of what
my actual data sizes and I want it now
it's available in the cloud and these
storages are secure as well because
we're talking about data data virginity
they need to be secure and Amazon
provides tools and Technologies through
which we can secure our data and these
are generally not found in the
on-premises storage system so let's talk
about the different types of storage in
general so S3 is cloud storage in AWS
and then we have elastic Block store now
elastic Block store is actually the SSD
hard drives that gets attached to our
ec2 instance you know it's like the C
drive it's like the D drive it's like
the e Drive that gets attached to our
instances now EFS is elastic file system
the underlying technology is kind of the
same but it differs from EBS in a way
that EBS can be accessed only by the
system that's attached to it meaning the
e- volumes and the d-volumes we spoke
about they can be accessed only if there
is an instance connected to it but these
EFS are actually shared file systems
elastic file system all right they are
shared systems they can be accessed by
multiple systems they can be accessed
from inside the Amazon environment it
can be accessed from on-premises
equipment as well a glacier is actually
the archiving solution in the cloud if
you want to dump a data and try to keep
them in the low cost as possible then
Glacier is the product we should be
using and then we have storage Gateway
if I want to safely move my data from my
local environment to the cloud
environment and also want to keep a copy
of the data locally so users locally can
access them and in a cloud users or
Internet users can access the data from
the cloud if that's your requirement
then storage Gateway is the one we would
be choosing and then we have snowball
snowball is really an data Import and
Export a system but it is actually an
Hardware that gets shipped to our
premises where we can copy our data into
it and then ship it back to Amazon and
Amazon would copy the data into whatever
destination we give them in our account
and if the data is really huge then I
can call for a snowmobile which is
actually a data center on a truck where
Amazon would send me a truck loaded with
the data center as you see that has
compute capacity lots and lots of
storage capacity and electricity AC and
a lot more so they get comparked near
our data center and cables run into our
data center we can copy data into it
send it back to Amazon and they would
copy it to our account in whatever
storage that we advise them so if the
data is really really huge then I would
be calling snowmobiles snowmobile is not
available in all the regions all right
let's take this example how things were
before S3 was introduced you know two
professionals are having a conversation
and one of them is finding it very
difficult to sort of manage all the data
in the organization well if the data is
small it can be easily managed but as
company grow and we are living in an era
where data is everything we want data to
backup every idea we want data to backup
every proof of concept that we provide
so data is everything so in this era
it's all about collecting data analyzing
them and saving you know not losing logs
you know saving them analyzing stuff
like that so coming back to our
discussion here one person finds it very
difficult to store and manage all the
data that they have so some of the data
that this person is having problem
storing is data stat application used to
running and then datas that gets sent to
the customers and data us that the
websites require the data that are
because of the email backups and a lot
more are the storages that an Enterprise
can have and this person is having
problem backing up all those data and
even if we think of increasing the local
storage capacity that's going to cost
the fortune and few things that make it
sometimes impossible to increase the
storage capacity in-house is you know we
will have to go and pay heavy to buy
hardware and software to run the
storages and we need to hire a team of
experts for maintaining them the
hardware and the software and anytime if
there is a dynamic increase in the
storage capacity the on-premises or
in-house Hardwares won't be able to
scale and data security data security is
very costly when it comes to building
our own storage environment in-house and
adding data security on top of it so the
other guy in the conversation was sort
of quietly listening everything the
manager was saying and then he slowly
introduced him to S3 because he knew
that all the problem that this manager
was worried about can be solved to S3 in
other words all the scalability all the
data security all the not being able to
provision hardware and software
components are all available with S3 so
that actually brings us to the
discussion about what S3 is S3 is simple
storage service it actually provides an
object storage service let me talk to
you about object and block storage
object storage is where you can store
things in the drive all right you can't
install anything in it and this object
storage can be accessed directly from
the internet whereas block storage is
something that needs to be attached to
an instance and we can't directly access
it but we can install software in it so
that's a high level difference between
object storage and block storage and S3
is an object storage what does that mean
we can store data from the internet we
can retrieve from the internet but we
can't install anything in S3 all right
so so S3 is an object based storage and
it's it's really built for storing and
recovering or retrieving any amount of
data or information from anywhere in the
internet few other things you need to
know about S3 is that this S3 is
accessible through the web interface the
storage one type of accessing or one way
of accessing S3 is by dragging dropping
content into it and another way of
retrieving data from S3 is go to a
browser click on download that's going
to let you download any content and the
data can be five terabytes in size now
we're talking about one file you know
you can have hundreds or thousands of
files like that one file can be as big
as five terabytes in size and S3 is
basically designed for developers where
they can push logs into S3 or drive logs
anytime they want instead of storing
locally in the server they can use S3 as
code repositories you know where they
can save the code and have the
applications read the code from there
and lot more if they want to safely
share the code with another person with
a lot of encryption and security added
on top of it that's possible as well so
there are few things about S3 and on top
of all this S3 provides 11 9 durability
and four nine availability meaning
durability is if I store the data will
it get lost Amazon is like no it's not
going to get lost you're going to have
the data because we provide 11 9
durability for the data and availability
is if you want the data now will you be
able to show it Amazon is like yes we
will be able to show it we have 99.99
availability and when you request the
data we will be able to show the data to
you all right so let's talk about the
benefits of S3 S3 is durable as we saw
it provides 11 9 durability S3 is low
cost out of all the storage options in
Amazon S3 is the cheapest ns3 is very
scalable like we were saying there
there's no required to pre-provision a
storage capacity you know if you need
more go ahead and use more if you need
even more go ahead and use even more and
once you're done some data needs to be
removed just remove the data so that
particular month you would be paying
less so it's very scalable in nature and
it's very available as well S3 isn't
regional service you know it's not based
on one availability zone so one
availability is on going down with an
Amazon the whole availability going down
it's not going to affect your ability to
access S3 storage and S3 is secure a lot
of security features like bucket policy
a lot of security features like
encryption and then MFA authentication
are possible with S3 that actually adds
a very good security layer on top of the
data that we have stored in S3 and not
only that this S3 is very flexible in
terms of cost flexible in terms of where
I want to store the data in terms of
cost there are a lot of pricing tiers
within S3 S3 itself is a cheap service
now within that we have a lot of price
and tiers depending on the durability so
I can always choose to put data on a
different storage tier or storage option
in S3 we're going to talk about it as
you stick along and in terms of
flexibility in region I can always
choose any of the region available in
the console or in the S3 console so put
my data to there is no restrictions on
where I can or cannot put the data in
the cloud as long as there are regions
available for me to move the data to and
data transferring with S3 is very simple
all I have to do is browse to the bucket
upload the data and the data gets
uploaded and we can also upload data
using CLI commands are very similar to
Linux commands are very similar to what
we would use in the Run command prompt
in Windows and what we would use in the
Run command prompt in the Powershell all
right let's talk about a basic building
block of S3 which is bucket and objects
now what's a bucket what's an object
object is the actual data and bucket is
the folder where the objects get stored
let me give you a new definition for
object so object is the actual data plus
some information that reference the data
like is it an JPEG file the name of the
file and at what time it was added to so
they're called metadata right so object
is actually data plus metadata and
bucket is actually a container that
receives the data and safely stores in
it and when we add a data in a bucket
Amazon S3 creates an unique version ID
and allocates it to the object so we can
easily identify it at a later Point let
me show you a quick lab on S3
foreign
console if you're wondering how I
reached here go to Services Under
storage and S3 is right here
and let's create a bucket called simply
learn
now the bucket names will have to be
unique
so I really doubt if simply learn will
be available let's let's check it anyway
all right it doesn't seem to be
available so
um
let me pick another name or let's call
it simply learn Dot
samuel.com
and I'm gonna put this in
Mumbai or I can choose Oregon
let me choose Oregon
yes let me choose Oregon
and let me create a bucket
sure enough a bucket got created let me
upload an object into the bucket
and you know these objects can be as big
as
five terabytes we talked about it right
all right let me upload an object
all right so that's my object so you get
the relation right here is a bucket
right here's my bucket and within that
is my object now object is the actual
file plus what type of file it is and
then
and then the size of the file the date
in which it got added and the storage
class it is in at the moment
so if I have to access it
I can simply access it through the
internet
so let's talk about how does this S3
bucket work anyway all right so how does
it work a user creates a bucket they
will specify the region in which the
bucket should be deployed we had an
option we could have chosen to deploy in
all the regions Amazon provides S3
service you know beat not Virginia beat
Mumbai B Tokyo beat Sydney uh beat uh
Oregon and we chose Oregon to be the
destination region where we want to
create bucket and save our data there
and when we upload data into the bucket
we can specify three types of storage
classes in our case we pick the default
which was a S3 standard as we saw on the
object data it was on S3 standard so
that's the basic thing later once the
object gets added we can always add a
bucket policies you know policies Define
who access it and who should not access
it what can they access are we going to
allow users only to read or to read
write as well stuff like that so that's
bucket policy we're defining the life
cycle or the lifespan of the data in the
S3 bucket now over the time do you want
the data to automatically move to a
different storage tier and at any point
do you want to sort of expire the data
you know get the data flushed out of
your account automatically you know
those things can be configured in
lifecycle policies and Version Control
is creating multiple versions if you're
going to use S3 for a code repository
creating multiple versions let's say if
you want to roll back to whatever you
had before a month how are you going to
do it if you kept updating the file and
never took a version of it so Version
Control helps us to keep different
versions and it helps us to roll back to
the older version anytime that is a need
so let's talk about the different
storage classes in S3 the different
storage classes in S3 begins with S3
standard now this is the default and
it's very suitable for use case is where
you need less or low latency for example
if you want to access the data of a
student's attendance you would retrieve
them very quickly as much as possible so
that's a good use case to store data in
S3 let's understand the different
storage classes in Amazon S3 now let's
take a school for example and the the
different data is present in a school
and the features of those data the
validity of the data all right so let's
take this example and there are
different storage options in S3 let's
take S3 standard for example and what
would be the actual candidate data that
can be stored in S3 standard let's talk
about that so S3 standard in general is
suitable for use cases where the latency
should be very low and in here the good
example are the good candidate file that
can be stored in S3 is a data that needs
to be frequently accessed and that needs
to be retrieved quickly something like
students attendance report or students
attendance sheet which we access daily
and then it needs to be retrieved
immediately as and when we need it the
other type of storage tier in S3 is
infrequent access or in frequent data
access just like the name says the use
case for that is less frequently
accessed data I mean the candidate data
that should go in infrequent access data
is a student's academic record you know
which we don't need to access on a daily
basis but if there is a requirement we
might want to go and look at it then
that's going to be quick so it's not a
data that we would access on a daily
basis but it's data that needs to show
up on your screen real quick and the
third option is Amazon Glacier now
Glacier is really an archival solution
in the cloud so for archives high
performance is not a requirement so
let's talk about some candidate data
that can go into archives something like
students admission fee and it's not
critical also now anytime if you want to
look at the data you can always wait to
retrieve the data so in other words you
know put it in the archive and
retrieving from the archival takes time
so a student's old record are a good
candidate to be put in the archives the
other options are one zone IA storage
class where the data is infrequently
accessed and the data is stored in a
single availability zone for example you
know by default Amazon stores data in
multiple availability zones and there is
a charge for that now it's not an option
but the charge includes storing data in
multiple availability zones but if your
data requirement is you want to keep the
charges in a low even further you can
choose a once own IA storage class where
it stores data in one availability Zone
and the candidate data that can go in a
one zone in frequent access is students
report card and the other option with
Amazon S3 is standard reduced redundancy
storage it is suitable for cases where
the data is not critical and the data
can be reproduced quickly for example
you know take a copy of the library book
take a copy of the PDF library book for
example now we would have a source PDF
and we would make copies of it and then
we make it available for the readers to
read the other option in S3 is reduced
redundancy storage here the use case is
data that's not critical and a data can
be reproduced quickly for example a
books in the library are not that
critical and we always have a copy of
the book we're talking about PDF so if
the customer facing or the student
facing book gets deleted I can always
copy the same PDF put it in the
destination folder and make it available
for the users to read that would be a
very good use case for reduced
redundancy storage all right let's
summarize everything that we learned
about different storage options in S3 so
S3 standard it's for frequently accessed
data it's the default storage if you
don't mention anything the data gets
stored in S3 standard it can be used for
cloud applications you know content
distribution gaming applications big
data analytic Dynamic websites they are
a very good use case for S3 standard
frequently accessed data the other one
on the contrary is S3 standard
infrequently accessed data just like the
name says the use case is this is good
for data that will be less frequently
accessed and and then the use case are
it's good for backups it's good for
disaster recovery and it's good for
lifelong storage of data Glacier on the
other hand is very suitable for
archiving data which is in frequent only
accessed and the Vault lock feature is
the security feature of the glacier that
also provides a long-term data storage
in the cloud this is the lowest storage
tier within S3 the other options are one
zone in frequent access storage class
just like the name says it's
infrequently accessed and it is stored
in just one availability Zone and use
cases are any data that doesn't require
any high level of security can be stored
here in one zone the fifth storage tier
is reduced redundancy storage this is
good for data that's frequently accessed
it's good for data that is non-critical
and that can be reproduced if it gets
lost and reduce redundancy storage or
RRS is and highly available solution
designed for sharing or storing data
that can be reproduced quickly all right
let's compare and contrast couple of
other features that are available in S3
for example durability availability SSL
support burst bike latency and life
cycle management so in standard the
durability is 11 9 durability it's the
same for standard standard IA one zone
IA Glacier except for reduced redundancy
the durability is 11 9 and availability
of all the storage classes is all the
same except for one zone IA where the
availability zone is 99.5 percentage all
of these products support SSL connection
and the first byte latency of these
products are most of them provide access
with millisecond latency except for
Glacier it provides retrieval time of
couple of minutes to a maximum of hours
and all of them can be used for a life
cycle management you know moving data
from one storage tier to another storage
here that's possible with the all of
these storage options all right now that
we've understood the different types of
storage options available in S3 let's
talk about some of the features that are
available on S3 lifecycle management now
lifecycle management is a service that
helps us to define a set of rules that
can be applied to an object or to the
bucket itself lifecycle is actually
moving the data from one storage tier to
another storage tier and finally
expiring it and and completing the life
cycle of the object with lifecycle
management we can manage and store our
objects in a very cost effective way it
has two features basically transition
actions and expiration actions let's
talk about transition actions with
transition action we can choose to move
the objects or move the data from one
storage class to another storage class
with lifecycle management we can
configure S3 to move our data from one
storage class to another storage class
at a defined time interval or at a
defined schedule let's talk about
transition actions in more detail let's
say we have our data in S3 at the moment
and we haven't used the data for quite
some time and it's that's how it's going
to be for the rest of the time so that
data is a very good candidate to move to
the infrequent axis because S3 standard
is a bit costlier and has three
infrequent access is a bit cheaper than
S3 standards so the kind of view set
sort of fits very well for moving that
data into infrequent access so using
lifecycle transition or lifecycle
management I can move the data to S3
infrequent access after 30 days and
let's say that the data stayed in
infrequent access for 30 more days and
then now I realize that nobody is
looking into the data so I can find an
appropriate storage here for that
particular data again and I can move it
to that particular storage which is
Glacier so in this case after 30 days or
in a total of 60 days from the time the
data got created the data can be moved
to Glacier and what is this really help
us with the life cycle management help
us to automatically migrate our data
from one storage cost to another storage
cost and by that it really helps us to
save the storage cost lifecycle
management can also help us with object
expiration meaning deleting the object
or flushing it out after a certain
amount of time let's say that our
compliance requirement requires that we
keep the data for seven years and we
have like thousands and thousands of
data like that it would be humanly
impossible to check all those or keep
track of all the dates and you know when
they need to be deleted stuff like that
but with lifecycle management it is very
much possible I can simply create a data
and set up lifecycle management for the
data to expire after seven years and
exactly after seven years the data is
going to expire meaning it's going to be
deleted automatically from the account
all right let me show you a lab on
lifecycle management and let me explain
to you how it's actually done so I'm
into the bucket that we have created and
here's a data into the bucket assume we
have thousands and thousands of data in
the bucket and that requires to be put
in a different storage tier over time
and that requires to be expired after
seven years let's say so the way I would
create lifecycle management is go to
management from inside the bucket and
click on life cycle and then add and
lifecycle rule just give it a name name
like expire so all the objects that's
present in this bucket meaning the
current version of it set a transition
for it so the transition is at the
moment they are in S3 so here I would
like to put them in infrequent access
after 30 days and then after it's been
an infrequent access for 30 days I would
like to move it to Glacier all right
plus 30 days so how do you read it so
for the first 30 days it's going to be
in Glacier so how do we actually read it
after I put the data in S3 the data is
going to get moved to standard IA and
then it's going to stay in standard IA
and after 60 days from the data creation
it's going to get moved to Glacier so on
the 31st day it's going to move to
standard IE on the 61st day it's going
to move to Glacier let's say if I want
to sort of delete the data if I want the
data to get deleted automatically after
seven years you know being in a glacier
how do I go about doing it let me open
up a quick calculator
365 into 7 that's
2555 days right after that the data is
gonna get deleted pretty much I have
created a life cycle so after on the
31st day it's going to get moved to
infrequent access and on the 61st day
Glacier and after seven years is over
any data that I put in the bucket it's
going to get deleted all right let's
talk about bucket policies bucket
policies are some permission files that
we can attach to an a bucket that allows
or denies access to the bucket based on
what's mentioned in the policy so bucket
policy is really an IM policy where you
can allow and deny permission to an S3
resource with bucket policy we can also
Define security rules that apply to more
than one file in a bucket now in this
case you know we can create an user or
let's say there's already a user called
simply learn we can allow or deny that
user connection to the s 3 bucket or
connecting to the S3 bucket using bucket
policy and bucket policies are written
in Json script and let's see how that's
done all right there are tools available
for us to help us create bucket policies
what you're looking at is a tool
available online that helps us to create
a bucket policy so let's use this tool
to create a bucket policy I'm going to
create a deny statement I'm going to
deny all actions to the S3 bucket and
what is the Arn to which we want to
attach the Arn of the resources actually
the name of the bucket but it really
expects us to give that key in a
different format so the Arn is available
right here copy bucket Arn so this is
actually going to deny everybody now we
wanted to deny a user just one user now
look at that now we have a policy that
has been created that's denying access
to the bucket and it's denying a user
called simply learn pretty much done so
I can use the policy and go over to
bucket policies so once I save it only
the user calls simply learn won't have
access to the bucket and the rest of
them will have access to it so once I
save it it gets added and only the user
simply learn will not have access to the
bucket because we're denying them
purposefully
the other features of S3 include data
protection we can protect our data in S3
with now one of which is bucket policy I
can also use encryptions to protect my
data I can also use IM policy to protect
my data so Amazon S3 provides a durable
storage not only that it also gives us
unprotected and scalable infrastructure
needed for any of our object storage
requirements so here the data is
protected by two means one is data
encryption and the other one is data
versioning data encryption is encrypting
the data so others won't get access or
even if they get access to the file they
won't be able to access the data without
the encryption key and versioning is
making multiple copies of the data so
let's talk about them in detail what's
data encryption now data encryption
refers to protecting the the data while
it is being transmitted and protecting
the data while it is at rest now data
encryption can happen in two ways one is
client encryption encryption at rest and
server side encryption encryption that's
in motion client-side encryption refers
to when client sends the data they
encrypt the data and send it across to
Amazon and server side encryption is
when the data is being transferred they
get encrypted and stay encrypted
throughout the transfer versioning is
another security feature let I mean it
helps us so our unintentional edits are
not actually corrupting the data for
example let's say you edited the data
and now you realize that the data is
incorrect and you want it all back now
how do you roll back without versioning
it's not possible in other words only
with versioning it's possible so
versioning it can be utilized to
preserve recovery and restore any early
versions of every object that we stored
in our Amazon S3 bucket unintentional
erases or overrides of the object can be
easily regained if we have versioning
enabled and it's possible only if we
have one file with the same name and
anytime we update the file it keeps the
file name but creates a different
version ID take this bucket and data for
example in a photo.pmg is a file that
was initially stored it attached a
version ID to it and then we edited it
let's say we added some Watermark we you
know added some graphic to it and that's
now the new version of it when we store
it we store it with the same file name
it accepts the same file name but
creates a new version ID and attaches it
anytime we want to roll back we can
always go to the console look at the old
versions pick the version ID and roll
back to the old version ID all right
let's take this example now I'm in a
bucket that we've been using for a while
and let me upload an object now before
we actually upload an object this bucket
needs to be versioning enabled so let me
watch enable this Bucket from this point
onwards this bucket is going to accept
versioning so let me upload an object
photo.jpg let me upload it all right it
successfully got uploaded good enough
it's uploaded now let me upload another
object with the same file name now look
at that it was uploaded at 7 40 35 am
let me upload another object with the
same file name that I have it stored
right here that got up to uploaded but
with the same name
all right so that's the other photo now
what if if I want to switch back the way
I would switch back is simply switch
back to the older version Look at that
this is the latest version and this is
the old version that I have I can simply
switch back to the old version that was
created at such and such time and I can
open it
that's going to open the old file so in
short it creates different version of
the data that I create as long as it's
with the same name and at any point I
can go and roll back to the original
data this is a very good use case if you
want to use S3 for storing our codes
let's talk about other feature like
cross region replication now cross
region replication is an very cool
feature if you want to automatically
keep a copy of the data in a totally
different region for you know data
durability for any additional data
durability or if you want to serve data
to your customers who live in another
country or who are accessing your data
from another country if you want to
serve the data with low latency cross
region replication is a very cool
feature that you can use and get
benefited from and let's see how that's
done so before we actually do a lab on
Cross region replication let's put
together a proper definition for it
cross region application is a feature
that provides automatic copying of every
object uploaded to our bucket or your
bucket Source bucket and it
automatically copies the data to the
destination bucket which is in a
different AWS region as you see here in
the picture I put data only in region
one it's going to copy the data to
region 2. and for us to use cross region
replication versioning must be turned on
so it creates versions and copies the
versions as well if tomorrow the
original region goes down let's say the
other region will be active and it has
the complete data that was present in
the original region or at any point if
we want to Simply you know cut the
region replication and use the other
bucket as in Standalone bucket it can be
used as well because it already has all
the data that was present in the master
or all the data that was present in the
original replication bucket let's see
how that's done so there are two things
needed one is versioning and another one
is role when we transfer data from a one
bucket to another bucket we need proper
permissions to do so and these roles
they give us proper permissions to
transfer data from one bucket to another
bucket let's see how that's done right
so here's my bucket a bucket in US organ
let's create another bucket
and
Mumbai
call it dot Mumbai
.com
and put it in
put it in Mumbai
or create that bucket in Mumbai there
you go we have one in Oregon we have one
in Mumbai we're planning to replicate or
create an application between these two
right create application between these
two buckets so go to the first bucket go
to management and start a replication
add a rule so all content in this bucket
is going to get replicated this is my
source bucket it's quite simple select
the destination bucket now my
destination bucket is going to be simply
learn.samual.mumbai.com all right it
says well you have versioning enabled in
your Source bucket but not on your
destination bucket do you want to enable
it now without which we won't be able to
proceed further so let's go ahead and
enable versioning through the console
that shows up and then like I said it's
going to require permission to put data
onto the other bucket now I can create
different roles these are different
roles that are used for different other
services I can also choose to create a
role that specifically gives permission
only to move the data from one bucket to
another bucket three months it's done so
if I go over to my source bucket and if
I add some data to it let's say
index.html assuming I'm adding some
files to it in theory they should move
to the other region automatically there
you go I'm in the Mumbai region and the
data is they got moved to the other
region automatically let's talk about
the other feature called transfer
acceleration now it's a very handy and a
helpful tool or a service to use if we
are transferring data which is very long
distance from us meaning from the client
to the SD bucket let's say from my local
machine which is in India if I transfer
the data over to Oregon let's say long
distance if it is going to go through
the internet it's going to go through
high latency connections and my transfer
might get delayed if it is an 1 gigabit
file it's okay but if we're talking
about anything that's uh you know five
terabyte size now if you're talking
about anything that's a five terabyte in
size then uh it's not going to be a
pleasant experience so in those cases I
can use transfer accelerator uh with
which in a secure way but in a fast way
or a fastest way transfers my data from
the laptop or from client to the S3
bucket and it makes use of a service
called cloudfront to transfer or to
enable the data acceleration so the way
it would do it is instead of copying the
data directly to the location instead of
copying the data directly to the
destination bucket it copies the data
locally into a cloudfront location which
is available very local to whatever
place we are in and from there it copies
the data directly to an S3 bucket not
going through the internet it helps
eliminate a lot of latency that could
get added when transferring the data so
let's see how that's done
right here I'm in the S3 bucket and
under properties I can find transfer
accelerator and if I enable transfer
accelerator so I'm in another bucket let
me go to properties and let me go to
transfer accelerator and enable transfer
accelerator so now if I put data into
this bucket they're gonna get copied to
the local cloudfront location and from
there they're gonna get copied to the S3
Bucket from the cloud front now if we
need to compare you know how the speed
is going to be compared to directly
putting the data to the internet and
using cloudfront there is a tool
available that actually runs for a while
and then comes with a report that tells
me how much will be the improved speed
if I use transfer accelerator and it
shows for all the regions available in
Amazon so from the source to the
destination if you want to put uh you
know what's the normal and what's the
accelerated speed when you transfer the
data those results we will get it in the
screen so at the moment this tool is
going through it testing like uploading
some file through the internet and
uploading some file using cloudfront and
it has come up with the calculation that
if I'm uploading file to San Francisco
compared to uploading through the
internet and through cloudfront it's 13
times faster so similar way it's going
to calculate for all the regions
available and it's going to give me a
result if you are looking to become an
AWS Cloud architect to excel your career
then have a look at AWS Cloud architect
Masters program by simply learn with
this certification course you will learn
architectural principles scalability
security and key components like S3 and
cloud formation become expert in the
tools like Amazon ec2 AWS Lambda Amazon
dynamodb Amazon cloud formation and much
more our AWS Cloud architect training is
great for beginners however AWS
recommends at least one year of relevant
experience before pursuing this program
so hurry up and try your hands on this
amazing course use discount coupon YT
be15 to get handsome discount find the
course Link in the description box if
you are looking to learn cloud computing
enroll into professional cloud computing
program in cloud computing and devops by
simply learn and gain hands-on
experience with industry projects one
year experiences prefer to grab the
course find the course Link in the
description box one day at office in a
growing company there was this question
going on between 2it personal it was
about the new role that his colleague
Jessica has taken Benjamin the guy
standing here wants to know about it and
Jessica's job is new different and
dynamic and interesting too she scale
and manage servers and operating systems
and apply security patches onto them and
monitor all of these at the same time to
ensure the best quality for application
is given to the users and Benjamin was
awestruck with the amount of work
Jessica is doing and with the time it
would take to complete all of them but
Jessica being a very Dynamic person very
suitable for the job said it was easy
for her to handle all of it and even
more but that easiness on the job did
not last longer as the environment grew
more and more it being a startup company
Jessica was getting drained and was not
really happy about all her job as she
used to be Jessica's annual way of
scaling and environment did not last
long and she was also finding out that
she missed to scale down some of the
resources and it's costing her a lot she
needs to pay for the service that she
was not at all using she sort of felt
that she was at the end of the role and
there was no way out from this manual
task she was very desperate and that's
when Benjamin suggested something and
that brought back the peace and joy
Jessica initially had Benjamin suggested
about a service called Lambda that can
ease the work that Jessica is doing at
the moment and Lambda as happy as it
looks like it's a solution that solves
the manual repetitive work and lot more
and Lambda introduced itself as the
problem solver and started to explain
the following things the very same thing
that we're gonna learn about in this
series so in this section we're going to
learn about the features of AWS Lambda
and we're going to talk about what
Lambda is and then we're going to talk
about where Lambda is being used in the
it or in the cloud environment as we
speak and then we're going to talk about
how Lambda works and some use cases of
Lambda and we're going to be
particularly discussing about the use
case about automatically backing up the
data that's put in the cloud storage
let's talk about Lambda in detail Lambda
automatically runs our code without
requiring us to provision or manage
servers just write the code and upload
it to Lambda and Lambda will take care
of it that means that we don't require
any server to run or to manage and all
you need to do is write the code and
upload it to Lambda and Lambda will take
care of it which also means that we can
stop worrying about provisioning and
managing service the only thing Lambda
expects from you is a code that's
working AWS Lambda also automatically
scales our application by running code
in response to each trigger our code
runs in parallel and processes each
triggers individually scaling precisely
with the size of the workload scaling
here is done automatically based on the
size of the workload Lambda can scale
the application running the code in
response to each and every trigger that
it receives billing in Lambda is meter
on the seconds we only pay for the
amount of time that our code is running
which means that we're not charged for
any of the servers the only payment
required is for the amount of time the
code is computed with AWS Lambda we are
charged for every 100 milliseconds we
are actually charged for 100
milliseconds our code executes and the
number of times our code is triggered
and we don't pay anything when the code
is not running let's talk about what is
AWS Lambda now Lambda is one of the
service that falls under the compute
section or the compute domain of
services that AWS provides along with
ec2 EBS elastic load balancer Lambda is
also a service that comes under the
bigger umbrella of compute services in
in AWS and Lambda allows us to execute
code or any type of application with AWS
Lambda we can run code for virtually any
type of application or backend Services
all we need to do is supply our code in
one of the languages that AWS Lambda
supports as we speak the languages that
are supported by AWS Lambda are node.js
Java c-sharp go and Python and Lambda
can be used to run code in response to
certain events from other services and
based on the event it can run functions
and those functions can be of node.js
java c-sharp Etc now let's talk about
where is Lambda used there are huge
number of use cases for Lambda and there
are many ways AWS Lambda is used
specifically in the business world let's
talk about some of them one use case is
AWS Lambda is used to process images
when it is uploaded in an S3 bucket
let's say the object gets uploaded in an
S3 bucket in a form met that we don't
expect it to be which means the file
needs to be formatted so it gets
uploaded in a raw format and AWS Lambda
is triggered anytime a new object is
added to the bucket and the images are
processed and converted into thumbnails
based on the devices the other end
device that would be reading the data it
could be in PC it could be an apple
machine it could be an Android phone it
could be an Apple phone it could be a
tablet What Not So based on the devices
different formats Lambda can get
triggered and convert video or convert
the picture into the different format
that it requires another use case for
Lambda is to analyze the social media
data let's say let's say we're
collecting the hashtag trending data and
the data is received and it's added into
the Kinesis stream to feed into the
Amazon environment and Lambda action get
triggered and it receives the data and
there the data is stored into the
database which can be used by businesses
for later analysis and some of the
companies that have gotten tremendous
benefit using Lambda or Thompson routers
benchlings Nordstrom Coca-Cola robot are
some of the companies to name at the
moment that have received tremendous
amount of benefit by using Lambda let's
look at how Lambda Works in other words
let's look at how the complicated
function behind the scenes work in a
simple and in a seamless way so here
clients send data now clients send data
to Lambda and clients could be anyone
who's sending requests to AWS Lambda it
could be an application or other Amazon
web services that's sending data to the
Lambda and Lambda receives the request
and depending on the size of the data or
depending on the amount or volume of the
data it runs on the defined number of
containers if it is a single request or
if it is less request it runs on a
single container so the requests are
given to the container to handle and the
container which contains the code the
user has provided to satisfy the query
would run and if you're sort of new to
Containers then let me pause here for a
while and explain to you what container
is now container image is a lightweight
Standalone executable package of a piece
of software that includes everything
needs to run it like the codes the
runtimes the system tools the system
libraries and any of the settings needed
and it is at the moment available both
on Linux and windows based application
and containerized software will always
run the same regardless of the
environment it's running on and
containers isolate software from its
surrounding for example there could be
difference between a development and
staging environment so that's sort of
isolated and this helps in reducing the
conflict between the teams running a
different software on the same
infrastructure all right now that we
know containers needed to understand
Lambda so if there are few requests it
sends to a single container but as and
when the request grows it actually
creates multiple containers shares the
multiple requests to the different
containers there and depending on the
volume depending on the size depending
on the number of sessions the more
number of containers are provisioned so
to handle those requests and when the
request reduce the number of containers
reduce as well and that helps in Saving
costs we're not running any resources
and paying for it when we're not using
them and in fact we're not at all paying
for the resources we're only charge for
the amount of time a function is running
inside these containers now consider a
situation where you have to set up a
temporary storage and as a system to
backup the data as soon as the data is
uploaded which is a near real time
backup now a near real-time manual
backups are nearly impossible and
they're not that efficient too and near
real-time manual backups that's what the
business demands but that's not near
real-time backup that too a manual one
that's not at all efficient even if we
come up with a solution of manually
backing up close to near real Time
That's not going to be efficient looking
at the amount of data that will be put
in and looking at the random times the
data will be put into the source bucket
and there is no way we can do a manual
backup and they keep it as real as
possible but if that's still your
requirement we can use AWS Lambda and
set things up so AWS Lambda manually
handles the backup in other words for an
impossible or a difficult situation like
that AWS Lambda comes for the rescue and
this is what we do create a 2s3 bucket
one would be the source bucket where the
data will be uploaded and the other one
is and destination bucket where the data
will be backed up from the source bucket
and for these buckets to talk to
themselves it's going to require an IM
role and then for the automatic copy
it's going to require an Lambda function
to copy the files from The Source bucket
to the destination bucket and what
triggers the Lambda function the Lambda
function is triggered every time there's
a change in the metadata for the bucket
and this data is then uploaded into the
destination bucket and after setting all
this up we can literally test it by
putting a data in the source bucket
that's going to automatically replicate
or that's going to automatically copy
the data from The Source bucket to the
destination bucket all right let's see
how we can replicate data between two
bucket well we have a feature to cross
region replicate in S3 that's a feature
that comes along with S3 what if you
want to replicate between two different
buckets in two different accounts in
those cases we can use Lambda to
replicate the data between the buckets
so you put one data or you put data in
the source bucket and that data gets
replicated to the destination bucket and
let's see how that's done the procedure
here would be to have two buckets to
begin with and then create an IM role
that lets you access to pull data from
The Source bucket and put data on the
destination bucket and then create
Lambda files or Lambda event or triggers
to actually look for event in the source
bucket and anytime a new data gets added
Lambda gets triggered copies the data
from The Source bucket and moves the
data to the destination bucket and it
uses the IAM role and policy for the
needed permissions and in just a couple
of clicks we have set up a temporary
backup system that can run seamlessly
without any manual intervention and that
can be as near real time as possible all
right that's the concept and let's see
how it is done real time through this
slab so to begin with we need two
buckets so I have here a source bucket
and a destination bucket and the source
bucket as of now do not have any data in
it so as the destination bucket all
right so that's one down the second
would be to create an IM role right so
let me create an IM role and the role is
going to have this policy in it a policy
that's allowing get object on the source
bucket and a policy that's allowing put
object on the destination bucket and
here I'm going to use or I'm going to
paste my source and destination buckets
Arn all right go to Services go to S3
Source bucket
all right click on the source bucket and
copy the bucket Arn
so that would be the source bucket Arn
right on on the destination bucket copy
the destination bucket Arn
and this is going to be my destination
bucket Arn so with this I'm going to
create a policy
all right
go to IM
and create a policy
now I I have already created a policy
with the same information all right
destination bucket Arn and a policy is
available with the name S3 bucket copy
Lambda attach the policy to the role
right go to roles create a role Lambda
is the service that's going to use it in
here attach the policy that we have
created
right give it the name
and then create a role now I have a role
created as well
all right copy Lambda and that's having
the policy that we have created sometime
back now create a Lambda function right
so go to Lambda
create a function give it a name like S3
bucket copy all right choose the rule
that we want to use
all right that's the rule that we want
to use copy one two create a function
all right
and in here we're going to use node.js
code right I have a sample code
this can be used as template in here
replace the source bucket with the name
of the source bucket and the destination
bucket with the name of the destination
bucket this is a node.js code that gets
run when an event get triggers now
what's an event anytime there is a new
object placed in the S3 bucket it
creates an event and the evil triggers
Lambda and Lambda checks this Source S3
bucket picks the data puts it in the
destination bucket all right paste it
here
paste it here and click on save all
right now before we click on Save just
ensure that you have the appropriate
rules defined that's all you got to do
click on save all right now I already
have created
a Lambda function
all right
which is the same thing same code and
the rule is attached to it
now it's running it's active now let's
put this to test go to S3
pick the source bucket
put some data in it
all right and in theory those data
should be present in the destination
bucket as well
there you go it's all done by Lambda so
today's session is on AWS plastic bean
stock so what's in it for you today
we'll be discussing about what is AWS
why we require AWS elastic bean stock
what is AWS elastic bean stock the
advantages disadvantages the components
of mean stock along with that the
architecture and the companies that are
primarily using the AWS bean stock so
let's get started and first understand
what is AWS AWS stands for Amazon web
services it's a cloud provider and that
offers a variety of services such as
compute power database storage content
delivery and many other resources so we
know that AWS is the largest cloud
provider in the market and so many
services are available in the AWS where
you can apply the business Logics and
create the solutions using the cloud
platforms now why AWS elastic bean stock
now what happened earlier and that
whenever the developer used to create
the software or the modules related to
the software has to be joined together
to create a big application now one
developer creates a module that has to
be shared with another developer and if
the developers are geographically
separated then it has to be shared over
a medium probably an internet so that is
going to take some time it would be a
difficult process and in return it makes
the application or a software
development a linear process the
building of the software development
will India process so there were
challenges which the developers were
facing earlier and to overcome that we
have the mean stock as a service
available in the AWS so why AWS elastic
bean stock is required AWS elastic being
stock has made the life of the
developers quite easy uh in terms of
that they can share the applications
across different devices at a shorter
time duration now let's understand what
is AWS elastic bean stock AWS elastic
bean stock is a service which is used to
deploy and scale web applications by
developers not only web application any
application that is being developed by
the developers this is a simple
representation of the AWS plastic mean
stock NOW along with that the AWS
elastic bean stock supports the
programming language the runtime
environments that are java.net PHP
node.js python Ruby go and Docker and in
case if you're looking for any other
programming language or a runtime
environment then you can make a request
with AWS to arrange that for you now
what are the advantages associated with
the elastic bean stock First Advantage
is that it's a highly scalable service
now when we talk about a scalability it
means that whenever we require the
resources in demand we can scale up the
resources or we can scale down the
resources so that is kind of a
flexibility we get in terms of changing
the type of resources whenever we need
it and in that case the elastic bean
stock is a highly scalable service now
that is something which is very
difficult to achieve in case of an
on-prem environment because you have to
plan for the infrastructure and in case
if you're short of the resources within
that infrastructure then you have to
procure it again the second Advantage
associated with the bean stock is that
it's a fast and simple to begin now when
we say it's fast and simple that means
that you just have to focus on the
development of an application building
an application and then you can just
deploy the application directly using
the Beanstalk what the bean stock is
going to do that every networking aspect
is being taken care by the Beanstalk it
deploys your application in the back end
on the servers and then you can directly
access your application using the URL or
through the IP address the third
Advantage is that it offers the quick
deployment that is what we discussed in
the fast and simple to begin as well so
why it offers a quick deployment you
don't have to bother about the
networking Concepts you just have to
focus on the application development and
then you can just upload your
application deploy that and then you are
good to go the other Advantage is that
it supports multi-tenant architecture
when we talk about tenants or multi
tenants that means we can have a virtual
environments for separate organizations
or the divisions within the
organizations that will be virtually
isolated so likewise you can have a
virtually isolated environments created
on the Beanstalk and they can be
separated used as a separate entities or
a separate divisions within the
organization and we know that it's a
flexible service since it's a scalable
then which is a flexible also now coming
to the simplifies operations as an
advantage now once the application is
deployed using the Beanstalk then it
becomes very easy to maintain and
support that application using the
Beanstalk Services itself and the last
advantage that we can have from the
Beanstalk is that it's a cost efficient
service the cost efficient as we know
that many of the AWS services are cost
effective the cost optimization can be
better managed using the AWS mean stock
as compared to if you are developing or
if you are deploying any kind of an
application or a solution on the on-prem
servers now there are some components
that are associated with the AWS
Beanstalk and it has to be created in
the form of a sequence manner so AWS
elastic Beanstalk consists of few
important components which are required
while developing an application now what
are these components these are four
components one is application the second
is application version the third is
environment and the fourth one is the
environment tier and we have to progress
while deploying our applications or the
softwares using the same sequence now
let's understand what are the different
components of the Beanstalk are the
application it refers to a unique label
which is used as a Deployable code for a
web application so generally you deploy
your web application or you create your
application and that is something which
is basically uh used as a unique label
then the second component is application
versions so it resembles a folder which
stores a collection of components such
as environments versions and environment
configurations so all these components
are being stored using the application
version the third most important
component is the environment in the
environment only the current versions of
the applications runs now remember that
elastic mean stock supports multiple
versions as well and using the
environment you can only run the current
version of the application file if you
wanted to have another version of an
application to be running then you have
to create another environment for that
then comes the environment tier and in
the environment here it is basically it
designates the type of application that
the environment runs on now generally
there are two types of environment here
one is the web and the other one is the
worker node and that's something which
we'll be discussing later as well now
let's understand how does elastic bean
stock in AWS works so first we have to
create an application and this is a task
that would be done by the developers and
for that you can actually select any
runtime environment or a programming
language like Java Docker Ruby gopal or
python as well and once you select that
environment you can develop your
application using that runtime
environments now after that once the
application is created then you have to
upload the version of an application on
the AWS and after that once the version
is uploaded and then you have to launch
your environment so just have to click
on the buttons that's it nothing more
you have to do once the environment is
launched then you can actually view that
environment using a web URL or using the
IP address now what happens in that case
is when you launch an environment in the
back end the elastic bean stock runs
automatically runs any ec2 instance and
using a metadata the mean stock deploys
your application within that ec2
instance that is something which you can
look into the ec2 dashboard as well so
you don't have to take care of the
security groups you don't have to take
care of the IP addressing and even you
don't have to login into the instance
and deploy your application it would be
done automatically by the Beanstalk it's
just that you just have to monitor the
environment and the statistics will be
available there itself in the bin stock
dashboard otherwise you can view those
statistics in the cloudwatch logs as
well now in case if you wanted to update
any kind of a version then you just
upload a new version and then just
deploy that and then monitor your
environment so these are the essentials
to create a local applications for any
platform whether it's a node.js python
Etc these are the things that you have
to actually take care and this is the
sequence you have to follow while
creating an environment so you can say
that it's a four steps creation of a
deployment of your application that's it
now after users upload their versions
the configuration is automatically
deployed with a load balancer yes and
with a load balancer that means you can
access the applications using the load
balancer DNS also and apart from load
balancer if you wanted to put any other
feature that includes the auto scaling
for example if you wanted to create your
ec2 instances where the application will
be deployed within the virtual private
cloud or in a particular subnet within
the VPC all those features that are
available and you can select them using
the mean stock itself you don't have to
move out to the VPC you don't have to
actually go to the ec2 dashboard and
select all those separately everything
would be available within the Beanstalk
dashboard so that's what it says in the
presentation that after creating an
application the deploy service can be
specifically accessed using the URL so
once the environment is created there
will be a URL defined now you can put a
URL name also that is something which
you wanted to put for your application
you can Define that you can check for
the availability of that URL and then
you have to use that URL to access your
application or the browser now once it
is done then in the monitor environment
it says the environment is monitored
provided capacity provisioning load
balancing Auto scaling and hand multi
features all those features are
available there itself in the mean stock
now let's understand the architecture of
AWS elastic bean stock now there are two
types of environments that you have to
select you can select one is the web
server environment and the other one is
the worker environment So based on the
client requirement Beanstalk gives you
two different types of environment that
you have to select generally the web
server environment is the front-end
facing that means the clients would be
accessing this environment directly
using a URL so mostly a web applications
are deployed using that environment the
worker environment is the back end
applications or on the micro apps which
are basically required to support the
running of the web applications now it
depends on the client requirement what
kind of an environment you wanted to
select now in the web server environment
it only handles the HTTP request from
the clients so that's why we use the web
server environment mostly for the web
applications or any application which
works on the HTTP https requests so it's
not only the HTTP you can use the https
as well the worker environment it
process background tasks and minimizes
the consumption of resources so again it
is just like a kind of a micro service
or an application services that are
running in the back end to support the
web server environment now coming to the
understanding of the AWS main stock so
this is how the arcade texture of the
AWS bean stock is designed and you can
refer to that image also now in the web
server environment let's say if we
select a web server environment and it
says that if the application receives
client request the Amazon Route 53 sends
his request to the elastic load balance
now obviously we discussed here that the
web server environment is primarily an
environment which receives the HTTP
requests it's a kind of a client-facing
environment now if the application
receives a client request Amazon from
the Amazon Route 53 this Route 53 is a
service which is primarily used for DNS
mapping it's a global Service and it may
route you can route the traffic from the
root 53 matching your domains towards
the load balancer and from the load
balancer you can point the traffic to
the web server environment obviously the
web server environment is nothing it's
just the ec2 instances that would be
running in the back end now here in the
diagram you can see that there are two
web server environments and they are
created in the auto scaling group that
means there is some kind of scaling
options that are defined as well and
these instances are created in an
availability zone or they can be created
in a different availability Zone also
for the redundancy as well and these web
application servers are further
connected to your databases which
primarily will be in a different
security groups probably it can be an
RDS database also so all these
functionalities all these features are
basically available on the elastic mean
stock dashboard itself now what happens
in that case is if the application
receives client requests Amazon Route 53
send these requests to the load balancer
later the load balancer shares those
requests among the ec2 instances how
does that happen it happens using a
predefined algorithm the equal
distribution of a load is distributed to
both the ec2 instances or n number of
ac2 instances running in the
availability zone now in the
availability zones every ec2 instance
would have its own Security Group they
can have a common Security Group also
they can have their own Security Group
as well now after the security group the
load balancer is then connected to the
Amazon ec2 instance which are part of
the auto scaling group so that's
something which we have discussed
already ready now this Auto scaling
group is would be defined from the
Beanstalk itself and there will be some
scaling options that will be created it
could be a possibility that it might be
the minimum number of instances that
would be running as of now and based on
the threshold defined it may increase
the number of ec2 instance and the load
balancer will keep on Distributing the
load to as many instances that will be
created inside the availability Source
obviously there will be an internal
health check that the load balancer will
be first doing before Distributing the
real-time traffic to this instances
created by the mean stock now what does
Auto scaling group does it automatically
starts the additional ec2 instance to
accommodate increasing load on your
application that's something which we
know that and also it monitors and
scales instances based on the workload
as well so depends on what kind of a
scaling threshold you have defined in
the auto scaling groups and when the
load of an application decreases the ec2
instance will also be decreased so
whenever we talk about the auto scaling
generally it comes in our mind is that
we scale up the resources that means we
it increases the ec2 instances in the
order scaling you might have the scale
down option also scale down policy also
created in which if the load minimizes
it can dominate the additional ec2
instances as well so that is something
which will be automatically managed all
these features can be achievable using
the elastic bean stock and with this
feature accommodated it gives you the
better cost optimization in terms of
managing your resources now it says that
elastic bean stock has a default
Security Group and the security group
acts as a Firefall for the instances now
here in this diagram it says about the
Security Group auto scaling also you
might create it in a default VPC also
you might create it in your custom EPC
also where you can have the additional
level of security is also created you
can have the nacls knuckles also defined
here before the security groups so that
would give you the additional filtering
option or the firewall option now it
says that with these groups with these
security groups it allows establishing
security groups to the database server
as well so every database would also
have its own Security Group and the
connection can be created between the
web servers environment that is created
by the Beanstalk to the database
security groups as well now let's
discuss about the worker environment now
understanding the worker environment
what happens is that the client the web
server environment is the client facing
the client sends a request for an access
to the web server and in this diagram
the web server further sends it to the
sqs which is a simple queue service and
the Q service send it to the worker
environment and then whatever the worker
environment is created for doing some
kind of a processing or some kind of an
application that is running in the back
end that environment initiates and then
send back the results to this sqs and
vice versa so let's understand the
architecture of a AWS elastic bean stock
with the worker environment so when a
worker environment here is launched AWS
elastic bean stock install the server on
every ec2 instance so that is in the
case of a web server environment also
and later the server passes the request
to the simple queue service now this
service is an asynchronous service
instead of a simple queue service you
can have other services also it is not
necessary that you need to have the SQL
so this is an example that we are
discussing about and the sqs shares
those message via a post request to the
HTTP path over the broker environment
and there are many case studies also
with respect to this kind of an
environment that is being created that
is being done on many customers and you
can search for these kind of a case
studies available on the internet now
the worker environment executes the task
given by the sqs with the HTTP response
after the operation is completed now
here what happens is a quick recap the
client request for an access of an
application to a web server using an
HTTP request the web server passes that
request to the Q service the queue
service shares the message with the
worker probably a worker might be the
manual worker and generally it's an
automatic worker so it would be shared
via the worker environment only and the
worker sends back the response with the
HTTP response back to the queue that
response can be viewed directly from the
queue service by the client using the
web server so this is one of the example
likewise as I said that there can be
many other examples also where you can
have the worker environments defined now
what are the companies that are using
the elastic bean stock these are few of
the companies that are primarily using
on a Zillow jelly button games then you
have League of Women Voters eBay these
are some of the few listed companies and
obviously you search on the AWS site and
you'll find many more organizations that
are using the elastic mean stock
primarily for deploying their
applications now the next thing is to go
with the practicals that how actually we
use the elastic Beanstalk so let's look
into the demo using the AWS elastic
Beanstalk now first you have to login
into the AWS console and I'm sure that
you might be having the accounts created
or you can use the IM credentials as
well and then you have to select the
region also now I am in the North
Virginia region likewise you can select
any of the regions that are listed here
now click on the services and you have
to search for the elastic bean stock you
can find the elastic bean stock under
the compute section so here itself
you'll find the elastic bean stock as a
service now open this service and there
it will give you an option to create an
environment you have to specifically
select an environment property a worker
environment or a web service environment
so let's wait for the service to open so
we have the dashboard now available with
us this is how the elastic bean stock
looks and this is the symbol
representation of a beanstalk now what
you have to do is we have to click on
get started and that will load and you
have to create a web app so instead of
creating a web app what we'll do we'll
create a new application so just click
on create a new application put an
application name let's say we put
something like x y z you can put any
description to your application let's
say it's a demo app and click on create
now it says you have to select an
environment now the environment the
application name XYZ is created you just
have to select an environment so click
on create one now and it is going to ask
you that what kind of an environment
here you wanted to select so as we
discussed that there are two types of
environments one is the web server and
the other one is the worker in one let's
look into it what is defined by the AWS
AWS says that it has two types of
environment tiers to support different
types of web applications web servers
are standard applications that listen
for and then process HTTP request
typically Uber port number 80. workers
are specialized applications that have a
background processing task that listens
for message on an Amazon sqs queue
workers application post those messages
to your application by using the HTTP
response so that's what we saw in the
case of the Beanstalk slides also now
the usability of a worker environment
can be anything now we'll do a demo for
creating a web server environment so
just click on select and you we have the
environment name created now we can
Define our own domain it ends with the
region dot elasticbeanstalk.com let's
say I look for a domain which is XYZ
only that's the environment name now
I'll check for the availability whether
that domain name is available with us or
not and it says we don't have that
domain name so probably I'll try to make
it with some other name and let's look
for the availability XYZ ABC and it says
yes it is available now once I deploy my
application I would be able to access
the application using this complete DNS
so you can put a description it's a demo
app that we are creating and then you
have to define a platform as well now
these are the platforms that are
supported by the AWS let's say I wanted
to run a node.js environment so I just
click on the node.js platform the
application codes is something which is
basically developed by the developers
and you can upload the App application
right now or you can do that later as
well once the environment is ready now
either you can select to create an
environment if you wanted to go with all
the default settings otherwise if you
wanted to customize it more you can
click on configure more options so let's
click on configure more options and here
you would be able to define various
different features like the type of an
instance for example what kind of an ec2
instance or a server that should be
running so that the Beanstalk can deploy
our applications over it if you wanted
to modify just click on a modify button
and here you can modify your instances
with respect to the storage as well now
apart from that if you wanted to do some
modification in the case of monitoring
in the case of databases in the case of
security or in the case of a capacity
let's look into the capacity so here you
can actually do the modification so in
the capacity you can select the instance
type also by default it is t2.micro but
in case if your application requires a
larger type of an instance then you can
actually go for the instance type as
well similarly you can Define your Emi
IDs also because obviously for the
application to run you would require the
operating system also so you can select
that particular Ami ID for your
operating system as well let's cancel
that likewise you have many other
features that you can actually Define
here from the dashboard and you don't
have to go to the ec2 dashboard to do
the modifications now let's go and
create an environment let's assume that
we are going with the default
configuration so this is going to create
our environment the environment is being
created and you can get the environment
and the logs defined in the dashboard
itself so you'll see that the Beanstalk
environment is being initiated then
environment is being started and in case
if there would be any errors or if it is
deployed correctly you will get all the
logs here itself now the environments
are basically color coded so there are
different color codings that are defined
if you get the environment in a green
color that means everything is good to
go so here you can see that it has
created an elastic IP it has checked the
health of the environment now it has
created the security groups and that
would be an auto security groups created
by the mean stock and the environment
creation has been started you can see
that uh elastic bean stock as Amazon S3
storage bucket for your environment data
as well this is the URL through which
you will be accessing the environment
but right now we cannot do that since
the environment is being created let's
click on the application name and here
you can see that it is in a gray color
that means right now the build is being
done it is being created once it will be
successfully created it should change to
the green color and then we will be able
to access our environment using the URL
now if I move to the ec2 instances and
see in the ec2 dashboard if I see
whether the instance is being created by
the Beanstalk or not so let's see and
let's see what are the differences in
terms of creating an instance manually
and getting it created from the
Beanstalk so click on the ec2 let's go
to the old ec2 experience that's what we
are familiar with and let's see what's
there in the dashboard so here you can
see one running instance let's open that
and the XYZ environment which was
created from the Beanstalk is being
initiated the instance is being
initiated and that is something which is
being done by the mean stock itself we
have not gone to the dashboard and
created it manually now in the security
groups if you see that here the AWS mean
stock security groups are defined it has
the elastic IPS also defined so
everything is being created by the
beanstock itself right now let's go back
to the Beanstalk and let's look into the
status of our environment whether the
color coding has been changed from Gray
to green or not and here you can see the
environment is successfully created and
we have that environment colored in
green we'll access the environment and
it says it's a web server environment
its platform is node.js running on
64-bit Amazon Linux Ami and it's a
sample app sample application health
status is okay now the other thing is
that if you do not want to use the web
console the Management console to access
the main stock then the Beanstalk offers
you the elastic bean stock CLI as well
so you can install the command line
interface and then you have the command
reference so CLI command references that
you can actually play with and get your
applications deployed using the bin
stock itself so this is one of the
sample CLI commands that you can
actually look into now let's look into
the environment let's click on the
environment and we be represented with
the URL it says health is okay these are
the logs that you have to follow in case
if there are any issues the platform is
node.js that is what we selected now the
next thing is you just have to upload
and deploy your applications so just
click on upload and deploy select the
version label or the name select file
and wherever your application is hosted
at just select that upload it and deploy
your application you'll see that the
like your environment is created
similarly your application will be
deployed automatically on the instance
and from this URL you will be able to
view the output it is as simple as just
like you have to follow these four steps
now let's see whether the node.js
environment is running on our instance
before deploying an application so we'll
just click on this URL since the
Beanstalk has already opened up the
security groups or HTTP Port 80 for all
we can actually view that output
directly from the URL so we have the
node.js running that's visible here and
after that you just have to upload and
deploy your application and then from
that URL you can get the output now this
URL you can map it with the root 53
service so using the root 53 DNS
Services the domain names can be pointed
to the elastic Beanstalk URL and from
there it can be pointed to the
applications that are running on the ec2
instance whether you wanted to point it
to the URL directly using the mean stock
you can do that otherwise as we saw in
the slides you can use the root 53
pointer to the load balancer and then
point it to the instances directly also
once it is created by the mean stock so
that was the demo guys with respect to
the bean stock and how we can actually
run the environments apart from the that
the operational task like system
operations you can manage all these
things from the environment dashboard
itself so you have the configurations
you have the logs you can actually check
the health status of your environment
you can do the monitoring and you can
actually get the alarms and the events
here so let's say if I wanted to if I
wanted to see the logs I can request for
the logs here itself and I'll be
represented with the full log report and
I can now download that log file and I
can view the logs so it's in the so we
have this bundle locks in the zip file
all right so if you want to see some
kind of logs with respect to elastic
bean stock activity it's in the form of
a notepad and here you can see what all
configurations the Beanstalk has done on
your environment on your instance
similarly you can go for the health
monitoring alarms events and all those
things if you are looking to become an
AWS Cloud architect to excel your career
then have a look at AWS Cloud architect
Masters program by simply learn with
this certification course you will learn
architectural principles scalability
security and key components like S3 and
cloud formation become expert in the
tools like Amazon ec2 AWS lambra Amazon
dynamodb Amazon cloud formation and much
more our AWS Cloud architect training is
great for beginners however AWS
recommends at least one year of relevant
experience before pursuing this program
so hurry up and try your hands on this
amazing course use discount coupon YT
be15 to get handsome discount find the
course Link in the description box if
you are looking to learn cloud computing
enroll into professional cloud computing
program in cloud computing and devops by
simply learn and gain hands-on
experience with industry projects
one-year experiences prefer to grab the
course find the course Link in the
description box
here we're going to talk about Amazon
ECS a service that's used to manage
Docker containers so without any further
Ado let's get started in this session we
would like to talk about some Basics
about AWS and then we're going to
immediately dive into why Amazon ECS and
what is Amazon ECS in general and then
it uses a service called Docker so we're
going to understand what Docker is and
there are competitive services available
for ECS I mean you could ECS is not the
on and only service to manage Docker
containers but why ECS advantage of ECS
we will talk about that and the
architecture of ECS so how it functions
what are the components present in it
and what are the functions that it does
I mean each and every component what are
all the functions that it does all those
things will be discussed in the
architecture of Amazon ECS and how it
works how it all connects together
that's something we will discuss and
what are the companies that are using
ECS what were the challenge and how ECS
helped to fix the challenge that's
something we will discuss and finally we
have a wonderful lab that talks about
how to deploy Docker containers on an
Amazon ECS so let's talk about what is
AWS Amazon web service in short call as
AWS is an web service in the cloud that
provides a variety of services such as
compute power database storage content
delivery and a lot of other resources so
you can scale your business and grow not
focus more on your it needs and the rest
of the ID demands rather you can focus
on your business and let Amazon scale
your it or let Amazon take care of your
it so what is that you can do with AWS
with AWS we can create deploy why any
application in the cloud so it's not
just deploying you can also create your
application in the cloud it has all the
tools and services required the tools
and services that you would have
installed in your laptop or you would
have installed in your on-premises
desktop machine for your development
environment you know the same thing can
be installed and used from the cloud so
you can use cloud for creating and not
only that you can use the same Cloud for
deploying and making your application
available for your end user the end user
could be internal internal users the end
user could be the could be in the
internet the end user could be kind of
spread all around the world it doesn't
matter so it can be used to create and
deploy your applications in the cloud
and like you might have guessed now it
provides service over the Internet
that's how your users worldwide would be
able to use the service that you create
and deploy right so it provides service
over the the internet so that's for the
End customer and how will you access
those Services that's again through the
internet it's like the extension of your
data center in the internet so it
provides all the services in the
internet it provides compute service
through the internet so in other words
you access them through the internet it
provides database service through the
internet over the internet in other
words you can securely access your
database through the internet and lot
more and the best part is this is a pay
as you go or pay only for what you use
there is no long term or you know
beforehand commitment here most of the
services does not have any commitment so
there is no long term and beforehand
commitment you only pay exactly for what
you use there's no overage there's no
overpaying right there's no buying in
advance right you only pay for what you
use let's talk about what ECS is so
before or ECS before containers right
ECS is a service that manages Docker
containers right it's not a product or
it's not a feature all by itself it's a
service that's dependent on Docker
container so before Docker containers
all the applications were running on VM
or on an host or on a physical machine
right and that's memory bound that's
latency bound the server might have
issues on and on right so let's say this
is Alice and she's trying to access her
application which is running somewhere
in her on-premises and the application
isn't working what could be the reason
some of the reasons could be Memory full
the server is currently down at the
moment we don't have another physical
server to launch the application a lot
of other reasons so a lot of reasons why
the application wouldn't be working in
on-premises some of them are Memory full
issue and server down issue very less
High availability or in fact single
point of failure and no high
availability if I if I need to tell it
correctly with ECS the services can kind
of breathe free right the services can
run seamlessly now how how is that
possible now those thing we will discuss
in the upcoming sessions so because of
containers and easiest managing
containers the applications can run in a
high available mode they can run in a
high available mode meaning if something
goes wrong right there's another
container that gets spun up and your
application runs in that particular
container very less chances of your
application going down that's what I
mean this is not possible with a
physical Host this is very less possible
with an a VM or at least it's going to
take some time for another VM to get
spun up so why ECS or what is ECS Amazon
ECS maintains the availability of the
Apple application and allows every user
to scale containers when necessary so it
not only meets the availability of the
application meaning one container
running your application or one
container hosting your application
should be running all the time so to
make that high availability availability
is making sure your service is running
24 7. so Container makes sure that your
services run 24 bar 7 not only that not
only that suddenly if there is an
increase in demand how do you meet that
Demand right let's say you have like
thousand users suddenly the next week
there are like 2000 users all right so
how do you meet that demand Container
makes it very easy for you to meet that
demand in case of VM or in case of
physical host you literally will have to
go buy another physical host or you know
add more RAM add more memory add more
CPU power to it all right or kind of
Club two three hosts together clustering
you would be doing a lot of other things
to me that the high availability and
also to meet that demand but in case of
ECS it automatically scales the number
of containers it automatically scales
the number of containers needed and it
meets your demand for that particular R
so what is Amazon ECS the full form of
ECS is elastic container service right
so it's basically a container Management
Service which can quickly launch and
exit and manage Docker containers on a
cluster so it's the function of ECS it
helps us to quickly launch and quickly
exit and manage Docker container so it's
kind of a Management Service for the
docker containers you will be running in
Amazon or running in the AWS environment
so in addition to that it helps to
schedule the placement of container
across your cluster so it's like this
you have two physical host you know
joined together as a cluster and ECS
helps us to place your containers now
where should your container be placed
should it be placed in host one should
be placed in host too so that logic is
defined in ECS we can Define it you can
also let ECS take control and Define
that logic most cases you will be
defining it so schedule the placement of
containers across your cluster let's say
two containers want to interact heavily
you really don't want to place them in
two different hosts all right you would
want to place them in one single host so
they can interact with each other so
that logic is defined by us and these
container services you can launch
containers using AWS Management console
and also you can launch containers using
SDK kids available from Amazon you can
launch through a Java program you can
launch container using an.net program
you can launch container using an
node.js program as an when the situation
demands so there are multiple ways you
can launch containers through Management
console and also programmatically and
ECS also helps to migrate application to
the cloud without changing the code so
anytime you think of migration the first
thing that comes to your mind is that
how will that environment be based on
that I'll have to alter my code what's
what's the IP what is the storage that's
being used what what are the different
parameters I'll have to include the
environment parameters of the new
environment with containers now that
worry is already taken away because we
can create an pretty exact environment
the one that you had on premises the
same environment gets created in the
cloud so no worries about changing the
application parameter no worries about
changing the code in the application
right you can be like if it ran in my
laptop a container that I was running in
my laptop it's definitely going to run
in the cloud as well because I'm going
to use the same container in the laptop
and also in the cloud in fact you're
going to ship it you're going to move
the container from your laptop to Amazon
ECS and make it run there so it's like
the same the very same image the very
same container that was running in your
laptop will be running in the cloud or
production environment so what is Docker
we know that it ECS helps to quickly
launch exit and manage Docker containers
what is Docker let's let's answer that
question what is a Docker now Docker is
a tool that helps to automate the
development of an application as a
lightweight container so that the
application can work efficiently in
different environments this is pretty
much what we discussed right before the
slide I can build an application in my
laptop or in on premises in a container
environment Docker can container
environment and anytime I want to
migrate right I don't have to kind of
rewrite the code and then rerun the code
in that new environment I can simply
create an image a Docker image and move
that image to that production or the new
Cloud environment and simply launch it
there right so no compiling again no
relaunching the application simply pack
all your code in a Docker container
image and ship it to the new environment
and launch the container there that's
all so Docker container is a light
weight package of software that contains
all the dependencies so because you know
when packing you'll be packing all the
dependencies you'll be packing the code
you'll be packing the framework you'll
be packing the libraries that are
required to run the application so in
the new environment you can be pretty
sure you can be guaranteed that it's
going to run because it's the very same
code it's the very same framework it's
the very same libraries that you have
shipped right there's nothing new in
that new environment it's the very same
thing that's going to run in that
container so you can be rest assured
that they are going to run in that new
environment and these Docker containers
are highly scalable and they are very
efficient suddenly you wanted like 20
more Docker containers to run the
application think of adding 20 more
hosts 20 more VMS right how much time
would it take and compared to that time
the amount of time that Docker
containers would require to kind of
scale to that amount like 20 more
containers it's very less or it's
minimal or negligible so it's a highly
scalable and it's a very efficient
service you can suddenly scale number of
Docker containers to meet any additional
demand very short boot up time because
it takes it's not going to load the
whole operating system and these Docker
containers you know they use the Linux
kernel and features of the kernel like C
group send name spaces to kind of
segregate the processes so they can run
independently any environment and it
takes very less time to boot up and the
data that are stored in the containers
are kind of reusable so you can have an
external data volume and I can map it to
the container and whatever the space
that's occupied by the container and the
data that the container puts in that
volume they are kind of reusable you can
simply remap it to another application
you can kind of remap it to the next
successive container you can kind of
remap it to the next version of the
container next version of the
application you'll be launching and you
don't have to go through building the
data again from the scratch whatever
data the container was using previously
or the previous container was using that
data is available for the next container
as well so the volumes that the
containers users are very reusable
volumes and like I said it's isolated
application so it kind of isolates by
its nature it kind of by the way it's
designed by the way it is created it
isolates one container from another
container meaning anytime you run
applications on different containers you
can be rest assured that they are very
much isolated though they are running on
the same host though they are running on
the same laptop let's say though they
are running on the same physical machine
let's say running 10 containers 10
different applications you can be sure
that they are well disconnected or well
isolated applications now let's talk
about the advantages of ECS the
advantage of ECS is improved security
it's security is inbuilt in ECS with ECS
we have something called as a container
registry you know that's where all your
images are stored and those images are
accessed only through https not only
that those images are actually and
encrypted and access to those images are
allowed and denied through identity and
access management policies IAM and in
other words let's say two container
running on the same instance now one
container can have access to S3 and the
others or the rest of the others are
denied access to S3 so that kind of
granular security can be achieved
through containers when we mix and match
the other security products available in
Amazon like IAM encryption accessing it
using https these containers are very
cost efficient like I've already said
these are lightweight processors right
we can schedule multiple containers on
the same node and this actually allows
us to achieve high density on an ec2
instance imagine an ec2 instance that
that's very less utilized that's not
possible with a container because you
can actually dance or crowd an ec2
instance with more container in it so to
best use those resources in ec2
straightforward you can just launch one
application but with when we use
containers you can launch like 10
different applications on the same ec2
server that means 10 different
applications can actually feed on those
resources available and can benefit the
application and ECS not only deploys the
container it also maintains the state of
the containers and it makes sure that
the minimum a set of containers are
always running based on the requirement
that's another cost efficient way of
using it right and anytime an
application fails and that has a direct
impact on the revenue of the company and
easiest make sure that you're not losing
any Revenue because your application has
failed and ECS is in pretty extensible
Services it's like this in many
organization there are majority of
unplanned work because of environment
variation a lot of firefighting happens
when we kind of deploy the chord from
one or kind of move the chord or
redeploy the code in a new environment a
lot of firefighting happens there right
this Docker containers are pretty
extensible like we discussed already
environment is not a concern for
containers because it's going to kind of
shut itself inside a Docker container
and anywhere the docker container can
run the application will run exactly the
way it performed in the past so
environment is not a concern for the
docker containers in addition to that
ECS is easily scalable we have discussed
this already and it improves it has
improved compatibility we have discussed
this already let's talk about the
architecture of ECS like you know now
the architecture of ECS is the ECS
cluster itself that's group of servers
running the ECS service and it
integrates with Docker right so we have
a docker registry Docker registry is a
repository where we store all the docker
images or the container images so it's
like three components ECS is of three
components one is the ECS cluster itself
right when I say easiest itself I'm
referring to easiest cluster cluster of
servers that will run the containers and
then the repository where the images
will be stored right the repository
where the images will be stored and the
image itself so container image is the
template of instructions which is used
to create a container right so it's like
what's the OS what is the version of
node that should be running and any
additional software do we need so those
question gets answered here so it's the
template template of instructions which
is used to create the containers and
then the registry is the service where
the docker images are stored and shared
so many people can store there and many
people can access or if there's another
group that wants to access they can
access the image from there or one
person can store the image and rest of
the team can access and the rest of the
team can store image and this one person
can pick the image from there and kind
of ship it to the customer or ship it to
the production environment all that's
possible in this container registry and
Amazon's version of the container
registry is ECR and there's a third
party Docker itself has a container
registry that's Docker Hub ECS itself
which is the the group of servers that
runs those containers so these two the
container image and the container
registry they kind of handle Docker in
an image format just an image format and
in ECS is where the container gets live
and then it becomes an compute resource
and starts to handle request now starts
to serve the page and starts to do the
batch job you know whatever your plan is
with that container so the class master
of servers ECS integrates well with
familiar services like VPC VPC is known
for securing VPC is known for isolating
the whole environment from rest of the
customers or isolating the whole
environment or the whole infrastructure
from the rest of the clients in your
account or from the rest of the
applications in your account on and on
so VPC is a service that provides or
gives you the network isolation ECS
integrates well with VPC and this VPC
enables us to launch AWS resources such
as Amazon ec2 instance in a virtual
private Network that we specified this
is basically what we just discussed now
let's take a closer look at the ECS how
does ECS work let's find answer for this
question how does ECS work ECS has got a
couple of components within itself so
these ECS servers can run a cross
availability Zone as you can see there
are two availability zones here they can
actually run across availability zones
and ECS has got two modes fargate mode
and the ec2 mode right here we're seeing
fargate more and then here we're seeing
nothing that means it's an ec2 mode and
then it has got different network
interfaces attached to it because they
need to be running in an isolated
fashion right so anytime you want
Network isolation you need separate IP
and if you want separate IP you need
separate network interface card and
that's what you have elastic network
interface card separate elastic network
interface card for all those tasks and
services and this runs within an VPC
let's talk about the far gate service
tasks are launched using the far gate
service so we will discuss about task
what is forget now fargate is a compute
engine in ECS that allows users to
launch containers without having to
monitor the cluster ECS is a service
that manages the containers for you
right otherwise managing containers will
be an full-time job so easiest manages
it for you and if you and you get to
manage ECS that's the basic service but
if you want Amazon to manage ECS and the
containers for you we can go for forget
so fargate is a compute engine in ECS
that allows users to launch containers
without having to monitor the ECS
cluster and the tasks the tasks that we
discussed the tasks has two components
you see task right here so they have two
components we have ECS container
instance and then the container agent so
like you might have guessed right now
easiest container instance is actually
an ec2 instance right capable of running
containers not all ec2 instances can run
containers so these are like specific
ec2 instance chances that can run
containers they are ECS container
instances and then we have container
agent which is the agent that actually
binds those clusters together and it
does a lot of other housekeeping work
right kind of connects clusters makes
sure that the version needed is present
so it's all part of that agent or it's
all job of that agent container
instances container instances is part of
Amazon ec2 instance which run Amazon ECS
container agent pretty straightforward
definition and then a container agent is
responsible for communication between
ECS and the instance and it also
provides the status of the running
containers kind of monitors the
container monitors the state of the
container make sure that the content is
up and running and if there's anything
wrong it kind of reports it to the
appropriate service to fix the container
on and on it's a container agent when we
don't manage container agent it runs by
itself and you really don't have to do
anything to make the container agent
better it's already better you really
won't be configuring anything in the
agent and then elastic network interface
car is in Virtual interface Network that
can be connected to an instance in VPC
so in other words elastic network
interface is how the container interacts
with another container and that's how
the container interacts with the ec2
host and that's how the container
interacts with the internet external
world and a cluster a cluster is a set
of ECS container instances it's not
something that's very difficult to
understand it's simply a group of ec2
instances that runs that ECS agent and
this cluster it cluster handles the
process of scheduling monitoring and
scaling the request we know that ECS can
scale the containers can scale how does
it scale that's all Monitor and managed
by this ECS cluster let's talk about the
companies that are using Amazon ECS
there are a variety of companies that
use ACS clusters to name a few okta
users easiest cluster and OCTA is a
product that use identity information to
Grant people access to applications on
multiple devices at any given point of
time they make sure that they have a
very strong security protection so OCTA
uses Amazon ECS to run their OCTA
application and serve their customers
and abima abhima is an TV channel and
they chose to use microservices and a
Docker containers they already had
microservices and Docker containers and
when they thought about a service that
they can use in AWS ECS was the only
service that they can immediately adapt
to and because in abima TV the engineers
have already been using Docker and
Docker containers it was kind of easy
for them to adapt themselves to ECS and
start using it along with the benefits
that ECS provides previously they had to
do a lot of work but now ECS does it for
them all right similarly remind and
Ubisoft GoPro or some of the famous
companies that use Amazon ECS and get
benefited from its scalability get
benefited from its cost gets benefited
from its Amazon managed Services get
benefited from the portability that ECS
and the migration option that ECS
provides let's talk about how to deploy
a Docker container on Amazon ACS the way
to deploy Docker container on ECS is
first we need to have an AWS account and
then set up and run our first ECS
cluster so in our lab we're going to use
the launch wizard to run an ECS cluster
and run containers in them and then task
definition task definition tells the
size of the container the number of the
container and when we talk about size it
tells how much of CPU do you need how
much of memory do you need and talking
about numbers you know it requires how
many numbers of container you're going
to launch you know is it 5 is it 10 or
is it just one running all the time now
those kind of information goes in the
task definition file and then we can do
some Advanced configuration on ECS like
a load balancers and you know what port
number you want to allow when you don't
want to allow you know who gets access
who shouldn't get access and what's the
IP that you want to allow and deny
requests from on and on and this is
where we would also mention the name of
the container so the differentiate one
container from the other and the name of
the service you know is it an backup job
is it a web application education is a
10. a data container is it going to take
care of your data data backend and the
desired number of tasks that you want to
be running all the time those details go
in when we try to configure the ECS
service right and then you configure
cluster you put in all the security in
the configure your cluster step or
configure cluster stage and finally we
will have an instance and bunch of
containers running in that instance all
right let's do a demo so here I have
logged in to my Amazon portal and let me
switch to the appropriate region I'm
going to pick not Virginia North
Virginia look for ECS and it tells ECS
is a service that helps to run and
manage Docker containers well and good
click on it I'm a not Virginia I just
want to make sure that I'm in the right
region and go to clusters and here we
can create cluster this is our forget
and and this is our ec2 type launching
for Linux and windows environment but
I'm going to launch through this
walkthrough portal right this gives a
lot of information here so the different
steps involved here is creating a
container definition which is what we're
going to do right now and then a task
definition and then service and finally
the cluster it's a four-step process so
in container definition we Define the
image the base image we are going to use
now here I'm going to launch an httpd or
a simple HTTP web page right so a simple
httpd 2.4 image is fair enough for me
and it's not an heavy application so 0.5
gigabit of memory is enough and again
it's not a heavy application so 0.25
virtual CPU is enough in our case right
you can edit it based on the requirement
you can always edit it and because I'm
using hdpd the port mapping is already
Port 80 that's how the container is
going to receive the request and there's
no health check as of now when we want
to design critical and complicated
environments we can include health check
right and this is the CPU that we have
chose we can edit it and I'm going to
use the bash commands to create an HTML
page right this page says that you know
Amazon ECS sample app right and then it
says Amazon ECS sample app your
application is running on a container in
Amazon ECS so that's the page the HTML
page that I'm going to create right
index.html so I'm going to create and
put it in an appropriate location so
those pages can be served from the
container right if you replace this with
any of your own content then it's going
to be your own content ECS comes with
some basic logs and these are the places
where they get stored that's not the
focus as of now all right so I was just
saying that you can edit it and
customize it to your needs we're not
going to do any customization now we're
just getting familiar with ECS now and
the task definition name of the task
definition is first run task definition
and then we are running it in a VPC and
then this is an fargate mode meaning the
servers are completely handled by Amazon
and the task memory is 0.5 gigabit and
the task CPU is 0.25 virtual CPU name of
the service is it a batch job is it an
you know a front end is it an back end
or is it a simple copy job what's the
service name of the service goes here
again this you can edit it and here's a
security group as of now I'm allowing
for 80 to the whole world if I want to
rest it to a certain IP I can do that
the default option for load balancing is
no load balancer but I can also choose
to have a load balancer and use port 80
to map that Port 80 to The Container
Port 80 right I can do that the default
is no load balancer all right let's do
one thing let's use load balancer let's
use load balancer and Port 80 that
receives information on Port 80 HTTP
what's going to be the cluster name when
the last step what is the cluster name
cluster name can be simply learn ECS
demo next we're done and we can create
so it's launching a cluster as you can
see and it's picking the task definition
file that we've created and it's using
that to launch and service and then
these are the log groups that we
discussed and it's creating your VPC
remember ECS clubs well with the VPC
it's creating a VPC and it's creating
two subnets here for high availability
it's creating that Security Group Port
80 allowed to the whole world and then
it's putting it behind and load balancer
right generally would take like five to
ten minutes so which is need to be
patient and let it complete its creation
and once this is complete we can simply
access these servers using the load
balancer URL and when this is running
let me actually take you to the other
products or the other services that are
integrated with the ECS it's getting
created our service is getting created
as of now ECR repository this is where
all our images are stored now as of now
I'm not pulling my image from ECR I'm
pulling it directly from the internet
Docker Docker Hub but all custom images
all custom images they are stored in
this repository so you can create a
repository call it app one create
repository so here's my repository so
any image that I create locally or any
Docker image that I create locally I can
actually push them push those images
using these commands right here and they
get stored here and I can make my ECS
connect with ECR and pull images from
here so they would be my custom images
and as of now because I'm using a
default image it's directly pulling it
from the internet let's go to ec2 and
look for a load balancer because we
wanted to access the application from
behind a load balancer right so here is
a load balancer created for us and
anytime I put the URL so cluster is now
created you see there's one service
running all right let's click on that
cluster here is the name of our
application and here is the tasks the
different containers that we are running
and if you click on it we have an IP
right IP of that container and it says
it's running it was created at such and
such time and started at such in such
time and this is the task definition
file that it this container uses meaning
the template the details to all the
version details they all come from here
and it belongs to the cluster called
simply learn ECS demo right and you can
also get some logs container logs from
here so let's go back and there are no
ECS instances here because remember this
is forget you're not managing any ECS
instance right so that's why you're not
seeing any ECS instance here so let's go
back to tasks and go back to the same
page where we found the IP pick that IP
put it in the browser and you have this
sample HTML page running from an
container so let me go back to load
balancer ec2 and then under ec2 I'll be
able to find a load balancer find that
load balancer pick that DNS name put it
in the browser and now it's accessible
to the load balancer URL right now this
URL can be mapped to other services like
DNS this URL can be emboded in this
section we're going to take a look at
Amazon dynamodb
Amazon dynamodb is a fast flexible nosql
database service for all applications
that need consistent single digit
millisecond latency at any scale
so basically it's a fully managed nosql
database that's suitable for document
and key Value Store models it's perfect
for mobile web gaming ad Tech Etc
let's take a look at some of the
benefits of dynamodb firstly it's always
stored on SSD storage so you get great
disk throughput
it also spread across three
geographically distinct data centers so
you get high availability built in
it's fast and flexible
and it's suited for read heavy
applications and it's worth remembering
that because you might see questions
saying I have a read intensive
application with a database what should
I use and dynamodb would be the answer
dynamodb offers two different
reconsistancy types
the first is eventually consistent reads
which is the default and then they're
strongly consistent reads
so what does this mean well when you
write something to your dynamodb the
changes have to be replicated to the
three geographically distinct data
centers on which dynamodb is being
stored and obviously this isn't instant
I mean it takes a couple of seconds or
less but it takes some time so you can
choose your read consistency depending
on whether you want users to be
guaranteed to have the correct data or
whether on occasions when they run a
query they might not get the latest
up-to-date version
eventually consistence reads which is
the default means that consistency is
usually reached within one second
I'm repeating a read after a short time
should return the correct data
and it gives the best read performance
but there is the danger of something
called Dirty reads which is where you
might request a read and you might not
get the latest up-to-date data
if it's essential that your application
gets the latest in correct data then you
can enable strongly consistent reads and
this guarantees that all rights have
committed prior to the read taking place
so it could result in a delay to your
results being returned
dynamodb is charged using something
called provisioned throughput capacity
when you create a dynamodb table you
define the capacity that you want to
reserve for reads and writes
for right farupa there's a charge per
hour for every 10 units of right
capacity
and this equates to 36 000 writes per
hour
the read throughput there's a charge per
hour for every 50 units of read capacity
and this equates to 180 000 strongly
consistent reads or 360
000 eventually consistent reads
what this shows is that dynamodb can be
expensive for writes but cheap for reads
so if your application has lots of reads
little writes and wants to be scalable
then you should be looking at dynamodb
rather than RDS
there's also a storage cost per gigabyte
for the amount of data you use with
dynamodb
and here's a very brief real-life case
study Duolingo the online language
learning site uses dynamodb to store 31
billion items for their online learning
site and they deliver lessons in 80
different languages
and their application hits 24 000 read
units per second and 3 300 right units
per second so you can see this is a very
read heavy application and is perfect
for dynamodb
this demonstration we're going to take a
very quick look at dynamodb
so we go to the database section and
we'll find dynamodb which is the manage
nosql database provided by AWS
so we'll click on that we're taken to
the dashboard I have no dynamodb tables
so we need to create a new table
and we need to give it a name so let's
call this
simply learn
underscore users that's going to be the
name of our table we have to give it a
key so a primary key so I'm going to put
in last name
because that's going to be the primary
key for this particular table and that's
going to be a string
now down here we have table settings so
we can use the default settings and it
says no secondary indexes his provision
capacity is set to five reads and five
rights and has basic alarms of 80 upper
thresholds so if an alarm if one of your
read or write capacities is greater than
80 of what you've set then you'll get an
alert so if we untick this you can see
we're not going to take a look at
secondary indexes but the thing that's
important is here we can change to read
and write capacity units so if we set it
both to one we'll get an estimated cost
of just 59 cents a month there
conversely if we push this way up
push it up to a thousand then you see
it's 580 dollars a month so it's fully
scalable and you just sort of pay for
what you need so let's just set this to
one and one because this is just a
demonstration
and we'll click on create table
okay so table has been created okay so
our table has been created so here we
can see the details of it and its
creation date
so what we now need to do is we can add
items to our table so we click on the
items tab
okay and we need to create an item so
you see this is now this should look
familiar to you because we're basically
doing Json code which is the same as for
the policies in the I am lesson so we
get to stick in a last name so let's put
let's put me in so we'll stick Weaver
and then we can add
more values to append let's append a
string and we'll put a
first name
and we'll put my name in there
and we could put more information if we
wanted but we're just going to leave it
as that so we click on Save
and there we are so now I'm in there so
I'll click on create item
and let's add a new user in there so
call the last name Smith
let's add another string so we'll have a
new field called first name
and we'll call this person Wendy and
we'll add another row let's put a string
in there and we'll call this uh position
so this shows what you can do with nosql
you can basically just keep
changing things and each row can be
different so without enter the position
of manager and we'll click save so now
we can see that it's different so you
can do whatever you want basically you
can add as much or as little information
to each row as you wanted
now you can also do a search so if we
wanted to add a filter we could say
search for last name
a bit of a string that equals
and there we are so we have Weaver Mark
and just to clear that we just remove
that and click on start search again so
that gives you a very basic overview of
what you can do with nosql there's
unstructured data and you can add as
much or as little as you want some other
things to look at the metrics tab here
you can see your read and write capacity
what what's actually happening so you
can see whether you're paying too much
or if you need to be paying too much
more
as the alarms alarms that we talked
about so they were set up for us some
basic alarms if the threshold goes over
0.08 reads or writes for a 12 minute
period will get notified
and here's the capacity so this is the
thing that you'll see in the exam
is that dynamodb you can just change
your capacity and as and when you want
on the fly so
our table's up and running but we could
just say we want five read capacity
units and five right capacity units
click save and we're done it's up and
running straight away
and if you knew how much you wanted and
you had a table that you were going to
be using for a look for a long period
like a couple of years you can click on
reserve capacity on the left hand side
and you can purchase Reserve capacity
and you'll get some significant
discounts by doing that so if you know
you have a table that's going to have
some heavy reads and writes going
forward you would purchase Reserve
capacity
okay so that's a very basic overview of
dynamodb you just need to remember that
it's instantly scalable and it's a nosql
database hi guys I'm Raul from Simply
learn and today I'd like to welcome you
all to the greatest debate of the
century today I am joined by two giants
of the cloud computing industry they'll
be going head to head with each other to
decide who amongst them is better it's
going to be one hell of fight now let's
meet our candidates on my left we have
AWS who's voiced by a bake shop hi guys
and on my right we have Microsoft Azure
who's voiced by Anjali hey there so
today we'll be deciding who's better on
the basis of their origin and the
features they provide their performance
in the present day and comparing them on
the basis of pricing market share and
options free tier and instance
configuration now let's listen to their
opening statements let's start with AWS
launched in 2006 AWS is one of the most
commonly used cloud computing platforms
across the world companies like Adobe
Netflix Airbnb HTC Pinterest and Spotify
have put their faith in AWS for their
proper functioning it also dominates the
cloud computing domain with almost 40
percent of the entire market share so
far nobody's even gotten close to
beating that number AWS also provides a
wide range of services that covers a
great number of domains domains like
compute networking storage migration and
so much more now let's see what Azure
has to say about that Azure was launched
in 2010 and is trusted by almost 80
percent of all Fortune 500 companies the
best best companies in the world choose
to work only with Azure Azure also
provides its services to more regions
than any other cloud service provider in
the world Azure covers 42 regions
already and 12 more are being planned to
be made Azure also provides more than
100 Services spanning a variety of
domains now that the opening statements
are done let's have a look at the
current market status of each of our
competitors this is the performance
route here we have the stats for the
market share of AWS Azure and other
cloud service providers this is for the
early 2018 period Amazon web services
takes up a whopping 40 percent of the
market share closely followed by sgr at
30 percent and other cloud services
adding 30 this 40 indicates most
organizations clear interest in using
AWS VR number one because of our years
of experience and Trust we've created
among our users sure you're the market
leader but we are not very far behind
let me remind you more than 80 percent
of the Fortune 500 companies trust Azure
with their cloud computing needs so it's
only a match of time before Azure takes
the lead the rest of the 30 person that
is in AWS or Azure accounts to the other
cloud service providers like Google
Cloud platform Rackspace IBM software
and so on now for our next round the
comparison round first we'll be
comparing pricing we'll be looking at
the cost of a very basic instance which
is a virtual machine of two virtual CPUs
and 8 GBS of RAM for AWS this will cost
you approximately
0.0928 US dollars per hour and for the
same instance in Azure it will cost you
approximately 0.096 US dollars per hour
next up let's compare market share and
options as I mentioned before AWS is the
Undisputed market leader when it comes
to the cloud computing domain taking up
40 of the market share by 2020 AWS is
also expected to produce twice its
current Revenue which comes close to 44
billion dollars not to mention AWS is
constantly expanding its already strong
roaster of more than 100 services to
fulfill the shifting business
requirements of organizations all that
is great really good for you but the
research company Gartner has released a
magic quadrant that you have to see you
see the competition is now neck to neck
between Azure and AWS it's only a matter
of time before Azure can increase from
its 30 market share and surpass AWS this
becomes more likely considering how all
companies are migrating from AWS to
Azure to help satisfy their business
needs Azure is not far behind AWS when
this comes to Services as well azure's
service offerings are constantly updated
and improved on to help users satisfy
their cloud computing requirements now
let's compare AWS and azure's free
offerings AWS provides a significant
number of services for free helping
users get hands-on experience with the
platform products and services the free
tier Services fall under two categories
services that will remain free forever
and the others that are valid only for
one year the always free category offers
more than 20 services for example Amazon
SNS sqs cloudwatch Etc and the valid for
your category offers approximately 20
services for example Amazon S3 ec2
elastic cache Etc both types of services
have limits on the usage for example
storage number of requests compute time
Etc but users are only charged for using
services that fall under the valid for a
year category after a year of their
usage a shop provides a free tier as
well it also provides services that
belong to the categories of free for a
year and always free there are about 25
plus always free services provided by
Azure these include app service
functions container service active
directory and lots more and as of the
valid for a year there are eight
services offered there's Linux or
Windows Virtual machines blob storage
SQL database and few more Azure also
provides the users with credits of 200
US dollars to access all their services
for 30 days now this is a unique feature
that Azure provides where users can use
their credits to utilize any service of
a choice for the entire month now let's
compare instance configuration the
largest instance that AWS offers is that
over whopping 256 GBS of RAM and 16
virtual CPUs the largest that Azure
offers isn't very far behind either 224
GBS of ram s16 virtual CPUs and now for
the final round each of our contestants
will be shown facts and they have to
give explanations for these facts we
call it the rapid fire round first we
have features in which AWS is good and
Azure is better AWS does not cut down on
the features it offers its users however
it requires slightly more Management on
the user's part Azure goes slightly
deeper with the services that fall under
certain categories like platform as a
service and infrastructure as a service
next we have hybrid Cloud where AWS is
good and Azure is better OK although AWS
did not emphasize on hybrid Cloud
earlier they are focusing more on
technology now Azure has always
emphasized on hybrid cloud and has
features supporting it since the days of
its inception for developers AWS is
better and Azure is good of course it's
better because AWS supports integration
with third-party applications well Azure
provides access to data centers that
provide a scalable architecture for
pricing both AWS and Azure are at the
same level it's good for AWS because it
provides a competitive and constantly
decreasing pricing model and in the case
of azure it provides offers that are
constantly experimented upon to provide
its users with the best experience and
that's it our contestants have finished
giving their statements now let's see
who won surprisingly nobody each cloud
computing platform has its own pros and
cons choosing the right one is based
entirely on your organization's
requirements if you are looking to
become an AWS Cloud architect to excel
your career then have a look at AWS
Cloud architect Masters program by
simply learn with this certification
course you will learn architectural
principles scalability security and key
components like S3 and cloud formation
become expert in the tools like Amazon
ec2 AWS lambra Amazon dynamodb Amazon
cloud formation and much more our AWS
Cloud architect training is great for
beginners however AWS recommends at
least one year of relevant experience
before pursuing this program so hurry up
and try your hands on this amazing
course use discount coupon YT be15 to
get handsome discount find the course
Link in the description box if you are
looking to learn cloud computing enroll
into professional cloud computing
program in cloud computing and devops by
simply learn and gain hands-on
experience with industry projects one
year experiences prefer to grab the
course find the course Link in the
description box
hi guys today we got something very
special in store for you we're going to
talk about the best cloud computing
platform available Amazon web services
uh Rahul I think you said something
wrong here the best cloud computing
platform is obviously Google Cloud
platform no it isn't AWS is more than
100 services that span a variety of
domains all right but Google Cloud
platform has cheaper instances what do
you have to say about that well I guess
there's only one place we can actually
discuss this a boxing ring so guys I'm
apeksha and I will be Google Cloud
platform and I'm Rahul I'll be AWS so
welcome to fight night this is AWS
versus gcp the winner will be chosen on
the basis of their origin and the
features they provide the performance in
the present day and comparing them on
the basis of pricing market share and
options the things they give you for
free and instance configuration now
first let's talk about AWS AWS was
launched in 2004 and is a cloud service
platform that helps businesses grow and
scale by providing them services in a
number of different domains these domain
attention to compute database storage
migration networking and so on a very
important aspect about AWS is its years
of experience now AWS has been in the
market a lot longer than any other cloud
service platform which means they know
how businesses work and how they can
contribute to the business growing also
AWS has over 5.1 billion dollars of
Revenue in the last quarter this is a
clear indication of how much faith and
trust people have in AWS they occupy
more than 40 percent of the market which
is a significant chunk of the cloud
computing Market they have at least 100
services that are available at the
moment which means just about every
issue that you have can be solved within
AWS service now that was great but now
can we talk about gcp I hope you know
that GCB was launched very recently in
2011 and it is already helping
businesses significantly with a suite of
intelligent secure and flexible Cloud
Computing Services it lets you build
deploy and scale applications websites
services on the same infrastructure as
Google the intuitive of user experience
that gcp provides with dashboards
Wizards is way better in all the aspects
gcp has just stepped in the market and
it is already offering a modest number
of services and the number is rapidly
increasing and the cost for a CPU
instance or Regional storage that JCP
provides is a whole lot cheaper and you
also get a multi-regional cloud storage
now what do you have to say on that I'm
so glad you asked let's look at present
day in fact let's look at the cloud
market share of the fourth quarter of
2017. this will tell you once and for
all that AWS is the leader when it comes
to cloud computing Amazon web services
contributes 47 of the market share
others like Rackspace or Verizon cloud
contribute 36 Microsoft Azure
contributes 10 the Google Cloud platform
contributes four percent and IBM
software contributes three percent 47 of
the market share is contributed by AWS
you need me to convince you any more
wait wait wait all that is fine but we
only started a few years back and have
already grown so much in such a less
amount of time having to heard the
latest news our revenue is already a
billion dollars per quarter wait for a
few more years and the world shall see
and AWS makes 5.3 billion dollars per
quarter it's going to take a good long
time before you can even get close to us
yes yes we'll see now let's compare a
few things for starters let's compare
price for AWS a compute instance of two
CPUs in 8GB ram costs approximately 68
US Dollars now a computer instance is a
virtual machine in which you can specify
what operating system Ram or storage you
want to have for cloud storage it costs
2.3 cents per GB per month with AWS you
really want to do that because gcp wins
this hands down let's take the same
compute instance of two CPUs with 8GB
Ram it will cost approximately 50
dollars per month with gcp and as per my
calculations that's a 25 annual cost
reduction when compared to AWS talking
about cloud storage costs it is only 2
cents per GB per month with gcp what
else do you want me to say let's talk
about market share and options now AWS
is the current market leader when it
comes to cloud computing now as you
remember we contribute at least 47 of
the entire market share AWS also has at
least 100 services available at the
moment which is a clear indication of
how well AWS understands businesses and
helps them grow yeah that's true but you
should also know that gcp is steadily
growing we have over 60 services that
are up and running as you can see here
and a lot more to come it's only a
matter of time when we will have as many
services as you do many companies have
already started adopting gcps a cloud
service provider now let's talk about
things you get for free with AWS you get
access to almost all the services for an
entire year with usage limits now these
limits include an hourly or by the
minute basis for example with Amazon ec2
you get 750 hours per month you also
have limits on the number of requests to
services for example with AWS Lambda you
have 1 million requests per month now
after these limits across you you get
charged standard rates with gcp you get
access to all Cloud platform products
like Firebase the Google Maps API and so
on you also get 300 in credit to spend
over a 12 month period on all the cloud
platform products and interestingly
after the free trial ends you won't be
charged unless you manually upgrade to a
paid account now there is also the
always free version for which you will
need an upgraded billing account here
you get to use a small instance for free
and 5 GB of cloud storage any usage
above this always free usage limits will
be automatically billed at standard
rates now let's talk about how you can
configure instances with AWS the largest
instances offered is of 128 CPUs and 40
views of ram now other than the
on-demand method like I mentioned before
you can also use Spartan sensors now
these are for situations where your
application is more fault or rent and
can handle an interruption now you pay
for the spot price which is effective at
a particular hour now these spot prices
do fluctuate but are adjusted over a
period of time the largest instance
offered with Google cloud is 160 CPUs
and 3.75 TBS Ram like spot instances of
AWS Google Cloud offers short-lived
compute instances suitable for back jobs
and fault tolerant workloads they are
called as preemptable instances so these
instances are available at 80 percent
off on on-demand price hence they've
reduced your compute engine costs
significantly and unlike AWS these come
at a fixed price Google Cloud platform
is a lot more flexible when it comes to
instance configuration you simply choose
your CPU and RAM combination of course
you can even create your own instance
types this way before we wrap it up
let's compare on some other things as
well Telemetry it's a process of
automatically collecting periodic
measurements from remote devices for
example GPS gcp is obviously better
because they have Superior Telemetry
tools which help in analyzing services
and providing more opportunities for
improvement when it comes to application
support AWS is obviously better since
they have years of experience under
their belt AWS provides the best support
that can be given to the customers
containers are better with gcp a
container is a virtual process running
in user space as kubernetes was
originally developed by Google gcp has
full native support for the tool other
cloud services are just fine-tuning a
way to provide kubernetes as a service
also the containers help with
abstracting applications from their
environment they originally run it the
applications can be deployed easily
regardless of their environment when it
comes to geographies AWS is better since
they have a head start of a few years
AWS in this span of time has been able
to cover a larger market share and
geographical locations now it's time for
the big decision so who's going to be
yeah who is it going to be gcp or AWS I
think I'm going to go for choosing the
right cloud computing platform is a
decision that's made on the basis of the
user or the organization's requirement
on that that note I believe it's time
for us to wrap this video up we hope you
guys enjoyed this video and learned
something new that AWS is better right
know that choosing a platform entirely
depends on you and your organization's
requirement if you're interested you
could also go through the AWS versus
visual video AWS as you are in gcp are
three of the world's largest cloud
service providers but how are they
different from each other let's find out
hey guys I'm Rahul and I'll be
representing Amazon web services I'm
chinmayin I'll be representing Microsoft
Azure and I am Shruti and I'll be
representing Google Cloud platform so
welcome to this video on AWS vs Azure
versus gcp talking about market share
Amazon web services leads with around 32
percent of the worldwide public cloud
share Azure owns up to 16 of the
worldwide market share and gcp owns
around nine percent of the world's
market share let's talk about each of
these service providers in detail AWS
provides services that enable users to
create and deploy applications over the
cloud these services are accessible via
the Internet AWS being the oldest of the
lot was launched in the year 2006. Azure
launched in 2010 is a Computing platform
that offers a wide range of services to
build manage and deploy applications on
the network using tools and Frameworks
launched in the year 2008 gcp offers
application development and integration
services for its end users in addition
to Cloud management it also offers
services for Big Data machine learning
and iot now let's talk about
availability zones these are isolated
locations within data center regions
from which public cloud services
originate and operate talking about AWS
they have 69 availability zones within
22 geographical regions this includes
regions in the United States South
America Europe and Asia Pacific they are
also predicted to have 12 more editions
in the future Azure available in 140
countries has over 54 regions worldwide
grouped into six geographies these
geographical locations have more than
100 data centers gcp is available in 200
plus countries across the world as of
today gcp is present in 61 zones and 20
regions with Osaka and Zurich being the
newly added regions now let's talk about
pricing these Services follow the pay as
you go approach you pay only For The
Individual Services you need for as long
as you use them without requiring
long-term contracts or complex licensing
now on screen you can see the pricing
for each of these cloud service
providers with respect to various
instances like general purpose compute
optimized memory optimized and GPU now
let's talk about the compute services
offered first off we have virtual
servers for AWS we have ec2 it is a web
service which eliminates the need to
invest in Hardware so that you can
develop and deploy applications in a
faster manner it provides virtual
machines in which you can run your
applications azure's virtual machines is
one of the several types of computing
resources that Azure offers Azure gives
the user the flexibility to deploy and
manage a virtual environment inside a
virtual Network G gcp's VM service
enables users to build deploy and manage
virtual machines to run workloads on the
cloud now let's talk about the pricing
of each of these Services awscc2 is free
to try it is packaged as part of aws's
free tier that lasts for 12 months and
provides 750 hours per month of both
Linux and Windows Virtual machines Azure
virtual machine service is a part of the
free tier that offers this service for
about 750 hours per month for a year the
user gets access to Windows and Linux
virtual machines gcp's VM service is a
part of a free tier that includes micro
instance per month for up to 12 months
now let's talk about platform as a
service or past services for AWS elastic
Beanstalk is an easy to use service for
deploying and scaling web applications
and services developed with java.net
node.js python and much more it is used
for maintaining capacity provisioning
load balancing Auto scaling and
application Health monitoring the past
backbone utilizes virtualization
techniques where the virtual machine is
independent of of the actual Hardware
that hosts it hence the user can write
application code without worrying about
the underlying Hardware Google app
engine is a cloud computing platform as
a service which is used by Developers
for hosting and building apps in Google
data centers the app engine requires the
apps to be written in Java or Python and
store data in Google bigtable and use
the Google query language for this next
let's talk about virtual private server
Services AWS provides light sale it
provides everything you need to build an
application or website along with the
cost effective monthly plan and minimum
number of configurations in simple words
VM image is a more comprehensive image
for Microsoft Azure virtual machines it
helps the user create many identical
virtual machines in a matter of minutes
unfortunately gcp does not offer any
similar service next up we have
serverless Computing Services AWS has
Lambda it is a serverless compute
service that lets you run your code
without facilitating and managing
servers you only pay for the compute
time you use it is used to execute
back-end code and scales automatically
when required Azure functions is a
serverless compute service
to run even triggered code without
having to
extra structure this allows the users to
build applications using serverless
simple functions with the programming
language of their choice gcp Cloud
functions make it easy for developers to
run and scale code in the cloud and
build image driven serverless
applications it is highly available and
fault tolerant now let's talk about
storage services offered by each of
these service providers First off we
have object storage AWS provides S3 it
is an object storage that provides
industry standard scalability data
availability and performance it is
extremely durable and can be used for
storing as well as recovering
information or data from anywhere over
the Internet blob storage is an Azure
feature that lets developers store
unstructured data in Microsoft's Cloud
platform along with storage it also
offers scalability it stores the data in
the form of tiers depending on how often
data is being accessed Google Cloud
Storage is an online storage web service
for storing and accessing data on Google
Cloud platform infrastructure unlike the
Google Drive Google can cloud storage is
more suitable for Enterprises it also
stores objects that are organized into
buckets Amazon provides EBS or elastic
Block store it provides high performance
block storage and is used along with ec2
instances for workloads that are
transaction or throughput intensive
Azure managed disk is a virtual hard
disk you can think of it like a physical
disk in an on-premises server but
virtualized these managed disks allow
the users to create up to 10 000 VM
disks in a single subscription
persistent storage is a data storage
device that retains data after power to
the device is shut off Google persistent
disk is durable and high performance
block storage for gcp persistent disk
provides storage which can be attached
to instances running in either Google
compute engine or kubernetes engine next
up we have Disaster Recovery Services
AWS provides a cloud-based recovery
service that ensures that your it
infrastructure and data are recovered
while minimizing the amount of downtime
that could be experienced used site
recovery helps ensure business
continuity by keeping business apps and
workloads running during outages it
allows recovery by orchestrating and
automating the replication process of
azure virtual machines between regions
unfortunately gcp has no disaster
recovery service next let's talk about
database Services first off for AWS we
have RDS or relational database service
it is a web service that's cost
effective and automates administration
tasks basically it simplifies the setup
operation and scaling of a relational
database Microsoft Azure SQL database is
a software as a service platform that
includes built-in intelligence that
learns app patterns and adapts to
maximize performance reliability and
data protection it also eases the
migration of SQL Server databases
without changing the user's applications
Cloud SQL is a fully managed database
service which is easy to set up maintain
and administer relational postgresql
MySQL and SQL Server databases is in the
cloud hosted on gcp cloud SQL provides a
database infrastructure for applications
running anywhere next we have nosql
database Services AWS provides dynamodb
which is a managed durable database that
provides security backup and restore and
in-memory caching for applications it is
well known for its low latency and
scalable performance Azure Cosmos DB is
Microsoft's globally distributed
multi-model database service it natively
supports nosql it natively supports
nosql created for low latency and
scalable applications gcp cloud data
store is a nosql database service
offered by Google on the gcp it handles
replication and scales automatically to
your applications load with cloud data
stores interface data can easily be
accessed by any deployment Target now
let's talk about the key Cloud tools for
each of these service providers for AWS
in networking and content delivery we
have AWS Route 53 and AWS cloudfront for
management we have AWS Cloud watch and
AWS cloud formation for development we
have AWS code star and AWS code build
for security we have IAM and Key
Management Service for Microsoft Azure
networking and content delivery we have
content delivery Network and express
route for management tools we have Azure
advisor and network Watcher for
development tools for management we have
Azure advisor and network Watcher for
development we have Visual Studio IDE
and Azure blob studio for security we
have Azure security Center and Azure
active directory for gcp we have the
following tools for networking and
content delivery we have Cloud CDN and
Cloud DNS for management we have
stackdriver and gcp monitoring for
development we have Cloud build and
Cloud SDK and finally for security we
have Google cloud IM and Google and
Cloud security scanner now let's talk
about the companies using these Cloud
providers for AWS we have Netflix
Unilever Kellogg's NASA Nokia and Adobe
Pixar Samsung eBay Fujitsu EMC and BMW
among others use Microsoft so as seen on
your screens the companies that use gcp
are Spotify HSBC Snapchat Twitter PayPal
and 20th Century Fox let's talk about
the advantages of each of these Services
Amazon provides Enterprise friendly
Services you can leverage Amazon's 15
years of experience delivering
large-scale Global infrastructure and it
still continues to hone and innovate its
infrastructure management skills and
capabilities secondly it provides
instant access to resources AWS is
designed to allow application providers
isvs and vendors to quickly and securely
host your applications whether an
existing application or a new SAS based
application Speed and Agility AWS
provides you access to its services
within minutes all you need to select is
what you require and you can proceed you
can access each of these applications
anytime you need them and finally it's
secure and reliable Amazon enables you
to innovate and scale your application
in a secure environment it secures and
hardens your infrastructure more
importantly it provides security at a
cheaper cost than on-premise
environments now talking about some of
the advantages of azure Microsoft Azure
offers better development operations it
also provides strong security profile
Azure has a strong focus on security
following the Standard Security model of
detect assess diagnose stabilize and
close Azure also provides a
cost-effective solution the cloud
environment allows businesses to launch
both customer applications and internal
apps in the cloud which saves an ID
infrastructure costs hence it's Opex
friendly let's now look at the
advantages of gcp Google bills in minute
level increments so you only pay for the
compute time you use they also provide
discounted prices for long-running
workloads for example you use the VM for
a month and get a discount gcp also
provides live migration of virtual
machines live migration is the process
of moving a running VM from one physical
server to another without disrupting its
availability to the users this is a very
important differentiator for Google
Cloud compared to other Cloud providers
gcp provides automatic scalability this
this allows a site container scale to as
many CPUs as needed Google Cloud Storage
is designed for 99.9 durability it
creates server backup and stores them in
an user configured location let's talk
about the disadvantages of each of these
services for AWS there's a limitation of
the ec2 service AWS provides limitations
on resources that vary from region to
region there may be a limit to the
number of instances that can be created
however you can request for these limits
to be increased secondly they have a
technical support fee AWS charges you
for immediate support and you can opt
for any of these packages developer
which costs 29 per month business which
costs more than 100 an Enterprise that
costs more than fifteen thousand dollars
it has certain network connectivity
issues it also has General issues when
you move to the cloud like downtime
limited control backup protection and so
on however most of these are temporary
issues and can be handled over time
talking about some of the disadvantages
of Microsoft Azure code base is
different when working offline and it
requires modification when working on
the cloud pass system is not as
efficient as iaas Azure Management
console is frustrating to work with it
is slow to respond and update and
requires far too many clicks to achieve
simple tasks Azure backup is intended
for backing up and restoring data
located on your on-premises servers to
the cloud that's a great feature but
it's not really useful for doing bare
metal industry stores of servers in a
remote data center let's now look into
the disadvantages of gcp so when it
comes to Cloud providers the support fee
is very minimal but in the case of gcp
it is quite costly it is around 150
dollars per month for the most basic
service similar to AWS S3 gcp has a
complex pricing schema also it is not
very budget friendly when it comes to
downloading data from Google cloud
storage and today we're going to talk
about AWS certifications so what exactly
is an AWS certification it represents a
degree of expertise in AWS it is
obtained after passing one or more exams
provided by Amazon and each different
role like thesis Ops administrator
developer or Solutions Arc has a
different exam associated with it this
in turn helps employees demonstrate and
validate technical Cloud knowledge which
means that the people who are certified
actually know what they're talking about
I have seen a common misconception among
people where they believe that just
because they're certified they're
entitled a job that's not true now
without proper practice or hands-on
experience the certification is wasted
on you so remember just because you're
certified does not mean you'll get a job
it only makes the job application
process slightly easier relative to
someone who's not been certified now why
is AWS given so much importance some of
the important reasons are an AWS
certified individual gains credibility
for their skills in AWS they also gain
access to free practice exams which help
them prepare for the next AWS
certification there's an increase in
monetary compensations which only means
that they get paid more and you gain
recognition for your Knowledge and
Skills in AWS right now we're in the AWS
certification website whose link will be
in the description and now we're going
to talk about the types of AWS
certification as you can see here there
are three levels of aw certification
there's the the foundational level
associate level and professional level
certification now the foundational level
certification only requires you to have
a basic understanding of how the AWS
Cloud works for the AWS certified dot
practitioner is optional for the
architect path to develop a path and
operations path it is mandatory for the
specialty certifications like the
advanced networking big data and
security certifications now the
associate level certifications are
mid-level certifications for a technical
role now a professional certification is
the highest level of certification that
you can have for a technical role now
you have the solutions architect for the
architect path and the devops engineer
certification for both the developer and
operations path so how do you decide
which of these certifications are
suitable for you so you've seen here
that AWS provides various certifications
for a number of job roles axisops
administrator solution architect
developer so you need to make the right
choice taking into consideration the
areas of your interest and the
experience level that you have now we're
going to talk about each of these
certifications in detail so first let's
talk about the AWS certified Cloud
practitioner now we all understand that
AWS is a while recognized product in the
market so this certification helps you
validate how well you know the AWS Cloud
so this is just the basic understanding
now it is optional for the developer
path on the operations path I would
suggest it's a good idea to start here
because it forms a solid Bedrock on all
the other things that you're going to
learn soon now more importantly it does
not require any technical knowledge of
other roles such as development
architecture Administration and so on so
it's a great place to start for
newcomers now you have the architect
role certifications now this is for you
if you are interested in becoming a
Solutions architect or a solution design
engineer or someone who just works with
designing applications or systems on the
AWS platform now first we have the AWS
certified Solutions architect associate
level certification now this
certification is for you if you want to
show off how well you can architect and
deploy applications on the AWS platform
now it is recommended that you have at
least a year of experience working with
distributed systems on the AWS platform
at the same time it's also required that
you understand the AWS services and be
able to recommend a service based on
requirements you need to be able to use
architectural best practices and you
need to estimate the AWS cost and how
you can reduce them next up you have the
AWS certified Solutions architect
professional level certification now you
will not get the certification unless
you're done with the AWS certified
Solutions architect associate level
certification this is the show of your
technical skills and experience in
designing distributed applications on
the AWS platform now this does require
you to have two years of experience
working with Cloud architecture on AWS
at the same time it also requires you to
be able to evaluate requirements and
then make architectural recommendations
you also need to provide guidance on the
best practices on architectural design
across a number of different platforms
the developer level certifications are
for you if you are interested in
becoming a software developer now the
AWS certified developer associate
certification is to test how well you
know how to develop and maintain
applications on the AWS platform it does
require you to have a year or more of
hands-on experience to design and
maintain AWS based applications like any
software developer role it is necessary
that you know know in depth at least one
high level programming language it's
also necessary that you understand the
core of AWS Services uses and basic
architectural best practices you need to
be able to design develop and deploy
cloud-based Solutions on AWS platform
and you need to understand how
applications can be created you need to
have experience in developing and
maintaining applications for a number of
AWS services like Amazon SNS dynamodb
sqs and so on now for the AWS certified
devops engineer professional level
certification note here that this
certification is exactly the same as the
one you have under the operation sold so
both of them are the same thing so here
it tests your ability to create operate
and manage distributed applications on
the AWS platform now it is necessary or
it is mandatory to have the AWS
certified developer associate
certification or the AWS certified
sysops administrator certification with
two or more years of hands-on experience
in doing the same in AWS environment it
requires you to be able to develop code
in at least one high level language you
need to be able to automate and test
applications via scripting and
programming and to understand agile or
other development processes the
operation certifications are for you if
you want to become a cisops
administrator systems administrator or
someone in devops role wants to deploy
applications networks and systems in an
automatable and beatable way the AWS
certified Sops administrator associate
certification tests your knowledge in
deployment management and operations on
the AWS platform now you need to have
one or more years of hands-on experience
in AWS based applications you need to be
able to identify and gather requirements
then Define a solution to be operated on
AWS you need to be able to provide
guidance for the best practices through
the life cycle of a project as well now
the specialty certifications are for you
if you're well versed in AWS and want to
showcase your expertise in other
technical areas the aw certified Big
Data certification showcases your
ability to design and Implement AWS
Services which can help derive value
from a large amount of complex data you
are however required to have completed
the foundational or associate level
certification before you can attempt
this you need a minimum of five years of
hands-on experience in the data and
analytics field as well next we have the
aw certified Advanced networking
certification this validates your
ability to design and Implement AWS
Solutions as well as other hybrid ID
Network architectures at scale this also
requires you to have completed the
foundational or associate level
certification you need to have a minimum
of five years of hands-on experience
architecting and implementing Network
Solutions and lastly we have the AWS
certified security certification it
helps showcase your ability to secure
the AWS platform you're required to have
an associate or Cloud practitioner level
of certification a minimum of five years
of it security experience and two years
of hands-on experience securing AWS
workloads now say I wanted to schedule
an examination so for example I want to
do the solutions architect certification
so first I would go there now here I can
click on register now and the process
continues or I can click on learn more
by doing this again I can show you the
examination here I can also get access
to other data like the number of
questions available the cost of an
examination the portions I need to study
and so on now let's talk about Solutions
architect certification with a little
more detail now this certification exam
costs 150 US Dollars and the practice
exam cost 20 US Dollars now here I can
schedule the examination or download the
exam guide I've already downloaded the
exam guide and here it is now this exam
guide tells you about what you need to
learn and what is expected from you here
they want you to define a solution based
on requirements and provide guidance in
its implementation it is also
recommended that you know about how the
AWS Services work one years of hands-on
experience with distributed systems on
AWS to identify and Define technical
requirements and so on the rest is
available in the exam guide and most
importantly they tell you the main
content domains and their weightages now
we have five domains first domain is to
design resilient architectures which
holds 34 percent of weightage it depends
too you have to Define performance
architectures three is to specify secure
applications and architectures cost
optimized architectures and five to
define operationally excellent
architectures now like you've seen here
you've selected one certification and
learned it in detail you can do the same
for any of these other certifications
you can press learn more and download
their exam guide and learn everything
that you need to know and welcome to
this session on AWS certification in
this session I'll be telling you about
several things like why AWS is so
important next we'll talk about the job
roles in AWS the career path to become
an AWS Solutions architect the
certifications that are available in AWS
and companies that are hiring in 2020
also guys if you have any questions
regarding the topic or AWS let us know
in the comment section below our team of
experts will help you as soon as
possible
now before we get on to this let's talk
about why or what exactly is AWS
AWS or Amazon web services is a cloud
computing platform that offers more than
175 different Services across several
domains
now among these several cloud service
providers in the market right now like
Microsoft Azure VMware Rackspace and so
on AWS is more commonly used than any of
its competitors
but what makes AWS so special why must
you get certified by them so to answer
our earlier question
let's have a look at 2019's third
quarters Cloud market share which is
provided by Canada's now AWS dominated
the cloud service market share with 32
percent of the entire share
and it had a growth of over 2.3 billion
dollars year over year now you can see
in this graph here that Azure and gcp
only account for 17 and seven percent
respectively the others like Rackspace
and IBM Cloud are contributing 44
percent
now if you do need more convincing as to
choosing AWS let me give you more
reasons why
firstly it provides pay-as-you-go
pricing which means you only pay for how
much you use
now you use what you need
and then the costs are scaled
automatically and accordingly now for
example say you use ec2 which provides
virtual machines you only use for how
much virtual machines you've created and
how long you use them
AWS is also very secure now this
provides end-to-end privacy
and security and Storage
AWS also has experience now AWS has over
four years of leg up over other cloud
service providers like Microsoft azure
this means over the years they have
skills and infrastructure management
that can give you more
next AWS is also very flexible
this allows users to select their
operating system language database and
other services with great ease
AWS is also very easy to use you can
host your applications very quickly
and securely whether it's existing or a
completely new application
AWS is also extremely scalable now if
you're a small scale company you don't
need a lot of resources
AWS understands that and scales down if
you're a larger organization obviously
you're going to need a lot more
resources
AWS can help with that as well
now let's talk about the job roles
within AWS
now here are some of them these are
these aren't all of them but we have AWS
Cloud architect sysops administrator
Cloud developer system integrator Cloud
devops engineer
Solutions architect Network specialist
and Big Data specialist
now getting each of these roles becomes
much easier when you're certified by AWS
now let's talk about the steps you need
to take so that you can become an AWS
Solutions architect
now we're selecting this one because
this is one of the most common and
talked about Job roles in AWS
now most of the job job roles I
mentioned earlier have similar steps but
but with a few steps added or remote
somewhere else
now first off you need an understanding
of operating systems
you'll need to have a basic
understanding of operating systems like
Windows or Linux now although this isn't
exactly mandatory this will help you
understand cloud services better
since most of them are based on Linux
next up you need to have experience with
programming languages
languages like SQL go Python c-sharp.net
and so on will help you develop
cloud-based applications based on
requirements from your organization now
although this isn't mandatory either
some services do require coding so that
you can use them properly in the next
step we have networking you need to
understand networking fundamentals like
virtual networks local area networks
wide area networks networking models and
so on now this is really important
because this will help you understand
how the backend infrastructure of the
cloud works
after that the most self-explanatory
step learn AWS now you need to focus on
learning the different services that are
offered by AWS now AWS provides a
year-long free tier where you get access
to several important services like ec2
and S3
so you can use it to get practical
experience with these services
now you can decide among several
different career paths that are offered
by AWS
paths like Cloud practitioner Cloud
architect Cloud developer operations
devops engineer and certain some others
and finally you need to be certified by
AWS
you need to choose from different levels
of certification that are provided by
AWS
these can help you showcase your
Proficiency in working with different
AWS Solutions
now let's talk about certifications
available in AWS now as you can see on
screen there are three levels four
actually firstly you have foundational
level certification that includes the
cloud practitioner certification
now for this you need to have six months
of basic Cloud knowledge and Industry
knowledge as well
so basically the cloud practitioner
certification access the starting point
for just about every other job role you
want to take beat architect operational
or developer
then in associate you need to have a
year of experience working with AWS
Cloud solving Solutions and implementing
Solutions you have options for Solutions
architect sysops administrator developer
and so on and finally you have
professional level certification now
after two years of experience with the
AWS Cloud you can take up the
certification you have options of
solutions architect professional and
devops engineer professional
now there are certain specialty
certifications that are provided by AWS
these are based on Advanced networking
big data security machine learning and
Alexa skill Builder
now let's have a look at aws's website
let's go to Google
AWS certification
and here you can see that
like like I told you here are the
certifications let's go to the bottom
here
and you can see that here all the
certifications that I mentioned are
divided into different levels like I
told you Cloud practitioner architect
developer operations and devops engineer
let's have a look at Cloud practitioner
so here you can see the steps you need
to take to become a cloud practitioner
now AWS suggests that you take up their
course and then follow it up by taking
the certification test
now similarly let's have a look at how
to become an AWS Cloud architect
you can select the link here
so like we saw with Cloud practitioner
they give you a set of courses you need
to take followed up by a
certification test
basically you need to first get the
associate level certification and then
move on to the professional level
certification after you've gotten some
experience and knowledge similarly you
can look at other job roles like
developer
very similar and operations likewise
everything else you can see here
now let's go back and talk about the
companies that are hiring in 2020 so
this is basically every company that's
hiring in 2020. so you have examples
like IMDb Lamborghini Adobe bitdefender
HTC and so much more this is just the
tip of the iceberg hi guys this is Raul
from Simply learn and today I'm going to
tell you how you can become an AWS
Solutions architect so who is an aw
Solutions architect now the main role of
an AWS Solutions architect is to help
you deploy your applications on the AWS
platform now this is nothing but a cloud
computing platform so it's not to say
that you can't deploy your applications
on the cloud computing platform yourself
it's just that when it comes to
organizations the applications that you
need to deploy become a whole lot more
complex that's where an aw Solutions
architect can help ever since cloud
computing became a thing companies
around the world have started migrating
their physical infrastructure onto the
cloud that's what an AWS Solutions
architect does they help you migrate
your physical infrastructure onto the
AWS Cloud companies around the world
work on a budget and an aw Solutions
architect will help design a cloud
infrastructure based on the
organization's budget before that can be
done however an aw Solutions architect
has to create a design with an intricate
and detailed blueprint of the cloud
infrastructure that they plan to set up
now aw Solutions Architects also have to
focus mainly on non-functional
requirements like usability reliability
scalability and performance of the cloud
infrastructure they're also responsible
when it comes to minimizing risks that
an organization can face when it comes
to cloud computing platforms now they
could face risks like security leaks
calculation mistakes and application
downtimes and AWS Solutions architect
has to ensure that these don't happen
before we can talk about how you can
become an AWS Solutions architect I have
some exciting news for you guys we've
launched our own YouTube Community
you'll get to see a lot of quizzes polls
offers and much more to make your
learning experience a whole lot more fun
you can find all of this on your
subscription feed or you can also click
on the top right corner right now to get
started and let's get back to how can
you become an aw solution architect now
to become an aw certified Solutions
architect you need to clear the aw
certified Solutions architect associate
level examination now here's some
details about it the exam score ranges
from 100 to 1000 marks and the minimum
passing score is 720. however there's a
catch the passing marks are actually set
using statistical analysis so they can
be changed based on how difficult the
examination actually is the exam fee is
150 dollars and you can also take a
practice examination which costs 20
dollars now regardless of the
examination you take be IT solutions
architect sysops administrator or
developer any associate level
examination costs 150 dollars for the
professional level examination it's 300
now if you want to learn more about the
AWS certifications I suggest you click
on the top right corner and watch our
video aw certifications in 10 minutes to
learn more now the exam duration is of
130 minutes and you have two types of
questions multiple choice and multiple
answer now the multiple choice questions
have four options out which one of them
is right and you have multiple answer
where you have five options out of which
two of them are correct you can take the
examination in English Japanese Korean
and simplify Chinese now let's talk
about how you can schedule an
examination first let's go to Google and
search for AWS certifications
click on the first link
so on this page you can go to the bottom
and find the different certifications
AWS provides
click on architect
and select the AWS certified Solutions
architect associate certification click
on register now
and here you need to click on the AWS
certification account
you can sign in with your Amazon account
or create a new one
I already have an Amazon account that
I'll be using for signing in here in
case you don't you can click on the
create your Amazon account button here
and create an account for yourself
now after that is done you can schedule
new examination
and you can scroll down to the bottom
here aw certified Solutions architect
associate level certification
click on schedule exam
press continue
and here you need to select your
language
which I'm assuming is English
I am from India so I don't need to
change anything here but you can change
your country or time zone and other
details based on your requirement and
select your preferred month I want to do
it in October
now select search for Exam Center
select the one that's closest to you
I'm going to select this one
and here you can select the day that you
want to take your test and the time
available I want to do from 15
select the time that I want
and press continue
now you can go through all the details
here change them if you want to
otherwise you can press continue
after this is done
close
and here we have the final step which is
the fees
you can enter your details here
and pay now to finish the process now
let's look at an outline of the exam
content now what you're going to see in
the exam are five major domains and here
you have each of the domains with their
respective weightages first you have a
34 designing resilient architectures at
24 you have both defining performant
architectures and specifying secure
applications and architectures a 10
person you have designing cost optimized
architectures and at six percent you
have defining operationally excellent
architectures now let's look at each of
these domains in detail now the first
domain or design resilient architectures
can be divided into four parts firstly
you need to know how you can choose
reliable or resilient storage using
services like AWS S3 AWS Glacier and AWS
EBS then you have how you can design
decoupling mechanisms using AWS Services
now this is possible with the help of
AWS SNS now these aren't the only
services that enable this these are just
some of the services then how you can
design a multi-tier architecture
solution now this is important because
you need to know how you can create a
solution that involves several other
services then you need to know how you
can design highly available and fall
tolerant architectures now for the
second domain defining performance
architectures first you have how to
choose performance storages and
databases services that are used are AWS
RDS AWS threadshift and AWS dynamodb the
second step is how you can apply caching
to improve performance a service that
can be used for this is AWS elastic
cache third how you can design Solutions
with elasticity and scalability you have
AWS Lambda AWS cloudwatch and AWS data
pipeline now for the third domain which
is specifying secure applications and
architectures you need to know how you
can secure applications using services
like AWS inspector AWS cloudtrail and
AWS iaf you need to know how to secure
data using Cloud HSM and AWS Macy and
and how you can Define the networking
infrastructure using cloudfront VPC and
elastic load balancer for the fourth
domain you have designing cost optimized
architectures firstly you need to know
how you can design cost optimized
compute Solutions you can use AWS ec2
plastic bean stock Lambda and AWS light
sail then you need to know how you can
design cost optimized Storage Solutions
using AWS S3 Glacier EBS and elastic
file system and the final domain to
define operationally excellent
architectures you need to set up design
features and solutions that enable
operational excellence now some of the
features are that you perform operations
as code you annotate documentation you
make frequent and small reversible
changes and anticipate and tactical
failures now let's have a look at the
job opportunities when it comes to AWS
Solutions architects now if you have a
look at this graph you can see that AWS
has always provided a lot more job
postings as compared to the other two
giants in the cloud computing domain
which are Microsoft Azure and Google
Cloud platform now if you do a little
bit of research you'll find out that
there's a significant lacking of
experienced Personnel when it comes to
AWS so I would suggest this is the best
time for you to get certified in AWS so
how can simply learn help you with your
AWS certifications now we're on the
sampling on home page
and here we have the AWS Solutions
architect course
on this course which is the AWS
Solutions architect certification
training course we have 36 hours of
instructor-led training 20 hours of
self-paced learning 16 live demos three
simulation exams three Hands-On practice
projects and so much more here our main
emphasis is on making sure that you have
enough hands-on experience with the
services that you crack the examination
at your first attempt
so here we also go through some of the
more important services that you need to
learn like IAM VPC ec2 S3 and so much
more we'll also talk about databases
application Services security practices
disaster recovery and so on we even have
practice examinations to help you
prepare for the real one if you are
looking to become an AWS Cloud architect
to excel your career then have a look at
AWS Cloud architect Masters program by
simply learn with this certification
course you will learn architectural
principles scalability security and key
components like S3 and cloud formation
become expert in the tools like Amazon
ec2 AWS lambra Amazon dynamodb Amazon
cloud formation and much more our AWS
Cloud architect training is great for
beginners however AWS recommends at
least one year of relevant experience
before pursuing this program so hurry up
and try your hands on this amazing
course use discount coupon YT be 15 to
get handsome discount find the course
Link in the description box if you are
looking to learn cloud computing enroll
into professional cloud computing
program in cloud computing and devops by
simply learn and gain hands-on
experience with industry projects
one-year experiences prefer to grab the
course find the course Link in the
description box welcome to another video
by simply learn today I am going to help
you understand how to pass the AWS Cloud
practitioner exam so let's get started
before we get started let us look at
today's agenda first we will go through
what is AWS Cloud practitioner and its
eligibility criteria then we will go
through the exam objectives and its exam
content further we will learn how to
prepare for the exam and look at a few
sample questions and finally we will
learn a few tricks related to the exam
now what is AWS Cloud practitioner AWS
has a variety of certifications under
foundational level comes AWS Cloud
practitioner going up we can achieve
professional and advanced level
certificates AWS gives a variety of
specialty certificates which is
something very unique it is a
foundational level certificate provided
by AWS it requires six months of AWS
cloud and Industry knowledge becoming an
AWS certified Cloud practitioner is
recommended it is an optional step
towards achieving an associate level a
specialty certification with the cloud
practitioner certificate you can get a
job as a cloud engineer or AWS
practitioner in many companies now let
us talk about the eligibility criteria
every exam has its own eligibility in
order to take the certification the
candidate must follow two important
criteria first he should have at least
six months of experience working with
AWS Cloud Concepts in various rules like
technical sales and finance next it is
mandatory for him to have knowledge of
the AWS Cloud platform and it's working
also he should know about the various ID
Services now let's talk about the exam
overview the exam duration is 90 minutes
the candidate should get 60 five percent
of the answers right in order to pass
the exam the question types are multiple
choice and multiple response questions
the cost of this exam is hundred dollars
every certificate is valid for a few
months or years AWS Cloud practitioner
is valid for two years after two years
this certificate needs to be renewed as
in the exam needs to be taken again
let's get into the exam objectives now
the AWS Cloud practitioner validate the
candidate strength in various domains
first he should be able to explain the
cloud knowledge and its platform he
should know how to manage the account he
should know about the billing and
pricing details the candidate should
have knowledge of key services available
on the AWS Cloud platform the candidate
should be able to explain the various
Cloud values the candidate should be
able to explain security Concepts
security model and compliance Concepts
the candidate should recognize the main
sources of technical assistance or
documentation the candidate must be able
to explain the key features of operating
and deploying in AWS cloud and finally
he should be able to explain the cloud
architecture and its basic principles in
order to excel in this exam we should
know the topics that occur in the exam
so let's take a look at the exam content
the exam has four major domains namely
Cloud Concepts security and compliance
technology and billing and pricing the
pie chart indicates the weightage of
each domain in the exam the cloud
Concepts contain 28 of the total exam
content security and compliance contains
24 of the exam content technology has
the maximum weightage of 36 percent of
the total exam content and billing and
pricing has the lowest weight rate of 12
of the entire exam content now let's
dive deep into each of these
Technologies first comes Cloud Concepts
in this you need to have some basic
knowledge of cloud computing Concepts
questions asked from this field are
broad now so to perform well you must
have high level cloud Concepts which
include elasticity scalability High
availability and fault tolerance next we
have security and compliance security is
marked as an important topic of whether
you're working with infrastructure or
not this certification exam includes a
variety of questions related to security
and culpable management you may find
some important topics regarding this
domain in your exam as follows shared
security model Cloud logs dtos
protection and IM that is managing users
password policies and MFA the next
domain is technology technology is the
most important domain of the AWS Cloud
practitioner exam if you want to adapt
in this section then you should have the
knowledge of core AWS Services some of
the AWS sources that you must know are
ec2 elb RDS S3 SNS and AWS Lambda and
finally we have billing and pricing the
AWS certified Cloud practitioner exam
constraints on the business applications
of AWS billing and pricing becomes
important topics that we should know you
must have knowledge of general account
information AWS support how certain
services are built how to connect
affiliate the cost of services and using
what tools Etc there may be some
questions which overlap with other exam
domains example AWS cost calculator
service may fall into billing and
pricing domain as well as technology
domain now that we know the domains
let's go ahead and see how to prepare
for the exam the first thing is we have
to start off with the AWS training
classes the following three AWS training
courses will help you pass the exam that
is AWS Cloud practitioner essential AWS
technical Essentials and AWS Business
Essentials moving forward the next step
is to read the AWS certified clock
practitioner certification exam guide
this guide would give an idea about the
essential area that needs to be
concentrated on it provides an overview
of all the exam objectives with
preparation instructions the next step
is to get familiar with the subject
areas before taking any exam knowing the
subject areas is very important the exam
content has been explained before so
review the subject areas carefully plan
and prepare how to attempt the test
accordingly moving forward we have to go
through the the AWS white papers these
white papers contain useful information
and cover many important topics some of
the popular AWS white papers are
overview of AWS how AWS pricing Works
maximizing value with AWS and many more
cell study is enough to pass the
certification exam the online training
will help you understand the subject
areas a lot of material can be found on
the AWS website also read the facts
related to the AWS cloud services
applications and moreover architecture
the next important step is to examine
sample questions and take free tests to
Ace the exam a practice test is always
necessary to see where you stand and
what subjects you need to concentrate on
AWS has many practice tests and you can
take them before the exam the final step
is to schedule the exam and get
certified so once you're prepared well
enough then enroll for the exam choose
any testing center near you at AWS
training website and register yourself
going forward let's look at few of the
sample questions that AWS website has so
all these questions are present under
the AWS website so you can go through
them now the first question is why is
AWS more economical than traditional
data centers for applications with
varying Computing workloads so you can
see the answer choices on the screen the
correct answer is Amazon ec2 instances
can be launched on demand when needed
the next question is which AWS service
would simplify migration of a database
to AWS the choices are AWS storage
Gateway AWS database migration service
Amazon elastic compute Cloud Amazon
appstream 2.0 so the correct answer is
AWS database migration service the next
question is which AWS offering enables
customers to find buy and immediately
start using software Solutions in the
AWS environment so you can see the
answer choices on the screen the correct
answer is AWS Marketplace which AWS
networking service enables a company to
create a virtual Network within AWS the
choices are AWS config AWS Route 53 AWS
direct connect Amazon virtual private
Cloud the correct answer is Amazon
virtual private Cloud which of the
following is aws's responsibility under
the AWS shared responsibility model so
the options are configuring third-party
applications maintaining physical
Hardware securing application access and
data managing custom Amazon machine
images so the correct answer is
maintaining physical Hardware the sixth
question is which component of AWS
Global infrastructure does Amazon
cloudfront used to ensure low latency
delivery the choices are AWS regions AWS
Edge locations AWS availability Zone
Amazon virtual private Cloud the correct
answer is AWS Edge locations so the
seventh question is how would a system
administrator add an additional layer of
login security to users AWS Management
console the answer choices are use AWS
Cloud directory audit AWS identity and
access management rules enable
multi-factor authentication enable AWS
Cloud trade so the correct answer is to
enable multi-factor Authentication Asian
the next question is which service can
identify the user that made the API call
when an Amazon elastic compute Cloud
instance is terminated so the options
are Amazon cloudwatch AWS cloudtrail AWS
x-ray AWS identity and access management
the correct answer is AWS cloudtrail the
next question is which service would you
use to send alerts based on Amazon Cloud
watch alarms the options are Amazon
simple notification service AWS
cloudtrail AWS trusted advisor Amazon
Route 53 the correct answer is Amazon
simple notification service and finally
the last question is where can a
customer find information about
prohibited actions on AWS infrastructure
so the correct answer for this is AWS
acceptable use policy so you can see we
get sample questions like these you can
go through the website and see all the
other preparation and exam guides you
need finally the last Topic in this
video is the tips while taking the exam
since the time is limited wasting time
of difficult questions is unavoidable
the next tip is to keep a track of time
and learn how to manage time to answer
all the questions third one is analyze
the questions first and then answer
because sometimes they may be traps and
finally as per the guidelines some
questions may have multiple answers so
read the questions properly and choose
the answers carefully hello everyone
in today's session we are going to talk
about AWS best practices so this is the
agenda for today we will talk about
billing best practices we will have a
look at Best Practices for ec2 which is
the Amazon elastic computer Cloud we
will walk through best practices for IAM
which is identity and access management
then we will walk through the best
practices for S3 which is simple storage
service and then we will have a general
look at the security best practices from
the Amazon account perspective as well
as from the general Amazon web services
perspective now let's start with billing
best practices so billing essentially is
the core of every company every company
wants to keep billing to the minimum and
cost to the Lower Side so one of the
best things about Amazon is that it
gives you a good way of controlling your
costs so from the billing specific side
from the billing console side there are
a lot of things you can do as a best
practice and also reduce cost
so Amazon gives you something called
billing budgets now this is interesting
so they give you an option to actually
create a budget kind of thing wherein
you specify you want x amount of dollars
for the next budget now whenever you hit
that budget let's say it's two hundred
dollars you get an email notification
now these budgets can be service
specific they can be specific to some
service like ec2 so if you want to have
a billing alert when the ec2 instances
of the account hit almost 200 you can
create that
one of the other best practices from
Amazon billing side is to give limited
access to IAM account users to billing
so by default Amazon does not allow
billing access to IAM users but you can
specifically allow them if you have got
some users who look at the billing
perspective of your account
now if you have a bill which crosses 10
to 15 dollars and it's not possible to
pay using a single credit card or debit
card Amazon gives you an option to pay
using multiple credit cards so as a best
practice if your bill has crossed a big
amount and you are not able to pay using
a single credit card if you are a good
Enterprise it's always good to have two
or three credit cards added to your
account so that even in the case when
your one card can't fulfill the bill you
can have another account which
automatically lets you pay the bill so
that way you don't risk your
infrastructure your infrastructure is
always safe
now one of the other best practices is
to have Regional taxation enabled in
your account
for every country there's a different
taxation regime and the tax methodology
and tax rates are different so instead
of getting a notice from your income tax
department or to avoid being
non-compliant to taxes it's the best
practice to have the billing number so
every country has the specific unique
billing number which you can put into
the Amazon account and then it
automatically charges that or files
taxes from Amazon's behalf on that
number
so at the end of the year when you are
doing tax filing you can easily show
that you have paid x amount of money
from Amazon so you don't have to repay
that or if in your country the tax is
collected at the end of the year in a
Consolidated way you are more compliant
now let's say that an Enterprise which
has got multiple accounts and Amazon web
services so let's say for project a you
have a different account for Project B
you have a different account so you can
consolidate the billing of those if you
don't want to have a headache of having
multiple account billing you don't want
to have to separate credit cards for
every account so what you can do is you
can consolidate the billing of each and
every account into a single account that
would essentially give you a single
Consolidated bill so you can also
consolidate your taxes there
in that case you can add a single
cooperate credit card which can have a
lot of limit there are credit cards
which give you no limit virtually so you
can use those kind of credit cards now
coming to ec2 best practices
one of the best practices from ec2 side
is Ami hardening
so Amazon gives you Amazon machine
images which are exact snapshots of
servers now these machine images are
used to spin up new machines so these
are like seat files for new servers
now you just can't have a stock OS image
and use that as a base for your entire
infrastructure you need to have a
hardened Ami
now when I say A hardened Ami what I
exactly mean is to have an Ami which has
got all the security practices
implemented into it
now as far as the Linux OS is concerned
an Ami which is hardened would have the
SSH Port changed SSH password-based
logins disabled security enhanced Linux
set to enabled if it's Santos if it's
Ubuntu maybe you can enable the firewall
and only allow port 22. the next thing
is VPC Port lockdown so apart from the
instant level firewall Port lockdown you
should also lock down ports specifically
to IP addresses if not possible to IP
addresses then at least you should make
sure that in your VPC all the security
groups have got only the specific ports
which are required by that server or
application open to specific IP
addresses
if you have to open something up to the
entire world make sure that you've got
adequate checks PPP inside your
application so you don't have a malware
user trying to do DDOS service attack
although Amazon has got a required
hardware in place to check the DDOS
attack but yes there can be still valid
amounts of attacks that go into your
application if you don't plan the
application correctly
one of the other things you should take
care when you are running instances on
ec2 is that you have private subnets for
everything other than the load balancer
or the web application tier
now this is important because you don't
want anyone to be directly able to
access your rediff CCC your database or
your internal backend systems
because most of these systems for
example let's say rediff don't have
authentication built into them so if you
have the port exposed anyone can go in
and pull all the keys
now from an application division
perspective you should make sure that
the application is divided into small
micro services
so it used to be known as service
oriented architecture SOA now the term
has changed and people call it
microservice architecture
so the basic idea behind microservice
architecture is that big monolithic
applications are divided into small
chunks of application which are exposed
over as a service to the internet to the
end users
now this helps you to scale them
individually and one of the best
advantages you get out of such a micro
service architecture is that even if one
of the applications is down the others
can actually still function without
affecting this one now this is very
important because in today's e-commerce
World a single minute of downtime can
cost millions of dollars of loss so when
you have Micro service architecture you
can scale vertically or horizontally
each microservice without worrying about
downtime
you should take care while designing
microservice architecture so that the
microservice architecture is independent
at every level
one of the mistakes which happens is
that you have a microservice
architecture at the web server or the
application server layer but then you
have a common database so how is it
going to affect the microservice
architecture in any way
I understand that it's very difficult to
have independent database microservice
architecture but yes there are companies
who do sync UPS of databases just to
make sure they have totally independent
microservices
now in such a scenario even if the
backend database of a particular
microservices down the other
functionality can work
take an example let's say there is login
functionality which talks to the back
end database and gets to data from there
let's say there is another functionality
which lets you buy stuff both of them
should have independent databases and
there should be a sync up every minute
or there should be push-based mechanism
where there's sync up only when they're
in one of the DBS
now if you have both these microservices
having independent databases your logins
will still work even if your payments
are down or your payments will still
work if your login functionality is
experiencing some degraded performance
you have stock images on Amazon that
Amazon provides you environment-based
key or Keys which are there for every
instance so every machine you launch on
Amazon has got a key you can't just use
the username and password to log in
now the advantage of key is that you
don't have to provide clear text
passwords but you have to make sure that
for every environment so let's say you
got a production environment QA test
environment make sure every environment
has got a different key the reason is if
you have a single key across all the
environments and let's say you give a
developer access to one of the
environments you can automatically gain
access to each and every environment so
that's not a good thing right you don't
want developers to access production
environments now if you have 100 ec2
instances and you have users to manage
let's say you should make 10 users are
there on every machine which are 10
members of your Dev team so it's very
difficult to create manually let's say
you also want to do password expiry and
a lot of intelligent stuff on these
users you can actually use something
called ldap this ldap is basically
centralized authentication system which
lets you authenticate users across
machines
so you can change the password for a
user on one ldap server and it's
reflected across all the machines
so let's have a look at IAM best
practices
so IAM is identity and access management
service of Amazon and it allows you to
have fine gain controls over the access
now as a best practice for your teams
you should have various roles to find
each of those roles should be specific
to what each member is doing
let's say you have a read-only Dev role
assigned to your developers you can also
have devops engineer role which you
assign to your devops engineering team
members
you can assign roles to the finance team
which has billing access
now this gives you the flexibility to
easily assign policies or attach
policies to various users without having
to manually do them
this is useful in a scenario wherein you
have a custom role wherein you have a
reporting engineer which needs access to
billing to create reports of CPU
utilization
now you can give them a specific access
by assigning specific policies
you can have new jointees Refreshers who
have joined your team have a read-only
access so that way they don't mess up
the account and they also have got full
access to the account so they can go and
they can see what the environment's like
they can read but they can't change
anything so that way you Safeguard your
account by any accidental Mistakes by
those freshers or new jointees
you can have service specific access so
let's say you have an ec2 instance which
requires access to S3 buckets so you
actually create a role in IAM and assign
that specific role to ec2 instances
you can also have special service
specific roles
now these service specific roles have
got access to specific service of Amazon
and you can use this let's say in case
you want to give some developer access
to some Dev environment of elastic
Beanstalk
so you can create a role which has the
access and the policies attached for
elastic Beanstalk read-only access to
that specific region so that way you can
control a specific service and specific
action
now let's have a look at S3 best
practices S3 is simple storage service
and it's one of Amazon's most important
Services it's one of the most used
Services as well
now in case of S3 you should make sure
that you have contact specific names now
these names can be the type of function
that S3 bucket is performing so let's
say images and then hyphen Dev and then
hyphen region
and then hyphen top level domain so you
have a unique name
so Amazon has a policy wherein they have
a single namespace for all the S3
buckets of the world which in other
words means that if you have a name
which is already chosen by someone in
the world of an S3 bucket you will not
get that name
so obviously you will have to have long
names since all shorter names would have
been taken
in order to make that long name
meaningful make sure you break that into
a specific context so that when you look
at that bucket you already know this is
what this bucket is doing
you should make sure you have bucket
policies for buckets which are
accessible by other users
these policies can be generated in the
policy generator or you can create
fine-grained self-made policies
for buckets which are very much live and
large and have got tons of data you
should archive them to Glacier because
Glacier gives you very cheap storage
although there is a retrieval time
associated with that but yes you should
make sure to Archive the buckets
so if you've got some critical objects
in a bucket you should make sure that
you also save a version of your object
because you can go back to a previous
version of that same object
now in this case you don't actually lose
any version of that file you always have
the important version of that object
from security best practices never share
your root Account Details with anyone
because if someone has got root Account
Details of your account you can actually
do anything
make sure people who leave your
organization you remove their access and
you don't keep them as stale accounts
you should also enable two-factor
authentication either with Google
Authenticator or some Hardware device
access token which flashes a six or four
digit random number so that it can be
added as an added layer of security you
also should enable IAM login don't give
root access to even trusted users so in
IAM you have a name and an ID which
shows you that this guy did this at this
time so here we wrap up this full course
feel free to ask questions in the
comment section below and our experts
will be happy to help you stay tuned
with simply learn for more such videos
thanks for watching
staying ahead in your career requires
continuous learning and upskilling
whether you're a student aiming to learn
today's top skills or a working
professional looking to advance your
career we've got you covered explore our
impressive catalog of certification
programs in Cutting Edge domains
including data science cloud computing
cyber security AI machine learning or
digital marketing designed in
collaboration with leading universities
and top corporations and delivered by
industry experts choose any of our
programs and set yourself on the path to
Career Success click the link in the
description to know more
hi there if you like this video
subscribe to the simply learn YouTube
channel and click here to watch similar
videos turn it up and get certified
click here
foreign