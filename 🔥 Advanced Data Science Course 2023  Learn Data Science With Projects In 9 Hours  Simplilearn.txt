welcome to the world of Advanced Data
science by cutting its techniques and
advanced analytics converts to unlock
the untapped potential of data in this
Dynamic and transformative course we
will develop into the depths of data
Universe equipping you with the skills
and knowledge needed to harness its
power for groundbreaking insights and
tangible business outcomes through a
carefully crafted curriculum we navigate
the intricacies of machine learning
artificial intelligence deep learning
and statistical modeling empowering you
to solve complex problems and
confidently make data driven decisions
you will embark on a journey that blends
Theory with hands-on experience enabling
you to master state-of-the-art tools and
Technologies utilized by the industry
leaders from data science fundamentals
to statistics from basics of machine
learning to critical machine learning
algorithms and from program coming
fundamentals to data science with python
and R this course pushes the boundaries
of traditional analytics unraveling the
true potential of Big Data through
practical projects and reliable case
studies you will gain the expertise to
extract valuable insights uncover hidden
patterns and transform data into a
strategic asset join us in the
transformative adventure and let
Advanced Data science Unleash Your
analytical powers and elevate your
career to unprecedented Heights if these
are the type of videos you'd like to
watch then hit the Subscribe button and
press the Bell icon to never miss on our
future content also data scientists
looking for online training and
certification programs from the best
universities or a professional who
elects to switch careers with data
science then try giving simply learns
postgraduate program in data science as
short the link in the description box
below should navigate to the home page
where you can find the complete overview
of the program being offered now over to
our training experts are you one of the
many who dreams of becoming a data
scientist keep watching this video if
you're passionate about data science
because we will tell you how does it
really work under the hood Emma is a
data scientist let's see how a day in a
life goes while she's working on data
science project well it is very
important to understand the business
problem first in our meeting with the
clients Emma asks relevant questions
understands and defines objectives for
the problem that needs to be tackled she
is a curious Soul who asks a lot of eyes
one of the many traits of a good data
scientist now she gears up for data
acquisition to gather and scrape data
from multiple sources like web servers
logs databases apis and online
repositories oh it seems like finding
the right data takes both time and
effort our data is gathered comes data
preparation this step involves data
cleaning and data transformation data
cleaning is the most time consuming
process as it involves handling many
complex scenarios here Mr deals with
inconsistent data types misspelled
attributes missing values duplicate
values and whatnot then in data
transformation she modifies the data
based on defined mapping rules in a
project
ETL tools like talent and Informatica
are used to perform complex
Transformations that helps the team to
understand the data structure better
then understanding what you actually can
do with your data is very crucial for
that Emma does exploratory data analysis
with the help of Eda she defines and
refines the selection of feature
variables that will be used in the model
development but what if Emma skips this
step she might end up choosing the wrong
way tables which will produce an
inaccurate model thus exploratory data
analysis becomes the most important step
now she proceeds to the core activity of
a data science project which is data
modeling she repetitively applies Force
machine learning techniques like k n
decision tree knife base to the data to
identify the model that best fits the
business requirement she trains the
models on the training data set and
tests them to select the best performing
model Emma prefers python for modeling
the data however it can also be done
using R and SAS well the trickiest part
is not yet over visualization and
communication Emma meets the clients
again to communicate the business
findings in a simple and effective
manner to convince the stakeholders she
uses tools like Tableau power bi and
click view that can help her in creating
powerful reports and dashboards and then
finally she deploys and maintains the
model she tests the selected model in a
pre-production environment before
deploying it in the production
environment which is the best practice
right after successfully deploying it
she uses reports and dashboards to get
real-time analytics further she also
wanted meters and maintains the
Project's performance well that's how
Emma completes the data science project
we have seen the daily routine of a data
scientist is a whole lot of fun has a
lot of interesting aspects and comes
with its own share of challenges now
let's see how data science is changing
the world data science techniques along
with genomic data provides a deeper
understanding of genetic issues in
reaction to particular drugs and
diseases logistic companies like DHL
FedEx have discovered the best routes to
ship the best suited time to deliver the
best mode of transport to choose thus
leading to cost efficiency with data
science it is possible to not only
predict employee attrition but to also
understand the key variables that
influence employee turnover also the
airline companies can now easily predict
flight delay and notify the passengers
beforehand to enhance their travel
experience well if you're wondering
there are various roles offered to a
data scientist like data analyst machine
learning engineer deep learning engineer
data engineer and of course data
scientist the median based salaries of a
data scientist can range from ninety
five thousand dollars to 165 000 so that
was about the data science Google had
gathered five exabytes of data between
the beginning of time 2003. this amount
of data started to be produced every two
days in 2010 and every 40 minutes by
2021 the responsibility of a data
scientist is to gather clean and present
data and have a keen business sense and
analytical abilities let us have a
discussion about it in the upcoming
slides how can you become a data
scientist as a beginner I guarantee you
after watching this video you will have
a clear understanding on how to drive
your career as a data scientist hey guys
welcome to Simply learn before
proceeding please make sure you
subscribe to Simply learns YouTube
channel and press the Bell cycle to
never miss any updates today we are
going to cover significance of data
scientists in Industries after that
prerequisites and Technologies required
for a data scientist and finally salary
of a data scientist I have a query for
you which technology is used by Google
Maps to predict traffic jams deep
learning machine learning natural
language processing data structure
please leave the answer in the comment
section below and stay tuned to get the
answer significance now we will see how
top industries are involved in the field
of data science by 2025 the data science
Industry is anticipated to grow to a
value of 16 billion dollars there is an
abundance of data science jobs all over
the world now let's list out crucial
areas where data science is used media
and entertainment the major player in
the media and entertainment sector such
as YouTube Netflix hot star Etc have
begun to use data science to better
understand their audience and provide
them with recommendations that are both
relevant and personalized e-commerce
data science has aided retail companies
in better meeting their expectations as
they bring a unique combination of deep
data knowledge technology skills and
statistical experience data scientists
are in high demand in the retail
industry top recruiters are Amazon
Flipkart Walmart myntra Etc digital
marketing large volumes of data are
currently being fetched from its users
through search Pages social networks
online traffic display networks movies
web pages Etc a high level of business
intelligence is needed to analyze such
large amount of data and this can only
be done with the proper use of data
science approaches top recruiters are
Amazon Flipkart Facebook Google Etc
cyber security data science and AI are
now being used by the cyber security
industry to prevent the growing usage of
algorithms for harmful purposes top
recruiters includes IBM Microsoft
Accenture Cisco and many more before
moving forward what is the response to
the Google Map question that I asked
answer is machine learning
coming to prerequisites now that when no
significance of data science in
Industries let us explore the
prerequisites and Technologies required
for a data scientist seeing the demand
of data scientists in every industry it
is obvious that the scope of a data
scientist is very high so how to start
there is no necessity that you should be
knowing any technology or programming
language you can be a Layman too data
scientists typically have a variety of
educational and professional experiences
most should be proficient in four
crucial areas important skill is
mathematical expertise three concepts
like linear algebra multivariable
calculus and optimization technique are
crucial because they aid in our
understanding of numerous machine
learning algorithms that are crucial to
data science
similar to that knowing statistics is
crucial because they are used in data
analysis additionally important to
statistics probability is regarded as a
must of for mastering machine learning
next is computer science in the field of
computer science there is a lot to learn
but one of the key inquiries that arises
in relation to programming is r or
Python language there are many factors
to consider when deciding which language
to choose for data science because both
have a comprehensive collection of
libraries to implement complex machine
learning algorithms in addition to
learning a programming language you
should learn the following computer
science skill fundamentals of algorithm
and data structures distributed
computing machine learning deep learning
Linux SQL mongodb Etc domain expertise
most individuals wrongly believe that
domain expertise is not crucial to data
science yet it is consider the following
scenario if you are interested in
working as a data scientist dentist in
the banking Industries and you already
know a lot about it for instance you are
knowledgeable about stock trading
Finance Etc this will be very
advantageous for you and the bank itself
will favor you over other applicants and
finally communication skill it covers
both spoken and written Communication in
a data science project the project must
be explained to others when finding from
the analysis have been reached this can
occasionally be a report that you
provide to your team or employer at work
sometimes it might be a blog entry it is
frequently a presentation to a group of
co-workers regardless a data science
project always involves some form of
communication of the Project's findings
therefore having a good communication
skill is a requirement for being a data
scientist apart from all this practicing
is very important keep using different
tools also start trading blogs on data
science start building projects on data
science which can be added to your
assume also you can find many
interesting courses on data science by
simply learn salary reward is the result
of good work now we shall discuss
salaries that a data scientist will get
it should come as no surprise that data
scientists may add significantly to a
business every step of the process from
data processing to data cleansing
requires persistence a lot of arithmetic
and statistics as well as scattering of
engineering skills one of the most
important factors in a data scientist
salary is experience at the beginner
level a data scientist can make 95 000
annually the typical annual compensation
for a mid-level data scientist is
between 130 000 and 195 000 a seasoned
data scientist typically earns between
165 000 and 250 000 per year
in India at the beginner level a data
scientist can make 9 lakh forty thousand
rupees on average per year at mid level
data scientists will get 20 lakhs rupees
per annum and if you are at the advanced
level you will get paid an average of
rupees 25 lakhs annually this salary
will vary in different countries the top
hiring businesses in the US that provide
the highest salaries for data scientists
are apple with 180 000 per annum next is
Twitter with 170 dollars per annum meta
technology
170 dollars annually
LinkedIn 160 000 per annum crypto
technology provides 17 lakh 50 000 per
annum IBM provides 14 lakhs per annum
and Accenture will provide you with 19
lakhs per annum and finally American
Express will provide an average of 13
lakhs per annum what this Buzz is all
about so now let's talk about the life
cycle of a data science project okay the
first step is the concept study in this
step it involves understanding the
business problem asking questions get a
good understanding of the business model
meet up with all the stakeholders
understand what kind of data is
available and all that is a part of the
first step so here are a few examples we
want to see what are the various
specifications and then what is the end
goal what is the budget is there an
example of this kind of a problem that
has been maybe solved earlier so all
this is a part of the concept study and
another example could be a very specific
one to predict the price of a 1.35 carat
diamond and there may be relevant
information inputs that are available
we want to predict the price the next
step in this process is data preparation
data Gathering and data preparation also
known as data munging or sometimes it is
also known as data data manipulation so
what happens here is the raw data that
is available may not be usable in its
current format for various reasons so
that is why in this step a data
scientist would explore the data he will
take a look at some sample data maybe
there are millions of Records pick a few
thousand records and see how the data is
looking are there any gaps is the
structure appropriate to be fed into the
system are there some columns which are
probably not adding value may not be
required for the analysis very often
these are like names of the customers
they will probably not add any value or
much value from an analysis perspective
this structure of the data Maybe the
data is coming from multiple data
sources and the structures may not be
matching what are the other problems
there may be gaps in the data so the
data all the columns all the cells are
not filled if you're talking about
structured data there are several blank
records or blank columns so if you use
that data directly you'll get errors or
you will get inaccurate results so how
do you either get rid of that data or
how do you fill this gaps with something
meaningful so all that is a part of data
munching or data manipulation so these
are some additional sub topics within
that so data integration is one of them
there are any conflicts in the data
there may be data may be redundant yeah
data resonant redundancy is another
issue then maybe you have let's say data
coming from two different systems and
both of them have customer table for
example customer information so when you
merge them there is a duplication issue
so how do we resolve that so that is one
data transformation as I said there will
be situations where data is coming from
multiple sources and then when we merge
them together they may not be matching
so we need to do some transformations to
make sure everything is similar we may
have to do some data reduction if the
data size is too big you may have to
come up with ways to reduce it
meaningfully without losing information
then data cleaning so there will be
either wrong values or your null values
or there are missing values so how do
you handle all of that few examples of
very specific stuff so if there are
missing values how do you handle missing
values or null values here in this
particular slide we are seeing three
types of issues one is missing value
then you have null value you see the
difference between the two right so in a
missing value there is nothing blank
null value it says null now the system
cannot handle if there are null values
similarly there is improper data so it's
supposed to be numeric value but there
is a string or a non-numeric value so
how do we clean and prepare the data so
that our system can work flawlessly so
there are multiple ways and there is no
one common way of doing this it can vary
from Project to project it can vary from
what exactly is the problem we are
trying to solve it can vary from data
scientist to data scientist organization
to organization so these are like some
standard practices people come up with
and and of course there will be a lot of
trial and errors somebody would have
tried out something and it worked and
that mechanics mechanism so that's how
we need to take care of data cleaning
now what are the various ways of doing
you know if values are missing how do
you take care of that now if the data is
too large and only a few records have
some missing values then it is okay to
just get rid of those entire rows for
example so if you have a million records
and out of which 100 records don't have
full data so there are some missing
values in about 100 cards so it's
absolutely fine because it's a small
percentage of the data so you can get
rid of the entire records which are
missing values but that's not a very
common situation very often you will
have multiple or at least you know large
number of a data set for example out of
million records you may have 50 000
records which are like having missing
values now that's a significant amount
you cannot get rid of all those records
your analysis will be inaccurate so how
do you handle such situation so there
are again multiple ways of doing it one
is you can probably if a particular
values are missing in a particular
column you can probably take the mean
value for that particular column and
fill all the missing values with the
mean value so that first of all you
don't get errors because of missing
values and second you don't get results
that are way off because these values
are completely different from what is
there so that is one way then a few
other could be either taken the median
value or depending on what kind of data
we are talking about so something
meaningful we will have put in there if
we are doing some machine learning
activity then obviously as a part of
data preparation you need to split the
data into training and test data set the
reason being if you try to test with a
data set which the system has already
seen as a part of training then it will
tend to give reasonably accurate results
because it has already seen that data
and that is not a good measure of the
accuracy of the system so typically you
take the entire data set the input data
set and split it into two parts and
again the ratio can vary from person to
person individual preferences some
people like to split it into 50 50 some
people like it has
63.33 and 33.3 is basically two third
and one third and some people do it as
80 20 80 for training and 24 testing so
you split the data perform the training
with the 80 percent and then use the
remaining 20 for testing all right so
that is one more data preparation
activity that needs to be done before
you start analyzing or applying the data
or putting the data through the model
then the next step is model planning now
this models can be statistical models
this could be machine learning models so
you need to decide what kind of models
you're going to use again it depends on
what is the problem you're trying to
solve if it is a regression problem you
need to think of a Direction algorithm
and come up with a regression model so
it could be linear regression or if you
are talking about classification then
you need to pick up an appropriate
classification algorithm like logistic
regression or decision tree or svm and
then you need to train that particular
model so that is the model building or
model planning process and the cleaned
up data has to be fed into the model and
apart from cleaning you may also have to
in order to determine what kind of model
you will use you have to perform some
exploratory data analysis to understand
the relationship between the various
variables and save the data is
appropriate and so on right so that is
the additional preparatory step that
needs to be done so little bit of
details about exploratory data analysis
so what exactly is exploratory data
analysis is basically to as the name
suggests you're just exploring you just
receive the data and you're trying to
explore and find out what are the data
types and what is the is the data clean
in in each of the columns what is the
maximum minimum value so for example
there are out of the box functionality
available in tools like R so if you just
ask for a summary of the table it will
tell you for each column it will give
some details as to what is the mean
value what is the maximum value and so
on fuck so this exercise or this
exploratory analysis is to get an
understanding of your data and then you
can take steps to during this process
you find there are a lot of missing
values you need to take step fix those
you will also get an idea about what
kind of model to be used and so on and
so forth what are the various techniques
used for exploratory data analysis
typically these would be visualization
techniques like you use histograms you
can use box plots you can use Scatter
Plots so these are very quick ways of
identifying the patterns or a few of the
trends of the data and so on and then
once your data is ready you've decided
on the model what kind of model what
kind of algorithm you're going to use if
you're trying to do machine learning you
need to pass your 80 percent the
training data or rather you use that
training data to train your model and
the training process itself is iterative
so the training process you may have to
perform multiple times and once the
training is done and you feel it is
giving good accuracy then you move on to
test so you take the remaining 20 of the
data remember we split the data into
training and test so the test data is
now used to check the accuracy or how
well our model is performing and if
there are further issues let's say a
model is still during testing in the
accuracy is not good then you may want
to retrain your model or use a different
model so this whole thing again can be
iterative but if the test process is
passed or if the model passes the test
then it can go into production and it
will be deployed all right so what are
the various tools that we use for model
planning R is an excellent tool in a lot
of ways whether you're doing regular
statistical analysis or machine learning
or any of these activities are in along
with our studio provides a very powerful
environment to do data analysis
including visualization it has a very
good integrated visualization or plot
mechanism which can be used for doing
exploratory data analysis and then later
on to do analysis detailed analysis and
machine learning and so on phone then of
course you can write python programs
python offers a rich library for
performing data analysis and machine
learning and so on Matlab is a very
popular tool as well especially during
education so this is a very easy to
learn tool so Matlab is another tool
that can be used and then last but not
least SAS SAS is again very powerful it
is proprietary tool and it has all the
components that are required to perform
very good statistical analysis or
perform data science so those are the
various tools that would be required for
or that that can be used for model
building and so the next step is model
building so we have done the planning
part we said okay what is algorithm we
are going to use what kind of model we
are going to use now we need to actually
train this model or build the model
rather so that it can then be deployed
so what are the various ways or where
are the various types of model building
activities so it could be let's say in
this particular example that we have
taken you want to find out the price of
1.35 carat diamond so this is let's say
a linear regression problem you have
data for various carrots of diamond and
you use that information you pass it
through a linear regression model or you
create a real linear regression model
which can then predict your price for
1.35 carat so this is one example of
model building and then a little bit
details of how linear regression works
so linear regression is basically coming
up with a relation between an
independent variable and a dependent
variable so it is pretty much like
coming up with equation of a straight
line which is the best fit for the given
data so like for example here Y is equal
to MX plus C so Y is is the dependent
variable and X is the independent
variable we need to determine the values
of M and C for our given data so that is
what the training process of this model
does at the end of the training process
you have a certain value of M and c and
that is used for predicting the values
of any new data that comes all right so
the way it works is we use the training
and the test data set to train the model
and then validate whether the model is
working fine or not using test data and
if it is working fine then it is taken
to the next level which is put in
production if not the model has to be
retrained if the 360 is not good enough
then the model is retrained maybe with
more data or you come up with a newer
model or algorithm and then repeat the
process so it is an iterative process
once the training is is completed
training and test then this model is
deployed and we can use this particular
model to determine what is the price of
1.35 carat diamond remember that was our
problem statement so now that we have
the best fit for this given data we have
the price of 1.35 carat diamond which is
10 000. so this is one example of how
this whole process works now how do we
build the model there are multiple ways
you can use Python for example and use
libraries like pandas or numpy to build
a model and implement it this will be
available as a separate tutorial a
separate video in this playlist so stay
tuned for that moving on once we have
the results the next step is to
communicate this results to the
appropriate stakeholders so which is
basically taking this results and
preparing like a presentation or a
dashboard and communicating these
results to the concern people so
finishing or getting the results of the
analysis is not the last step but you
need to as a data scientist take this
results and presented to the team that
has given you this problem in the first
place and explain your findings explain
the findings of this exercise and
recommend maybe what steps they need to
take in order to overcome this problem
or solve this problem so that is the
pretty much once that is accepted and
the last step is to operationalize so if
everything is fine your data scientists
presentations are accepted then they put
it into practice and thereby they will
be able to improve or solve the problem
that they stated in step one okay so
quick summary of the life cycle you have
a concept study which is basically
understanding the problem asking the
right questions and trying to see if
there is enough data to solve this
problem and then even maybe gather the
data then data preparation the raw data
needs to be manipulated you need to do
data munging so that you have a data in
a certain proper format to be used by
the model or our analytics system and
then you need to do the model planning
what kind of a model what algorithm you
will use for a given problem and then
the model building so the exact
execution of that model happens in step
four and you implement and execute that
model and put the data through the
analysis in this step and then you get
the results this results are then
communicated packaged and presented and
communicated to the stakeholders and
once that is accepted that is
operationalized so that is the final
step data scientist is one of the
hottest highest paid and most
enthusiastic job roles in the current ID
industry business organizations are
actively hiring professionals who could
collect data analyze the market to help
the companies diagnose and overcome
economic barriers to lead the industry
many mncs including the fan companies
are actively upscaling their employees
to bridge the gap in the world of
fast-paced and intelligent Technologies
according to the U.S Bureau of Labor
Statistics now is the best time to start
a career in data science as a number of
jobs requiring data science skills alone
is expected to grow by 27.9 by 2026.
the potential recruiters are actively
looking for data scientists and are
willing to compensate their employees
with handsome salaries according to the
leading websites like indeed classdoor
naukri and Linkedin the average salary
of a data scientist is anywhere between
10 lakhs Rupees to as high as 80 lakh
rupees per annum in India depending on
your experience in the United States of
America the salary of a data scientist
ranges from hundred thousand dollars to
two hundred and fifty thousand dollars
per annum on an average should you be
interested in becoming a successful data
scientist but you don't know how don't
you worry as we have got you covered
with our rigorous survey reports which
will help you to not only choose from
the top 10 data scientist certifications
but will also help you develop and
enhance your data science skills to give
your career the Boost it deserves so
without much further Ado let's get
started with the top 10 best data
scientist certifications in 2022. while
we are added please feel free to get
subscribed and hit the Bell icon to keep
yourself updated with the latest ID
Trends in the industry at number 10 we
have cap which is AKA certified
analytics professional will be a great
addition to your resuming as a
certification is valued by many data
science and analytics driven
organizations if you're looking forward
to becoming a data scientist this
certification is proof that you possess
the critical and lucrative technical
expertise understand risk management
information security procedures and
possess relevant soft skills required by
a data science and analytics
professional aspiring data scientists
with a bachelor's degree and five years
of professional analytical experience
all master's degree with three years of
experience are eligible to face cap
certification exam
there are numerous benefits to the
certification apart from getting
exposure to new and updated technical
skills the certification also paves way
for new opportunities as the
certification proves your credibility
for the role we have the open certified
data scientist AKA opencds or number
nine
opencds is considered the broadest and
encyclopedic certification for a data
scientist organizations choose opencds
certified professionals to fill up the
most critical job roles responsible for
producing an effective analysis of data
to improve the business the
certification is available in three
different levels level 1 certified data
scientist level 2 Master Certified data
scientist level 3 distinguished
certified data scientists however there
is a prerequisite for candidates where
the applicants are expected to
demonstrate skills and experience
against a set of conformance
requirements through written
applications and peer interviews next on
our list is an interesting one tap your
data scientists and desktop specialist
Tableau is one of the leading business
intelligence tools leading in Gartner's
quadrant Tableau offers a wide range of
courses but this certification is
absolutely for free and other similar
certification is based on day data
analytics for interested candidates
pursuing this certification will help
you with data science fundamentals and
also enhance your data visualization
skills the course covered data prep with
Tableau visual analytics data science
with Tableau and finally to put your
learning to the test
the course also includes a data
scientist skills assessment the course
requires the candidates to possess some
fundamentals of data preparation and
exploration creating and sharing
insights from data and some hands-on
experience with Tableau up ahead we have
SAS Academy for data science
certification in seventh position
statistical analytical system is a
sophisticated tool purpose built for
carrying out Advanced analytics and
complex statistical operations
getting certified as an sas expert will
help you resume me to stand out the
course features hands-on experience with
powerful tools like learn using SAS are
python big Hive and Hadoop the global
recognized tailor-made certification
will help you prepare for all three
exams namely data curation professional
Advanced analytics professional Ai and
machine learning professionals as well
the course will enhance your learning
with practical application based on
actual C studies that will feature data
curation Advanced analytics Ai and
machine learning your way at your own
pace we have tell EMC proven
professional certification program in
the top six crate the Dell EMC proven
professional certification program is a
tried and tested certification with
affirm results Dell claims about 73
percent of individuals found the program
to be effective in their career 23 of
candidates obtained a decent hike with
promotion as well about 86 percent of
HRS admitted to selecting the candidates
with this certification with these
groundbreaking numbers this
certification has nothing more to prove
the program is segmented into two
categories the associate level
certification and specialist level
certification the main growth areas
include Advanced Analytical methods
Hadoop and pig Heights Edge base social
network analysis natural language
processing and visualization methods the
development of these skills and the use
of these methods allows the data
scientists to identify and communicate
conclusions and recommendations to solve
business challenges across many domains
if you are interested in wondering about
the pre-requisites needed that's good
news for you the program expects only
fundamental programming and analytical
skills and have a good understanding of
cloud computing on the top fifth
position we have tensorflow developer
certification the main goal of this
certification is to help aspiring data
scientists get well versed with
tensorflow and showcase a expertise in
machine learning the tensorflow
developer certification can be
considered as one of the best
foundational certifications for
demonstrating training and deployment of
the machine learning models the
candidates appearing for the
certification are expected to have a
fundamental knowledge of image
recognition object detection test
recognition algorithms with deep neural
networks con delusional neural networks
and exploring strategies to prevent
overfitting including augmentation and
dropouts the best part of the
certification is it does not have an
expiration date
one amongst the top five is simply
learns postgraduate program in data
science simply learn collaborates with
IBM and with the best and world-renowned
Purdue University for providing
postgraduate certification in data
science simply learns data science
certification is ranked as the number
one by Economic Times the course focuses
on the comprehensiveness of content and
skills required to tackle real world
business problems the best part of the
certification program is that it does
not concentrate on one language like the
others here you get into the master data
science with both Python and R
programming languages the course then
navigates you through advanced machine
learning paradigms with real-time
industry professionals and experienced
faculty from Purdue University
by the end of the program you will get
hands-on experience with three Capstone
projects
also you will have lifetime access to 25
plus real-time business problems and
their solutions for future reference the
projects are related to top
organizations like Amazon Uber Walmart
Comcast to name a few the IBM data
science professional certificate stands
at the third position of top data
science certifications you will learn to
important clean data sets analyze and
visualize data and build and evaluate
machine learning models and pipelines
using python you will apply various data
science skills techniques and tools to
complete a project and publish a
real-time report by the end of the
course the course revolves around python
so it happens to be one of the major
prerequisites and along with outcomes
SQL and jupyter notebook IBM claims that
the course gives people the foundational
blocks to learn and master data sources
we are now at the top second
certification the Google Certified
professional data engineer you might be
wondering why are we discussing data
engineering it is a fact that to become
a successful data scientist you need to
possess a good grip over the data that
you are working with data engineer
certification will give you an edge over
the other candidates proving your
caliber while handling data this
certification will take you through
various stages where you will learn to
design and build data processing systems
operationalize machine learning models
and deliver the best solution having a
comfortable understanding of Google
Cloud would be necessary a good one year
of experience with data management and
analytics could add to taking up the
certification on the number one position
we have Google data machine learning
engineer most of the time data
scientists are comfortable working with
Jupiter notebook this could be feasible
in normal conditions but if you wish to
send your model production on the
website or mobile devices things can
escalate real quick unexpected issues
might pop up from nowhere and hinder the
production environment although it is
obvious and many educational programs
teach this concept it is preferred to
learn it from the experts in Google's
data machine learning Engineer program
you will get hands-on experience in
various domains a few important ones
like framing and developing machine
learning problems Automation and
orchestrating machine learning pipelines
monitoring optimizing and maintaining
machine learning Solutions the best part
of the certification program is you do
not need to have any concrete
prerequisite to take up the search
education program with that we have come
to the end of this session on the top 10
data scientist certifications for 2022
so what is data science as the name
suggests it is nothing but study of
using data and trying to find out some
insights or extracting some insights or
knowledge using all the data that is at
your disposal so that's pretty much what
data science is all about so you take
the data and apply certain methodologies
certain algorithms and your business
domain knowledge as well and of course a
certain amount of creativity to extract
some insights very often they are also
known as actionable insights because
once you have the insights you should be
in a position to take some action to
let's say solve a problem or improve a
situation there are a lot of areas where
data science can be used one of the very
common one is fraud detection or fraud
prevention there are a lot of fraudulent
activities or transactions primarily on
the Internet it's very easy to commit
fraud and therefore we can use data
science to either prevent or detect
fraud there are certain algorithms
machine learning algorithms that can be
used like for example some outlier
techniques clustering techniques that
can be used to detect fraud and prevent
fraud as well so who is a data scientist
rather it is actually a very generic
role that defines somebody who is
working with data is known as a data
scientist but they can be very specific
activities and the roles can be actually
much more specific what exactly a person
does within the area of data science can
be much more specific but broadly
anybody working in the area of data
science is known as a data scientist so
what does a data scientist do these are
some of the activities data acquisition
data preparation data mining data
modeling and then model maintenance we
will talk about each of these in a great
detail but at a very high level the
first step obviously is to get the raw
data which is known as data acquisition
it can be in all kinds of format and it
could be multiple sources but obviously
that raw data cannot be used as it is
for performing data mining activities or
data modeling activities so the data has
to be clamped and prepared for using in
the data models or in the data mining
activity so that is the data preparation
then we actually do the data mining
which can also include some exploratory
activities and then if we have to do
stuff like machine learning then you
need to build a machine learning model
and test the model get insights out of
it and then if the model is fine you
deploy it and then you need to maintain
the model because over a period of time
it is possible that you need to tweak
the model because of change in the
process or changing the data and so on
so that all comes under the model
maintenance so let's take deeper look at
each of these activities let's start
with data acquisition so the stage of
data acquisition basically the data
scientist will collect raw data from all
possible sources so this could be
typically an rdbms which is a relational
database or it can also be a non rdbms
or it could be flat files or
unstructured data and so on so we need
to bring all that data from different
sources if required we need to do some
kind of homogeneous formatting so that
it all fits into a you know looks at
least format from a format perspective
it looks homogeneous so that may be
requiring some kind of transformation
very often this is loaded into what is
known as data warehouse so this can also
be sometimes referred to as ETL or
extract transform and load so a data
warehouse is like a common place where
data from different sources is brought
together so that people can perform data
science activities like reporting or
data mining or statistical analysis and
so on so data from various sources is
put in a centralized place which is
known as data warehouse so that is also
known as ETL and in order to do this
there can be data scientists can take
help of some ETL tools there are some
existing tools that a data scientist can
take help of like for example data stage
or Talent OR Informatica these are
pretty good tools for performing this
ETL activities and getting the data the
next stage now that you have the raw
data into a data warehouse you still
probably are not in a position to
straight away use this data for
performing the data mining activities so
that is where data preparation comes
into play and there are multiple reasons
for that one of them could be the data
is dirty there are some missing values
and so on and so forth so a lot of time
is actually spent in this particular
stage so a data scientist spends a lot
of time almost 60 to 70 percent of the
time in this part of the project or the
process which is data preparation so
there are again within this there can be
multiple sub activities starting from
let's say data cleaning you will
probably have missing values the data
there is some columns the values are
missing or the values are incorrect
there are null values and so on and so
forth so that is basically the data
cleaning part of it then you need to
perform certain Transformations like for
example normalizing the data and so on
right so you could probably have to
modify a categorical values into
numerical values and so on and so forth
so these are transformational activities
then we may have to handle outliers so
the data could be such that there are a
few values which are Way Beyond the
normal behavior of the data for whatever
reason either people have keyed in wrong
values or for some reason some of the
values are completely out of range so
those are known as outliers so there are
certain ways of handling these outliers
and detecting and handling these
outliers so the this is a part of what
is known as exploratory analysis so you
quickly explore the data to find out
others so and you can use visual tools
like plots and identify what are the
outliers and see how we can get rid of
the outliers and so on then the next
part could be data Integrity data
Integrity is to validate for example if
there are some primary keys that all the
primary keys are populated there are
some foreign keys and at least most of
the foreign Keys should be populated and
otherwise when we are trying to query
the data you may get wrong values and so
on so that is the data Integrity part of
it and then we have what is known as
data reduction sometimes we may have
duplicate values we may have columns
that may be duplicated because they are
coming from different sources the same
values are there and so on so a lot of
this can be done using what is known as
data reduction and thereby you can
reduce the size of the data drastically
because very often this could be written
and data which can be removed so let's
take a look at what are the various
techniques that are used for data
cleaning so we need to ensure that the
data is valid and it is consistent and
uniform and accurate so these are the
various parameters that we need to
ensure as a part of the data cleaning
process now what are the techniques that
that are used for data cleaning or so we
will see what each of these are in this
particular case and so what is the data
set that we have we have data about a
bank and its customer details so let's
take an example and see how we go about
cleaning the data and in this particular
example we are assuming we are using
python so let's assume we loaded this
data which is the raw file.csv this is
how the customer data looks like and we
will see for example we take a closer
look at the geography column we will see
that there are quite a few blank spaces
so how do we go about when we have some
blank spaces or if it is a string value
then we put a empty string here or we
just use a space or empty string there
are numerical values then we need to
come up with a strategy for example we
put the mean value so wherever it is
missing we find the mean for that
particular column so in this case let's
assume we have credit score and we see
that quite a few of these values are
missing so what do we do here we find
the mean for this column for all the
existing values and we found that the
mean is equal to 638.6 so we kind of
write a piece of code to replace
wherever there are blank values Nan is
basically like null and we just go ahead
and say fill it with the mean value so
this is the piece of code we are writing
to fill it so all the blanks or all the
null values get replaced with the mean
value now one of the reasons for doing
this is that very often if you have some
such situation many of your statistical
functions may not even work so that's
the reason you need to fill up these
values or either get rid of these
records or fill up these values with
something meaningful so this is one
mechanism which is basically using a
mean there are few others as we move
forward we can see what are the other
ways for example we can also say that
any missing value in a particular row if
even one column the value is missing you
just drop that particular row or delete
all rows where even a single column has
missing values so that is one way of
dealing now the problem here can be that
if a lot of data has let's say one or
two columns missing and we drop many
such rows then overall you may lose out
on let's say 60 of the data as some
value or the other missing sixty percent
of the rows then it may not be a good
idea to delete all the rows like in that
manner because then you're losing pretty
much 60 percent of your data Therefore
your analysis won't be accurate but if
it is only five or ten percent then this
will work underway is only to drop
values where or rather dropped rows
where all the columns are empty which
makes sense because that means that
record is of really no use because it
has no information in it so there can be
some situations like that so we can
provide a condition saying that drop the
records where all the columns are blank
or not applicable we can also specify
some kind of a threshold let's say you
have 10 or 20 columns in a row you can
specify that maybe five columns are
blank or null then you drop that record
so again we need to take care that such
a condition such a situation the amount
of data that has been removed or
excluded is not large if it is like
maybe five percent maximum 10 percent
then it's okay but by doing this if
you're losing out on a large chunk of
data then it may not be a good idea you
need to come up with something better
what else we need to do next is the data
preparation part is done so now we get
into the data mining part so what
exactly we do in data mining primarily
we come up with ways to take meaningful
decisions so data mining will give us
insights into the data what is existing
there and then we can do additional
stuff like maybe machine learning and so
on to get perform Advanced analytics and
so on so one of the first steps we do is
what is known as data Discovery and
which is basically like exploratory
analysis so we can use tools like
Tableau for doing some of this so let's
just take a quick look at how we go
about that so Tableau is excellent data
mining or actually more of a reporting
or a bi tool and you can download a
trial version of Tableau at tableau.com
or there is also a tableau public which
is free and you can actually use an play
around however if you want to use it for
Enterprise purpose then it is a
commercial software so you need to
purchase license and you can then run
some of the data mining activities say
your data source your data is in some
Excel sheet so you can select the source
as Microsoft Excel or any other format
and the data will be brought into the
Tableau environment and then it will
show you what is known as dimensions and
measures so dimensions are all the
descriptive columns so and Tableau is
intelligent enough to actually identify
these dimensions and measure so measures
are the numerical valve so as you can
see here our customer ID gender
geography these are all Dimensions
non-numerical values whereas age balance
credit score and so on are numeric
values so they come under measures so
you've got your data into Tableau and
then you want to let's say build a small
model and you want to let's say solve a
particular problem so what is the
problem problem statement all right
let's say we want to analyze why
customers are leaving the bank which is
known as exit and we want to analyze and
see what are some of the factors for
exiting the bank and we want to Let's
assume consider these three of them like
let's say gender credit card and
geography these as a criteria and
analyze if these are in any way
impacting or have some bearing on the
customer exiting or the customer exit
Behavior okay so let's use Tableau and
very quickly we will be able to find out
how these parameters are affecting all
right so let's see so this is our
customer data so from our Excel sheet we
have data set about let's say 10 000
rows and we want to find out what is the
criteria let's start with gender let's
say we want to first use gender as a
criteria so Tableau really offers an
easy drag and drop kind of a mechanical
so that makes it really really easy to
perform this kind of analysis so what we
need to do is exit it says whether the
customer has exited or not so it has a
value of 0 and 1 and then of course you
have gender and so on so we will take
these two and simply drag and drop okay
so exit it and then we will put gender
and if we drag and drop it to the
analysis side of tabloop all right so
here what we are doing is we are showing
male female as two different columns
here and zero for people who did not
exist and one for people who exited and
that is color coded so the blue color
means people who did not exist and this
yellow color means people who did exit
all right so now if we pull the data
here create like bar graphs this is how
it would look so what is yellow let's go
back so yellow is uh who exited and for
the mail only 16.45 percent have exited
and we can also to draw a reference line
that will help us or even provide
aliases so these are a lot of fancy
stuff that is provided by Tableau you
can create aliases and so that it looks
good rather than basic labels and you
can also add a reference line so you add
a reference line something like this
from here we can make out that on an
average female customers exit more than
the male customers right so that is what
we are seeing here on an app
so we have analyzed based on gender we
do see that there is some difference in
the male and female Behavior now let's
take the next criteria which is the
credit card so let's see if having a
credit card has any impact on the
customer exit Behavior so just like
before we drag and drop the credit card
has credit card a column if we drag and
drop here and then we will see that
there is pretty much no difference
between people having credit card and
not having credit card
20.81 percent of people who have no
credit card have exited and similarly
20.18 of people who have credit card I
have also exited so the credit card is
not having much of an impact that's what
this piece of analysis shows last we
will basically go and check how the
geography is impacting so once again we
can drag and drop geography column onto
this side and if we see here there are
geographies like I think there are about
three geographies like France Germany
and Spain and we see that there is some
kind of impact with the geography as
well okay so what we derive from this is
that the credit card is really we can
ignore the credit card variable or
feature from our analysis because that
doesn't have any impact but gender and
geography we can keep and do further
analysis okay all right so what are some
of the advantages of data mining bit
more detailed analysis can help us in
predicting the future Trends and it also
helps in identifying customer Behavior
patterns okay so you can take informed
decisions because the data is telling
you or providing you with some insights
and then you take a decision based on
that if there is any fraudulent activity
data mining will help in quickly
identifying such a fraud as well and of
course it will also help us in
identifying the right algorithm for
performing more Advanced Data Mining
activities like machine learning and so
on all right so the next activity now
that we have the data we have prepared
the data and performed some data mining
activity the next step is model building
let's take a look at model building so
what is a model building if we want to
perform a more detailed data mining
activity like maybe perform some machine
learning then you need to build a model
and how do you build a model first thing
is you need to select which algorithm
you want to use to solve the problem on
hand and also what kind of data that is
available and so on and so forth so you
need to make a choice of the algorithm
and based on that you go ahead and
create a model train the model and so on
now machine learning is kind of at a
very high level classified into
supervised and unsupervised so if we
want to predict a continuous value could
be a price or a 10 temperature or a
height or a length or things like that
so those are continuous values and if
you want to find some of those then you
use techniques like regression linear
regression simple linear regression
multiple linear regression and so on so
these are the algorithms on the other
hand there will be situations or there
may be situations where you need to
perform unsupervised learning case of
unsupervised learning you don't have any
historical labeled data so to learn from
so that is when you use unsupervised
learning and some of the algorithms in
unsupervised learning are clustering
k-means clustering is the most common
algorithm used in unsupervised learning
and similarly in supervised learning if
you want to perform some activity on
categorical values like for example that
is not measured but it is counted like
you want to classify whether this image
is a cat or a dog whether you want to
classify whether this customer will buy
the product or not or if you want to
classify whether this email is Ram or
not spam so these are examples of
categorical values and these are
examples of classification then you have
algorithms like logistic regression K
nearest neighbor or k n and support
Vector machine so these are some of the
algorithms that are used in this case
and similarly in case of unsupervised
learning if you need to perform on
categorical values you have some
algorithms like Association analysis and
hidden Markov model okay so in order to
understand this better let's take an
example and take you through the whole
process and then we will also see how
the code can be written to perform this
now let's take our example here where we
want to perform a supervised learning
which is basically we want to do a
multi-linear regression which means
there are multiple independent variables
and then you want to perform a linear
regression to predict certain value so
in this particular example we have World
happiness data so this is the data about
the high happiness quotient of people
from various countries and we are trying
to predict and see whether our how our
model will perform so what is the
question that we need to ask first of
all how to describe the data and then
can we make a predictive model to
calculate the happiness score right so
based on this we can then decide on what
algorithm to use and what model to use
and so on so variables that are
available or used in this model this is
a list of variables that are available
there is a happiness rank I'll load the
data and or I'll show you the data in a
little bit so it becomes clear what are
these so that is what is known as
happiness rank happiness score which is
happiness score is more like absolute
value whereas rank is what is the
ranking and then which country we are
talking about and within that country
which region and what kind of economy
and whether the family which family and
health details and freedom trust
generosity and so on and so forth so
there are multiple variables that are
available to us and the specific details
probably are not required and there can
be in another example the variables can
be completely different so we don't have
to go into the details of what exactly
these variables are but just enough to
understand that we have a bunch of these
variables and now we need to use either
all or some of these variables and then
which we also sometimes refer to as
features and then we need to build our
model and train our model all right so
let's assume we will use python in order
to perform this analysis or perform this
machine learning activity and I will
actually show you in our lab in in a
little bit this whole thing we will run
the Live code but quickly I will run you
through the slides and then we will go
into the lab so what are we doing here
first thing we need to do is import a
bunch of libraries in Python which are
required to perform our analysis most of
these are for manipulating the data the
preparing the data and then scikit-learn
or sklearn is the library which you will
use at actually for this particular
machine learning activity which is
linear regression so we have numpy we
have pandas and so on and so forth so
all these libraries are imported and
then we load our data and the data is in
the form of a CSV file and there are
different files for each year so we have
data for 2015 16 and 70 and so we will
load this data and then combine them
concatenate them to prepare a single
data frame and here we are making an
assumption that you are familiar with
python so it becomes easier if you are
familiar with Python programming
language or at least some programming
language so that you can at least
understand by looking at the code so we
are reading the file each of these files
for each year and this is basically we
are creating a list of all the names of
the columns we will be using later on we
will see in the code so we have loaded
2015 then 2016 and then also 2017. so we
have created three data data frames and
then we concatenate all these three data
frames this is what we are doing here
then we identify which of these columns
are required which for example some of
the categorical values do we really need
we probably don't then we drop those
columns so that we don't unnecessarily
use all the columns and make the
computation complicated complicated we
can then create some plots using plotly
library and data some powerful features
including creation or creation of maps
and so on just to understand the pattern
the happiness quotient or how the
happiness is across all the countries so
it's a nice visualization we can see
each of these countries how they are in
terms of their happiness score this is
the legend here so the lighter colored
countries have lower ranking and so
these are the lower ranking ones and
these are higher ranking which means
that the ones with these dark colors are
the happiest ones so as you can see here
Australia and maybe this side uh us and
so on are the happiest ones okay the
other thing that we need to do is the
correlation between the happiness score
and happiness rank we can find a
correlation using a scatter plot and we
find that yes they are kind of inversely
proportion which is obvious so if the
score is high happiness score is high
then are ranked number one for example
highest scored as number one so that's
the idea behind this so the happiness
score even here and the happiness rank
is actually given here so they are
inversely proportional because the
higher the score the absolute value of
the rank will be lower right so number
one has the highest value of the score
and so on so they are inversely
correlated but there is a strong what
this graph shows is that there is a
strong correlation between happiness
Rank and happiness score and then we do
some more plots to visualize this we
determine that probably Rank and score
are pretty much conveying the same
message so we don't need both of them so
we will kind of drop one of them and
that is what we are doing here so we
drop the happiness Rank and similarly so
this is one example of how we can remove
some columns which are not adding value
so we will see in the code as well how
that works moving on this is a
correlation between pretty much each of
the columns with the other columns so
this is a correlation you can plot using
plot function and we will see here that
for example happiness score and
happiness score are correlated strong
score relation right because every
variable will be highly correlated to
itself so that's the reason so the
darker the color is the higher the
correlation and as so the and
correlation in numerical terms goes from
zero to one so one is the highest value
and it can only be between 0 and 1
correlation between two variables can be
only have a value between 0 and 1. so
the numerical value can go from 0 to 1
and 1 here is dark color and 0 is kind
of dark but it is blue colors from Red
it goes down the dark blue color
indicates pretty much no correlation so
from this heat map we see that happiness
and economy and family are probably also
Health probably are the most correlated
and then it keeps decreasing after
Freedom kind of keeps decreasing and
coming to pretty much
zero all right so that is a correlation
graph and then we can probably use this
to find out which are the columns that
need to be dropped which do not have
very high correlation and we take only
those columns that we will need so this
is the code for dropping some of the
columns once we have prepared the data
when we have the required columns then
we use scikit-learn to actually split
the data first of all this is a normal
machine learning process you need to
split the data into training and test
data set in this case we are splitting
into 80 20 so 80 is the training data
set and 20 is the test data set so
that's what we are doing here so we use
train test flip method or function so
you have all your training data uh in X
underscore train the labels in y
underscore train similarly X underscore
test has the test data the inputs
whereas the labels are in y underscore
test so that's how and this value where
it is 80 20 or 50 50 that is all
individual preference so in our case we
are using 80 20. all right and then the
next is to create a linear regression
instance so this is what we are doing
we're creating an instance of linear
regression and then we train the model
using the fit function and we are
passing X and Y which is the x value and
the label data regular input and the
label data table information then we do
the test We Run The or we perform the
evaluation on the test data set so this
is what we are doing with the test data
set and then we will evaluate how
accurate the model is and using the
scikit-learn functionality itself we can
also see what are the various parameters
and what are the various coefficients
because in linear regression you will
get like a equation of like a straight
line Y is equal to beta0 plus beta 1 x 1
plus beta 2 x 2 so those beta 1 beta 2
Beta 3 are known as the coefficients and
beta0 is The Intercept after the
training you can actually get these
information of the model what is The
Intercept value what are the
coefficients and so on by using these
functions so let's take quickly go into
the lab and take a look at our code okay
so this is my lab this is my jupyter
notebook where the code I have the
actual code and I will take you through
to the score to run this linear
regression on the world happiness data
so we will import a bunch of libraries
numpy pandas plot plotly and so on also
yeah scikit-learn that's also very
important so that's the first step then
I will import my data and the data is in
three parts there are three files one
for each year 2015 2016 and 2017 and it
is a CSV file so I've imported my data
let's take a look at the data quickly
glance at data so this is how it looks
we have the country region happiness
Rank and then happiness score there are
some standard errors and then what is
the per capita family and so on so then
we will keep going we will create a list
of all these column names we will be
using later so for now just I will run
this code and no need of major
explanation at this point we know that
some of these columns probably are not
quiet so you can use this drop
functionality to remove some of the
columns which we don't need like for
example region and standard error will
not be contributing to our model so we
will basically drop those values out
here so we use the drop and then we
created a vector with these names column
names that's what we are passing here
instead of giving the names of the
columns here we can pass a vector so
that's what we are doing so this will
drop from our data frame it will remove
region and standard error these two
columns then the next step we will read
the data for 2016 and also 2017 and then
we will concatenate this data so let's
do that so we have now data frame called
Happiness which is a concatenation of
both all the three files let's take a
quick look at the data now so most of
the unwanted columns have been removed
and you have all the data in one place
for all the three years and this is how
the data looks and if you want to take a
look at the summary of The Columns you
can say describe and you will get this
information for example for each of the
columns what is the count what's what is
the mean value standard deviation
especially the numeric values okay not
the categorical values so this is a
quick way to see how the data is and
initial little bit of exploratory
analysis can be done here so what is the
maximum value what's the minimum value
and so on for each of the columns all
right so then we go ahead and create
some visualizations using plotly so let
us go and build a plot so if we see here
now this is the relation correlation
between happiness Rank and happiness
score this is what we have seen in the
slides as well we can see that there is
a tight correlation between them only
thing is it is inverse correlation but
otherwise they are very tightly
correlated which also says that they
both probably provide the same
information so there is no not much of
value add so we'll go ahead and drop the
happiness rank as well from our columns
so that's what we are doing here and now
we can do the creation of the
correlation heat map let us plot the
correlation heat map to see how each of
these columns is correlated to the
others and we as we have seen in the
slides this is how it looks so happiness
score is very highly correlated so this
is the legend we have seen in the slide
as well so blue color indicates pretty
much zero or very low correlation deep
red color indicates very high
correlation and the value correlation is
a numeric value and the value goes from
0 to 1. if the two items or two features
or columns are highly correlated then
they will be as close to one as possible
and two columns that are not at all
correlated will be as close to zero as
possible so that's how it is for example
here happiness score and happiness score
every column or every feature will be
highly correlated to itself so it is
like between them there will be
correlation value will be one so that's
why we see deep red color but then
others are for example with higher
values are economy and then health and
then maybe family and freedom so these
are generosity and Trust are not very
highly correlated to happiness score so
that is uh one quick exploratory
analysis you can do and therefore we can
drop the country and the happiness rank
because they also again don't have any
major impact on the analysis on our
analysis so now we have prepared our
data there was no need to clean the data
because the data was clean but if there
were some missing values and so on as we
have discussed in the slides we would
have had to perform some of the data
cleaning activities as well but in this
case the data was clean all we needed to
do was just the preparation part so we
removed some unwanted columns and we did
some exploratory data analysis now we
are ready to perform the machine
learning activity so we we use scikitler
for doing the machine learning circuit
learn is python library that is
available for performing our machine
learning once again we will import some
of these libraries like pandas and numpy
and also scikit-learn first step we will
do is put the data in 2080 format so you
have all the test data which is 20 of
the data is test data and 80 percent is
your training data so this test size
indicates how much of it is in the what
is the size of the test data the
remaining which is uh here we are saying
0.2 therefore that means training is 0.8
so training data is eighty percent all
right so we have executed that split the
data and now we create an instance of
the linear regression model so LM is our
linear regression model and we pass X
and Y the training data set and call the
function fit so that the model gets
drained so now once that is done
training is done training is completed
and now what we have to do is we need to
predict the values for the test data so
the next step is using so you see your
fit will basically run the Training
Method predict will actually predict the
values so we are passing the input
values which is the independent
variables and we are asking for the
values of the dependent variable which
is which we are capturing in y
underscore thread and we use the predict
method here lm.predict so this will give
us all the predicted y values and
remember we already have y underscore
test has the actual values which are the
labels so that we can use these two to
compare and find out how much of it is
error so that's what we are doing here
we are trying to find the difference
between the predicted value and the
actual value y underscore test is the
actual value for the test data and Y
underscore predict is the predicted
value we just found out the predictor
right so we will run that and we can do
a quick check as to how the data looks
how is the difference so in some cases
it is positive some cases it is negative
but in most of the cases I think the
difference is very small this is
exponential to the power of 0 minus 0 4
and so on so looks like our model has
performed reasonably well we can now
check some of the parameters of our
model like the intercept and the
coefficients so that's what we are doing
here so these are the coefficients of
the various parameters that we are the
coefficients of the various independent
variables okay so these are the values
then we can quickly go ahead and list
them down as well against the
corresponding independent variables so
the coefficients against the
corresponding independent variable so
1.0051 is the coefficient for economy
0.9983 is for family coefficient for
family and health and so on and so forth
right so that's what this is showing now
we can use the functionality readily
available functionality of scikit-learn
and then plot that to find some of the
parameters which determine the accuracy
of this model like for example what is
the mean square error and so on so
that's what we are doing here so let's
just go ahead and run this so you can
see here that root mean square error is
pretty low which is a good sign and
which is a one of the measures of how
well our model is performing we can do
one more quick plot to just see how the
actual values and the predicted values
are looking and once again you can see
that as we have seen from the root mean
square error root mean square error is
very very low that means that the actual
values and the predicted values are
pretty much matching up almost matching
up and this plot also shows the same so
this line is going through the predicted
values and the actual values and the
differences very very low so again this
is actually later this is one example
where the accuracy is high and the
predicted values are pretty much
matching with the actual values but in
real life you may find that these values
are slightly more scattered and you may
get the error value can be relatively On
The Higher Side the root mean Square
okay so this was a good quick example of
the code to perform data science
activity or machine learning or data
mining activity in this case we did what
is known as linear regression so let's
go back to our slides and see what else
is there so we saw this these are the
coefficients of each of the features in
our code and we have seen the root mean
square error as well and we can take a
few hundred countries certain values and
actually predict to see if how the model
is performing and I think we have done
this as well and in this case as we have
seen pretty much the predicted values
and the actual values are pretty much
matching which means our model is almost
100 accurate as I mentioned in real life
it may not be the case but in this
particular case we have got a pretty
good model which is very good also
subsequently we can assume that this is
how the equation in linear regression
the model is nothing but an equation
like Y is equal to beta0 plus beta 1 x 1
plus beta 2 x 2 where plus beta 3 x 3
and so on so this is what we are showing
here so this is our intercept which is
beta0 and then we have beta 1 into
economy value beta 2 into the family
value beta 3 into health value and so on
so that is what is shown here okay so I
think the next step once we have the
result from the data mining or machine
learning activity the next step is to
communicate these results to the
appropriate stakeholders so that is what
we will see here now so how do we
communicate usually you take these
results and then either prepare a
presentation or put it in a document and
then show them these actionable results
or actionable insights and you need to
find out who are your target audience
and put all the results in context and
maybe if there was a problem statement
you need to put this results in the
context of the problem statement what
was our initial goal that we wanted to
achieve so that we need to communicate
here based on remember we started off
with what is the question and what is
the data and so on and then what is the
answer so we we need to put the results
and then what is the methodology that we
have used all that has to be input and
clearly communicated in business terms
so that the people will understand very
well from a business perspective so once
the model building is done once the
results are published and communicated
the last part is maintenance of this
model now very often what can happen is
the model may have to be subsequently
updated or modified because of multiple
reasons either the the data has changed
the way the data comes has changed or
the process has changed for whatever
reason the accuracy may keep changing
once you have a trained model the for
example we got a very high accuracy but
then over a period of time there can be
various factors which can cause that so
from time to time we need to check
whether the model is performing well or
not the accuracy needs to be tested once
in a while and if required you may have
to rebuild or retrain the model so you
do the assessment you you see if it
needs any tweaks or changes and then if
it is required you need to probably
retrain the model with the latest data
that you have and then you deploy it you
build the model train it and then you
deploy it so that is like the
maintenance cycle that you may have to
take the model through also data
scientists looking for online training
and certification programs from the best
universities or a professional who Alex
to switch careers with data science then
try giving simply learns postgraduate
program in data science as short the
link in the description box below should
navigate to the home page where you can
find the complete overview of the
program being offered now over to our
training experts
hey everyone welcome to Simply learn
today's video will compare and contrast
artificial intelligence deep learning
machine learning and data science but
before we get started consider
subscribing to simplylearn's YouTube
channel and hit the Bell icon that way
you'll be the first to get notified when
we post similar content before moving on
let me ask you two interesting queries
which among the following is not a
branch of artificial intelligence data
analysis machine learning deep learning
neural networks and the second query is
what is the main difference between
machine learning and deep learning
please leave your answer in the comments
section below and stay tuned to get the
answer
first we will unwrap deep learning deep
learning was first introduced in the
1940s deep learning did not develop
suddenly it developed slowly and
steadily over seven decades many Theses
and discoveries were made on deep
learning from the 1940s to 2000. thanks
to companies like Facebook and Google
the term deep learning has gained
popularity and may give the perception
that it is a relatively New Concept deep
learning can be considered as a type of
machine learning and artificial
intelligence or AI that imitates how
humans gain certain types of knowledge
deep learning includes statistics and
predictive modeling deep learning makes
processes quicker and simpler which is
advantageous to data scientists to
gather analyze and interpret massive
amounts of data having the fundamentals
discussed let's move into the different
types of deep learning neural networks
are the main component of deep learning
but neural networks comprise three main
types which contain artificial neural
networks or an
convolution neural networks or CNN and
recurrent neural networks or RNN
artificial neural networks are inspired
biologically by the animal brain
convolutional neural networks surpass
other neural networks when given inputs
such as images Voice or audio it
analyzes images by processing data
recurrent neural networks uses
sequential data or series of data
convolutional neural networks and
recurrent neural networks are used in
natural language processes speech
recognition image recognition and many
more machine learning the evolution of
ml started with the mathematical
modeling of neural networks that served
as the basis for the invention of
machine learning in 1943 neuroscientist
Warren McCulloch and logician Walter
Pitts attempted to quantitatively map
out how humans make decisions and carry
out thinking processes therefore the
term machine learning is not new machine
learning is a branch of artificial
intelligence and computer science that
uses data and algorithms to imitate how
humans learn gradually increasing the
system's accuracy there are three types
of machine learning which include
supervised learning what is supervised
learning well here machines are trained
using label data machines predict output
based on this data now coming to
unsupervised learning models are not
supervised using a training data set it
is comparable to the learning process
that occurs in the human brain while
learning something new and the third
type of machine learning is
reinforcement learning here the agent
learns from feedback it learns to behave
in a given environment based on actions
and the result of the action this
feature can be observed in robotics
now coming to the evolution of AI the
potential of artificial intelligence
wasn't explored until the 1950s although
the idea has been known for centuries
the term artificial intelligence has
been around for a decade still it wasn't
until British polymath Alan Turing posed
the question of why machines couldn't
use knowledge like humans do to solve
problems and make decisions
we can Define artificial intelligence as
a technique of turning a computer-based
robot to work and act like humans
now let's have a glance at the types of
artificial intelligence
weak AI performs only specific tasks
like Apple Siri Google assistant and
Amazon's Alexa you might have used all
of these Technologies but the types I am
mentioning after this are under
experiment General AI can also be
addressed as artificial general
intelligence it is equivalent to human
intelligence hence an AGI system is
capable of carrying out any task that a
human can
strong AI aspires to build machines that
are indistinguishable from the human
mind
both General and strong AI are
hypothetical right now rigorous research
is going on on this matter there are
many branches of artificial intelligence
which include machine learning deep
learning natural language processing
robotics expert systems fuzzy logic
therefore the correct answer for which
is not a branch of artificial
intelligence is option a data analysis
now that we have covered deep learning
machine learning and artificial
intelligence the final topic is data
science
Concepts like deep learning machine
learning and artificial intelligence can
be considered a subset of data science
let us cover the evolution of data
science the phrase data science was
coined in the early 1960s to
characterize a new profession that would
enable the comprehension and Analysis of
the massive volumes of data being
gathered at the time
since its Beginnings data science has
expanded to incorporate ideas and
methods from other fields including
artificial intelligence machine learning
deep learning and so forth data science
can be defined as the domain of study
that handles vast volumes of data using
modern tools and techniques to find
unseen patterns derive meaningful
information and make business decisions
therefore data science comprises machine
learning artificial intelligence and
deep learning now let's talk about
python for doing data science we need
some kind of a programming language or a
tool and so on so this session will be
about python there are other tools like
for example R and we will probably do a
separate video on that but this session
is on Python and you must have already
heard python is really becoming very
popular everybody is talking about
python not only data science in iot and
Ai and many other places so it's a very
popular it's getting very popular so if
you are not yet familiar with python
this may be a good time to get started
with it so why do we want to use Python
so basically python is used as a
programming language because it is for
data science because it has some rich
tools from a mathematics and from a
statistical perspective it has some rich
tools so that is one of the reasons why
we use Python and if you see some of the
trends if you're probably tracking some
of the trends you will see that over the
last few years python has become
programming language of choice and
especially for data science SAS was
earlier one of the most popular tools
but now increasingly python is being
used for doing data science and of
course as well as r one of the reasons
of course is that Python and R are open
source compared to SAS which is a
commercial product so that could
definitely be one explanation but beyond
that I think it is the ease of
understanding this language the ease of
using this language which is also making
it very popular in addition to the
availability of fantastic libraries for
performing data science what are the
other factors there are speed then there
are availability of number of packages
and then of course the design goal all
right so what are each of these design
goals primarily the syntax rules in
Python are relatively intuitive and easy
to understand thereby it helps in
building applications with the con size
and readable code base so with the few
lines of code you can really achieve a
lot of stuff and then there are a lot of
packages that are available that have
been developed by other people which can
be reused so we don't have to reinvent
the wheel and last but not least the
speed so python is relatively faster
language of course it is not as fast as
let's say CRC plus plus but then
relatively it is still faster so these
are the three factors which make python
the programming language of choice so if
you want to get started with python the
first thing obviously is to install
python so there is some documentation
there are some steps that you need to
follow so we will try to briefly touch
upon that otherwise of course there are
a lot of material available on how to
install Python and so on you can always
look around but this is one of the again
there are different ways in which you
can also install python so we will use
the Anaconda path there is a packaging
tool called Anaconda so we will use that
path you can also directly install
python but in our session we will use
the Anaconda route so the first thing
you need to do is download Anaconda and
this is the path for that and once you
click on this you will come to a page
somewhat like this and download you can
do the corresponding download Based on
whether you have a Windows or Ubuntu
there is a also a download possible for
our package available for Ubuntu if you
are doing something on Ubuntu So based
on which operating system in fact this
page will automatically detect which
operating system you are having and it
will actually suggest so for example you
see here if you're running Mac OS then
it will automatically medically detect
that you have Max and the corresponding
installers will be displayed here
similarly if you are on some flavor of
Linux like Ubuntu or any other then you
will get the corresponding download
links here and then beyond that you can
also select which version of python you
want to install of course the latest
version is in the three point x range at
the time of recording this 3.6 is one of
the latest versions but some of you may
want to do or start with the earlier
version which is Python 2.7 to point x
and you can download that as well if you
don't have anything installed then my
suggestion is start with python 3.6 all
right so once you do that you will be
able to install Python and you will be
able to run Jupiter notebook okay so now
that you know how to install Python and
if you have installed python let's take
a look at what are the various libraries
that are available so python is a very
easy language to learn and there are
some basic stuff that you can do for
example adding or printing a hello world
statement and so on without importing
any specific libraries but if you want
to perform data analysis you need to
include or import some specific
libraries so we are going to talk about
those as we move forward so pandas for
example is used for structured data
operations so if you let's say are
performing something on a CSV file you
import a CSV file create a data frame
and then you can do a lot of stuffs like
data munging and data preparation before
you do any other stuff like for example
machine learning or so on so that's
pandas sci-fi as the name suggests it is
kind of it provides more scientific
capabilities like for example it has
linear algebra it has Fourier transform
and so on and so forth then you have
numpy which is a very powerful library
for performing n-dimensional or creating
and dimensional arrays and it also has
some of the stuff that is there in
sci-fi like for example linear algebra
and Fourier transform and so on and so
forth then you have matplotlib which is
primarily for visualization purpose it
has again very powerful features for
visualizing your data for doing the
initial what is known as exploratory
data analysis for doing univariate
analysis by variate analysis so this is
extremely useful for visualizing the
data and then scikit-learn is used for
performing all the machine learning
activities if you want to do anything
like linear regression classification or
any of this stuff then the scikit-learn
library will be extremely helpful in
addition to that there are a few other
libraries for example networks and I
graph then of course a very important
one is tensorflow so if you are
interested in doing some deep learning
or AI related stuff then it would be a
good idea to learn about tensorflow and
tensorflow is one of the libraries there
is a separate video on tensorflow you
can look for the that and this is one of
the libraries created by Google open
source Library so once you're familiar
with machine learning data analysis
machine learning then that may be the
next step to go to deep learning and AI
so that's where tensorflow will be used
then you have beautiful soup which is
primarily used for web scraping and then
you take the data and then analyze and
so on then OS library is a very common
Library as the name suggests it is for
operating system so if you want to do
something on creating directories or
folders and things like that that's when
you would use OS all right so moving on
let's talk in a little bit more detail
about each of these libraries so sci-fi
as the name suggests is a scientific
library and it very specifically it has
some special functions of our
integration and for ordinary
differential equations so as you can see
these are a mathematical operations or
mathematical functions so these are
readily available in this library and it
has linear algebra modules and date is
built on top of numpy so we will see
what is there in numpy so this is a
again as the name suggests the num comes
from numbers so it is a mathematical
library and one of its key features is
availability of an n-dimensional array
object that is a very powerful object
and we will see how to use this and then
of course you can create other let's say
objects and so on and it has tools for
integrating with C C plus plus and also
Fortran code and then it of course also
has linear algebra and Fourier
transformation and so on all these
scientific capabilities okay what else
pandas is another very powerful Library
primarily for data manipulation so if
you are importing any files you will
want to create it like a table so you
will create what is known as data frames
these are very powerful data structures
that are used in Python Programming so
pandas Library provides this capability
and once you import a data import the
data into Data frame you can pretty much
do whatever you're doing like in a
regular database so people who are
coming from a database background or SQL
background would really like this
because it is very they will feel very
much at home because it feels like
you're using you're viewing a table or
using a table and you can do a lot of
stuff using the pandas library now there
are two important terms or components in
pandas series and the data frame I was
just talking about the data frame so
let's take a look at what our series and
what is a data frame So within pandas we
have series and data frame so series is
primarily some of you may also be
knowing this as let's say an array so
it's a one-dimensional structure data
structure if you will so in some other
languages we may call it as an array or
maybe some others probably an equivalent
of a list in R perhaps I'm not very sure
on that aspect but yes so this is like a
one-dimensional storage of information
so that is what is series whereas data
frame is like a table so you have a
two-dimensional structure you have rows
and you have columns and this is where
people as I said who are familiar with
SQL and databases will be able to relate
to this very quickly so you have like a
table you have rows and columns and then
you can manipulate the data so if you
want to create a series this is how you
would create a code snippet and as you
can see the programming in Python is
very simple there are no major overheads
you just need to import some libraries
which are essential and then start
creating objects so you don't have to do
additional Declaration of variables and
things like that so that is I think one
key difference between Python and other
programming languages and what does this
series contain it has to contain these
numbers 6346 and X is my object
consisting of this series so if you
display if you just say x it will
display the contents of X and you will
see here that it creates a default index
then you have data frame so if you want
to create a data frame as you can see
the series is like a one-dimensional
structure there is just like a row one
row of items whereas a data frame looks
somewhat like this it is a two
dimensional structure so you have
columns in one dimension and then you
have rows in the other dimension how do
you create a data frame you need to
create you need to rather import pandas
and then you import in this case we are
basically creating our own data so
that's the reason we are importing numpy
which is one of the libraries we just
refer to a little bit uh before so we
are using one of the functionalities
within numpy to create some random
numbers otherwise this is not really
mandatory you probably will be importing
the data from outside maybe some CSV
file and import into the data frame so
that's what we are doing here so in this
case we are creating our own test data
that's the reason we are importing numpy
as NP and then I create a data frame
saying PD dot data frame so this is the
keyword here similarly here in this case
while creating series we set PD dot
series and then you pass the value
similarly here using PD dot data frame
now in order to create the data frame it
needs the values in each of these cells
what are the values in the rows and what
are the values in the column so that in
our example we are providing using this
random number generator so NP speed or
random is like a class or a method that
is available in numpy and then you are
saying okay generate some random numbers
in the form of a 4x3 matrix or 4x3 data
frame the 4 here indicates the number of
rows and the three here indicates the
number of columns so these are the
columns 0 1 2 are the columns and these
are the rows here 0 this is one this is
two this is three okay and once again it
will when you display DF it will give us
a default index there are ways to Omit
that but at this point we will just keep
it simple so it will display the default
index and then the actual values in each
of these rows and columns so this is the
way you create a data frame so now that
we have learned some of the basics of
pandas let's take a quick look at how we
use this in real life so let's assume we
have a situation where we have some
customer data and we want to kind of
predict whether the customer's loan will
be approved or not so we have some
historical data about the loans and
about the customers and using that we
will try to come up with a way to maybe
predict whether loan will be approved or
not so let's see how we can do that so
this is a part of exploratory analysis
so we will first start with exploratory
analysis we will try to see how the data
is looking so what kind of data so we
will of course I'll take you into the
jupyter notebook and give you a quick
live demo but before that let's quickly
walk through some of the pieces of this
program in slides and then I will take
you actually into the actual code and do
a demo of that so the Python program
structure looks somewhat like this the
first step is to import your all the
required libraries now of course it is
not necessary that you have to import
all your libraries right at the top of
the code but it is a good practice so
you if you know you are going to need a
certain set of libraries it may be a
good idea to put from a readability
perspective it's a good practice to put
all the libraries that you're importing
at the beginning of your call however it
is not mandatory so in the middle of the
code somewhere if you feel that you need
a particular Library you can import that
library and then start using it in the
middle of the code so that's also
perfectly fine it will not give any
errors or anything however as I said
it's not such a good practice so we will
import all the required libraries in
this case we are importing pandas numpy
and matplotlib and in addition if we
include this piece of code percentage
matplotlib inline what will happen is
all the graphs that we are going to
create the visualizations that we are
going to create will be displayed within
the notebook so if you want to have that
kind of a provision you need to have
this line so it's always a good idea
when you're starting off I think it's a
good idea to just include this line so
that your graphs are shown in line okay
so these are the four we will start with
these four lines of then the next step
is to import your data so in our case
there is a training data for loans by
the name loan PE underscore train.csv
and we are reading this data so in this
case you see here unlike the previous
example where we created a data frame
with some data that we created ourselves
here we are actually creating a data
frame using some external data and it's
the method is very very straightforward
so you use the read underscore CSV
method and it is a very intuitive
function name and you say APD dot read
underscore CSV and give the path of the
file CSV file that's about it and then
that is read into the data frame DF this
can be any name we are calling it DF you
can call XYZ anything there's a name
just name of the object so head is one
of the methods within the data frame and
it will give us the first five so this
is just to take a quick look now you
have imported the data you want to
initially have a quick look how your
data is looking what are the values in
some of the columns and so on and so
forth right so typically you would do a
head DF dot head to get the sample of
let's say the first few lines of your
data so that's what has happened here so
it displays the first few lines and then
you can see what are the columns within
that and what are the values in each of
these cells and so on and so forth you
can also typically you would like to see
if there are any null values or are
there any is the data for whatever
reason is invalid or looking dirty for
whatever reason some unnecessary
character so this will give a quick view
of that so in this case pretty much
everything looks okay then the next step
is to understand the data a little bit
overall for each of the columns what is
the information so the describe function
will basically give us a summary of the
data what else can we do pandas also
allows us to visualize the data and this
is more like a power of what we call it
as univariate analysis that means each
and every column you can take and do
some plots and visualization to
understand data in each of the columns
so for example here the loan amount
column we can take and then the hist
basically hist method will create a
histogram so you take all the values
from one column which is loan amount and
you create a histogram to see how the
data is distributed right so that's what
is happening here and as you can see
there are some extreme values so this is
again to identify do we have to do some
data preparation because if the data is
in a completely haphazard way the
analysis may be difficult so we these
are the initial or exploratory data
analysis is primarily done to understand
that and see if we need to do some data
preparation before we get into the other
steps like machine learning and
statistical modeling and so on so in
this case we will see that here by
plotting this histogram we see that
there are some extreme values so there
are some values a lot of it is around
100 range but there is also something
one or two observations in the 700 range
so it's pretty scattered in that sense
or they're not really scattered
distributedly scattered but it is
randomly scattered so the range is
really huge so what can we do about this
so there are some steps that we need to
do normalization and so on so we'll see
that in a bit so this is for one of the
columns let's take another column which
is applicant income similar kind of
similar situation you have most of your
observations in this range but there are
also some which are far off from where
most of the observations are so this is
also pretty this also has some extreme
values so we'll have to see what can be
done grid history is a binary value so
some people have a zero value and some
will have credit history of one this is
just like a flag so this basically is
telling us how many people have one and
how many people have zero so looks like
majority of them have a value of one and
a few about 100 of them have a value of
zero okay what else can we do so we now
understood a little bit about the data
so we need to do some data wrangling or
data munging and see if we can some
bring in some kind of normalization of
all this data and we will kind of try to
understand what is data wrangling and
before we actually go into it okay so
data wrangling is nothing but a process
of cleaning the data if let's say there
are there are multiple things that can
happen in this particular example there
were no missing values but typically
when you get some data very often it
will so happen that a lot of values are
missing either they are null values or
there are a lot of zeros now you cannot
use such data as it is to perform some
let's say predictive analysis or perform
some machine learning activities and so
on so that is one part of it so you need
to clean the data the other is unifying
the data now if these ranges of this
data are very huge some of them are
going from some columns are going from
zero to a hundred thousand and some
columns are just between 10 to 20 and so
on these will affect the accuracy of the
analysis so we need to do some kind of
unifying the data and so on so that is
what wrangling data wrangling is all
about so before we actually perform any
analysis we need to bring the data so to
some kind of a shape so that we can
perform additional analysis actual
analysis on this and get some insights
now how do we deal with missing values
is a very common issue when we take data
or when we get data from the business
when a data scientist gets the data from
the business so we should never assume
that all our data will be clean and all
the values filled up and so on because
in real life very often there will be
the data will be dirty so data wrangling
is the process where you kind of clean
up the data first of all identify
whether the data is dirty and then clean
up so how do we find some data is
missing so there are a few ways you can
write a small piece of code which will
identify if for a given column or for
given row any of the observations are
null primarily so this line of code for
example is doing that it is trying to
identify how many null values or missing
values are there for each of the columns
so this is a Lambda function and what we
are saying is find out if a value is
null and then you add all of them how
many observations are there where this
particular column is null so it does
that for all the columns so here you
will see that for loan ID obviously it's
an ID so there are no null values or
missing values gender has about 13
observations where the values are
missing similarly marital status has
three and so on and so forth so we'll
see here for example loan amount has 21
observations where the values are
missing loan amount term has 14
observations and so on so we'll see how
to handle this missing values so there
are multiple ways in which you you can
handle missing values if the number of
observations are very small compared to
the total number of observations then
sometimes one of the easy ways is to
completely remove that data so or delete
that record exclude that record so that
is one way of doing it so if there are
let's say a million records and maybe 10
records are having missing values it may
not be worth doing something to fill up
those values it may be better off to get
rid of those observations right so that
is the missing values are
proportionately very small but if there
are relatively large number of missing
values if you exclude those observations
then your accuracy may not be that very
good so there the other way of doing it
is we can take a mean value or for a
particular column and fill up wherever
there are missing values fill up those
observations or cells with the mean
value so that way what happens is you
don't get give some value which is too
high or too low and it somehow fits
within the range of the observations
that we are seeing so this is one
technique again there are it can be case
to Case and you may have to take a call
based on your specific situation but
these are some of the common method if
you see in the previous case loan amount
had 21 and now we went ahead and filled
all of those with the mean value so now
there are zero with missing values okay
so this is one part of a data wrangling
activity so what else you can do you can
also check what are the types of the
data so DF dot d types will give us what
are the various data types so all right
so you can also perform some basic
mathematical observations we have
already seen that mean we found out so
similarly if you do call the mean method
for the data frame object it will
actually perform or display or calculate
the mean for pretty much all the
numerical columns that are available in
this right so for example here applicant
income application income and all these
are numerical values so it will display
the mean values of all of those now
another thing that you can do is you can
actually also combine data frames so
let's say you import data from one CSV
file into one data frame and another CSV
file into another data frame and then
you want to merge these because you want
to do an analysis on the entire data
okay one example could be that you have
data in the form of CSV files one CSV
file for each month of the year January
February March each of these are in a
different so you can import them into
let's say 12 data frames and then you
can merge them together as a single data
frame and then you perform your analysis
on the entire data frame of the entire
data for the year so that is one example
so how do we do that this is how we do
again in this case we are not importing
any data we are just creating some
random values using some random values
so let's assume I have a data frame
which is by the name one and I assign
some random values here which is a 5x4
format so there are five rows and four
columns and this is how my data frame
one looks and then I create another data
frame which is data Frame 2 again random
numbers of the format five by four and I
have something like this now I want to
combine these two how do I combine these
two I can use the concatenate or concat
method and I can combine these two so PD
Dot concat and it takes the the data
frames 1 and 2 if you have more of them
you can provide them and it will just
simply add all of them merge all of them
or concatenate whatever you call
whichever term you call so it will so of
course we have to make sure that the
structure Remains the Same like I said
this could be let's say sales data
coming for 12 different months but each
of the files has the same structure so
now you can combine all of them merge
all of them by using the concat method
if we have let's say structure is not
identical then what what will happen
let's say we have these two data frames
one has a column by the name key and the
second column is L well and a second
data frame which has a column by the
name key but the second column by the
name r well not L well so you see here
the structure is not identical so you
can still combine them but then the way
they get combined or merged is somewhat
like this so it takes the key as a
common parameter between them some
common column has to be there otherwise
this will not work and then we have to
use merge instead of concatenate and
when we do a merge then we get the
result will be in this format what it
does is it uses the key as the Common
Thread between them and then it kind of
populates the values accordingly so if
you see here the first one had four and
bar for key and then it had L values of
1 and 2 right so if we go back 4 and bar
had one and two L values so that's what
we see here 1 and 2 whereas in the right
data frame we had Foo bar and bar as a
second time and then R values are 3 4
and 5. so what it has done for Foo it
has put for the existing right for 4 is
already existing because it has come
from left so it will just put the value
of r value here which is three similarly
it will put 4 here because for bar if
you go back for bar it is the value is 4
and since it has one more value of bar
it will go and add this 5 as well the
only thing here is that this one had for
example left had only two values and
only one value for bar but since we are
appending or merging and there are two
key values with the bar therefore it
will kind of repeat the value of L Val
here so that's what we are seeing in
this case right so L value appears twice
the number 2 appears twice but that is
because R value that are two of them
okay all right so that is how when you
don't have identical structure that's of
you merge now we will talk a little bit
about psychic learn so scikit-learn is a
library which is used for doing machine
learning of work for performing machine
learning activities so if you want to do
linear regression logistic regression
and so on there are easily usable apis
that you can call and that's the
advantage of psychic learn and it
provides a bunch of algorithms so I
think that is the good part about this
Library so if you want to use
scikit-learn obviously you need to
import these modules and also there are
some sub modules you may have to import
based on what you're trying to use like
for example if we know if we want to use
logistic regression again people who are
probably not very familiar with machine
learning that is a separate module for
machine learning you may want to take a
look at that but we will just touch upon
the basics here so machine learning has
some algorithms like linear regression
logistic regression and random Forest
classification and so on so that is what
we are talking about here so those
algorithms are available and again if
you want to use some of them you need to
import them and from the scikit learn
Library so psychic learn is the top
level Library which is basically SQL
learn right and then it has kind of sub
parts in it you need to import those
based on what exactly you will be or
which algorithm you will be using so
let's take an example as we move and we
will see that whenever we perform some
machine learning activity those of you
who are familiar with machine learning
will already know this we split our
labeled data into two parts training and
test now there are multiple ways of
splitting this data how do we either
some people do it like 50 50 some people
do it 80 20 which is training is 80 and
test it is 20 and so on so it is an
individual preference there are no hard
and fast Rules by and large we have seen
that training data set is larger than
the test data set and again we will
probably not go into the details of why
do we do this at this point but that's
one of the steps in machine learning so
scikit-learn offers a readily available
method to do this which is train test
split all right so in this example let's
say we are taking the values X and Y are
our values X is the independent
variables and Y is our dependent
variable okay and we are using these two
and then I want to split this into train
and test data so what do we do we import
the train test split sub module from
within scikit-learn which is SK learn
right so within that we import drain
test split and then you call this train
test split method or function or
whatever you call it and pass the data
so X is the all the values of the
independent variables and Y is our
labels so you pass X and Y and then you
specify what should be your size of the
test data so only one you need to
specify so if you say test size is 0.25
it is understood that train size will be
0.75 so you're telling what should be
the ratio of the split so technically it
doesn't nothing prevents you from giving
whatever you like here so you can give
test as 80 and train as 20 so whichever
way but then there's normal practices
you will have the training data set
would be larger than the test data set
and typically it would be 80 20 75 25 or
65 35 something like that right so that
is the second parameter and this is just
to say that you know the data has to be
randomly split so it shouldn't be like
you take the first 75 percent and put it
in training and then the next 25 percent
and put it in test so that so such a
thing shouldn't happen so we first set
the state random state so that the the
splitting is done in a very random way
so if they are randomly picked up the
data and then put it into training and
test and then this results in these four
data frames so explain and X test and
white rain and white test okay so that
is basically the result it will now that
the splitting is done let's see how to
implement or execute logistic regression
so in logistic regression what we try to
do is try to develop a model which will
classify the data logistic regression is
an algorithm for supervised learning for
performing classification so logistic
regression is for classification and
usually it is binary classification so
binary classification means there are
two classes so either like a yes no or
for example customer will buy or will
not buy so that is a binary
classification so that's where we use
logistic regression so let's take a look
at the code how to implement something
like that using scikit-learn so the
first thing is to import this logistic
regression submodule or subclass
whatever you call it and then create an
instance of that so our object is
classifier so we are creating an object
by the name this is a name by the way
you can give any name in our case we are
saying classifier we say classifier is
equal to logistic regression so we are
creating an instance of the logistic
regression variable or class or whatever
okay and you can pass a variable or a
parameter rather which is the random
state is equal to 0 and once you create
the object which in our case is named
classifier you can then train the object
by calling the method fit so this is
important to note we don't call any
there is no method like train here but
we call what is known as there is a
method called fit so you are basically
by calling the fit method you are
training this model and in order to
train the model you need to pass the
training data set so X underscore train
is your independent variables the set of
independent variables and Y underscore
train is your dependent variable or the
label so you pass both of these and and
call the fit function or fit method
which will actually result in the
training of this model classifier now
this is basically showing what are the
possible parameters that can be passed
or initiated when we are calling the
logistic or the instance of logistic
regression so this is but you can also
look up the help file if you have
installed python so some of these are
very intuitive but some you may want to
take a look at the details of what
exactly they do all right so moving on
once we train the model by calling fit
then the next step is to test our model
so this is where we will use the test
data you need to pay attention here here
I am calling so there are two things one
is in order to test our data we have to
actually call what is known as the
method known as predict right so here
this is where so the training is done
now is the time for inference isn't it
so we have the model now we want to
check whether our model is working
correctly or not so what do you do you
have your test data remember we split it
25 percent of our data was stored here
right we split it into test and training
so that 25 percent of the data we pass
to and call the method predict so that
the model will now predict the values
for y right so that's why here we are
calling it as y underscore predict and
um if we display here as I said this is
the logistic regression which is
basically binary classification so it
gives us the results like yes or no in
this particular case and then you can so
this is what the model has predicted or
model has classified now but we also
know we already have the labels for this
so we need to compare with the existing
labels with the known labels whether
this classification is correct or not so
that is where is the next step which is
basically calculating the accuracy and
so on will come into play okay so in
this case the first thing most important
thing to note is we do the prediction
using predict and here we are passing X
underscore test and not train right in
this case we did x n and whiten so again
one more point to be noted here in case
of training we will pass both the
independent variables and also the
dependent variables because the system
has to internally it has to verify that
is what is the training process so what
it will do it will take the X values it
will try to come up with the Y value and
compare with the actual y value right so
that is what is the Training Method so
that's why we have to pass both X as
well as y whereas in case of predict we
don't pass both we only pass because we
are pretending as if this is the actual
data so in actual data you will not have
the labels isn't it so we are just
passing the independent variables and
the system will then come up with the Y
values which we will then okay remember
we also know the actual value so we will
compare this with the actual values and
we will find out whether how accurate
the model is so how do we do that we use
what is known as a confusion Matrix so
this is also readily available in the
python Library so so we import this
confusion Matrix and some of you who
already know machine learning will find
this familiar but those who are new to
machine learning this confusion Matrix
is nothing but this Matrix this kind of
a matrix which basically tells how many
of them are correctly predicted and how
many of them are incorrectly predicted
so the some of the characteristics let's
quickly spend some time on this
confusion Matrix itself this the total
numbers out here these are just the
numbers these are like number of
observations then the accuracy is
considered to be highest when the the
numbers or the sum of the numbers across
the diagonals is maximum okay and the
numbers outside of the diagonal should
be minimum so which means that if this
model was 100 accurate then the sum of
these two there would have been only
numbers in these two along the diagonal
this would have been 0 and this would
have been zero okay so that is like a
hundred percent accurate model that is
very rare but just that you are aware so
just to give an idea okay all right so
once you have the confusion Matrix you
then try to calculate the accuracy which
is in a percentage so there are two
things that we can do from a confusion
Matrix so that we can calculate from a
confusion Matrix one is the accuracy and
the other is the Precision what is the
accuracy accuracy is basically a measure
of how many of the observations have
been correctly predicted okay so let's
say this is a little bit more detailed
view of the confusion Matrix it looks
very similar like as we saw in this case
right so this is a two by two Matrix
that's what we are seeing here 18 27
2103 so 18 27 2103 now but what are
these values that is what is kind of the
labels are shown here in this so there
are altogether 150 observations so as I
said the sum of all these four right 18
plus 27 plus one zero three plus two is
equal to 150 that's the first thing we
have to observe the sum of all these
values will be equal to the sum of test
observation number of test observations
we have 150 test observations because
remember we had about 500 we split that
into 2575 so that is why we have 150
here and I think 350 in the training
data set okay so that we get the number
correct so that's the first thing now
this next thing is let's take a look at
the actual values this view is the
actual view so there are actually right
in the actual data we have labels yes
and no so as per the actual data there
are 45 observations tagged as no and
similarly there are 105 observations
that are tagged as yes or labeled as yes
okay now I know for the first time when
you are seeing this it may be a little
confusing but just stay with me okay so
this is the actual part of it and this
side tells us the predicted part of it
so our model is predicted and it has
totally predicted 20 of them as no right
so that is what this is totally 20 of
them as predicted as no and it has
predicted 130 of them as yes okay I hope
this part is clear so before we go into
the middle part let us first understand
what exactly are these numbers so
actually tag does no there are 45 total
actually tagged as yes there are 105 and
predicted no there are 20 predicted as
yes there are 130. this is the result
from our model okay this is the result
from our model and this is the actual
value which we already know because this
is our label data that's the first thing
now now let us take a look at each of
these individually okay now what are the
options we have once again okay so now
what is happening here let us look at
these these values so this 18 says that
these are actually tagged as no and the
model is also predicted as no which
means this is what is known as a true
positive right or true negative sorry
right which means that our model is
predicted is it correctly it is negative
because it says no so and it has also
predicted no so it is known as what is
known as true negative okay now let's
come to this side of it that way we are
talking about the diagonal remember I
said most of the values should be in the
diagonal okay so that means these 18 are
correctly tagged they are labeled as no
and our model is predicted as no so
these are correctly tagged and these are
known as true negative okay similarly if
we come diagonal it down there are 103
observations which are labeled as yes
actual value is s and our model is also
predicted as yes and these are known as
true positive values positive because of
this yes okay right so what is important
is this is true this is also true so we
have to make sure that the maximum
number of values are in the true section
okay true positive and true negative
that's the reason I said the Sum along
the diagonal should be maximum now let's
say if your model was 100 accurate this
sum in this case it is only 103 plus 103
plus 18 which is 121 but if our model
was accurate the sum of these two would
have been 150 that means it's a perfect
model okay all right now what else since
we covered these two let's also cover
these two so here this says that 27 of
them were actually labeled no but our
model is predicted as yes that means
this is wrong right similarly these are
two of them where the actual value is
yes but our model is predicted as no
that means it's a wrong prediction so
you get the point so therefore along the
diagonals are the correct values whereas
in other places it is all wrong values
or wrong predictions okay now how do we
calculate accuracy from this information
so the way to calculate accuracy is so
we say okay there are total observations
are 150 and what are the correctly
predicted values these are the correctly
predicted values which is 18 plus 103 so
this will give us our accuracy so 103
plus 18 which is 121 divided by our
total observations which is 150 is our
accuracy which is 0.8 or we can say it
is 80 percent okay now there is another
concept called Precision so Precision is
given by the formula true positives
divided by the predicted positives
totally predicted positives okay what do
we mean by that which are the true
positives here remember which are the
true positives we just recall we just
talked in the previous slide which are
the true positives you see here so this
hundred and three are the true positives
which means that the value is positive
actual value is positive predicted value
is also positive so that's why it's
called a true positive so 103 divided by
so that is our true positive 103 divided
by totally predicted as yes now what is
totally predicted is yes remember 130 of
them I have all together been predicted
as yes not that they are correctly
predicted only 103 have been correctly
predicted but 130 of them have been
predicted as yes so Precision is
basically the ratio of these two out of
the totally predicted how many of them
are actually true that ratio so 103 by
130 which is again about 80 percent is
the Precision that's how you calculate
Precision so this is just a simple
formula in the term that you need to
remember so accuracy is you need to take
total of true positive and true negative
divided by the total number of
observations whereas Precision is true
positives divided by the totally
predicted positives okay so that is our
accuracy and precision now what we did
the accuracy calculation was manual but
we can also use some libraries which are
already existing and the functions
within that Library so a psychic learn
provides one such method so for example
accuracy underscore score is one such
method so if you use that and pass your
test and predicted values only the y u
need to pass right the dependent
variable values so if you pass that it
will calculate it for you so in this
case again as you can see it still
calculates the same which is 80 percent
which we have seen here as well okay so
this can be done using the method great
so that's pretty much what we have done
here before we conclude let me take you
into the code and show you how it
actually looks okay so this is our code
let me run it okay one by one we have
already seen most of the steps in the
slides so I will but I will run this in
the actual jupyter notebook some of you
if you are not yet familiar with jupyter
notebook again there are other videos we
created on how to install Jupiter
notebook and how to set up jupyter
notebook and so on in this tutorial also
we there was one slide on how to install
Python and jupyter notebook if you have
not yet done please do that so that then
you can actually walk through this code
while you're watching this okay so what
are we doing here we are importing the
library's required libraries recall here
we have pandas we have numpy and for uh
visualization we have matplotlib and
this line is basically reading the CSV
file so we have the CSV file locally on
our local drive and this is where I'm
checking the data just so that I'm
starting with my exploratory analysis
how the data is looking so it looks good
I don't know major missing values or
anything like that so it will display
all the columns and it will show me the
first five rows if when I'm using this
head function and then I want to see a
kind of a summary of all the each of the
numerical columns so that's what I'm
doing here so these are the numerical
columns and it gives a summary like how
many observations are there what is the
mean standard deviation minimum maximum
and so on and so forth for each of them
and then you can do some visualization
so this is the visualization for for
this okay the next step is to view the
data data visualization and we will do
that using a histogram for a couple of
these columns so in this case I'm taking
a look at the loan amount and if I
create a histogram it displays the data
here in the form of a histogram one
thing that we gather from this as I
mentioned in the slides as well is how
the data is kind of scattered so while
most of the values are in this range 0
to 300 range there are a few extreme
values around the 700 range so that is
one information we get from this
histogram similarly for the applicant
income if we draw a histogram something
similar we can see that while most of
the values are in this range 0 to 20 000
range there are a few in the range of 80
000 and probably 65 000 and so on okay
so the next step is to perform data
wrangling where we will check if any
data is missing and how to fill those
missing values and so on so in this case
we will just check for all the columns
how many data or how many entries are
there with missing values so this is the
result so loan ID has all the columns or
all the cells filled gender has 13
missing values marital status has three
missing values and so on and so forth
loan amount as 21 and this is what we
are going to show you how to remove
these missing values so when you have
missing values as I mentioned in the
during the slides there are a couple of
ways of handling that one is you can
completely remove those or you fill in
with some meaningful values so in this
case we will fill the missing values
with the mean value of the loan amount
so let's go ahead and do that and now if
we check here now loan amount number of
missing values is 0 because what we did
was for all these 21 cells where the
values were missing we filled with the
mean value of the low number so now
there are no more missing values for
loan amount we can do this for other
columns as well but this was just one
example so we have shown it here okay so
we will run this for credit history and
loan amount term as well and then if we
calculate the mean of pretty much all
the numerical columns that's the method
call so DF dot mean will give us the
mean of all the numerical values and
another thing that we can do is we if we
want to find out what are the data types
of each of these columns so you can call
DF dot d types and get the data types of
course it may not be that very useful
most of the cases is an object but for
example this one it shows as int64 and
there are float64 and so on and so forth
now in addition to doing the exploratory
data analysis we can do some machine
learning activity as well so in this
case we are going to do logistic
regression so this is the example that I
have shown you in the slides as well
this is the actual code for that all
right so the first step here is to
import the libraries and then the next
step is to separate the independent
variables and and the dependent
variables so X is our independent
variable and Y is our dependent variable
so we separate the data into two parts
and this will be our Target as well
right so that's how we separate it now
we have to split the data into training
and test data sets as I mentioned in the
during the slides we use the train test
split method and when we call this and
pass the independent variables and the
dependent variables and we specify the
test size to be 0.25 which means the
training size will be 0.75 which is
nothing but you split the data into
training data set which is 75 percent
and test data set in which is 25 percent
okay so once you split that you will
have all your independent variables data
in extrane the training data which is 74
percent of it similarly independent
variables for test will be in X
underscore test and dependent variables
train will be in the underscore train
and dependent variable test will be
Wireless contest once we do this we have
to do a small exercise for scaling
remember we had some data which was kind
of very scattered there were some
extreme values and so on so this will
take care of that so that the data is
normalized so that before we pass to our
algorithm the data is normalized so that
the performance will be much better the
next step is to create the instance of
logistic regression object so that's
what we are doing here so classifier is
our logistic regression instance right
classifier is equal to logistic
regression we are saying so one instance
of plastic regression is created and
then we call the Training Method the
name of the method actually is fit but
what it is doing is it is taking the
training data X is the training data or
the independent variables and Y is the
dependent variables so we are taking
both of these and the model gets trained
so the method for calling the training
is fit okay so it gives us the output
and then once we are done with the
training we do the testing and once
again just to recall in the slides when
I was showing you the slides also I
mentioned we don't pass y here while we
are testing while for training we do
pass y but right so for fit we are
passing X and Y but for test we are only
passing X something you need to observe
because y will be calculated by the
model and we will then compare that with
the known value of y to measure the
accuracy so that's what we will do here
and the method that is called here is
predict so this will basically create or
predict the values of Y now we have in
this case a binary classification so the
outputs are yes or no y indicates yes
and then indicates no so y or n is the
output now how do we measure the
accuracy as we have seen earlier I
described how configuration Matrix works
and how we can use confusion Matrix for
calculating the accuracy that's what we
are seeing here so this is the confusion
Matrix and then then you want to do the
measure the accuracy you can directly
use this method and we find that it is
80 so we in the slides we have seen when
we calculate manually as well we get an
accuracy of 80 also data scientists
looking for online training and
certification programs from the best
universities or a professional who
elects to switch careers with data
science then try giving simply learns
postgraduate program in data science as
short the link in the description box
below should navigate to the home page
where you can find the complete overview
of the program being offered now over to
our training experts so let's talk about
this amazing Library tensorflow which is
also one of my favorites so tensorflow
is a library for high performance
numerical computations with around 35
000 GitHub comments and a Vibrant
Community of around 1500 contributors
and it's used across various scientific
domains it's basically a framework where
we can Define and run computations which
involves tensors and tensors we can say
partially defined computational objects
again where they will eventually produce
a value that was about tensorflow let's
talk about the features of tensorflow so
tensorflow is majorly used in deep
learning models and neural networks
where we have other libraries like torch
and piano also but tensorflow has hands
down better computational graphical
visualizations when compared to them
also tensorflow reduces the error
largely by 50 to 60 percent in neural
machine translations it's highly
parallel in a way where you can train
multiple neural networks and multiple
gpus for highly efficient and scalable
models this parallel Computing feature
of tensorflow is also called pipelining
also tensorflow has the advantage of
seamless performance as it's backed by
Google it has quicker updates frequent
new releases with the latest of features
now let's look at some applications
tensorflow is extensively used in speech
and image recognition text-based
applications time series analysis is in
forecasting and various other
applications involving video detection
so favorite thing about tensorflow that
is already popular among the machine
learning community and most are open to
trying it and some of us are already
using it now let's look at an example of
a tensorflow model in this example we
will not dive deep into the explanation
of the model as it is beyond the scope
of this video so here we're using amnest
dataset which consists of images of
handwritten digits handwritten digits
can be easily recognized by building a
simple tensorflow model let's see how
when we visualize our data using
matplotlab Library the inputs will look
something like this then we create our
tensorflow model to create a basic
tensorflow model we need to initialize
the variables and start a session then
after training the model we can validate
the data and then predict the accuracy
this model has predicted 92 accuracy
let's see which is pretty well for this
model so that's all for tensorflow if
you need to understand this tutorial in
detail then you can go ahead and watch
deep learning tutorial from Simply learn
as shown in the right corner interesting
right let's move on to the next library
now let's talk about a common yet a very
powerful python Library called numpy
numpy is a fundamental package for
numerical competition in Python it
stands for numerical python as the name
suggests it has around 18 000 comments
on GitHub with an active community of
700 contributors it's a general purpose
array processing package in a way that
it provides high performance
multi-dimensional objects called arrays
and tools for working with them also
numpy addresses the slowness problem
partly by providing these
multi-dimensional arrays that we talked
about and then functions and operators
that operate efficiently on these arrays
interesting right now let's talk about
features of numpy it's very easy to work
with large arrays and mattresses using
numpy numpy fully supports object
oriented approach for example coming
back to ND array once again it's a class
possessing numerous methods and
attributes ndra provides for larger and
repeated computations numpy offers
vectorization it's more faster and
compact than traditional methods I
always wanted to get rid of loops and
vectorization of numpy clearly helps me
with that now let's talk about the
applications of numpy numpy along with
pandas is extensively used in data
analysis which forms the basis of data
science it helps in creating the
powerful n-dimensional array whenever we
talk about numpy the mention of the
array we cannot do it without the
mention of the powerful n-dimensional
array also number is extensively used in
machine learning when we are creating
machine learning models as in where it
forms the base of other libraries like
sci-fi scikit-learn
Etc when you start creating the machine
learning models in data science you will
realize that all the models will have
their basis numpy or pandas also when
number is used with sci-fi and plot live
it can be used as a replacement of
Matlab now let's look at a simple
example of an array in numpy as you can
see here there are multiple array
manipulation routines like their basic
examples where you can copy the values
from one array to another we can give a
new shape to an array from maybe one
dimensional do we can make it as a two
dimensional array we can return a copy
of the array collapse into one dimension
now let's look at an example where this
is a Jupiter notebook and we will just
create a basic array and for detailed
explanation you can watch our other
videos which Targets on these
explanations of each libraries so first
of all whenever we are using any library
in Python we have to import it so now
this NP is the Alias which we will be
using let's create a simple array
let's look what is the type of this
array
so this is an indiary type of array Also
let's look what's the shape of this
array
so this is a shape of the array now here
we saw that we can expand the shape of
the array
so this is where you can change the
shape of the array using all those
functions now let's create an array
using arrange functions if I give
arrange 12 it will give me a one day
array of 12 numbers like this now we can
reshape this array
3 comma 4 or we can write it here itself
so this is how arrange function and the
reshape function works for numpy now
let's discuss the next Library which is
PSI Pi so this is another free and open
source python Library extensively used
in data science for high level
computations so this Library as the name
suggests stands for Scientific Python
and it has around 19 000 commits on
GitHub with an active community of 600
contributors it is extensively used for
scientific and Technical computations
also as it extends numpy it provides
many user-friendly and efficient
routines for scientific calculations now
let's discuss about some features of
sci-fi so scipy has a collection of
algorithms and functions which is built
on the numpy extension of python
secondly it has various high level
commands for data manipulation and
visualization also the ndmh function of
scipy is very useful in
multi-dimensional image processing and
it includes built-in functions for
solving differential equations linear
algebra and many more so that was about
the features of scipy now let's discuss
its applications so cyber is used in
multi-dimensional image operations it
has functions to read images from disk
into numpy arrays to write arrays to
discuss images resize images
Etc solving differential equations
Fourier transforms then optimization
algorithms linear algebra Etc let's look
at a simple example to learn what kind
of functions are there in sci-fi here
I'm importing the constants package of
scipy Library so in this package it has
all the constants
so here I'm just mentioning C or H or
any and this Library already knows what
it has to fetch like speed of light
Planck's constant Etc so this can be
used in further calculations data
analysis is an integral part of data
science data scientists spend most of
the day in data munching and then
cleaning the data also hence mention of
pandas is a must in data science
lifecycle yes pandas is the most popular
and widely used python library for data
science along with numpy and matplotlib
the name itself stands for python data
analysis with around 17 000 comets on
GitHub and an active community of 1200
contributors it is heavily used for data
analysis in cleaning as it provides fast
flexible data structures like data
frames CVS which are designed to work
with structured data very easily and
intuitively now let's talk about some
features of pandas so pandas offers this
eloquent syntax and Rich functionalities
that there are various methods in pandas
like Drop n a fill any which gives you
the freedom to deal with missing data
also Partners provides a powerful apply
function which lets you create your own
function and run it across a series of
data now forget about writing those four
Loops while using pandas also this
library's high level abstraction over
low level numpy which is written in pure
C then it also contains these high level
data structures and manipulation tools
which makes it very easy to work with
pandas like their data structures and
series now let's discuss the
applications of pandas so pandas is
extensively used in general data
wrangling in data cleaning then pandas
also Finds Its usage in ETL jobs for
data transformation and data storage as
it has excellent support for loading CSV
files into its data frame format then
pandas is used in a variety of academic
and Commercial domains including
statistics Finance Neuroscience
economics web analytics Etc then pandas
is also very useful in Time series
specific functionality like date range
generation moving window linear
regression date 15 Etc now let's look at
a very simple example of how to create a
data frame so data frame is a very
useful data structure in pandas and it
has very powerful functionalities so
here I'm only enlisting important
libraries in data science you can
explore more of our videos to learn
about these libraries in detail so let's
just go ahead and create a data frame
I'm using Jupiter notebook again and in
this before using pandas here I am
importing the pandas Library
let me go and run this so in data frame
we can import a file a CSV file Excel
files there are many functions doing
these things and we can also create our
own data and put it into Data frame so
here I am taking random data and putting
in a data frame also I'm creating an
index and then also giving the column
names so PD is the Alias we've given for
pandas random data of 6x4 index which is
taking a range six numbers and column
name I'm giving as ABCD now let's go
ahead and look at it
so here it has created a data frame with
my column names ABCD my list has six
numbers 0 to 5 and a random data of six
by four so data frame is just another
table with rows and columns where you
can do various functions over it also I
can go ahead and describe this data
frame to see so it's giving me all these
functionalities where count and mean and
standard deviation
Etc okay so that was about pandas now
let's talk about next library and the
last one so matplot live for me is the
most fun Library out of all of them why
because it has such powerful yet
beautiful visualizations we'll see in
the coming slides plot and mac.lib
suggest that it's a plotting library for
python it has around 26 000 comments on
GitHub and a very Vibrant Community of
700 contributors and because of such
graphs and plots that it produces it's
majorly used for data visualization and
also because it provides an object
oriented API which can be used to embed
those plots into our applications let's
talk about the features of matplotlib
the pi plot module of matplotlab
provides Matlab like interface so
matplotlib is designed to be as usable
as Matlab with an advantage of being
free and open source also it supports
dozens of backends and output types
which means you can use it regardless of
which operating system you are using or
which output format you wish pandas
itself can be used as wrappers around
matplotlips API so as to drive matrodly
via cleaner and more modern apis also
when you start using this Library you
will realize that it has a very little
memory consumption and a very good
runtime Behavior now let's talk about
the applications of matplotlib it's
important to discover the unknown
relationship between the variables in
your data set so this Library helps to
visualize the correlation analysis of
variables also in machine learning we
can visualize 95 confidence interval of
the model just to communicate how well
our model fits the data then matpatliff
Finds Its application in outlier
detection using scatter plot Etc and to
visualize the distribution of data to
gain instant insights now let's make a
very simple plot to get a basic idea
I've already imported the libraries here
so this function matplotlib inline will
help you show the plots in the Jupiter
notebook this is also called a magic
function I won't be able to display my
plots in the jupyter notebook if I don't
use this function I am using this
function in numpy to fix random state
for reproducibility now I'll take my n
as 30 and will assign random values to
my variables so this function is
generating 30 random numbers here I am
trying to create a scatter plot so I
want to decide the area let's
put this so just multiplying 30 with
random numbers to the power 2 so that we
get the area of the plot which we will
see in just a minute so using the
scatter function and the Alias of
matplotlip as PLT I've created this if I
don't use this in a very small circles
as my scatter plot it's colorful it's
nice so that's one very easy plot I
suggest that you Explore More of
matplotlib and I'm sure you will enjoy
it let's create a histogram so I'm using
my the style is GG plot and assigning
some values to these variables any
random values
now we are assigning bars and colors and
Alignment to the plot and here we get
the graph so we can create different
type of visualizations and plots and
then work upon them using matplotlib and
it's just that simple so that was about
the leading python libraries in the
field of data science but along with
these libraries data scientists are also
leveraging the power of some other
useful libraries for example like
tensorflow Keras is another popular
Library which is extensively used for
deep learning and neural network modules
Keras wraps both tensorflow and theano
back-ends so it is a good option if you
don't want to dive into details of
tensorflow then scikit-learn is a
machine learning library it provides
almost all the machine learning
algorithms that you need and it is
designed to interpolate with numpy and
sci-fi then we have c bond which is
another library for data visualization
we can say that c born is an enhancement
of matplotlib as it introduces
additional plot types R is a programming
language for statistics Optical
Computing and Graphics it was developed
at at T Bell Laboratories by Robert
gentleman and Ross ihaka it's a GNU
project and is similar to the S language
it can be considered as another
implementation of s however there are a
few important differences but much of
the code written for S can be run
unchanged in r R offers various
statistical and graphical techniques
which make it extensible
R is a free and open source language the
present R is the result of contributions
and collaborations from highly active
community members from all over the
world R is capable of compiling and
running code on all platforms such as
Linux Mac and windows due to its
underlying philosophy and design R is
useful for statistical computation and
graphic visualization
R is a lot more than just a programming
language it has a worldwide repository
system called comprehensive R archive
Network or cran which can be accessed
through the given link cran is a network
of web servers and ftps spread across
the world this network stores similar
codes and updated documentation of r as
per the data of 2011 there were more
than 3 000 packages hosted on cran while
there were many more on other websites
as well at present the repository
includes about
7429 packages
do you know there are some limitations
of R2 one of the limitations is that it
has a steep learning curve it takes some
time for a learner to understand the
actual power of R however it's much
better than other statistical languages
for a novice it may be difficult to be
understood although there are a few easy
to use guis available but they do not
offer the Polish of the commercial
offerings it's documentation is
sometimes very concise which makes it
less understandable for a
non-statistician however a few high
standard books are trying to plug in the
documentation gaps in addition R has
memory or Ram limitations when working
with large data sets its commands offer
less attention to memory management so
they may consume all the available
memory quickly therefore when doing data
mining it can be a major restriction as
a solution one may choose to use 64-bit
operating systems over 32-bit ones
here is a list of various reputed
companies that use R for various
purposes
Microsoft uses it in video gaming for
purposes like Xbox matchmaking and
scientific Revenue using R Developers
can perform functions like identifying
trouble spots and game levels
the New York Times uses R for purposes
like data visualization data journalism
and interactive features like election
forecast and dialect quiz
Bing uses this language for social
search awareness
the New Scientist magazine is another
company that uses R4 data visualization
data journalism and the analysis of news
articles
Ford uses R for data-driven decision
making
Facebook also uses it for purposes like
experimental analysis exploratory data
analysis Human Resources Big Data
visualization and the analysis of user
Behavior with respect to status updates
and profile pictures
the famous recommendation engine of
Foursquare is backed up by r
Uber uses R for the purpose of
statistical analysis in addition Google
uses it for economic forecasting
advertising Effectiveness and Big Data
statistical modeling
do you want to know more about r one of
the main advantages of R is that users
can write their custom functions and own
programs with ease the syntaxes are so
easy to learn that even users with no
programming experience can use them
R can be used as a powerful environment
to do complex analyzes on almost each
data type
here are a few more pointers that will
help you understand r
our statements or commands can be
separated by a semicolon or a new line
the assignment operator used in R is as
given although the equal to operator
also works
all characters after hash are treated as
comments
in addition there are no multi-line or
Block Level comments
the dollar operator is analogous to a
Don operator in other languages
the use of the operator is shown on the
screen
in this lesson you'll learn about
installing r on operating systems
including Windows Mac OS X and Linux
let's first learn to install r on
Windows from cran website for this you
can download the package from the given
website as the first step you would need
to download the given Windows executable
from this link
once the package is downloaded open the
executable and follow the instructions
if you are installing r on a 64-bit
Windows the options will include 32 or
64-bit versions of R and the default is
to install both you can choose to leave
the settings as default and click next
to proceed
once you install R successfully you will
see its icon on the desktop you can
start the r console like any other
program
this demo will show the steps to install
r on a Windows PC
in this demo you'll learn how to install
r on a Windows PC
download the latest r package from the
given URL
double-click the executable file to
begin the installation
the r installation wizard will open
proceed with the installation by
clicking the next button
select your preferred installation
directory otherwise leave the settings
as default
click the Finish button to finish the
installation if everything goes well you
will see the r icon on the desktop
rstudio is an IDE or integrated
development environment for r
it's a project that provides most of the
desired features for an IDE in an
Innovative way which makes it easier and
more productive to use R it includes a
console and tools for history plotting
workspace and debugging management it
also includes an editor that highlights
the syntaxes which supports execution of
the code directly some other notable
ide's are listed in the table shown on
the screen along with the supported
platforms
our studio is available in various
commercial and open source editions it
can be run on a desktop or through a web
browser connected to our studio server
or rstudio server Pro
the desktop version is available for
Windows Mac OS X and Linux platforms
to install our studio on a Windows or
Mac OS X system you would need to Simply
download its self-installing binary for
this you would need to go to the rstudio
official website given on the screen and
download the appropriate version for
your system next you would need to
double-click the downloaded executable
file and start the installation process
however for Linux the installation
process varies for dbn distribution
including Ubuntu you can install the
rstudio using its regular package
management tools
this demo will show the steps to install
our studio on a Windows PC
in this demo you'll learn how to install
rstudio on a Windows PC
download the latest Windows rstudio
package from the given URL double-click
the executable file to begin the
installation
the rstudio installation wizard will
open proceed with the installation by
clicking the next button
select your preferred installation
directory otherwise leave the settings
as default
click the Finish button to finish the
installation
when R starts it undergoes the process
steps as shown on the screen it starts
in the working directory which is also
called the workspace the dot R profile
files commands are executed next if they
are present at the end if present the
dot R data file is loaded
the r workspace is the current working
environment of our it includes
user-defined objects such as matrices
lists data frames functions and vectors
at the end of an R session you can save
a snapshot of the current workspace the
workspace reloads automatically the next
time R starts in this you can enter
commands interactively at the r user
prompt you can also use up and down
arrow keys to browse through your
command history here are a few examples
of such commands
get WD is the command that prints the
current working directory and set WD is
the command that sets the working
directory
to set the workspace in our studio you
can click tools and then Global options
as shown on the screen
R has many functions that can be used to
perform sophisticated tasks at the core
and has above 1000 functions in addition
several new ones are created all the
time
every function of r has its own help
page to access the same you can type the
name of the function in the console
preceded by a question mark the table
given on the screen shows the commands
to get help for some common functions in
r
this demo will show the steps to access
the help document
in this demo you'll learn how to access
the help document
to Simply load the help document in a
browser type the given command
to access the help document for a
particular package type the given
commands
to access the search help document by
keyword type the given command
for more help options try using the name
of functions or package prefixed with a
question mark a few examples are given
on the screen
many data scientists programmers and
statisticians use R to design tools for
analyzing data and to contribute their
codes as pre-assembled collections of
functions and objects called packages
our packages are defined as the
collections of data our functions and
compiled data these packages are stored
in a directory the library all these
packages are hosted at the given URL
R includes a set of standard packages
while there are a few others that can be
downloaded and installed to use them you
would have to load them into a session
some of the available are packages and
their functions are listed in the given
table
note that not all packages are loaded by
default but they can be loaded or
installed on demand
you can install in our package by
clicking guirstudio tools and then
install packages as shown on the screen
this demo will show the steps to install
and load a package in r
in this demo you'll learn how to install
and load a package in r
to install a package in our type the
given command
to load a package in R type the given
command
to list the contents of a package type
the given command
to unload a package type the given
command
let's summarize the topics covered in
this lesson R is a programming language
developed as an alternative to the S
language R is available across all
platforms Windows Mac and Linux
R is most useful for statistical
computation and visualization
R has a steep learning curve and it's
working with large data sets is limited
by the Ram size
R can be downloaded from the cran
website
the rstudio program can run on a desktop
or through a web browser
our store's user-defined objects in the
workspace by allowing the user to take a
snapshot of the current workspace and by
automatically reloading it the next time
R starts
each R function comes with its own help
page
not all packages are loaded by default
but can be loaded or installed on demand
let's begin this lesson by defining the
term statistics statistics is a
mathematical science pertaining to the
collection presentation analysis and
interpretation of data it's widely used
to understand the complex problems of
the real world and simplify them to make
well-informed decisions
several statistical principles functions
and algorithms can be used to analyze
primary data build a statistical model
and predict the outcomes
an analysis of any situation can be done
in two ways statistical analysis or a
non-statistical analysis
statistical analysis is the science of
collecting exploring and presenting
large amounts of data to identify the
patterns and Trends statistical analysis
is also called quantitative analysis
non-statistical analysis provides
generic information and includes text
sound still images and moving images
non-statistical analysis is also called
qualitative analysis
although both forms of analysis provide
results statistical analysis gives more
insight and a clearer picture a feature
that makes it vital for businesses
there are two major categories of
Statistics descriptive statistics and
inferential statistics
descriptive statistics helps organize
data and focuses on the main
characteristics of the data it provides
a summary of the data numerically or
graphically numerical measures such as
average mode standard deviation or SD
and correlation are used to describe the
features of a data set
suppose you want to study the height of
students in a classroom in the
descriptive statistics you would record
the height of every person in the
classroom and then find out the maximum
height minimum height and average height
of the population
inferential statistics generalizes the
larger data set and applies probability
Theory to draw a conclusion it allows
you to infer population parameters based
on the sample statistics and to model
relationships within the data modeling
allows you to develop mathematical
equations which describe the inner
relationships between two or more
variables consider the same example of
calculating the height of students in
the classroom in inferential statistics
you would categorize height as tall
medium and small and then take only a
small sample from the population to
study the height of students in the
classroom
the field of Statistics touches our
lives in many ways from the daily
routines in our homes to the business of
making the greatest cities run the
effect of Statistics are everywhere
there are various statistical terms that
one should be aware of while dealing
with statistics
population sample variable quantitative
variable qualitative variable discrete
variable continuous variable
a population is the group from which
data is to be collected
a sample is a subset of a population
a variable is a feature that is
characteristic of any member of the
population differing in quality or
quantity from another member
a variable differing in quantity is
called a quantitative variable for
example the weight of a person number of
people in a car
a variable differing in quality is
called a qualitative variable or
attribute for example color the degree
of damage of a car in an accident
a discrete variable is one which no
value can be assumed between the two
given values for example the number of
children in a family
a continuous variable is one in which
any value can be assumed between the two
given values for example the time taken
for a 100 meter run
typically there are four types of
statistical measures used to describe
the data they are measures of frequency
measures of central tendency measures of
spread measures of position
let's learn each in detail
frequency of the data indicates the
number of times a particular data value
occurs in the given data set the
measures of frequency are number and
percentage
central tendency indicates whether the
data values tend to accumulate in the
middle of the distribution or toward the
end
the measures of central tendency are
mean median and mode
spread describes how similar or varied
the set of observed values are for a
particular variable
the measures of spread are standard
deviation variance and quartiles
the measure of spread are also called
measures of dispersion
position identifies the exact location
of a particular data value in the given
data set
the measures of position are percentiles
quartiles and standard scores
statistical analysis system or SAS
provides a list of procedures to perform
descriptive statistics they are as
follows
proc print proc contents proc means proc
frequency proc univariate
proc G chart
proc box plot
proc G plot
proc print it prints all the variables
in a SAS data set
proc contents it describes the structure
of a data set
proc means it provides data
summarization tools to compute
descriptive statistics for variables
across all observations and within the
groups of observations
proc frequency it produces one way to
in-way frequency and cross tabulation
tables frequencies can also be an output
of a SAS data set
proc univariate it goes beyond what proc
means does and is useful in conducting
some basic statistical analyzes and
includes high resolution graphical
features
proc G chart
the g-chart procedure produces six types
of charts block charts horizontal
vertical bar charts Pi donut charts and
star charts
these charts graphically represent the
value of a statistic calculated for one
or more variables in an input SAS data
set the tread variables can be either
numeric or character
proc box plot the box plot procedure
creates side-by-side box and whisker
plots of measurements organized in
groups a box and whisker plot displays
the mean quartiles and minimum and
maximum observations for a group
proc G plot G plot procedure creates
two-dimensional graphs including simple
Scatter Plots overlay plots in which
multiple sets of data points are
displayed on one set of axis plots
against the second vertical axis bubble
plots and logarithmic plots
in this demo you'll learn how to use
descriptive statistics to analyze the
mean from the electronic data set let's
import the electronic data set into the
SAS console
in the left plane right-click the
electronic.xlsx dataset and click import
data
the code to import the data generates
automatically copy the code and paste it
in the new window
the proc means procedure is used to
analyze the mean of the imported data
set
the keyword data identifies the input
data set in this demo the input data set
is electronic
the output obtained is shown on the
screen
note that the number of observations
mean standard deviation and maximum and
minimum values of the electronic data
set are obtained
this concludes the demo on how to use
descriptive statistics to analyze the
mean from the electronic data set so far
you've learned about descriptive
statistics let's now learn about
inferential statistics
hypothesis testing is an inferential
statistical technique to determine
whether there is enough evidence in a
data sample to infer that a certain
condition holds true for the entire
population
to understand the characteristics of the
general population we take a random
sample and analyze the properties of the
sample
we then test whether or not the
identified conclusions correctly
represent the population as a whole
the population of hypothesis testing is
to choose between two competing
hypotheses about the value of a
population parameter
for example one hypothesis might claim
that the wages of men and women are
equal while the other might claim that
women make more than men
hypothesis testing is formulated in
terms of two hypotheses
null hypothesis which is referred to as
H null
alternative hypothesis which is referred
to as H1
the null hypothesis is assumed to be
true unless there is strong evidence to
the contrary
the alternative hypothesis assumed to be
true when the null hypothesis is proven
false
let's understand the null hypothesis and
alternative hypothesis using a general
example
null hypothesis attempts to show that no
variation exists between variables and
alternative hypothesis is any hypothesis
other than the null
for example say a pharmaceutical company
has introduced a medicine in the market
for a particular disease and people have
been using it for a considerable period
of time and it's generally considered
safe
if the medicine is proved to be safe
then it is referred to as null
hypothesis
to reject null hypothesis we should
prove that the medicine is unsafe if the
null hypothesis is rejected then the
alternative hypothesis is used
before you perform any statistical tests
with variables it's significant to
recognize the nature of the variables
involved based on the nature of the
variables it's classified into four
types
they are categorical or nominal
variables ordinal variables interval
variables and ratio variables
nominal variables are ones which have
two or more categories and it's
impossible to order the values examples
of nominal variables include gender and
blood group
ordinal variables have values ordered
logically however the relative distance
between two data values is not clear
examples of ordinal variables include
considering the size of a coffee cup
large medium and small and considering
the ratings of a product bad good and
best
interval variables are similar to
ordinal variables except that the values
are measured in a way where their
differences are meaningful
with an interval scale equal differences
between scale values do have equal
quantitative meaning
for this reason an interval scale
provides more quantitative information
than the ordinal scale
the interval scale does not have a true
zero point a true zero point means that
a value of zero on the scale represents
zero quantity of the construct being
assessed examples of interval variables
include the Fahrenheit scale used to
measure temperature and the distance
between two compartments in a train
ratio scales are similar to interval
scales and that equal differences
between scale values have equal
quantitative meaning
however ratio scales also have a true
zero point which give them an additional
property for example the system of
inches used with a common ruler is an
example of a ratio scale
there is a true zero point because zero
inches does in fact indicate a complete
absence of Link
in this demo you'll learn how to perform
the hypothesis testing using SAS
in this example let's check against the
length of certain observations from a
random sample
the keyword data identifies the input
data set
the input statement is used to declare
the Aging variable and cards to read
data into SAS
let's perform a t-test to check the null
hypothesis
let's assume that the null hypothesis to
be that the mean days to deliver a
product is six days
so null hypothesis equals six Alpha
value is the probability of making an
error which is five percent standard and
hence Alpha equals 0.05
the variable statement names the
variable to be used in the analysis
the output is shown on the screen
note that the p-value is greater than
the alpha value which is 0.05 therefore
we fail to reject the null hypothesis
this concludes the demo on how to
perform the hypothesis testing using SAS
let's now learn about hypothesis testing
procedures there are two types of
hypothesis testing procedures they are
parametric tests and non-parametric
tests
in statistical inference or hypothesis
testing the traditional tests such as
t-test and Anova are called parametric
tests they depend on the specification
of a probability distribution except for
a set of free parameters
in simple words you can say that if the
population information is known
completely by its parameter then it is
called a parametric test
if the population or parameter
information is not known and you are
still required to test the hypothesis of
the population then it's called a
non-parametric test
non-parametric tests do not require any
strict distributional assumptions there
are various parametric tests they are as
follows t-test Anova
chi-squared linear regression let's
understand them in detail
t-test
a t-test determines if two sets of data
are significantly different from each
other
the t-test is used in the following
situations
to test if the mean is significantly
different than a hypothesized value
to test if the mean for two independent
groups is significantly different
to test if the mean for two dependent or
paired groups is significantly different
for example
let's say you have to find out which
region spends the highest amount of
money on shopping
it's impractical to ask everyone in the
different regions about their shopping
expenditure
in this case you can calculate the
highest shopping expenditure by
collecting sample observations from each
region
with the help of the t-test you can
check if the difference between the
regions are significant or a statistical
fluke
Anova
Anova is a generalized version of the
t-test and used when the mean of the
interval dependent variable is different
to the categorical independent variable
when we want to check variance between
two or more groups we apply the Anova
test
for example let's look at the same
example of the t-test example now you
want to check how much people in various
regions spend every month on shopping in
this case there are four groups namely
East West North and South with the help
of the Anova test you can check if the
difference between the regions is
significant or a statistical fluke
chi-square
chi-square is a statistical test used to
compare observed data with data you
would expect to obtain according to a
specific hypothesis
let's understand the chi-square test
through an example
you have a data set of mail Shoppers and
female shoppers
let's say you need to assess whether the
probability of females purchasing items
of 500 or more is significantly
different from the probability of males
purchasing items of 500 or more
linear regression
there are two types of linear regression
simple linear regression and multiple
linear regression
simple linear regression is used when
one wants to test how well a variable
predicts another variable
multiple linear regression allows one to
test how well multiple variables or
independent variables predict a variable
of interest
when using multiple linear regression We
additionally assume the predictor
variables are independent
for example finding relationship between
any two variables say sales and profit
is called Simple linear regression
finding relationship between any three
variables say sales cost telemarketing
is called multiple linear regression
some of the non-parametric tests are
wilcoxan rank sum test and kresco
Wallace h test
will coxin rank some test the wilcoxon
signed rank test is a non-parametric
statistical hypothesis test used to
compare two related samples or matched
samples to assess whether or not their
population mean ranks differ
in wilcoxon rank sum test you can test
the null hypothesis on the basis of the
ranks of the observations
kruskel Wallace h test
kresco Wallace h test is a rank-based
non-parametric test used to compare
independent samples of equal or
different sample sizes in this test you
can test the null hypothesis on the
basis of the ranks of the independent
samples
the advantages of parametric tests are
as follows
provide information about the population
in terms of parameters and competence
intervals
easier to use in modeling analyzing and
for describing data with Central
Tendencies and data transformations
Express the relationship between two or
more variables
don't need to convert data into rank
order to test
the disadvantages of parametric tests
are as follows
only support normally distributed data
only applicable on variables not
attributes
let's Now list the advantages and
disadvantages of non-parametric tests
the advantages of non-parametric tests
are as follows simple and easy to
understand
do not involve population parameters and
sampling Theory
make fewer assumptions
provide results similar to parametric
procedures
the disadvantages of non-parametric
tests are as follows
not as efficient as parametric tests
difficult to perform operations on large
samples manually also data scientists
looking for online training and
certification programs from the best
universities or a professional who
elects to switch careers with data
science then try giving simply learns
postgraduate program in data science as
short the link in the description box
below should navigate to the home page
where you can find the complete overview
of the program being offered now over to
our training experts so now let's dive
into the definition of the probability
distribution function
what is probability distribution
function a function which defines the
relationship between a random variable
and its probability such that you can
find the probability of the variable
using the function is called a
probability density function
in simple words probability density is
the relationship between an observation
and the probability
some outcomes of a random variable will
have low probability density and other
outcomes will have a very high
probability density basically the
probability of a variable X happening or
occurring will vary and it can sometimes
take on a slower value or it can take on
a way higher value
the overall shape of the probability
density is referred to as probability
distribution and the calculation of
probabilities for specific outcomes of a
random variable is performed by a
probability density function a PDF for
short
now consider a variable x with a PDF of
f of x
this is what your probability density
function will look like there might be a
point where the probability of X
occurring is very high hence your
probability distribution function or f
of x will also be very high at other
points the distribution or the
probability of X happening or occurring
is going to be very low hence your f of
x is also going to have a very small
value
basically given the random sample of a
variable we might want to know things
like the shape of the probability
distribution this here is something
called a normal distribution where a
probability distribution function takes
on a bell shape
however this is not
the probability density function that
might always occur there are different
probability distribution functions and
all of the graphs look very different
from each other
knowing the probability distribution for
a random variable can help you calculate
movements of the distribution like the
mean and variance but it can also be
useful for other more General
considerations like determining whether
an observation is unlikely or very
unlikely and might be an outlier or an
anomaly like consider this graph itself
in this graph
these points over here which have very
less probability distribution
are outliers which means that the chance
of them occurring is very low and
basically this is not something that
you're gonna see in your regular
scenario for your variable X now let's
consider two points A and B which are
values that are variable X can take
P of A and P of P just represent the
probability of a and the probability of
B which can be found out by drawing a
straight line and coinciding it with our
graphs
the area under the graph over here which
is going to give you your probability of
this region occurring can be written as
probability of a less than equal to X
which is a probability that we're
searching for here less than equal to
probability of B what does this mean
exactly this means that this area is
always going to be greater than or equal
to the probability of a but less than or
equal to the probability of B this gives
us the narrow region
of the probability which is present over
here and doing this we can find the
probability of occurrence for any value
of x
suppose you want to find the probability
of B happening for a probability
distribution function
the probability of B happening is not
simply this point here but the entire
area of the graph which is taking place
before this point itself so if you want
to find the probability between these
regions you're gonna have to find the
entire area and not simply the
probability at one point
now so far we've been talking about
different types of variables which is
discrete random variables and continuous
variables what exactly do these mean
a variable which can only take a value
within a certain range is called a
discrete random variable the value is
usually within a certain distance of
another finite value
an example of this would be the sum of
two dices basically values which are
well defined are called discrete values
or and a variable which has well-defined
values will be called a discrete random
variable
this variable can only take values which
fall within a certain set of values
let's say you roll a dice the dice can
only give you specific outcomes which
range from 1 to 6. this is what you
would call a discrete output
on the other hand a continuous random
variable can take on infinite different
values within a range of values for
example the height of a student the
height of a student is not fixed even if
the height is 1.7 meters in reality the
height can be 1.77 or 1.765 or 1.789
the exact height is very hard to
determine because it's not easy for us
to find the precise value of the height
of a student
so basically the height can take on an
infinite different range of values we
are trying to define the values that a
continuous random variable can take we
usually say it in the form of a range of
values which means that the value can
fall in that range and can take on any
value in that range it's not like a
discrete random variable where you can
Define definitive values
now let's understand a probability
density function with the help of a
graph consider the graph below which
shows the rainfall distribution in any
inner city the x-axis has the rainfall
in inches or the amount of rain that
we're getting and the y-axis has the
probability density function of getting
that amount of rain
the probability of some amount of
rainfall is obtained by finding the area
of the curve to the left of it
so let's say we have a 0.3
if we want to find the probability of 3
inches of rainfall occurring we would
have to find the area of the curve which
falls to the left of three when we draw
a line from 3 which intercepts the graph
and further extend it onto the y-axis we
get a value of 0.5
simply put this means that the
probability of 3 inches of rainfall
occurring is going to be lesser than or
equal to 0.5 the exact probability can
be found out by finding the area of the
Curve
which falls to the left of 3.
how do we find the probability
distribution function
the first step is to summarize your
density with the help of a histogram the
first step in a density estimation is to
create a histogram of the observations
in the random sample
now what is a histogram a histogram is a
plot which involves first grouping the
observation into bins and Counting the
number of events that fall in each bin
the counts of frequency of observation
in each bins are then plotted as a bar
graph with the bins on the x-axis and
the frequency on the y-axis
the choice of the number of bins is
important as it controls the coarseness
of the distribution and in turn how well
the density of the observation is
plotted it is a good idea to experiment
with different bin sizes for a given
data sample to get multiple perspectives
of views on the same data
at the same time the number of bins is
important as it determines how many bars
the histogram will have and their widths
this will change not only the shape of
the graph but also how the graph is read
this will also determine how a density
is plotted now let's see how we can
summarize our density with histograms
using python first let's import all of
our necessary modules which we're gonna
require we're gonna require matplotlib
to plot graphs we're going to need
the normal random function so that we
can get a normal distribution we're
going to import mean and standard
deviation from numpy to use on our
graphs we're also going to normalize our
data so we're going to import the norm
function from PSI pi
we finished importing all of our
necessary modules now let's generate a
sample
which has a size of thousand and it's
going to be a normal distribution and
we're going to also plot this with the
help of a histogram in bins of 10.
so as you can see here you get a normal
distribution which is nothing but a
almost bell shaped curve and we have 10
bins here which are centered at 0 and
which extend from minus 3 to 3.
how will our graph look if we change the
number of bins though
let's run it and see so you still have a
normal distribution but it's not as well
defined because of how less the number
of bins are you lose a majority of the
data which will contribute to your
normal distribution it doesn't look like
a proper nominal distribution but looks
more like discrete data at this point
now let's take a look at the next step
of finding a probability distribution
function
the next step is called parametric
density estimation
what exactly is parametric density
estimation
the probability density function is of
many types the shape of your histogram
will help you determine what type of a
function it is we can also calculate the
parameters associated with the function
to get our density
now different probability distribution
functions will have
different graphs which will have
different shapes and which will also
have different parameters like mean
standard deviation Etc associated with
them
using these parameters we can find
important points of our data
hence it's very important for us to
recognize what type of a distribution it
is common distributions will occur again
and again in different and sometimes
unexpected domains
getting familiar with common probability
distributions will help you identify a
distribution from a histogram and once
identified you can attempt to estimate
the density of the random variable with
the chosen probability distribution this
can be achieved by estimating the
parameters of the distribution from a
random sample of data
now an example of this would be a normal
distribution which has two main
parameters the mean and standard
deviation given these two parameters we
will now know the probability
distribution function these parameters
can be estimated from data by
calculating the sample mean and Sample
standard deviation
this entire process is known as
parametric density estimation and it
includes identifying your probability
distribution function and getting the
parameters which are associated with it
now once you have estimated the density
we can check if it's a good fit
this can be done in three different ways
one is plotting the density of the
function and comparing the shape to the
histogram the next is sampling the
density function and comparing the
generated sample to the real sample and
the last one is using a statistical test
to confirm if the data fits the
distribution now over here as you can
see all we've done is taken our data and
plotted the density function
on top of a histogram and we've compared
this shape so the distribution so the
density function that we're actually
considering here is a normal
distribution and from this graph we can
see that it's almost an exact fit to a
histogram
now let's see how we can perform
parametric density estimation using
python to begin with
let's generate a random sample of
thousand observations from a normal
distribution with a mean of 50 which is
determined by the loc parameter and the
standard deviation of 5 which is
determined by the scale parameter
now just to show you what the
distribution looks like we're going to
plot it in the form of a histogram
so this is what the histogram looks like
but this is just to give you a basic
idea of our data and what it looks like
once plotted but let's assume that we
don't know
the probability distribution and and we
don't know what it looks like as a
histogram and we don't know that's that
it's normal
so now if we just assume that it's
normal we can calculate the parameters
of the distribution specifically the
mean and the standard deviation
we would not expect the mean and
standard deviation to be 50 and 5
exactly given the small sample size and
the noise in the sampling data
so because of this noise and the small
sample size we have a mean of almost 50
and a standard deviation of a little
more than five
now let's define the distribution as
normal so now using this we've defined a
normal distribution we've used the norm
method of the Sci-Fi Library
and we're doing this
with the mean and the standard deviation
that we've obtained from our samples
so up until now we're just assuming that
it's a normal distribution and because
of that the parameters that we've
calculated is the mean and standard
deviation
and using the calculated mean and
standard deviation we've gotten a normal
distribution
and up until now again keep in mind we
do not know for certain that it is a
normal distribution so far all we have
is this data
so the next thing that we're gonna do is
fit the distribution with these
parameters
and then sample the probabilities for a
distribution for a range of values in
our domain which in this case is 30 and
70. so all we're doing is we're
calculating probabilities for a range of
outcomes and in this case we've taken 30
and 70 as our domain
so these are the probability
distribution values for the normal
distribution that we've defined over
here and this is gonna this is basically
gonna give you the outline of your
normal distribution
these are the points at which your
normal distribution will be plotted uh
now what we're basically going to do is
we're gonna plot our histograms using
the samples that we've already generated
along with the values and probabilities
of the normal function that we defined
over here
so as you can see it's an all it's
almost a complete fit the normal
distribution that we have here is made
using the mean and the standard
deviation of our actual samples
the reason we took mean and standard
deviation was because we assumed it was
a normal distribution
and the parameters associated with the
normal distribution our mean and
standard deviation
using the mean and standard deviation we
got the normal distribution we
calculated
probabilities for this normal
distribution using a random domain of 30
and 70.
and we plotted the probabilities and the
values on top of our histogram to see if
the normal distribution was a fit to a
histogram
if it was not a fit you would have to go
and do the same procedure with other
common probability density functions
until you found a function which was a
proper fit to your histogram now let's
move on to the final step which is used
in the calculation of a PDF
this final step is called non-parametric
density estimation and it's only used
when the shape of a histogram doesn't
match a common probability density
function or it cannot be made to fit one
in this case we will calculate the
density using all samples in our data
using certain algorithms
this is only done when a data sample
does not resemble a common probability
distribution or it cannot be easily made
to fit the distribution and this is
often the case when the data has two
peaks this is also called a bimodal
distribution or it has many Peaks which
is also called a multimodal distribution
in this case the parametric density
estimation will not be feasible and
alternative methods can be used that do
not use a common distribution instead
you will use an algorithm which is used
to approximate the probability
distribution of the data without a
predefined distribution which is also
referred to as a non-parametric method
because we are not using any predefined
parameters
the distribution will still have
parameters but these are not
controllable in the same way as a simple
probability distribution
for example a non-parametric method
might estimate the density using all
observations in a random sample in
effect making all observations in the
sample parameters
now consider this graph which has two
peaks you this is not a normal
distribution or any other sort of
distribution that we are familiar with
so for this we're not going to use a
parametric estimation method but which
is going to calculate the parameters for
every single sample point in this
perhaps the most common non-parametric
approach for estimating the probability
density function of a continuous random
variable is called kernel smoothing or
kernel density estimation or KDE for
short
kernel density estimation is a
non-parametric method for using a data
set to estimate probabilities for new
points
it uses a mathematical function and
smoothing probabilities so this so the
sum of the resultant probabilities is
always one now in this case a kernel is
a mathematical function that returns a
probability for a given value of a
random variable the kernel effectively
Smooths or interpolates the
probabilities across a range of outcomes
for a random variable such that the sum
of probabilities always equals one a
requirement of well-behaved
probabilities you also have a parameter
called a smoothing parameter which
controls the scope or the window of
observations from the data samples that
contributes to estimating the
probability for a given sample as such
the kernel density estimation is such is
sometimes referred to as your powers in
rosenbault window
now at the end you also have a basis
function which is a function which is
chosen to control the contribution of
samples in the data set towards
estimating the probability of a new
point this is only done to make sure
that you are not learning from a lot of
noise and that you are not using a lot
of the outliers again let's see how we
can perform non-parametric density
estimation with the help of python
so first we'll start by importing all
the necessary modules along with the
kernel density estimation which can be
imported from SK loan
now let's create a bimodal distribution
by combining two different samples
sample one and Sample two sample one has
300 examples with a mean of 20 and a
standard deviation of 5 while sample 2
has 700 examples
with a mean of 40 and a standard
deviation of 5.
we're then going to use Edge stack to
come to merge both of them together to
get a final sample
the means that we've chosen which is 20
and 40 are chosen close together to
ensure that the distributions overlap in
the combined sample
so this is what our distribution is
let's just plot it so you get a basic
idea of what a graph looks like
so this is what a graph looks like
now we already know that none of the
various different uh probability
distribution functions fit these graphs
so now we're going to perform
non-parametric estimations
to perform non-parametric estimations
we're going to use the scikit-learn
machine learning library which provides
the kernel density class that implements
kernel density as sorry that implements
kernel density estimation
first the class is constructed with the
desired bandwidth or window size of two
and your basis function
which in this case is a gaussian
function
it's a good idea to at least test
different configurations to your data
and in this case we're only going to try
a bandwidth of Two and a gaussian kernel
uh but usually there are multiple
different kernels that you can uh you
know like that you can play around with
and you can also tweak your bandwidth to
exactly fit the distribution that you
have
now let's run this
uh so now we've got in our kernel
density estimation
uh now we can evaluate how well the
density estimates matches our data by
calculating probabilities
for a range of observations and
comparing shapes to the histogram just
like we did for the parametric case
before
so again we're gonna just calculate
different probabilities using the kernel
density function
and we're just going to plot it on top
of a histogram to see how well this the
kernel density function is estimating
for our data
so these are the probabilities that
we've gotten finally with the con with
the kernel density estimation and now
we're going to plot it on top of our
histograms
so as you can see it's almost a complete
fit it's just left out some of these
outlier values which again are ranging
very high but overall we have a pretty
good fit
uh the only problem is it's not very
smooth and you can again
try tweaking the bandwidth to different
values uh so let's just in this case try
tweaking it to three and see how well it
runs okay so now we've got a new
probabilities and let's run it on top of
our bandwidth
so again we using a bandwidth of three
you can see that we're fitting our data
even better and we're again
ignoring a lot of the outliers which are
out there so this is going to give us a
better estimation
so what is the population
in statistics population is the entire
set of items from which data is drawn
for a statistical study it can be a
group of individuals a set of items Etc
it constitutes the data pool for a study
in general a population is the entire
group that you want to draw conclusions
about usually when we talk about
population you think about the people
living in an area at a time but in
research a population doesn't always
refer to people it can mean a group
containing elements of anything you want
to study such as objects events
organizations countries species
organisms
Etc
for example all undergraduate students
in the Netherlands or say you would like
to know whether there's an association
between job performance and the amount
of home working hours per week in the
specific case of Belgian data scientists
in this case the population may be
Belgian data scientists however if the
scope of the study is more narrow then
the population will be more specific and
include only workers who meet that
certain criteria the point is that the
population should only include people to
whom the results will apply
since in this case and many others it is
impossible to observe the entire
statistical population due to time
constraints constraints on geographical
accessibility and constraints on the
researchers resources a researcher would
instead absorb the statistical sample
from the population in order to attempt
to learn something about the population
as a whole
this brings us to the topic what is a
sample
a sample represents the group of
interests from the population which we
will use to represent our data the
sample is a unbiased set of the
population which best represents the
whole data a sample consists of some
observations drawn from the population
so a part or a subset of the population
the sample is a group of elements who
actually participated in the study the
size of the sample is always less than
the total size of the population
typically the population is very large
making a census or a complete and
numeration of all values in the
population in Practical or impossible
the sample represents a subset of
manageable size samples are collected
and statistics are calculated from the
sample so that one can make
interferences or extrapolations from the
sample to the population this process of
collecting information from a sample is
referred to as sampling
say you are testing the effects of a new
fertilizer on crop yield all the crop
Fields represent your population whereas
the 10 crop Fields you tested correspond
to your sample
now let's look at the differences
between population and samples with the
help of some examples
the first example would be that all
students in a class would constitute the
population of that class
a sample would be only the top 10
students within the same class the
sample is a small subset of the
population in this case
advertisements for IT jobs in India
would mean the complete search results
that pops up when you search IT jobs in
India
a sample of that would be the top 50
advertisements for the same jobs
another example is all countries of the
world this represents a population of
all the countries
a sample would be countries with
published data on birth rates and GDP
since 2000s
next let's look at how to collect data
from a population
data from a population is collected when
your research question needs a large
amount of data or information about
every member of the population is
available
populations are used when your research
question requires or when you have
access to data from every member of the
population usually it's only
straightforward to collect data from a
whole population when it is small
accessible and cooperative
population data is used when the data
pool is small and Cooperative to giving
all of the required data for larger
populations we can use sampling to
represent part of the population it is
hard to collect data from
for example a high school administrator
wants to analyze the final exam scores
of all graduating seniors to see if
there is a trend
since there are only interested in
applying the findings to the graduating
seniors in this high school they can use
the whole population data set
for larger and more dispersed
populations it is often difficult or
impossible to collect data from every
individual for example every 10 years
the federal U.S government aims to count
every person living in the country using
the U.S census this data is used to
distribute funding Across the Nation
however historically marginalized and
low-income groups have been difficult to
contact locate and encourage
participation from because of
non-responses the population count is
incomplete and biased towards some
groups which results in disproportionate
fundings across the country
in cases like this sampling can be used
to make more precise interferences about
the population
and for that we can collect data from a
sample
how do we go about collecting data from
a sample
samples are used when the population is
large in size scattered or it's
generally hard to collect data on
individual instances within it we can
use a small sample of the population to
make overall hypotheses
when your population is too large in
size geographically dispersed or
difficult to contact it is necessary to
use a sample with statistical analysis
you can use sample data to make
estimates or test hypotheses about
population data
for example you want to study political
attitudes in young people your
population is a 3000 undergrads in the
Netherlands because this is not
practical to collect data from all of
them you use a sample of 300
undergraduate volunteers from three
Dutch universities this is the group who
will complete your online study
ideally a sample should be randomly
selected and representative of the
population using probability sampling
methods such as simple random sampling
or stratified sampling reduces the risk
of sampling bias and enhances both
internal and external validity
we'll discuss the three main types of
probability distribution that is normal
binomial and poisson distribution
so let's move ahead
so what is normal distribution
normal distribution is a continuous
probability density that has a
probability density function which gives
us a symmetrical bell curve
now data can be distributed or spread
out in different ways but there are many
cases where the data tends to be around
a central value with no bias to the left
or right which means that it doesn't
show any particular spikes towards the
left or the right and it gets close to a
normal distribution
half of the data will fall on the left
of the mean and the other half will fall
on the right
now let's take a look at a graph which
shows the height distribution in a glass
as you can see the average height is in
the middle and the data to the left of
the average height represents the short
people and the data to the right of it
represents the taller people
the y-axis shows us the likelihood of
any of these Heights occurring the
average height has the most distribution
or it has the most number of cases in
the class
and as the height decreases or increases
the number of people who have that
height also decreases
this kind of a distribution is called a
normal distribution where the average or
the mean is always the highest point and
any other point after that or before
that is significantly lower
the resulting data gives us a bell curve
and as we can see there is no abrupt
bias or spike in the data anywhere
except for the average height
so this kind of a curve is called a bell
curve and it's usually seen in a normal
distribution
the reason we call this a normal
distribution is because the data is
normally distributed with the average
being the highest and all the other data
points having a lower likelihood
now we came across two terms which are
associated with normal distribution
continuous probability density
and probability density function
what is continuous probability density
continuous probability density is a
probability distribution where the
random variable X can take any given
value because there are infinite values
that X could assume the probability of X
taking on any specific value is zero for
example let's say you have a continuous
probability density for men's height
what is the probability that a man will
have the exact height of 70 inches
it is impossible to find this out
because the probability of one man
measuring exactly 70 inches is very low
it is more probable that he will measure
around 70.1 inches or maybe
69.97 inches and it doesn't stop there
the fact is that it's impossible to
exactly measure any variable that's on a
continuous scale and because of this
it's impossible to figure out the
probability of one exact measurement
which is occurring in a continuous
probability density
next we have the probability density
function it's nothing but a function or
an expression which is used to define
the range of values that a continuous
random variable can take an example of
this would be to Gorge the risk and
reward of a stock
a probability density function is a
statistical measure which is used to
Gorge the likelihood of a discrete value
a discrete variable can be measured
exactly while a continuous variable can
have infinite values
however for both continuous as well as
discrete variables we can define a
function
which gives us the range of values
within which
these variables will fall and that
function is known as the probability
density function
now let's take a look at standard
deviation
what is standard deviation
standard deviation is used to measure
how the values in your data differ from
one another or how spread out your data
is
a standard deviation is a statistic that
measures dispersion of a data set
relative to its mean the standard
deviation is calculated as a square root
of variance by determining each data
Point's deviation relative to the mean
if the data points are further from the
mean that means that there's a higher
deviation within the data set and then
the data is said to be more spread out
this leads to a higher standard
deviation too
let's take an example of income in rural
and urban areas in rural areas let's say
such as farming areas the income doesn't
differ that much more or less everyone
earns the same
because of this a bell curve has a very
low standard deviation and it has a very
narrow Peak
however in urban areas the well
distribution is very uneven some people
can have very high incomes and can be
earning a lot while other people can
have very low incomes the furthermore
the data distribution between these two
income points is going to be more spread
out because there are a lot more people
living there who work in various fields
and who have various incomes because of
this our standard deviation is more
spread out and a bell curve will also
have a wider Peak
now how can we find the standard
deviation standard deviation is obtained
by subtracting each data value from the
mean and finding the squared average of
these values let's look at how we can do
this with the help of an example these
values correspond to the height of
various dogs
we can find the mean by finding the
average of all these values which is
nothing but adding all the values and
dividing it by the total number of
values the mean that we get is 394. this
means that the average height of a dog
is 394 millimeters
to find the standard deviation first we
need to subtract the height from the
mean this will tell us how far from the
mean our data points actually are
next we will square up all of these
differences and add them up and again
divide it by the total number of values
that we have this is called the variance
the variance that we get in this case is
21704
finally when we find the square root of
this value we will get the standard
deviation the standard deviation here is
147. the standard deviation will tell us
how our data points differ from the
average
and it gives us the basic values
suggesting how spread out our data is
from the very middle or from the mean so
when we plot these values this value 147
will mean that a curve will have a width
of 147 points around the mean
now what is the standard normal
distribution
the standard normal distribution
is a type of normal distribution that
has a mean of 0 and a standard deviation
of 1. this means that the normal
distribution has its Center at 0 and it
has intervals which increase by 1. all
normal distributions like the standard
normal distribution are uni model and
symmetrically distributed with a
bell-shaped curve however a nominal
distribution can take on any value as
its mean and standard deviation in the
standard normal distribution however the
mean and standard deviation are always
fixed when you standardize a normal
distribution the mean becomes 0 and the
standard deviation becomes 1. this
allows you to easily calculate the
probability of certain values occurring
in your distribution
or to compare data sets with different
mean and standard deviations the curve
shows a standard normal distribution as
you can see again the data is centered
at 0.
this does not mean that the data
necessarily starts at zero this means
that after standardizing this point is
where a mean will lie in a standard
terminal distribution the standard
deviation is one so all the data points
will increase or decrease in steps of 1.
let's better understand the standard
normal distribution with the help of an
example
again as you can see the data is
centered around 0 which is nothing but
the mean
let's again consider the weights of
students in class 8th the average weight
here is around 50 kgs and the data
increases and decreases in steps of 5.
the data over here in this curve is
evenly distributed along these steps
this is what a standard nominal
distribution will look like
we already know that the mean of our
data is 50 and because the data is
increasing and decreasing in equal steps
we can just standardize it and take it
to mean that the data is increasing and
decreasing in steps of 1. this is what a
standard normal distribution looks looks
like and when you have a data which
looks like this you can always
standardize it and convert it into a
standard normal distribution now
standard normal distribution has a
couple of properties which makes
calculation comparatively easy
the first one is that 68 percent of the
values fall within the first standard
deviation which means that 68 of all
data values on this Curve will fall
between the range of -1 to 1 or the
first interval ranging from -1 to 1.
the second property is that 95 percent
of the rest of the values are within the
second standard deviation or from the
second negative point to the second
positive point
and finally
99.7 of the values fall within the third
standard deviation or from the third
negative point to the third positive
point this makes calculations and
standard normal distribution fairly easy
you can compare scores on different
distributions with different means and
standard deviations you can normalize
scores for statistical decision making
using standard normal distribution you
can find the probability of observations
in a distribution which fall above or
below a given value and find you can
find the probability that it means
significantly differs from a population
mean
now let's take a look at z-score
so what is a z-score a z-score is used
to tell us how far from the mean a data
point actually is
it is calculated using the mean and
standard deviation so it can be said
that the Z score is how many standard
deviations below the mean our data is
basically by using the Z score we can
get an approximate location of where our
data point lies on the graph with
regards to the mean
now the z-score is given by subtracting
the data point from the mean and
dividing it by standard deviation
this can also be written as x minus mu
divided by Sigma now any normal
distribution can be standardized by
converting its values into Z scores the
z-score will tell you how many standard
deviation from the mean each values lie
while data points are referred to as X
and normal distribution they are called
Z or Z scores in the Z distribution a
z-score is the standard score that will
tell you how many standard deviations
away from the mean an individual point
will lie a positive z-score will mean
that your x value is greater than the
mean and a negative Z score will mean
that your x value is less than the mean
a z-score of 0 will mean that your x
value is equal to the mean
and again to standardize a value from a
normal distribution all we have to do is
convert it to a z-score by subtracting
the mean from our individual value and
dividing it by the standard deviation
now let's see how we can find the
z-score from data points with the help
of a solved example
let's do a case study
in this case study we'll be taking the
summary of daily travel time of a person
who's commuting to and from work all
these values are in minutes and using
these values we have to calculate the
mean the standard deviation and the Z
score these values are as shown as we
can see there are 13 values in total
let's start by finding the mean the mean
is the average and can be gotten by
adding all of these values and dividing
it by the total number of values
this gives us a value of 38.6
the mean tells us the average of all our
data points which means on an average he
travels
for 38.6 minutes to reach work
next let's subtract the individual
values from our mean
and calculate the variance and standard
deviation
the values on the left give us the
values that we get after subtracting it
from the mean and the variance can be
calculated by squaring all of these
values adding up all of the squared
values and dividing it by the total
number of values
at the end of the day we get a variance
of 140.
to calculate the standard deviation all
we have to do is take a square root of
the variance which gives us a value of
11.8
now the means signifies the average of
our values and we already know this it
gives us the average time which is taken
to travel but the standard deviation
will tell us the average value of how
much our data points differ from the
mean it tells us the deviation within
our own data and it tells us
how far away on an average a point is
from the mean now the value that we get
is 11.8 which means that on an average a
single data point is around 11.8 data
points away from the mean
now let's calculate the Z score
the z-score is given by subtracting
individual data points from the mean and
dividing it by the standard deviation we
know that we have a standard deviation
of 11.8 and a mean of 38.6
using these values we can calculate the
Z scores for individual X values
now we know that a negative Z score
means that our x value is lower than our
mean but what does the number 1.06 mean
this means that the Z score for 26 is
1.06 standard deviations
away from the mean the negative symbol
here means that our x value is less than
the mean
and by how less
1.06 times the standard deviation now we
know that the negative value of a z
score means that
our x value is less than our mean but
what does the number 1.06 mean this
means that the z-score is 1.06 times the
standard deviation less than the mean
the same thing can be said for the
z-score of 33 it is 0.47 times the
standard deviation less
than the mean
the z-score of 65 is 2.23 times the
standard deviation more than the mean
that means it has to be added to the
mean the reason that we know it's more
than the mean is because this has a
positive value so this means that using
Z's course we can know where a data
points fall relative to other points on
the graph the search score will tell us
how far away from the mean a point is in
steps of a standard deviation
Basics and terminology
the first one is outcome
whenever we do an experiment like
flipping a coin or rolling a dice we get
an outcome for example if we flip a coin
we get an outcome of heads or tails and
if you roll a die we get an outcome of
one two three four five or six
random experiment
a random experiment is any well-defined
procedure that produces an observable
outcome that could not be perfectly
predicted in advance
a random experiment must be well defined
to eliminate any vagueness or surprise
it must produce a definite observable
outcome so that you know what happened
after the random experiment is run
random events
consider a simple example
let us say that we toss a coin up in the
air
what can happen when it gets back it
either gives a head or a tail
these two are known as outcome and the
occurrence of an outcome is an event
thus the event is the outcome of some
phenomenon
the last one is sample space
a sample space is a collection or a set
of possible outcomes of a random
experiment
the sample space is represented using
the symbol S
the subset of all possible outcomes of
an experiment is called events and a
sample space may contain a number of
outcomes that depends on the experiment
if it contains a finite number of
outcomes then it is known as a discrete
or finite sample spaces
now let's discuss what is random
variable
a random variable is a numerical
description of the outcome of a
statistical experiment
a random variable that may assume only a
finite number of values is said to be
discrete one that may assume any value
in some interval on the real number line
is said to be continuous
let's see an example
let X be a random variable defined as a
sum of numbers when two dices are ruled
X can assume the values 2 3 4 5 6 7 8 9
10 11 and 12. notice there is no one
here because the sum of the two dice can
never be one
now that we know the basics let's move
on to binomial distribution
the binomial distribution is used when
there are exactly two mutually exclusive
outcomes of a trial these outcomes are
appropriately labeled success and
failure
the binary distribution is used to
obtain the probability of observing X
successes in N number of trials with the
probability of success on a single trial
denoted by P
the bannable distribution assumes that P
is fixed for all the trials
here's a real life example of a binomial
distribution
suppose you purchase a lottery ticket
then either you are going to win the
lottery or not in other words the
outcome will be either success or
failure that can be proved through
binomial distribution
there are four important conditions that
needs to be fulfilled for an experiment
to be a binomial experiment
the first one is there should be a fixed
number of entras carried out
the outcome of a given trial is only two
that is either a success or a failure
the probability of success remains
constant from trial to trial it does not
changes from one trial to another
and the trials are independent the
outcome of a trial is not affected by
the outcome of any other trial
to calculate the binomial coefficient we
use the formula which is NCR into P to
the power R into 1 minus P to the power
n minus r where R is the number of
success in N number of Trials and P is
the probability of success one minus P
denotes the probability of a failure now
let's use this formula to solve an
example
suppose a dice tossed three times what
is the probability of No 5 turning up
one five and three fives turning up
to calculate the Note 5 turning up here
RS is equal to 0 and N is equal to 3.
substituting the value in the formula we
have 3 C 0 into 1 by 6 to the power 0
into 5 by 6 to the power 3 where 1 by 6
is the probability of success and 5 by 6
is the probability of failure
calculating this equation we'll get the
value to be 0.5787
in a similar manner to calculate the
probability of 1 5 turning up we'll
replace r with 1 and N will be 3 so p x
1 will be is equal to 3 c 1 into 1 by 6
to the power 1 into 5 by 6 to the power
2 which will come out to be 0.347
and for three five turning up we
substitute r equal to 3 and the formula
will remain the same and we'll get the
value to be 0.0046
now that we are done with the concepts
of animal probability distribution
here's a problem for you to solve
post your answers in the comment section
and let us know
a poison distribution is a probability
distribution used in statistics to show
how many times an event is likely to
happen over a given period of time
to put it another way it's a count
distribution
distribution are frequently used to
comprehend independent event at a
constant rate over a given interval of
time
the positive distribution was developed
by French mathematician Simon Dennis
poison in 1837.
a poison distribution is used in cases
where the chances of any individual
event being a success is very small
the number of defective pencils per box
of a 6000 pencil
the number of plane crash in India in
one year
all the number of printing mistakes in
each page of a book
all of these examples can have use of
person distribution
the poison distribution can be used to
calculate How likely it is that
something will happen X number of times
a random variable X has a poisson
distribution with parameter Lambda and
the formula for that is e to the power
minus Lambda into Lambda to the power x
divided by X factorial
where X can be the number of times the
event is happening
the value of e is taken as 2.7182
let's discuss some application of
poisson distribution
if you want to calculate the number of
deaths per day or week due to rare
disease in a hospital you can use the
poison distribution in a similar manner
the count of bacteria per CC in blood or
the number of computers infected as
virus per week
the number of mishandled baggage per
thousand passengers can also have an
application for poisson distribution
let's discuss one example to see how we
can calculate the poisson distribution
suppose on an average a cancer kills 5
people each year in India
what is the probability that a one
person is killed this year
we'll assume all these events are
independent random events
so by the formula
we have X is equal to 1 because we have
to calculate the probability of one
person that is killed this year
so p x equal to 1 will be equal to e to
the power minus 5 into 5 to the power 1
divided by 1 factorial which will come
out to be
0.033 which will be near to 3.3 percent
so the probability that only one person
is killed this year due to cancer is 3.3
percent
now we have one question for you do try
to answer in the comment section
suppose the trains arrive at a railway
station with an average arrival rate of
4 trains per hour
what is the probability exactly six
trains will arrive in a two hour period
and here are your options
please do let us know your answers in
the comment section
also data scientists looking for online
training and certification programs from
the best universities or a professional
who elects to switch careers with data
science then try giving simply learns
postgraduate program in data science as
short the link in the description box
below should navigate to the home page
where you can find the complete overview
of the program being offered now over to
our training experts let's start by
defining what is mean squared error
error measures the amount of error in a
statistical model
it calculates the average square
difference between the observed and
predicted values
in regression line the mean squid error
represents the average square residual
as you can see in the graph as the data
points fall closer to the regression
line the model has a less error
decreasing the mean squared error a
model with less error produces more
precise predictions
now let's jump to our Excel workbook and
discuss it with an example
here we have a small table with the
actual and predicted values
we'll start by finding the error
between the actual and predicted value
which will be the difference between the
actual minus predicted values so here 10
minus 7 will become 3 16 minus 14 will
become 2 13 minus 17 will become minus 4
19 minus 20 becomes minus 1
and 7 minus 4 becomes 3.
now just calculate the total of the
errors while calculating the total we'll
treat this negative value as a positive
value
so here the total of the errors will
become 3 plus 2 5 plus 4 9 1 10 and 13.
then we'll calculate the square of the
errors
so here 3 Square will become 9
will become 4 4 Square becomes 16.
1 square will become 1 and 3 Square
again is 9.
and we'll do the total of the square
errors so this will become 9 plus 4 13
13 plus 1 14 14 plus 16 30 and 39.
to find the MSE
that is the mean squared error we'll
divide the total of the square error
values with the total number of Records
so here MSC
will be equal to
39 divided by and we have total number
of records that is 5.
which will be 7.8
let's name this
as Model A
now for the comparison purpose we'll
create another model that is model B
we'll have all the columns here
and we'll keep the actual values here
same
we'll just change the predicted values
so let's take some random values
um
we'll keep this as 13 we'll take this as
20.
take this as 12 18 and 3.
now to calculate the error we'll
subtract the predicted values from the
actual values so 10 minus 13 will become
minus 3 16 minus 20 will become minus 4
this will become 1 this will be 1 and
this will be
4
now to calculate the square error we'll
just Square the error values
so this will be 9 this will be 16.
this will be 1 this will be 1 and this
will be 16.
the total of all the square values will
come out to be 43.
now to calculate the MSE again we'll
divide the total square values with the
total number of Records so here this
will be equal to 43
divided by 5.
which will come out to be 8.6
If You observe here the total of the
errors in both Model A and model B is
same that is 13.
so which model to prefer
since the mean squared error of model A
is lesser than model B we'll prefer
Model A
because
we are okay with the model that has
small small errors but in model B there
are some significantly large errors
suppose in this case 16 minus 20 that is
minus 4 minus 3 and this here 4. so we
do not want a model where the error is
larger is specifically are more frequent
we can have a model with a small errors
that will be ok
so we'll prefer Model A over model B
most of you must be using SnapChat to
apply filters on your photos but do you
know how Snapchat recognizes your photo
on the screen and puts filters on it
even if there are multiple faces on the
photo it applies filters in appropriate
position Snapchat actually does this
using a technique called facial
recognition which in turn uses machine
learning the machine learning algorithm
detects the features on your face like
the nose the eyes and it knows where
exactly your eyes are where exactly your
nose is and accordingly it applies the
filters we will take a few more examples
as we move along and try to understand
how machine learning algorithms can be
applied to solve some of our real life
problems so what will you learn from
this video we will talk about some real
world applications of machine learning
we will also see and understand what
exactly is machine learning and how it
works we will also see the process
involved in machine learning the types
of machine learning algorithms and we
will also see a few Hands-On including
some code python code of the following
algorithms linear regression logistic
regression decision tree and random
forest and K nearest neighbors okay so
let's get started let's consider some of
the real world applications of machine
learning it's no longer just a buzzword
machine learning is being used in a
variety of Industries to solve a variety
of problems facial recognition is one of
them it's becoming very popular these
days for security for police for solving
crime a lot of areas facial recognition
is being used voice recognition is
another area it's becoming very common
these days some of you must be using
Siri that's an example of machine
learning and voice recognition
healthcare industry is another big area
where machine learning is adopted in a
very big way as you all may be aware
Diagnostics needs analysis of images
let's say like x-ray or MRI and
increasingly because of the shortage of
doctors machine learning and artificial
intelligence is being used to help and
support doctors in analyzing these
images and identifying the Advent of any
diseases weather forecast is another
area and in fact Netflix has actually
come up with a very interesting use case
you all must be aware of the House of
Cards Show on Netflix so they did an
analysis on their customer Behavior they
got data of their 30 million customers
information about where they pause where
they fast forward a video and they used
this information they provided this
information to their playwrights and
told them that this is what we want this
is this is what our audience is looking
for these are the areas that interest
them and these are the areas where they
get bored and the playwrits wrote the
scripts accordingly this is really
really interesting and it actually
brings a new era so what is machine
learning machine learning is a science
of making computers learn and act like
humans so here when we say computers
very often what comes to our mind is
writing a piece of code or program and
telling the computer step by step what
to do but in machine learning we don't
do that the system learns on its own we
just provide past data historical data
what we call as labeled data and the
system learns during the process process
what is known as training process we
tell the system whether the outcomes are
right or wrong and that feedback is
taken by the system and it corrects
itself and that's how it learns till it
gives the correct output for most of the
cases obviously it won't be 100 correct
but the aim is to get as accurate as
possible so let's take you through step
by step process of machine learning the
first step in machine learning is data
Gathering machine learning needs a lot
of past data especially we will see a
little later supervised learning we will
see what that is in a little while but
that's the most common form of learning
so the first step there is data
Gathering you need to have sufficient
historical data then the second step is
pre-processing of this data so that this
can be used for the machine learning
process the raw data cannot be used
directly so it needs to be pre-processed
before it is fed into the machine
Learning System The Next Step is to
choose a model so what kind of algorithm
and what kind of model within that
algorithm are we going to use and then
we need to train this model so before
training the model it's just like a
blank model and after training it
becomes a trained model and once we
train the model we need to also test it
to make sure it is predicting correctly
it's working fine with minimum errors
and subsequently it will be deployed and
once again it may have to be tuned from
time to time or fine tune from time to
time to improve the accuracy and so on
and so forth after training the model we
have to test the model and during
testing we may have to we may find out
that the accuracy is not good enough so
we may have to tune the model we may
change some parameters and run through
this process once again perform the
training once again so it can be an
iterative process and once the model is
ready then we deploy it to do the
prediction
what are the different types of machine
learning algorithms machine learning
algorithms are broadly classified into
three types the supervised learning
unsupervised learning and reinforcement
learning supervised learning in turn
consists of techniques like regression
and classification and unsupervised
learning we use techniques like
Association and clustering and
reinforcement learning is the recently
developed technique and it is very
popular in gaming some of you must have
heard about alphago so this was
developed using reinforcement learning
primary difference between supervised
learning and unsupervised learning
supervised learning is used when we have
historical data and we have labeled data
which means that we know how the data is
classified so we know the classes if we
are doing classification or we know the
values when we are doing regression so
if we have historical data with these
values which are known as labels they
then we use supervised learning in case
of unsupervised learning we do not have
past labeled data historical labeled
data so we use techniques like
Association and clustering to maybe form
clusters or new classes maybe and then
we move from there in case of
reinforcement learning the system learns
pretty much from scratch there is an
agent and there is an environment the
agent is given a certain Target and it
is rewarded when it is moving towards
that Target and it is penalized if it is
moving in a direction which is not
achieving that Target so it's more like
a carrot and stick model so what is the
difference between these three types of
algorithms supervised algorithms or
supervised learning algorithms are used
when you have a specific Target value
that you would like to predict the
target could be categorical having two
or more possible outcomes or classes if
you will that is what is classification
or the target could be a value which can
be measured and that's where we use
regression like for example whether
forecasting you want to find the
temperature whereas in classification
you want to find out whether this is a
fraud or not a fraud or if it is email
spam whether it is Spam or not spam so
that is a classification example so if
you know or this is known as labeled
information if you have the labeled
information then you use supervised
learning in case of unsupervised
learning we have input data but we don't
have the labels or what the output is
supposed to be so that is when we use
unsupervised learning techniques like
clustering and Association and we try to
analyze the data in case of
reinforcement learning it allows the
agent to automatically determine the
ideal Behavior within a specific context
and it has to do this to maximize the
performance like for example playing a
game so the agent is told that you need
to score the maximum score possible
without losing lives so that is a Target
that is given to the agent and it is
allowed to learn from scratch play the
game itself multiple times and slowly it
will learn the behavior which will
increase the score and keep the lives to
the maximum that's example of
reinforcement learning so we will
discuss about some of the most popular
algorithms in machine learning a few of
them are listed here this is by no means
an exhaustive list there are really a
lot of algorithms available out there
but these are some of the most common
ones what are these algorithms linear
regression logistic regression decision
tree random forest and K nearest
neighbors so let's look at each of these
in detail linear regression a little
history about linear regression Sir
Francis the Alton is credited with the
discovery of the linear regression model
so what he did was he started studying
the heights of Father and Son to predict
the sun's height or the child's height
even before he or she is born so he
collected enough data of the heights of
father and their respective Sons he
plotted this data on the X and Y axis
and he drew a line in such a way that
the distance of these points from the
line was the least which is now what we
call it as mean squarer so at that time
this term was probably not there but
that's what he did and then he was able
to use this line to predict the height
of the child which was yet to be born
based on the height of the father so
that was the very beginning or very
initial phase of linear regression
algorithm so that was a little bit of
history but what is linear regression so
linear regression is a way of modeling a
linear model creating a linear model to
find the relationship between one or
more independent variables denoted by X
and a dependent variable which is also
known as the Target and denoted as y a
few examples are shown here let's say we
are going to plot the sales of ice cream
and the temperature so temperature on
the x-axis and the sales on the y-axis
and this is how the data would look and
if we draw a line in such a way that the
distance of each of this points from
this line is minimum that is known as
the regression line or the best fit line
so regression is all about finding this
line and it is called linear regression
because it is a straight line linear and
the equation doesn't have any non-linear
component which means we do not have x
to the power of 2 or 3 or any of this so
this is simple linear regression and
simple linear regression is when there
is only one independent variable so
there is only one X so that is simple
linear regression so let's see how this
is actually done so linear regression is
all about finding the best fit line and
the way it is done is in a recursive
manner so first a random line is draw on
and the distance is calculated from this
line of all the points as you can see in
this example and that distance is known
as the error and to ensure that there
are no negative values this is squared
so we actually take the square of the
distance of the point from the line and
add it up and we make sure that at the
end this sum which is known as the sum
of squared errors is the minimum so as
you can see this is where we start and
then we keep changing the line in such a
way and calculate the distance once
again the sum of the squares once again
and here again you will see that this is
probably not minimum and we keep
changing this unless until we get a line
where this value is minimum so here we
find that the sum of the squares of the
distance which is also the sum of
squared errors capital D denoted by
capital D is minimum so now we found the
best fit regression line so this is a
recursive process it's an iterative
process and this is what behind the
scenes this is what happens when we try
to do linear regression now we'll take
an example of a linear regression and we
will actually demonstrate this using
Python and we will be using jupyter
notebook for this case you are familiar
with Jupiter notebook I think that would
be very helpful and if you need the data
set please put a comment under this
video so that we can provide you with
the data set before we go there just
let's try to understand what this
example is all about let's say there is
a new joining joining the company and we
want to determine what should be the pay
or the salary for this new journey and
for that we already have the labeled
data which is the details of our
employees based on their years of
experience and their salaries and for
the new Journey we have the experience
and we need to determine what should be
the salary so let's take a look at the
demo so before we go into the jupyter
notebook let's quickly review the code
what the code is doing the first section
is about importing the libraries some of
you who are familiar with python will
already be knowing this you don't have
to go into details and then we import
the data set and then we visualize the
data just get a quick idea about how the
data is looking and then we split the
data into training and test data sets
this is a common procedure in machine
learning process any machine learning
process and the overall training and
test process we do with two different
data sets so that's what we are doing
here and then we built or train our
model the linear regression model and
then we do the testing and we find out
what is the errors and visualizer
results and this is how the test results
look and this is how the training result
looks all right and then we calculate
the residuals residuals are nothing but
the errors there are a couple of ways of
measuring the accuracy the root mean
square error is the most common one rmsc
and in this case we got root mean square
error of 58 which is pretty good and
that's our best fit all right so now
let's go into Jupiter notebook and take
a look at by running it live okay so
this is our code for the linear
regression demo and this is how the
jupyter notebook looks and this code in
the Jupiter notebook looks I will walk
you through the code pretty much line by
line and let's see how this works the
linear regression so the first part is
pretty much a standard template in
pretty much all our code we will see
this is importing the required Library
so numpy is a library matplotlib is a
library pandas and so on so these
libraries are required each of these
libraries have a different purpose of
some of them are required for
manipulating your data some for plotting
as the name suggests matplotlib and so
on and so forth okay so let's go ahead
and import all these libraries and then
we will import our data so in this
example what we are trying to do the use
case in this particular example is we
have some historical value of salary
data and now we want to build a model so
that we can predict the salary for new
employee a person who is joining new and
we will use the same the characteristics
that were available to us or the
features that were available to us and
we will try to predict what will be this
salary of this new person okay so that's
the kind of the use case so let's go
ahead and load the data and let me
introduce a cell and see how the data
looks so salary
underscore data
so we have basically two features right
so it's pretty much like a simple linear
regression we are trying to do so we
have years of experience and salary and
in this case what we are trying to do is
this is our predictor so years of
experience is our predictor and we are
trying to predict what the salary would
do so once again so X is what is known
as the predictor and Y is the target so
number of years of experience we are
taking as input and we will try to find
what will be the salary of this person
based on that okay so that's what we are
going to do so I have shown what is how
the overall how the data that has been
imported looks now let's take a quick
look at these extracted values or
extracted columns as well so let me put
in X here and see how it looks so
there's basically one column and same
way if we put y here we will see what
exactly I'm sorry this has to be in
small lowercase so this is the salary
information okay so this is our data is
and then we can do a little bit of
plotting of the data to play around a
little bit because we imported the data
typically we want to do some exploratory
analysis how they compare with each
other and so on and so forth how they
are correlated so let's draw a quick
plot a bar plot and see how the years of
experience are being seen here right so
this is how the part plot looks
similarly let's do one more which is
basically more like a this is horizontal
bar plot okay so how many people like
with what experience and so on and so
forth so what this shows is uh the count
uh how many records are there with a
given experience and things like that
okay so this is another way of
visualizing the data and
this is a third View and this is one
more View and then we can do a quick
heat map so there are there's only one
or actually there are only two variables
so there is a there's not a lot of
plotting that we can do or not a lot of
visualization that we can do since there
are only two variables but nevertheless
uh whatever is possible uh we can do a
quick to get a quick idea about how the
data is looking and how the variables
are related to each other is there a
correlation and things like that all
right so once we are done with that this
is the most important part of our demo
here which is basically this is the
beginning of our training process so the
first thing before we start the model
building and model training processes to
split the data into training and test
data sets Okay now whenever we do any
machine learning exercise especially
supervised learning we never use the
entire label data for training purpose
the reason being then we will not be
able to correctly evaluate how well the
training has happened okay so what we do
is we split the data we take a portion
of the data we we call that as the
training data set and we set aside some
portion of the data we call that as a
test data set we use the training data
set to actually perform the training to
train our model and once that is done we
use the test data set to check how well
the model has been trained how
accurately it is able to predict okay
now the reason is in for our with our
test data set also since that is also
labeled we allow the model to predict
the values and we compare it with the
labeled information to see whether it is
predicted correctly or not okay so that
is the reason in all machine learning
activities whenever we perform machine
learning we especially training of a
model we split this data into training
and s data set and that's what we're
going to do here now we there is no need
to write any separate code for that
there is a readily available function
for that drain underscore test
underscore split so that's what we are
going to use here it takes the data set
so X and Y is the data set and in
addition it takes a parameter which
tells how the data has to be split so
for example here it says test size is
equal to 1 by 3 that means it is 33
percent right 33.33 so one third of the
data you want to set aside for test now
there are no hard and fast rules as to
what should be the split of test and
training data set is a matter of
individual preferences some people would
like to have 50 50 some people would
prefer 80 20 and so on and so forth so
it is completely up to the individuals
to to decide on that so in this
particular case what we are doing is we
are setting aside one third of the data
set for testing purpose and two thirds
of of the data set for training purpose
okay so that's exactly what we are doing
here now the next step is to create an
instance of linear regression so linear
regression model is readily available so
we create an instance of the linear
regression model and give it a name like
LR and then we call the fit method of
the linear regression model now this fit
method is common across all the
algorithms so any algorithm you use if
you want to start the training process
you call the fit method okay and then we
pass the training data set now training
data set we send the predictor as I was
saying or the independent variable and
also the dependent variable we pass both
of them okay and the system will
basically learn based on this so now the
training has happened training is
completed and now we need to see whether
it is predicting correctly or not we
need to test it right so for test
testing we have another method called
predict again this is common across all
algorithms any algorithm you use you
will have this predict method for
testing your your model okay or for
actually predicting the values whether
during testing or when you actually
deploy the model okay now here predict
will take only one parameter which is
the independent variables right the
reason being the dependent variable is
what it will predict right so why
underscore test we don't have to pass
whereas here y underscore train we
passed for training purpose but why
underscore test we do not pass because
that is what the model will predict so
we don't have to pass that and then the
model will predict and that is what we
call it as y underscore thread but we
will use y underscore test to compare
with what the model has predicted and
thereby we can determine how accurate
the model is okay so let me go ahead and
run this code word so it has done with
the testing or with the prediction of
the values so these are the predicted
values and now we can visualize we can
generate some plots to visualize this
information so let's plot this training
set results information and see how it
looks so what what we are doing here is
basically we have the training data set
plotted the salary versus the years of
experience right this is the training
data the the dots basically and then
this line is what our model has so this
is the best fit so linear regression
what is linear regression process we
find a equation of a line which is the
best fit so our model this is the
equation or this is the line that our
model has kind of come up with saying
that this is the best fit model okay so
as you can see intuitively it feels okay
because it is passing through pretty
much the middle of our data set okay so
this is for the the training part now we
do the same for our test as well and see
how it is doing or how it looks here
also it looks pretty good because the
line passes pretty much in the middle of
the overall data set that's what we are
trying to do here all right and then how
do we measure the accuracy so this
residuals are nothing but the errors the
term residuals is nothing but the errors
we have seen in the slides as well so we
calculate the accuracy of our model by
calculating the various residuals we
have like mean square error then we have
the mean absolute error and then we have
the root mean square error they are
pretty much they're very closely related
this is nothing but the error and the
actual value right so for example let me
explain with this this particular line
so what our model says is if the value
of the years of experience right so let
me take at this point if the value of
the years of experience is 4 then the
salary will be according to our data
that salary probably should be what is
this maybe around 58
000 right and there are of course uh
three points three data points pretty
much around this four years experience
uh one maybe around 58 or in fact two of
them are around 58 and there is one more
which is probably 60 okay now these are
the three data points whereas our model
predicts that if the years of experience
is 4 then the salary should be sixty
thousand let's say okay let's assume
this goes to sixty thousand which means
that one of the data points is very
accurately predicted by our model but
there are two of them which are off so
there is an error for these two values
and what is that error the error is
nothing but the distance of this point
from this line right the distance of
this point of each of these data points
from the from the line is the error okay
so that's basically for each of the
values it will correct so it will
calculate rather and uh and instead of
taking the absolute value we kind of
take a square of that so because the
value can be positive or negative so in
order to avoid a cancellation so there
are some positive values there are some
negative values and they get canceled
out uh what we do is we take a square of
that so that is basically what is the
root mean square or the mean square
error and then if we take a square root
of that it becomes root mean square
error right so this was explained
probably in the slides as well so that
is what we will calculate here and uh we
we print it okay and these values the
root mean square error the mean squared
error and the mean absolute error the
lower these values are the beta so the
accuracy is higher if these values are
lower so in a way it is inversely
proportional okay so that's the way we
measure the accuracy of our linear
regression model all right so that
brings us to the end of this demo and we
will continue with the other demos all
right now that we have seen linear
regression let's take a look at our next
machine learning algorithm which is
logistic regression now what's
interesting about this algorithm is that
while it says regression the name has
regression in it but keep in mind this
is not used for regression but this
algorithm is used for classification now
many people get confused by this name so
you need to be aware of it linear
regression is used to solve regression
problems where we are trying to predict
a value whereas logistic regression is
used to solve a classification problem
so we are trying to find for example
whether a person will repay the loan or
not or whether we want to find whether
this image is of a cat or a dog so this
is a classification problem okay so just
that you should be aware of the name so
in this slide we are talking about
whether a person credit card user or
credit card holder will default on the
payment so we can create a profile we
have historical data and of people their
income and their credit card balance and
we have examples of people who have
defaulted and people who have not
defaulted and based on that we let the
system learn to predict this particular
value whether the person will default on
the payment or not now how does the
logistic regression work if we try to
draw a straight line to determine the
probability so we are trying to predict
whether this person will default or not
okay so if we draw a straight line then
the predicted value can exceed 0 and 1
which is not a good idea so the
probability is calculated between 0 and
1 using what is known as a sigmoid curve
so it is not a straight line but we use
a different formula a different
mechanism to find find the probability
and find the value between 0 and 1 and
the formula for that is p is equal to 1
by 1 plus e to the power of minus Z here
Z is actually the linear equation that
we used in Figure 1 which is m1x plus c0
so this is fed to p and the value is
calculated between 0 and 1 and we will
see actually mathematically you will see
here that if Z is negative and much much
higher a very high value it will the
equation will be 1 by 1 plus 0 which is
equal to 1 so we will see here that if
the value of Z is positive and it is
some high number then the value of P can
at the most be equal to 1 because this
portion here is 1 by a very high number
let's say infinity so 1 by Infinity is 0
so this whole thing becomes one by one
so that is equal to 1 p is equal to one
so that is where the maximum value it
can achieve is similarly if the value is
negative if the Z value is negative or
negative and very high value then this
will become very high and therefore the
denominator will become Infinity so 1 by
Infinity is 0 and therefore p is equal
to 0. so that's the way the sigmoid
function works this is how the graph
looks and there has to be a threshold
value which is like in this case 0.5 so
if the value is greater than 0.5 we
consider the output as 1 whereas if the
value is less than 0.5 we consider the
value as 0 because remember let me go
back in this case it doesn't exactly
give us a one or a zero okay so we need
to keep that in mind it doesn't give us
exactly a one or a zero it will give a
value between 0 and 1. irrespective of
what the value of Z is it will give us a
value between 0 and 1. it's like the
probability between 0 and one now we
have to have a threshold and then based
on what the value is if the value is
greater than 0.5 then we say okay this
probability is one and if the value is
less than 0.5 the probability is zero so
we decide that based on the cutoff or a
threshold so for example if we continue
on that if a person having a balance of
maybe this is 1750 then the probability
that he will repay the loan is 0.2 which
means Which is less than 0.5 the
threshold value which we have said that
means the probability is 0 which means
the person will not repay whereas if the
balance is somewhere in the range of
2250 maybe which is this red dot then
the sigmoid function calculates the
probability of default as 0.8 which
means the output is 1 which means the
person will default on payment okay so
that is how the logistic regression
works so in this case it says that okay
this person will default it is
classified as a default in this case the
the second one and in the first case it
is a non-default so these are the two
different classes of these two different
cases so let's move on and see how
logistic regression is implemented so
this is another example of logistic
regression can we predict whether a
person is going to buy an SUV based on
their age and their estimated salary so
these are our two inputs and based on
that can we predict whether this person
will buy an SUV or not so this is a
logistic regression problem and we are
going to demonstrate this using python
code and in jupyter Notebook and before
going into jupyter notebook let's take a
look at how we go about solving this
problem and how the code looks so this
is the implementation of logistic
regression this is the code the python
code I just take you very briefly at a
high level what each parts are so first
section of course is importing the
libraries and then we import the data
set and within this data set we just
take for performing or for training our
model we only take the independent
variables into one create one vector and
then we extract the labels separately
and then we visualize our data this is
how the data looks and then we split the
data into training and test set like we
did in linear regression and we do some
feature scaling as well which improves
the performance of our model and then we
train and test the model so these are
the test results and we visualize the
test results so this is the
visualization of the train results they
look pretty good the classification they
are reasonably accurately classified the
red dots and the green dots and there
are a few of course miscalculations or
misclassifications but by and large it
looks pretty good and this is the
visualization of the test results again
looks pretty good and then we evaluate
our model and this is what we'll be
doing in the code as well and for this
we use what is known as confusion Matrix
now let's try to understand what this
confusion Matrix is now I know the name
itself is confusing but actually it is
very simple now once we predict these
values and compare with the actual
values we can create or represent the
results in the form of a matrix so we
have all together 134 observations and
if we put it in a tabular form the
predicted values and the actual values
first of all in order to identify
whether our model is more accurate or
less accurate the criteria is that the
values along the diagonal should be
maximum so for example this is 79 and
this is 38 the sum of these should be
maximum so what do we mean by that that
means that if we have a perfect model
then this sum will be equal to 1 134 and
the values here will be zero so that is
an ideal perfect logistic regression
model that is of course very rare but
that is the the sum of these numbers are
the maximum numbers should be in the
diagonal and in the other cells there
should be as less as possible okay so
here we see in this case that it is 79
plus 38 and so that is equal to 117. so
117 of them have been correctly
predicted which gives us an accuracy of
87 percent so out of 134 117 have been
correctly predicted and the 6 plus 11 17
of them have been incorrectly so this is
6 plus 11 17 of them have been
misclassified so which is about 0.13
percent so we have an accuracy of 87
percent okay so I hope the way we
calculate or with the way we find
accuracy from confusion Matrix I hope it
is clear by the way once again a quick
reminder if you need this data set to
perform this on your own please put a
comment and under this video and we will
send you the data set all right so let's
go and check in Python notebook how
exactly this is done all right so this
is the demo of logistic regression and
here what we're doing is we've taken an
example of a data set and a scenario
where we will predict whether a person
is going to buy an SUV or not and we
will use logistic regression for this
and the parameters we will take are for
example the person's age his salary and
a few other parameters we will see very
quickly what those are okay so the first
step we import the required libraries so
that's what we are doing in this
particular cell like for example numpy
matplotlab pandas so for performing any
preparation of the data before we
actually launch into the learning
process and then we load the data so let
me just introduce one more cell and see
how the data looks so why don't we do
that here and if I say data set and then
if I and this this is how our data looks
so these are the parameters or the
features as we call it in machine
learning language we have gender age and
estimated salary user ID is also there
but it's not really a feature which will
probably not contribute so we will not
be using this we will primarily be using
these three columns and in technical
terms these columns are known as
predictors and then we have the labeled
value this is known as our Target and in
this is the labeled value so it has zero
or one so we are basically performing a
binary classification zero or one
whether the person will purchase or not
purchase zero means you will not
purchase one means he will purchase okay
and the the other way of also looking at
it is from a more from a mathematical
perspective these are our independent
variables gender age and estimated
salary are our independent variables and
purchased is our dependent variables so
in our equation like Y is equal to
something something in 1X plus M to X
and so on this is our y okay all right
so now let's move forward what we will
do next is to extract this the or
separate out the independent variables
and the dependent variables how do we do
that so we load them into x capital x
and y and what we are doing here is we
are taking the second and the third
column we are taking these three right
so gender age and estimated salary so
that is what we are taking here 2 up to
3 and then Y is basically the last
column which is the fourth column so
let's go ahead and extract that and once
again why don't we take a look at how
the data is looking so let me just print
X so we only yeah we are basically
taking 2 and 3 so what is 2 and 3 2 and
3 is our age and our estimated salary
yeah that's right so these are the only
two independent variables and our
dependent variable is obvious obviously
the last one whether the person will buy
or not okay so we have the data loaded
now let's now that we have the data in
Python let's take a quick look at how
the data looks so in terms of
visualization let's visualize the data
and perform a little bit of what is
known as exploratory analysis right so
as a data scientist whenever you get new
data you just play around and see how
the data is looking before you actually
launch it to the actual training or the
modeling part of it so let's run a small
heat map and see how the data looks so
we have just passed the entire data set
here and this is how the heat map looks
how how they are related how the various
what you call features are related to
each other and this is a scale here very
dark means basically there is a pretty
much no no correlation this is in a way
to measure the correlation as well and
light means there is a very high
correlation so as you can see why value
or a feature will have very high
correlation of 1 to itself so the user
ID and this is user ID it has a very
high correlation to itself similarly
purchased and purchase data so the
diagonal will be very high so that is
really not relevant but then the other
values is what we have to see for
example there is a correlation of 0.6 in
this area which is basically if you take
age and purchase right whether the
person has purchased and the age so
that's that's a quick look at exploring
the data so that's all we are doing here
here's where the actual uh the Crux of
this code is so from here onwards what
we do is the first step before we start
the training process is split our data
into train and test training data set
and test data set so whenever we perform
any machine learning activity we have
let's say our labeled data we never pass
the entire data to the model because
when we are then when we are measuring
the accuracy whether the model is
performing well or not we will not be
able to do that if we use up the entire
data for training purpose so we split
the data into what is known as a
training data set we train the model
with the training data set and then this
test data set which is kept separately
which the model has not yet seen we use
that to check whether it is able to
identify perform the predictions
correctly or not okay that will give us
a higher accuracy or a better estimate
of how the model is doing all right so
this is a very common practice in
machine learning so whenever we have
whenever we perform training of the
model we split the data into training
and tested now how do we do this we
don't have to write any special code
there is already a method available
which is train underscore test
underscore split and we just call this
method only thing is it takes these
parameters and we have to specify
especially this parameter which tells
the the method how the data should be
split now when we are splitting the data
into training and test there are no hard
and fast rules how we split this data
Some people prefer 50 50 Some people
prefer 80 20 and so on and so forth so
that is flexible and it could be to some
extent individual preferences so in our
case we are splitting this data into
7525 which means 75 percent of the data
will we will use for training 25 values
for test so that is what we are
specifying here as a parameter we say
test underscore size is equal to 0.25
that means put or keep uh 25 percent of
the data set aside 25 of the data as
test data and therefore the remaining 70
percent will be used for training all
right so let's move on I execute this
then we perform what is known as feature
scaling so what is feature scaling
usually what happens is uh the the
values that we have in the data
sometimes they can be uh some numbers
will be very large some will be very
small so that there is a huge variation
so in order to if we have that and if we
use that as it is the accuracy of our
model may come down so we have to
somehow normalize these values so that
is what scaling does feature scaling
does and uh again we don't have to write
any special code for this there is a
standard method available or standard
class available called standard scale up
so we just create an instance of that
and pass our data for scaling purpose
okay so let's go ahead and do that now
so this is all what we have done is more
like a data preparation so we we chose
what are the parameters what features we
want we did the feature scaling and we
split the data now everything is ready
next is to to start the actual training
process so this is the most crucial part
of the code so here as we said we will
use the logistic regression model so we
have to create we create an instance of
regression model so I call that as
classifier you can give any name and we
just do some kind of a initialization
random value initialization so we we
call this as a random state or we assign
it as random state is equal to zero and
we we have an instance of the logistic
regression model and we use that model
the fit method so any any algorithm that
we have when we want to perform the
training we have to use the fit method
okay we call the fit method now let's
say instead of logistic regression we
are using K uh K nearest neighbors
algorithm you will have K nearest
neighbor dot fit or linear regression
linear regression dot fit right so that
is where that is the way it works and we
pass the training data set the x is the
independent variables and wire the
labels so we pass both of them for the
training purpose so once the training is
done we get the results of or the model
is trained okay the model is straight
now the next step is to test the model
how well the model is performing this is
where we use the test data set we pass
the test test data set and here there is
a another method that we have to use
that is known as the predict method so
again this is a predict method is common
across all uh algorithms that we have so
any algorithm we have we have to call
the predict method for performing the
test or even when we deploy it when new
data comes in we are trying to predict
for new data set we always use predict
okay so fit is only for the for training
okay then predict is used for testing
and for the actual performance of the
prediction now one thing we need to
observe here again is we only pass the X
part of it and not the Y part of it so
this can always sometimes this can lead
to confusion in when we are calling fit
we are passing X and Y when we are
calling predict we are only calling X
the reason is the Y part is what the
model will predict so you don't have to
pass the Y part right so Y is what the
model will predict for us so you don't
pass here we have to pass it because the
model has to learn from the existing
values so that's why we had to pass
label as well but when we are testing it
the model will calculate this and that's
the reason we do not pass the Y value
okay so now when we run predict it will
calculate those values or find those
values and it will put it in y
underscore thread and that is what is
being displayed here and as you can see
the values can be either a 0 or 1 which
which means the person will buy the SUV
which means which is one which is which
is a value of one or zero means the
person will not buy the SUV okay now one
more thing we need to remember is while
we have not passed the Y values or the
labels to the system but we have these
labels available with us right y
underscore test is available with us
that is what we have to use to find out
how accurate the system is so it has
given us the results now we will compare
these results which the system has
predicted with the actual values that we
have which is available in y underscore
test so we will see how well how that is
done how that accuracy is calculated in
a little bit okay now that the training
is completed we can go ahead and do some
visualization of the training result so
let me execute this code and take a look
uh in a in a form of a plot so this is
how the classification is done so these
are like the class this is like the
class boundary the green color belongs
to the class 1 and the red color belongs
to class 0 and these dots indicate it
has been misclassified so some of them
have been misclassified so that's what
we are seeing some red dots in the green
area and some green dots in the red area
right whereas if you probably it's not
very clear but you can see here that
there are also red dots in the red area
and the green dots in the green area so
those are the data points which are
correctly predicted and these in the
opposite locations are the ones that
have been the misclassified rather same
for test as well so test data set also
if we visualize we plot we will see
something similar but this also gives at
a high level
um whether is that a major uh in a major
way are there major misclassifications
or there are only few here and there it
gives a little bit of an idea about the
accuracy as well but then that is not
sufficient for us right so we have to
quantify this accuracy so how do we do
that we do this using what is known as
confusion Matrix so it's a readily
available method or a class which is
there and it takes two parameters which
is the you remember I told you we will
be using Y test so this is where we will
be using Y underscore test which is our
labeled value or label data set and this
is what is predicted by our model OKAY y
underscore thread is predicted are the
predictions so these are the values out
here we saw this is y underscore thread
okay so we we pass these two parameters
and we create a matrix and this Matrix
is known as confusion Matrix now what is
the significance of this first thing is
that the total number of values here is
equal to the number of data sets in your
test data okay so let's go ahead and in
the next cell let me just add up these
values so 65 plus 3 plus 8 plus 24 or so
how many are there there are 100 in
numbers so there are totally hundred
observations in your test data set that
is the first point then the second point
is the number in the diagonals right the
total value in the diagonals if that is
a high that indicates a higher accuracy
the higher that number is the total in
the diagonal the higher the accuracy
okay and if we have quite a few numbers
and non-diagonal locations that means
accuracy is not very high so here looks
like the accuracy is pretty good now we
can actually quantify the accuracy by
using these numbers so what we do is the
sum along the diagonal we have to take
and divide that by the total observation
so 65 plus 24 is what we have along the
diagonal and we have 100 observations so
89 by 100 is our percentage so we got
about 90 percent accuracy for this
particular model okay so that brings us
to the end of this demo and we will move
on to the next demo all right so the
next algorithm is decision tree and
random Forest they are being taken
together because they are very closely
related so what is a decision tree this
is human tree is another algorithm and
unlike logistic regression which is only
used for binary classification decision
tree can be used for classification as
well as regression even though it is
more popular for classification and it
can be used to classify multiple classes
as well not just binary classification
so how does decision tree work one of
the good things about decision trees is
that it is easy to represent and show
how exactly it works and therefore it is
very easy to understand as well now
let's take an example let's say a person
receives a job offer and he needs to
decide whether to accept the job offer
or not so we will use decision tree to
come to the decision to accept or not to
accept so this is how first of all a
decision tree looks it is actually an
inverted tree so the root is at the top
okay so and that is known as the root
node the node where the tree starts is
known as the root node and then we have
some inner nodes or decision nodes and
we have the nodes where we have a
decision either positive or negative
these are known as Leaf nodes or
terminal nodes okay so we have the root
node and we have Leaf nodes or terminal
nodes there can be multiple of them and
in between we have decision nodes or
internal nodes there are different terms
used so need not be hung up by the exact
terminology now let's say we have to use
this decision tree to find out whether
this person will accept the job offer or
not so first thing he considers is the
salary is the salary greater than 60 000
if no know it's a clear decision the
offer will be rejected so we reach the
decisions therefore this is a leaf node
now if the salary is greater than 60 000
it is not a clear-cut decision because
there are probably other factors based
on which the decision needs to be made
for example what is the commute time if
the commute time is greater than 1 R
then the is rejected okay so even though
the salary is greater than 60 000. so
this is again a leaf node if the commute
time is less than one hour still it is
not a confirmed decision because there
are still a few other factors for
example Performance incentives are there
sufficient performance incentives as a
part of this offer if no then again the
offer is rejected and this is another
Leaf node if yes if there are sufficient
performance incentives then the offer is
accepted so this is another Leaf node by
the way keep in mind that these are all
the leaf nodes this is a leaf node this
is a leaf node this is a leaf node and
they of course belong to different
categories in this case it is still a
binary classification so this belongs to
one class which is accept offer and
these Leaf nodes belong to a different
class which is reject offer so let's
take an example and see how we can solve
this problem using decision tree let's
say we have to implement a
classification algorithm for kyphus's
patient and the problem is a bunch of
kids have been have undergone a kyphosis
surgery and we need to predict whether
kyphosis is present in them or not and
how can we do this using decision tree
algorithm so this is how the
classification tree for kyphosis looks
so it starts with if the age is greater
than 8.5 so this is how the decision
tree looks so the first criteria is the
vertebra the number on which the surgery
has been performed if it is greater than
8.5 then we need to perform further
analysis and look at other criteria if
it is less than 8.5 it is clear here
that kyphosis is present and if it is
greater than 8.5 then we check whether
the vertebra operated upon is greater
than 14.5 or not if it is greater than
14.5 then kyphosis is absent if not then
the next criteria is the H if the
person's age is less than 55 years then
kyphosis is absent but if it is greater
than 55 years then a further analysis is
required to see if the person's age is
greater than 111 years which is of
course almost impossible but yes so if
if no then there is no kyphosis then we
check if the age is greater than 111
years which means the person is between
55 and 111 years if the person is
between 55 and 111 years that is this
path that means kyphosis is present
otherwise it is absent so this is very
easy to understand decision tree and now
what we're going to do is we're going to
implement this in Python in our Jupiter
notebook and we have a data set that is
available for this and once again if
anyone wants this data set please put a
comment under this video and we will be
more than happy to share the data set
with you before we go into the jupyter
notebook let's take a look at what the
code is doing what are the various
sections of the code so first one of
course is loading the libraries as usual
then we import the data and then extract
the independent variables and then
separate the labels which is the
dependent variable and then we visualize
the data this is how it looks and then
we as usual split the data into training
and test data sets and then perform the
training of the decision tree and then
we test that model with our test data
and then since this is a classification
problem we evaluate this model using the
confusion Matrix remember we talked
about this in the previous example as
well so this is the confusion Matrix for
this particular problem and here you can
see that 16 plus 3 19 of them have been
correctly predicted out of 25 which
gives us a 64 percent accuracy and this
four plus two six of them have been
misclassified just to recap Once Again
how do we determine from the confusion
Matrix whether this is accurate or not
the numbers in this diagonal should be
maximum so here we see that out of 25 n
is equal to 25 which is the total number
of observations out of 25 in this case
in this diagonal we have 16 plus 3 19 of
the values are here so that's the reason
we feel there is good accuracy which is
64 percent they have been correctly
classified 64 percent of the
observations have been correctly
classified by this model whereas 24
percent have been misclassified which
consists of this 4 plus this two so
that's how we use the confusion Matrix
to determine the accuracy of our
decision tree model so let's go into the
Jupiter notebook and take a look run the
code and see how it locks so this is our
python notebook for decision tree and I
will take you through the code not line
by line of course but we will see the
blocks as always the first block is to
import the required libraries like
pandas and numpy and so on and then we
import the data for this we are using
kyphosis dot CSV file and then we
extract the independent variables and
then the dependent variable dependent
variable is the target which is a or
classification and also we call it as
the label data whether it is whether
this person individual has kyphosis or
not and then we will run a little bit of
initial exploratory analysis on the data
so as a data scientist whenever you get
new data you do that so we plot age
against whether the person has kyphosis
or not and then we plot the various
parameters against each other so for
example this is like age versus the
number and by the way the data how does
the data look let's take a look at the
data this has primarily three columns
one attribute against the other so for
example this is age versus the number
the number versus the start and so on
and so forth so this is just to give get
a quick idea about the overall how the
overall data looks so we can do a few
more visualizations like with respect to
the age and the count the number of
vertebrae that have been operated upon
and and the red color indicates kyphosis
is absent and the blue color indicates
kyphosis is present so this is another
view and so on so we can basically do as
much of exploratory analysis uh we can
and then we start training our model so
this is where we split the data into
training and test data set like in all
other machine learning examples earlier
also and we can visualize the data or
just quick take a quick look at how the
data looks just the first four or five
entries here if we do the head that's
what the head function does and
similarly y as well we can take a look
at how the target is looking and then we
can get into the training of the
decision tree so we train our model with
the training data set using this fit
method and then we test whether the
model is working fine or not using the
test data set and this is the result of
our test data and then we evaluate the
model and for evaluating we use the
confusion metering so just let's take a
quick look at the confusion Matrix and
see what is the accuracy so we get about
16 right as I mentioned the diagonal
values is what matters we have 16 of
them correctly identified out of 25 so
that gives us about 62 I think 70
percent accuracy so that is using
decision tree now instead of decision
tree let's check what happens if we use
random Forest so we use let's say the
our forest consists of 100 trees and we
do the same we train our model and and
with the test data set we try to test
the model and here we see that we get 19
of them out of 25 correct so that is a
much better accuracy so this is about 76
percent so 19 out of 25 is 76 percent
accuracy and we saw in by using the
decision tree we got only 64 percent
that is 16 by 25 was only 64 percent so
this is a quick demo of implementation
of decision tree so once we evaluate the
model the next thing we can do is what
happens if we want to improve the
performance so that's where random
Forest comes into play so in instead of
using a decision tree what happens if we
use a random forest and what is a random
Forest it's very similar to decision
tree only thing is that instead of using
one tree we use a number of trees that's
why it's called a forest so we use
multiple trees and we take like a
average or a voting of all the trees for
each observation and we finally
calculate or we finally classify each
observation based on that so in this
case it helps in improving the accuracy
so as we have seen here if we take in
decision tree the accuracy now comes to
76 percent right in the previous case it
was only 64 percent using just the
decision tree now when we use random
Forest the performance has improved to a
good extent and it has become 76 percent
so Random Forest usually helps in
increasing the accuracy when we are
using decision trees as our algorithm
okay the last algorithm is the K nearest
neighbors algorithm this is again a
classification algorithm and it is
actually very simple and straightforward
very easy to understand as well in this
case let's say we have historical data
of heights and weights and we also have
the labels so if the height and weight
is in this range these are cats and if
the Heights and weights are in this
range these are dogs so this is one
class and this is another class and now
if we get a new point or new data set
and we find that for this particular
animal the height and weight if we plot
it is here so how do we determine
whether it belongs to the cat class or
the dog class that is where the K here
comes into play so when we are using
this algorithm we need to specify what
should be the value of K so let's say we
say we determine that the value of K is
3. so what we will do is we will find
out the nearest objects nearest three
objects to this data point that we have
so let's go to the next slide here okay
and we Define K as 3 so what happens we
find the nearest three objects to this
data point and then we find out what is
the maximum number of items which class
the maximum number of items belong to so
in this case there are two cats and
there is one dog therefore we determine
that this new data point belongs to the
cat class now what happens if we say k
is equal to 7 in that case these are the
seven data points which are nearest to
this new data point and now you see that
instead of cats dogs are more so which
means that now the point or this data or
this new creature belongs to the class
Dom so this is while this is a slight
drawback but by by trial and error we
should be able to find out the right
value of K and once we use that that's
how we train the model now let's use K
nearest neighbors and take an example to
solve one of our previous problems that
we did using logistic regression whether
a person is going to buy an SUV or not
based on the age and estimated salary so
before we go into Jupiter note let's
again take a quick look at the code so
what are the various sections in the
code as always we import the libraries
we load the data set we visualize the
data we split the data into training and
test data set we do some feature scaling
and then we train our model and then we
test our model we visualize the training
set results and then we visualize our
test results and both of these seem to
be looking pretty good and then we
evaluate our model using the confusion
Matrix so remember whenever we use a
classification algorithm the way to
evaluate our model to find out the
accuracy of our model is the confusion
Matrix we have seen in logistic
regression we have seen in decision tree
and now in k n as well so in this case
once again the total number of
observations is 100 and we see that
along the diagonal which is the number
we need to consider there are 93 values
which means we achieve 93 percent
accuracy and a few of them that is only
seven of them are misclassified so this
is a pretty good accuracy compared to
our previous example so let's go and
check how this looks in Jupiter notebook
so this is my python notebook for K
nearest neighbor implementation
and I will quickly walk you through this
code the first block is as usual we are
importing the various libraries like
numpy pandas and matplotlib and then we
load the data set and we split the data
into training and test data sets and
then we do feature scaling and then we
start training our model and here we are
specifying K as 5 okay and now the data
set is trained now we go and and test it
with our test data set then we evaluate
the model and here we get that as you
can see in the diagonal there are 73
entries so total is 73 which means out
of total of 80 73 have been classified
correctly which is about 90 percent
accuracy we are getting and seven of
them have been misclassified we can now
visualize the training results so this
is these are the zeros and these are the
ones this is the training set so there
are a few as you can see
misclassifications the green ones in the
red area and the red ones in the green
area are the misclassified values and
same way we can check the visualize the
results of the test data set as well and
here again we see there are a few little
bit of misclassification and that brings
us to the end of this python notebook
demo for KNN classification as you know
in today's world data is becoming
increasingly important entire
professions are dedicated to studying
understanding and manipulating the data
hence it is important to know about the
different types of data and the
associated properties so in this session
we'll discuss how to calculate a
distortion in the normal curve using
skewness and kurtosis so let's get
started
we will start this tutorial by
understanding symmetrical distribution
then we'll move on to discuss Q
distribution and PSS coefficient of
skewness
finally we will discuss curtosis
so let's get started
symmetrical distribution
a distribution is said to be symmetrical
distribution when it is equally
distributed on both the sides of the
mean value
the distribution shape can either be a
bell shaped or u-shaped
in symmetrical distribution the values
of mean median and mode are all equal as
you can see in the graph
skewed distribution
skewness is used to measure the level of
asymmetry in our graphs it is the
measure of asymmetry that occurs when
our data deviates from the norm
sometimes the normal distribution tends
to be tilt more on one side this is
because the probability of data being
more or less than the mean is higher and
hence makes the distribution
asymmetrical
this also means that the data is not
equally distributed
the squares can be of two types
positively skewed and negatively skewed
let's discuss them separately
negatively skewed in a negatively skewed
distribution the data points are more
concentrated towards the right hand side
of the distribution
as you can see in the graph this makes
the mean median and mode Bend towards
the right hence these values are always
negative
in this distribution mode is greater
than median and median is greater than
mean
now let's move on to positively skewed
distribution
in a distribution that is positively
skewed the values are more concentrated
towards the right side
and the left tail is spread out as you
can see in the graph
hence the statistical results are bent
towards the left hand side
therefore the mean median and mode are
always positive in this distribution
mean is greater than median and median
is greater than more
the measure of skewness always help us
to know to what degree and in which
direction positive or negative the
frequency distribution has a departure
from symmetry
positive or negative skewness can be
detected graphically depending on
whether the right tail or the left tail
is longer but we don't get the idea of
the magnitude
in a negatively skewed symmetry the
values are always negative
this is the symmetrical distribution and
in a positively skewed the values are
always positive
now let's discuss ps's coefficient of
skewness
the median is always the middle value
and the mean and the mode are the
extremes
so we can derive the formula to capture
the horizontal distance between the mean
and the mode using the PSS first
coefficient
so the formula to calculate the PSS
coefficient is mean minus mode divided
by the standard deviation
the division by the standard deviation
will help you to scale down the
difference between the mode and the mean
this will scale down their values in the
range of minus 3 to Plus 3. in case
where the mode is indeterminate we can
calculate the PSS coefficient by mean
minus 3 median minus 2 mean divided by
the standard deviation
so the value of coefficient is between
minus 0.5 and 0.5 the distribution of
the value is always symmetrical
if the value of coefficient is 0 then
the distribution is symmetrical
if the value of coefficient is positive
then the distribution is positively
skewed another coefficient is negative
then the distribution will be negatively
skewed
now that we have discussed this
symmetrical and skew distribution let's
move on to understand what is criticism
kurtosis is used to find the presence of
outliers in our data it gives us the
total degree of outliers present
the data can be heavily tailed and the
peak can be flatter almost like punching
the distribution or squishing it
while skewness signifies the extent of
the Symmetry kurtosis measures the
degree of weakness of a frequency
distribution
when the peak of the curve becomes
relatively High then the curve is called
leptocurtic
and when the curve is flat it is called
practical tick
when the curve is similar to the normal
curve it is called mesocortic
the expected value of cortices is 3.
this is observed in a symmetrical
distribution a courtes is greater than 3
will indicate positive cortices
so the occurrency is less than 3 will
mean a negative cortices the range of
value for a negative courtesy is from -2
to Infinity
the greater the value of quarters is the
higher is the peak as you can see from
the graph also data scientists looking
for online training and certification
programs from the best universities or a
professional who elects to switch
careers with data science then try
giving simply learns postgraduate
program in data science as short the
link in the description box below should
navigate to the home page where you can
find the complete overview of the
program being offered now over to our
training experts today we're going to be
looking at random Forest one of the many
powerful tools in the machine learning
library before we dive into the topic
let's start by looking at a few of the
uses for random Forest currently today
is used in remote sensing for example
they're used in the etm devices if
you're a space buff that's the enhanced
thematic mapper they use on satellites
which see far outside the human Spectrum
for looking at land masses and they
acquire images of the Earth's surface
the accuracy is higher and training time
is less than many other machine learning
tools out there also object detection
multi-class object detection is done
using random Forest algorithms a good
example is a traffic we try to sort out
the different cars buses and things and
it provides a better detection and
complicated environments it's very
complicated up there and then we have
another example connect and let's take a
little closer look at connect connect
they use a random Forest as part of the
game console and what it does is it
tracks the body movements and it
recreates it in the game and let's see
what that looks like we have a user who
performs a step in this case it looks
like Elvis Presley going there
that is then recorded So that connect
registers the movement and then it marks
the user based on accuracy and it looks
like we have prints going on this one
from Elvis Presley to Prince it's great
so Mark's user base on the accuracy if
we look at that a little closer we have
a training set to identify body parts
where are the hands where the feet
what's going on with the body
that then goes into a random Forest
classifier that learns from it once
we've trained the classifier
and then identifies the body parts while
the person's dancing and it's able to
represent that in a computer format and
then based on that it scores the game
and how accurate you are as being Elvis
Presley or prince in your dancing let's
take an overview of what we're going to
cover today what's in it for you we're
going to start with is what is machine
learning we're not going to go into
detail on that we're going to
specifically look how the random Force
fits in the machine learning hierarchy
then we're going to look at some
applications of random Forest what is
classification which is its primary use
why use random Force what's the benefits
of it and how does it actually come
together what is random forest and then
we'll get into random forest and the
decision tree how that's like the final
step in how it works and finally we'll
get some python code in there and we'll
use the case the iris flower analysis
now if you don't know what any of these
terms mean or where we're going with
this don't worry we're going to cover
all the basics and have you up and
running and even having doing some basic
script in Python by the end let's take a
closer look at types of machine learning
specifically we're going to look at
where the decision tree fits in with the
different machine learning packages out
there we'll start with the basic types
of machine learning there's supervised
learning where you have lots of data and
you're able to train your models there's
unsupervised learning where it has to
look at the data and then divide it
based on its own algorithms without
having any training and then there's
reinforcement learning where you get a
plus or negative if you have the answer
correct this particular tool belongs to
the supervised learning let's take a
closer look at that what that means in
supervised learning supervised learning
falls into two groups classification and
regression we'll talk about regression a
little later and how that differs this
particular format goes underneath
classification so we're looking at
supervised learning and classification
in the machine learning tools
classification is a kind of problem
wherein the outputs are categorical in
nature like yes or no true or false
false or 0 or 1. in that particular
framework there's the k n n where the NN
stands for nearest neighbor Nave Bays
the decision tree which is part of the
random Forest that we're studying today
so why random Forest It's always
important to understand why we use this
tool over the other ones what are the
benefits here and so with the random
Forest the first one is there's no
overfitting if you use of multiple trees
reduce the risk of overfitting training
time is less overfitting means that we
have fit the data so close to what we
have as our sample that we pick up on
all the weird parts and instead of
predicting the overall data you're
predicting the weird stuff which you
don't want high accuracy runs
efficiently on large database for large
data it produces highly accurate
predictions in today's world of Big Data
this is really important and this is
probably where it really shines this is
where why random Forest really really
comes in it estimates missing data data
in today's world is very messy so when
you have a random Forest it can maintain
the accuracy when a large proportion of
the data is missing what that means is
if you have data that comes in from five
or six different areas and maybe they
took one set of Statistics in one area
and they took a slightly different set
of Statistics in the other so they have
some of the same shared data but one is
missing like the number of children in
the house if you're doing something over
demographics and the other one is
missing the size of the house it will
look at both of those separately and
build two different trees and then it
can do a very good job of guessing which
one fits better even though it's missing
that data let us dig deep into the
theory of exactly how it works and let's
look at what is random forests random
forests or random decision Forest is a
method that operates by constructing
multiple decision trees the decision of
the majority of the trees is chosen by
the random Forest as the final decision
and let's uh we have some nice Graphics
here we have a decision tree and they
actually use a real tree to denote the
decision tree which I love and given a
random some kind of picture of a fruit
this decision tree decides that the
output is it's an apple and we have a
decision tree two where we have that
picture of the fruit goes in and this
one decides that it's a limit and the
decision three tree gets another image
and it decides it's an apple and then
this all goes together in what they call
the random forest and this random Forest
then looks at it and says okay I got two
votes for apple one vote for lemon the
majority is Apples so the final decision
is apples to understand how the random
Forest works we first need to dig a
little deeper and take a look at the
random forest and the actual decision
tree and how it builds that decision
Tree in looking closer at how the
individual decision trees work we'll go
ahead and continue to use the fruit
example since we're talking about trees
and forests a decision tree is a tree
shaped diagrammed used to determine a
course of action each branch of the tree
represents a possible decision
occurrence or reaction so in here we
have a bowl of fruit and if you look at
that it looks like they switch from
lemons to oranges we have oranges
cherries and apples and the first
decision of the decision tree might be
is a diameter greater than or equal to
three and if it says false it knows that
they're cherries because everything else
is bigger than that so all the cherries
fall into that decision so we have all
that data we're training we can look at
that we know that that's what's going to
come up is the color orange well goes
hmm orange or red well if it's true then
it comes out as the orange and if it's
false that leaves apples
so in this example it sorts out the
fruit in the bowl or the images of the
fruit a decision tree these are very
important terms to know because these
are very Central to understanding the
decision tree when working with them the
first is entropy everything on the
decision tree and how it makes this
decision is based on entropy entropy is
a measure of Randomness or
unpredictability in the data set then
they also have Information Gain
the leaf node the decision node and the
root node we'll cover these other four
terms as we go down the tree but let's
start with entropy so starting with
entropy we have here a high amount of
Randomness what that means is that
whatever's coming out of this decision
if it was going to guess based on this
data it wouldn't be able to tell you
whether it's a lemon or an apple it
would just say it's a fruit
so the first thing we want to do is we
want to split this apart and we take the
initial data set we're going to create a
data set one and a data set two we just
split it in two and if you look at these
new data sets after splitting them the
entropy of each of those sets is much
less so for the first one whatever comes
in there it's going to sort that data
and it's going to say okay if this data
goes this direction it's probably an
apple and if it goes into the other
direction it's probably a lemon so that
brings us up to Information Gain it is a
measure of decrease in the entropy after
the data set is split what that means in
here is that we've gone from one set
which has a very high entropy to two
lower sets of entropy and we've added in
the values of E1 for the first one and
E2 for the second two which are much
lower and so that information gain is
increased greatly in this example and so
you can find that the information grain
simply equals decision E1 minus E2 as
we're going down our list of definitions
we'll look at the leaf node and the leaf
node carries the classification or the
decision
so we look down here to the leaf node we
finally get to our set one or our set
two when it comes down there and it says
okay this object's gone into set one if
it's gone into set one it's going to be
split by some means and we'll either end
up with apples on the leaf node or a
lemon on the leaf node and on the right
it'll either be an apple or lemons those
Leaf nodes are those final decisions or
classifications that's the definition of
leaf node in here if we're going to have
a final Leaf where we make the decision
we should have a name for the nodes
above it and they call those decision
nodes a decision node decision node has
two or more branches and you can see
here where we have the five apples and
one lemon and in the other case the five
lemons and one apple they have to make a
choice of which tree It Goes Down based
on some kind of measurement or
information given to the tree and that
brings us to our last definition the
root node the top most decision node is
known as the root node and this is where
you have all of your data and you have
your first decision it has to make or
the first split in information
so far we've looked at a very general
image with the fruit being split let's
look and see exactly what that means to
split the data and how do we make those
decisions on there let's go in there and
find out how does a decision tree work
so let's try to understand this and
let's use a simple example and we'll
stay with the fruit we have a bowl of
fruit and so let's create a problem
statement and the problem is we want to
classify the different types of fruits
in the bowl based on different features
the data set in the bowl is looking
quite messy and the entropy is high in
this case so if this ball was our
decision maker it wouldn't know what
choice to make it has so many choices
which one do you pick Apple grapes or
lemons and so we look in here we're
going to start with a training set
so this is our data that we're training
our data with and we have a number of
options here we have the color and under
the color we have red yellow purple we
have a diameter three three one three
three one and we have a label Apple
lemon Grapes apple lemon grapes and how
do we split the data we have to frame
the conditions to split the data in such
a way that the Information Gain is the
highest it's very key to note that we're
looking for the best gain we don't want
to just start sorting out the smallest
piece in there we want to split it the
biggest way we can and so we measure
this decrease in entropy that's what
they call it entropy there's our entropy
after splitting and now we'll try to
choose a condition that gives us the
highest gain we will do that by
splitting the data using each condition
and checking the gain that we get out of
them the conditions that give us the
highest gain will be used to make the
first split so let's take a look at
these different conditions we have color
we have diameter and if we look
underneath that we have a couple
different values we have diameter equals
three color equals yellow red diameter
equals one and when we look at that
you'll see over here we have one two
three four threes that's a pretty hearty
selection so let's say the condition
gives us a maximum gain of three so we
have the most pieces fall into that
range so our first split from our
decision node is we split the data based
on the diameter is it greater than or
equal to three if it's not that's false
it goes into the great bolt and if it's
true it goes into a bowl fold of lemon
and apples the entropy after splitting
has decreased considerably so now we can
make two decisions if you look at
they're very much less chaos going on
there this node has already attained an
entropy value of zero as you can see
there's only one kind of label left for
this Branch so no further splitting is
required for this node however this node
on the right is still requires a split
to decrease the entropy further so we
split the right node further are based
on color if you look at this if I split
it on color that pretty much cuts it
right down the middle it's the only
thing we have left in our choices of
color and diameter too and if the color
is yellow it's going to go to the rifle
and if it's false it's going to go to
the left Bowl so the entropy in this
case is now zero so now we have three
moles with zero entropy there's only one
type of data in each one of those bowls
so we can predict a lemon with a hundred
percent accuracy and we can predict the
Apple also with 100 accuracy along with
our grapes up there so we've looked at
kind of a basic tree in our forest but
what we really want to know is how does
a random Forest work as a whole so to
begin our random Forest classifier let's
say we already have built three trees
and we're going to start with the first
tree that looks like this just like we
did in the example this tree looks at
the diameter if it's greater than or
equal to three it's true otherwise it's
false so one side goes to the smaller
diameter one side goes to larger
diameter and if the color is orange it's
going to go to the right true we're
using oranges now instead of lemons and
if it's red it's going to go to the left
false we build a second tree very
similar but split differently instead of
the first one being split by a diameter
this one when they created it if you
look at that first Bowl it has a lot of
red objects so it says is the color red
because that's going to bring our
entropy down the fastest and so of
course if it's true it goes to the left
if it's false it goes to the right and
then it looks at the shape false or true
and so on and so on and tree three is
the diameter equal to one and it came up
with this because there's a lot of
cherries in this bowl so that would be
the biggest split on there is is the
diameter equal to one that's going to
drop the entropy the quickest and as you
can see it splits it into true if it
goes false and they've added another
category does it grow in the summer and
if it's false it goes off to the left if
it's true it goes off to the right let's
go ahead and bring these three trees you
can see them all in one image so this
would be three completely different
trees categorizing a fruit and let's
take a fruit now let's try this and this
fruit if you look at it we've blackened
it out you can't see the color on it so
it's missing data remember one of the
things we talked about earlier is that a
random Forest works really good if
you're missing data if you're missing
pieces so this fruit has an image but
maybe as a person had a black and white
camera when they took the picture and
we're going to take a look at this and
it's going to have they put the color in
there so ignore the color down there but
the diameter equals three we find out it
grows in the summer equals yes and the
shape is a circle and if you go to the
right you can look at what one of the
decision trees did this is the third one
is the diameter greater than equal to
three is a color orange well it doesn't
really know on this one but if you look
at the value it say true and go to the
right tree two classifies it as cherries
is a color equal red is is a shape a
circle true it is a circle so this would
look at it and say oh that's a cherry
and then we go to the other classifier
and it says is the diameter equal one
well that's false does it grow in the
summer true so it goes down and looks at
as oranges so how does this random
Forest work the first one says it's an
orange the second one said it was a
cherry and the third one says it's an
orange
and you can guess that if you have two
oranges and one says it's a cherry when
you add that all together the majority
of the vote says orange so the answer is
it's classified as an orange even though
we didn't know the color and we're
missing data on it I don't know about
you but I'm getting tired of fruit so
let's switch and I did promise you we'd
start looking at a case example and get
into some python coding today we're
going to use the case the iris flower
analysis
oh this is the exciting part as we roll
up our sleeves and actually look at some
python coating before we start the
python coding we need to go ahead and
create a problem statement wonder what
species of Iris do these flowers belong
to let's try to predict the species of
the flowers using machine learning in
Python let's see how it can be done so
here we begin to go ahead and Implement
our python code and you'll find that the
first half of our implementation is all
about organizing and exploring the data
coming in let's go ahead and take this
first step which is loading the
different modules into Python and let's
go ahead and put that in our favorite
editor whatever your favorite editor is
in this case I'm going to be using the
Anaconda Jupiter notebook which is one
of my favorites certainly there's
notepad plus plus and eclipse and dozens
of others or just even using the python
terminal window any of those will work
just fine to go ahead and explore this
python coding so here we go let's go
ahead and flip over to our Jupiter nope
book and I've already opened up a new
page for Python 3 code and I'm just
going to paste this right in there and
let's take a look and see what we're
bringing into our python the first thing
we're going to do is from the
sklearn.datasets import load Iris now
this isn't the actual data this is just
the module that allows us to bring in
the data the load Iris and the iris is
so popular it's been around since 1936
when Ronald Fisher published a paper on
it and they're measuring the different
parts of the flower and based on those
measurements predicting what kind of
flower it is and then if we're going to
do a random Forest classifier we need to
go ahead and import our random forest
classifier from the sklearn module so
sklearn dot Ensemble import random force
classifier and then we want to bring in
two more modules and these are probably
the most commonly used modules in Python
and data science with any of the other
modules that we bring in and one is
going to be pandas we're going to import
pandas as PD p D is a common term used
for pandas and pandas is basically
creates a data format for us where when
you create a pandas data frame it looks
like an Excel spreadsheet and you'll see
that in a minute when we start digging
deeper into the code panda is just
wonderful because it plays nice with all
the other modules in there and then we
have numpy which is our numbers Python
and the numbers python allows us to do
different mathematical sets on here
we'll see right off the bat we're going
to take our NP and we're going to go
ahead and Seed the randomness with it
with zero so
np.random.seed is seating that as zero
this code doesn't actually show anything
we're going to go ahead and run it
because I need to make sure I have all
those loaded and then let's take a look
at the next module on here the next six
slides including this one are all about
exploring the data remember I told you
half of this is about looking at the
data and getting it all set so let's go
ahead and take this code right here the
script and let's get that over into our
Jupiter notebook and here we go we've
gone ahead and run the Imports and I'm
going to paste the code down here
and let's take a look and see what's
going on the first thing we're doing is
we're actually loading the iris data and
if you remember up here we loaded the
module that tells it how to get the IRS
data now we're actually assigning that
data to the variable Iris and then we're
going to go ahead and use the DF to
Define data frame
and that's going to equal PD and if you
remember that's pandas as PD so that's
our pandas
and Panda data frame and then we're
looking at Iris data and columns equals
Irish feature names
and we're going to do the DF head and
let's run this so you can understand
what's going on here
the first thing you want to notice is
that our DF has created what looks like
an Excel spreadsheet and in this Excel
spreadsheet we have set the columns so
up on the top you can see the four
different columns and then we have the
data iris.data down below it's a little
confusing without knowing where this
data is coming from so let's look at the
bigger picture and I'm going to go print
I'm just going to change this for a
moment and we're going to print all the
virus and see what that looks like
so when I print all a virus I get this
long list of information and you can
scroll through here and see all the
different titles on there
what's important to notice is that first
off there's a brackets at the beginning
so this is a python dictionary
and in a python dictionary you'll have a
key or a label and this label pulls up
whatever information comes after it so
feature names which we actually used
over here under columns is equal to an
array of simple length simple width
petal length petal width these are the
different names they have for the four
different columns and if you scroll down
far enough you'll also see data down
here oh goodness it came up right
towards the top and data is equal to the
different data we're looking at
now there's a lot of other things in
here like Target we're going to be
pulling that up in a minute and there's
also the names the target names which is
further down and we'll show you that
also in a minute let's go ahead and set
that back
to the Head
and this is one of the neat features of
pandas and Panda data frames
is when you do df.head or the panda
dataframe dot head it will print the
first five lines of the data set in
there along with the headers if you have
them in this case we have the column
header set to Iris features and in here
you'll see that we have 0 1 2 3 4 in
Python most arrays always start at zero
so when you look at the first five it's
going to be zero one two three four not
one two three four five so now we've got
our IRS data imported into a data frame
let's take a look at the next piece of
code in here and so in this section here
of the code we're going to take a look
at the Target and let's go ahead and get
this into our notebook this piece of
code so we can discuss it a little bit
more in detail so here we are in our
jupyter notebook I'm going to put the
code in here and before I run it I want
to look at a couple things going on so
we have DF species and this is
interesting because right here you'll
see where I have DF species in Brackets
which is uh the key code for creating
another column and here we have
iris.target now these are both in the
pandas setup on here so in pandas we can
do either one I could have just as
easily done Iris and then in Brackets
Target depending on what I'm working on
both are acceptable let's go ahead and
run this code and see how this changes
and what we've done is we've added the
target from the iris data set as another
column on the end
now what species is this is what we're
trying to predict so we have our data
which tells us the answer for all these
different pieces and then we've added a
column with the answer that way when we
do our final setup we'll have the
ability to program our our neural
network to look for these this different
data and know what a setosa is or a Vera
color which we'll see in just a minute
or virginica those are the three that
are in there and now we're going to add
one more column I know we're organizing
all this data over and over again it's
kind of fun there's a lot of ways to
organize it what's nice about putting
everything onto one data frame is I can
then do a printout and it shows me
exactly what I'm looking at and I'll
show you where you where that's
different where you can alter that and
do it slightly differently but let's go
ahead and put this into our script up to
that now and here we go we're going to
put that down here
and we're going to run that
and let's talk a little bit about what
we're doing now we're exploring data
and one of the challenges is knowing how
good your model is did your model work
and to do this we need to split the data
and we split it into two different parts
they usually call it the training and
the testing and so in here we're going
to go ahead and put that in our database
so you can see it clearly and we've set
it DF remember you can put brackets this
is creating another column is train so
we're going to use part of it for
training and this equals NP remember
that stands for numpy DOT random.uniform
so we're generating a random number
between 0 and 1 and we're going to do it
for each of the rows that's where the
length DF comes from so each row gets a
generated number and if it's less than
0.75 it's true and if it's greater than
0.75 it's false this means we're going
to take 75 percent of the data roughly
because there's a Randomness involved
and we're going to use that to train it
and then the other 25 percent we're
going to hold off to the side and use
that to test it later on on so let's
flip back on over and see what the next
step is so now that we've labeled our
database for which is training and which
is testing let's go ahead and sort that
into two different variables train and
test and let's take this code and let's
bring it into our project and here we go
let's paste it on down here and before I
run this let's just take a quick look at
what's going on here is we have up above
we created remember there's our def dot
head which prints the first five rows
and we've added a column is train at the
end and so we're going to take that
we're going to create two variables
we're going to create two new data
frames one's called train one's called
test 75 percent in train 25 in test
and then to sort that out
we're going to do that by doing DF our
main original data frame with the iris
data in it and if DF is trained equals
true
that's going to go in the train and if
DF is train equals false it goes in the
test and so when I run this
we're going to print out the number in
each one let's see what that looks like
and you'll see that it puts 118 in the
training module and it puts 32 in the
testing module which lets us know that
there was 150 lines of data in here so
if you went and looked at the original
data you could see that there's 150
lines and that's roughly 75 percent in
one and 25 percent for us to test our
model on afterward so let's jump back to
our code and see where this goes in the
next two steps
we want to do one more thing with our
data and let's make it readable to
humans I don't know about you but I hate
looking at zeros and ones so let's start
with the features and let's go ahead and
take those and make those readable to
humans and let's put that in our code
let's see here we go paste it in and
you'll see here we've done a couple very
basic things we know that the columns in
our data frame again this is a panda
thing the DF columns
and we know the first four of them 0 1 2
3 that'd be the first four are going to
be the features or the titles of those
columns and so when I run this
you'll see down here that it creates an
index sepa length sepa width petal
length and petal width and this should
be familiar because if you look up here
here's our column titles going across
and here's the first four
one thing I want you to notice here is
that when you're in a command line
whether it's Jupiter notebook or you're
running command line in the terminal
window if you just put the name of it
it'll print it out this is the same as
doing print
features
and the shorthand is you just put
features in here if you're actually
writing a code
and saving the script and running it by
remote you really need to put the print
in there but for this when I run it
you'll see it gives me the same thing
but for this we want to go ahead and
we'll just leave it as features because
it doesn't really matter and this is one
of the fun thing about jupyter notebooks
is I'm just building the code as we go
and then we need to go ahead and create
the labels for the other part so let's
take a look and see what that for our
final step in prepping our data before
we actually start running the training
and the testing is we're going to go
ahead and convert the species on here
into something the computer understands
so let's put this code into our script
and see where that takes us
all right here we go we've set y equal
to PD dot factorize train species of
zero so let's break this down just a
little bit we have our pandas right here
PD factorize what is factorize doing I'm
going to come back to that in just a
second let's look at what train species
is and why we're looking at the group 0
on there
and let's go up here and here is our
species
remember this on we created this whole
column here for species
and then it has cytosis cytosis cytosis
cytosa and if you scroll down enough
you'd also see virginica and Vera color
we need to convert that into something
the computer understands zeros and ones
so the trained species of zero because
this is in the format of a of an array
of arrays so you have to have the zero
on the end and then species is just that
column factorize goes in there and looks
at the fact that there's only three of
them so when I run this you'll see that
y generates an array that's equal to in
this case it's the training set and it's
zeros ones and twos representing the
three different kinds of flowers we have
so now we have something the computer
understands and we have a nice table
that we can read and understand and now
finally we get to actually start doing
the predicting so here we go we have two
lines of code oh my goodness that was a
lot of work to get to two lines of code
but there is a lot in these two lines of
code so let's take a look and see what's
going on here and put this into our full
script that we're running and let's
paste this in here and let's take a look
and see what this is we have we're
creating a variable clf and we're going
to set this equal to the random forest
classifier and we're passing two
variables in here and there's a lot of
variables you can play with as far as
these two are concerned they're very
standard in jobs all that does is to
prioritize it not something to really
worry about usually when you're doing
this on your own computer you do in jobs
equals two if you're working in a larger
or big data and you need to prioritize
it differently this is what that number
does is it changes your priorities and
how it's going to across the system and
things like that and then the random
state is just how it starts zero is fine
for here
but let's go ahead and run this
we also have clf.fit train features
comma Y and before we run it let's talk
about this a little bit more clf dot fit
so we're fitting we're training it we
are actually creating our random Forest
classifier right here this is a code
that does everything and we're going to
take our training set remember we kept
our test off to the side and we're going
to take our training set with the
features and then we're going to go
ahead and put that in and here's our
Target the Y so the Y is 0 1 and 2 that
we just created and the features is the
actual data going in that we put into
the training set let's go ahead and run
that
and this is kind of an interesting thing
because it printed out the random Force
classifier
and everything around it
and so when you're running this in your
terminal window or in a script like this
this automatically treats this like just
like when we were up here and I typed in
y and it printed out y instead of print
y
this does the same thing it treats this
as a variable and prints it out but if
you're actually running your code that
wouldn't be the case and what is printed
out is it shows us all the different
variables we can change and if we go
down here you can actually see in jobs
equals two
you can see the random State equals zero
those are the two that we sent in there
you would really have to dig deep to
find out all these the different
meanings of all these different settings
on here some of them are
self-explanatory if you kind of think
about it a little bit like Max features
is auto so all the features that we're
putting in there is just going to
automatically take all four of them
whatever we send it it'll take some of
them might have so many features because
you're processing words there might be
like 1.4 million features in there
because you're doing legal documents and
that's how many different words are in
there at that point you probably want to
limit the maximum features that you're
going to process in leaf nodes that's
the end notes remember we had the fruit
and we're talking about the leaf nodes
like I said there's a lot in this we're
looking at a lot of stuff here so you
might have in this case there's probably
only think three leaf nodes maybe four
you might have thousands of leaf nodes
at which point you do need to put a cap
on that and say okay you can only go so
far and then we're going to use all of
our resources on processing this and
that really is what most of these are
about is limiting the process and making
sure we don't overwhelm a system and
there's some other settings in here
again we're not going to go over all of
them warm start equals false alarm start
is if you're programming it one piece at
a time externally since we're not we're
not going to have like we're not going
to continually to train this particular
Learning Tree and again like I said
there's a lot of things in here that
you'll want to look up more detail from
the SK learn and if you're digging in
deep and running a major project on here
for today though all we need to do is
fit or train our features and our Target
y so now we have our training model
what's next if we're going to create a
model
we now need to test it remember we set
aside the test feature test group 25
percent of the data so let's go ahead
and take this code and let's put it into
our script and see what that looks like
okay here we go
and we're going to run this
and it's going to come out with a bunch
of zeros ones and twos which represents
the three type of flowers the Sentosa
the virginica and the Versa color and
what we're putting into our predict is
the test features and I always kind of
like to know what it is I am looking at
so real quick we're going to do test
features and remember features is an
array
of simple length simple width pedal
length pedal width so when we put it in
this way it actually loads all these
different columns that we loaded into
features so if we did just features let
me just do features in here seeing so
what features looks like this is just
playing with the with pandas data frames
you'll see that it's an index so when
you put an index in like this
into test features into test it then
takes those columns and creates a panda
data frames from those columns and in
this case
we're going to go ahead and put those
into our predict
so we're going to put each one of these
lines of data
the 5.0 3.4 1.5.2 and we're going to put
those in and we're going to predict what
our new Forest classifier is going to
come up with and this is what it
predicts it predicts uh zero zero zero
one two one one two two two and and
again this is the flower type sotosa
virginica and Versa color so now that
we've taken our test features let's
explore that let's see exactly what that
data means to us so the first thing we
can do with our predicts is we can
actually generate a different prediction
model when I say different we're going
to view it differently it's not that the
data itself is different so let's take
this next piece of code and put it into
our script
so we're pasting it in here and you'll
see that we're doing uh predict and
we've added underscore proba for
probability so there's our clf DOT
predict probability so we're running it
just like we ran it up here but this
time with this we're going to get a
slightly different result and we're only
going to look at the first 10.
so you'll see down here instead of
looking at all of them which was what 27
you'll see right down here that this
generates a much larger field on the
probability and let's take a look and
see what that looks like and what that
means
so when we do the predict underscore
praba for probability it generates three
numbers so we had three leaf nodes at
the end and if you remember from all the
theory we did this is the predictors the
first one is predicting a one for setosa
it predicts a zero for virginica and it
predicts a zero for Versa color and so
on and so on and so on and let's uh you
know what I'm going to change this just
a little bit let's look at 10
to 20 just because we can
and we start to get a little different
of data and you'll see right down here
it gets to this one this line right here
and this line has 0 0.5 0.5 and so if
we're going to vote and we have two
equal votes it's going to go with the
first one so it says uh satosha gets
zero votes virginica gets 0.5 votes
Versa color gets 0.5 votes but let's
just go with the virginica since these
two are equal and so on and so on down
the list you can see how they vary on
here so now we've looked at both how to
do a basic predict of the features and
we've looked at the predict probability
let's see what's next on here so now we
want to go ahead and start mapping names
for the plants we want to attach names
so that it makes a little more sense for
us and this we're going to do in these
next two steps we're going to start by
setting up our predictions and mapping
them to the name so let's see what that
looks like
and let's go ahead and paste that code
in here and run it and this goes along
with the next piece of code so we'll
skip through this quickly and then come
back to it a little bit so here's Iris
dot Target names
and uh if you remember correctly this
was the the names that we've been
talking about this whole time the setosa
virginica versus color and then we're
going to go ahead and do the prediction
again we've run we could have just hit a
variable equal to this instead of
re-running it each time but we're going
ahead and run it again clf dot predict
test features remember that Returns the
zeros the ones in the twos and then
we're going to set that equal to
predictions so this time we're actually
putting it in a variable and when I run
this
it distributes it it comes out as an
array and the array is setosis cytosis
cytosa we're only looking at the first
five we could actually do let's do the
first 25 just so we can see a little bit
more on there and you'll see that it
starts mapping it to all the different
flower types the Versa color and the
virginica in there and let's see how
this goes with the next one so let's
take a look at the top part of our
species in here and we'll take this code
and put it in our script
and let's put that down here and paste
it there we go and we'll go ahead and
run it and let's talk about both these
sections of code here
and how they go together the first one
is our predictions and I went ahead and
did predictions through 25 let's just do
five
and so we have cytosis cytosis cytosis
cytosis that's what we're predicting
from our test model
and then we come down here we look at
test species I remember I could have
just done
test.species.head and you'll see it says
cytosis cytosis cytosa and they match so
the first one is what our forest is
doing
and the second one is what the actual
data is now is we need to combine these
so that we can understand what that
means we need to know how good our
forest is how good it is at predicting
the features so that's where we come up
to the next step which is lots of fun
we're going to use a single line of code
to combine our predictions and our
actuals so we have a nice chart to look
at and let's go ahead and put that in
our script in our jupyter notebook here
let's see let's go ahead and paste that
in and then I'm going to because I'm on
the jupyter notebook I can do a control
minus you can see the whole line there
there we go resize it and let's take a
look and see what's going on here we're
going to create in pandas remember PD
stands for pandas and we're doing a
cross tab this function takes two sets
of data and creates a chart out of them
so when I run it you'll get a nice chart
down here and we have the predicted
species
so across the top you'll see the Sentosa
versus color virginica and the actual
species cystosa versicolor virginica and
so the way to read this chart and let's
go ahead and take a look on how to read
this chart here when you read this chart
you have setosa where they meet you have
versicolor where they meet and you have
virginica where they meet and they're
meeting where the actual and the
predicted agree so this is the number of
accurate predictions so in this case it
equals 30. if you had 13 plus 5 plus 12
you get 30. and then we notice here
where it says virginica but it was
supposed to be versacolor this is
inaccurate so now we have two two
inaccurate predictions and 30 accurate
predictions so I will say that the model
accuracy is 93 that's just 30 divided by
32 and if we multiply it by a hundred we
can say that it is 93 accurate so we
have a 93 accuracy with our model I did
want to add one more quick thing in here
on our scripting before we wrap it up so
let's flip back on over to my script in
here we're going to take this line of
code from up above I don't know if you
remember it but predicts equals the iris
dot Target underscore names so we're
going to map it to the names
and we're going to run the prediction
and we read it on test features but you
know we're not just testing it we want
to actually deploy it so at this point I
would go ahead and change this and this
is an array of arrays this is really
important when you're running these to
know that
so you need the double brackets and I
could actually create data maybe let's
let's just do two flowers so maybe I'm
processing more data coming in and we'll
put two flowers in here
and then I actually want to see what the
answer is so let's go ahead and type in
preds and print that out and when I run
this
you'll see that I've now predicted two
flowers so maybe I measured in my front
yard as versacolor and versacolor
not surprising since I put the same data
in for each one
this would be the actual end product
going out to be used on data that you
don't know the answer for
what is naive Bayes let's start with a
basic introduction to the Bayes theorem
named after Thomas Bayes from the 1700s
who first coined this in the western
literature naive Bayes classifier works
on the principle of conditional
probability as given by the Bayes
theorem before we move ahead let us go
through some of the simple Concepts in
the probability that we will be using
let us consider the following example of
tossing two coins here we have two
quarters and if we look at all the
different possibilities of what they can
come up as we get that they could come
up as head heads they come up as head
tell tell head and Telltale when doing
the math on probability we usually
denote probability as a p a capital P so
the probability of getting two heads
equals one-fourth you can see in our
data set we have two heads and this
occurs once out of the four
possibilities and then the probability
of at least one tail occurs three
quarters of the time you'll see in three
of the twin tosses we have tails in them
and out of four that's three fourths and
then the probability of the second coin
being head given the first coin is tell
is one half and the probability of
getting two heads given the first coin
is a head is one half we'll demonstrate
that in just a minute and show you how
that math works now when we're doing it
with two coins it's easy to see but when
you have something more complex you can
see where these Pro these formulas
really come in and work so the Bayes
theorem gives us a conditional
probability of an event a given another
event B has occurred in this case the
first coin toss will be B and the second
coin toss a this could be confusing
because we've actually reversed the
order of them and go from B to a instead
of a to B you'll see this a lot when you
work in probabilities the reason is
we're looking for event a we want to
know what that is so we're going to
label that a since that's our focus and
then given another event B has occurred
in the Bayes theorem as you can see on
the left the probability of a occurring
given B has occurred equals the
probability of B occurring given a has
occurred times the probability of a over
the probability of B this simple formula
can be moved around just like any
algebra formula and we could do the
probability of a after a given B times
probability of b equals the probability
of B given a times probability of a you
can easily move that around and multiply
it and divide it out let us apply a
Bayes theorem to our example here we
have our two quarters and we'll notice
that the first two probabilities of
getting two heads and at least one tail
we compute directly off the data so you
can easily see that we have one example
HH out of four one fourth and we have
three with tails in them giving us three
quarters or three four seventy-five
percent the second condition the second
set three and four we're gonna explore a
little bit more in detail now we stick
to a simple example with two coins
because you can easily understand the
math the probability of throwing a tail
doesn't matter what comes before it and
the same with the head so still going to
be fifty percent or one half but when
that com when that probability gets more
complicated let's say you have a D6 dice
or some other instant then this formula
really comes in handy but let's stick to
the simple example for now in this
sample space let a be the event that the
second coin is head and B be the event
that the first coin is tells again we
reversed it because we want to know what
the second event is going to be so we're
going to be focusing on a and we write
that out as a probability of a given B
and we know this from our formula that
that equals the probability of B given a
times the probability of a over the
probability of B and when we plug that
in we plug in the probability of the
first coin being tells given the second
coin is heads and the probability of the
second coin being heads given the first
coin being over the probability of the
first coin being Tails when we plug that
data in and we have the probability of
the first coin being Tails given the
second coin is heads times the
probability of the second coin being
heads over the probability of the first
coin being tails you can see it's a
simple formula to calculate we have one
half times one-half over one-half or
one-half equals 0.5 or 1 4. so the Bayes
theorem basically calculates the
conditional probability of the
occurrence of an event based on prior
knowledge of conditions that might be
related to the event we will explore
this in detail when we take up an
example of online shopping further in
this tutorial understand standing naive
Bayes and machine learning like with any
of our other machine learning tools it's
important to understand where the naive
Bayes fits in the hierarchy so under the
machine learning we have supervised
learning and there is other things like
unsupervised learning there's also
reward system This falls under the
supervised learning and then under the
supervisors learning there's
classification there's also a regression
but we're going to be in the
classification side and then under
classification is your naive Bayes let's
go ahead and glance into where is naive
Bayes used let's look at some of the use
scenarios for it as a classifier we use
it in face recognition is this Cindy or
is it not Cindy or whoever or it might
be used to identify parts of the face
that they then feed into another part of
the face recognition program this is the
eye this is the nose this is the mouth
weather prediction is it going to be
rainy or sunny medical recognition news
prediction it's also used in medical
diagnosis we might diagnose somebody as
either as high risk or not as high risk
for cancer or heart disease disease or
other elements and news classification
we look at the Google news and it says
well is this political or is this world
news or a lot of that's all done with a
naive Bayes understanding naive Bayes
classifier now we already went through a
basic understanding with the coins and
the two heads and two tells and head
tell tale heads Etc we're going to do
just a quick review on that and remind
you that the naive Bayes classifier is
based on the Bayes theorem which gives a
conditional probability of event a given
event B and that's where the probability
of a given b equals the probability of B
given a times probability of a over
probability of B remember this is an
algebraic function so we can move these
different entities around we can
multiply by the probability of B so it
goes to the left hand side and then we
could divide by the probability of a
given B and just as easy come up with a
new formula for the probability of B to
me staring at these algebraic functions
kind of gives me a slight headache
it's a lot better to see if we can
actually understand how this data fits
together in a table and let's go ahead
and start applying it to some actual
data so you can see what that looks like
so we're going to start with the
shopping demo problem statement and
remember we're going to solve this first
in table form so you can see what the
math looks like and then we're going to
solve it in Python and in here we want
to predict whether the person will
purchase a product are they going to buy
or don't buy very important if you're
running a business you want to know how
to maximize your profits or at least
maximize the purchase of the people
coming into your store and we're going
to look at a specific combination of
different variables in this case we're
going to look at the day the discount
and the free delivery and you can see
here under the day we want to know
whether it's on the weekday you know
somebody's working they come in after
work or maybe they don't work weekend
you can see the bright colors coming
down there celebrating not being in work
or holiday and did we offer a discount
that day yes or no did we offer free
delivery that day yes or no and from
this we want to know whether the person
is going to buy based on these traits so
we can maximize them and find out the
best system for getting somebody to come
in and purchase our goods and products
from our store now having a nice visual
is great but we do need to dig into the
data so let's go ahead and take a look
at the data set we have a small sample
data set of 30 rows we're showing you
the first 15 of those rows for this demo
now the actual data file you can request
just type in below under the comments on
the YouTube video and we'll send you
some more information and send you that
file as you can see here the file is
very simple columns and rows we have the
day the discount the free delivery and
did the person purchase or not and then
we have under the day whether it was a
weekday a holiday was it the weekend
this is a pretty simple set of data and
long before computers people used to
look at this data and calculate this all
by hand so let's go ahead and walk
through this and see what that looks
like when we put that into tables also
note in today's world we're not usually
looking at three different variables in
30 rows nowadays because we're able to
collect data so much we're usually
looking at 27 30 variables across
hundreds of rows the first thing we want
to do is we're going to take this data
and based on the data set containing our
three inputs Day discount and free
delivery we're going to go ahead and
populate that to frequency tables for
each attribute so we want to know if
they had a discount how many people buy
and did not buy did they have a discount
yes or no do we have a free delivery yes
or no on those dates how many people
made a purchase how many people didn't
and the same with the three days of the
week was it a weekday a weekend a
holiday and did they buy yes or no as we
dig in deeper to this table for our
Bayes theorem let the event buy ba now
remember we looked at the coins I said
we really want to know what the outcome
is did the person buy or not and that's
usually event a is what you're looking
for and the independent variables
discount free delivery in day BB so
we'll call that probability of B now let
us calculate the likelihood table for
one of the variables let's start with
day which includes weekday weekend and
holiday and let us start by summing all
of our our rows so we have the weekday
row and out of the weekdays there's nine
plus two so it's 11 weekdays there's
eight weekend days and eleven holidays
that's a lot of holidays and then we
want to sum up the total number of days
so we're looking at a total of 30 days
let's start pulling some information
from our chart and see where that takes
us and when we fill in the chart on the
right you can see that 9 out of 24
purchases are made on the weekday 7 out
of 24 purchases on the weekend and 8 out
of 24 purchases on a holiday and out of
all the people who come in 24 out of 30
purchase you can also see how many
people do not purchase on the weekdays
two out of six didn't purchase and so on
and so on we can also look at the totals
and you'll see on the right we put
together some of the formulas the
probability of making a purchase on the
weekend comes out at 11 out of 30. so
out of the 30 people who came into the
store throughout the weekend weekday and
holiday 11 of those purchases were made
on the weekday and then you can also see
the probability of them not making a
purchase and this is done for doesn't
matter which day of the week so we call
that probability of no buy would be 6
over 30 or 0.2 so there's a twenty
percent chance that they're not going to
make a purchase no matter what day of
the week it is and finally we look at
the probability of B of A in this case
we're going to look at the probability
of the weekday and not buying two of the
no buys were done on the weekend out of
the six people who did not make
purchases so when we look at that
probability of the week day without a
purchase is going to be 0.33 or 33
percent let's take a look at this at
different probabilities and based on
this likelihood table let's go ahead and
calculate conditional probabilities as
below the first three we just did the
probability of making a purchase on the
weekday is 11 out of 30 or roughly 36 or
37 percent 0.367 the probability of not
making a purchase at all doesn't matter
what day of the week is roughly 0.2 or
20 percent and the probability of a
weekday no purchase is roughly two out
of six so two out of six of our no
purchases were made on the weekday and
then finally we take our P of a b if you
look we've kept the symbols up there we
got P of probability of B probability of
a probability of B if a we should
remember that the probability of a if B
is equal to the first one times the
probability of no buys over the
probability of the weekday so we could
calculate it both off the table we
created we can also calculate this by
the formula and we get the 0.36 7 which
equals or 0.33 times 0.2 over 0.367
which equals 0.179 or roughly 17 to 18
percent and that'd be the probability of
no purchase done on the weekday and this
is important because we can look at this
and say as the probability of buying on
the weekday is more than the probability
of not buying on the weekday we can
conclude that customers will most likely
buy the product on a weekday now we've
kept our chart simple and we're only
looking at one aspect so you should be
able to look at the table and come up
with the same information or the same
conclusion that should be kind of
intuitive at this point next we can take
the same setup we have the frequency
tables of all three independent
variables now we can construct the
likelihood tables for all three of the
variables we're working with we can take
our day like we did before we have
weekday weekend and holiday and we
filled in this table and then we can
come in and also do that for the
discount yes or no did they buy yes or
no and we fill in that full tail table
so now we have our probabilities for a
discount and whether the discount leads
to a purchase or not and the probability
for free delivery does that lead to a
purchase or not and this is where it
starts getting really exciting let us
use these three likelihood tables to
calculate whether a customer will
purchase a product on a specific
combination of Day discount and free
delivery or not purchase here let us
take a combination of these factors day
equals holiday discount equals yes free
delivery equals yes let's dig deeper
into the math and actually see what this
looks like and we're going to start with
looking for the probability of them not
purchasing on the following combinations
of days we're actually looking for the
probability of a equal no buy no
purchase and our probability of B we're
going to set equal to is it a holiday
did they get a discount yes and was it a
free delivery yes before we go further
let's look at the original equation the
probability of a if B equals the
probability of B given the condition a
and the probability times the
probability of a over the probability of
B occurring now this is basic algebra so
we can multiply this information
together so when you see the probability
of a given B in this case the condition
is b c and d or the three different
variables we're looking at and when you
see the probability of B that would be
the conditions we're actually going to
multiply those three separate conditions
out probability of you'll see that just
a second in the formula times the full
probability of a over the full
probability of B so here we are back to
this and we're going to have let a equal
no purchase and we're looking for the
probability of B on the condition a
where a sets for three different things
remember that equals the probability of
a given the condition B and in this case
we just multiply those three different
variables together so we have the
probability of the discount times the
probability of freedom delivery times
the probability is the day equal holiday
those are our three variables of the
probability of a if B and then that is
going to be multiplied by the
probability of them not making a
purchase and then we want to divide that
by the total probabilities and they're
multiplied together so we have the
probability of a discount the
probability of a free delivery and the
probability of it being on a holiday
when we plug those numbers in we see
that one out of six were no purchase on
a discounted day two out of six or a no
purchase on a free delivery day and
three out of six or a no purchase on a
holiday those are our three
probabilities of a of B multiplied out
and then that has to be multiplied by
the probability of a no purchase and
remember the probability of a no buy is
across all the data so that's where we
get the 6 out of 30. we divide that out
by the probability of each category over
the total number so we get the 20 out of
30 had a discount 23 3 out of 30 had a
yes for free delivery and 11 out of 30
were on a holiday we plug all those
numbers in we get
0.178 so in our probability math we have
a 0.178 if it's a no buy for a holiday a
discount and a free delivery let's turn
that around and see what that looks like
if we have a purchase I promise this is
the last page of math before we dig into
the python script so here we're
calculating the probability of the
purchase using the same math we did to
find out if they didn't buy now we want
to know if they did buy and again we're
going to go by the day equals a holiday
discount equals yes free delivery equals
yes and let a equal buy now right about
now you might be asking why are we doing
both calculations why why would we want
to know the no buys and buys for the
same data going in well we're going to
show you that in just a moment but we
have to have both of those pieces of
information so that we can figure it out
as a percentage as opposed to a
probability equation and we'll get to
that normalization here in just a moment
let's go ahead and walk through this
calculation and as you can see here the
probability of a on the condition of b b
being all three categories did we have a
discount with a purchase do we have a
free delivery with a purchase and did we
is a day equal to Holiday and when we
plug this all into that formula and
multiply it all out we get our
probability of a discount probability of
a free delivery probability of the day
being a holiday times the overall
probability of it being a purchase
divided by again multiplying the three
variables out the full probability of
there being a discount the full
probability of being a free delivery and
the full probability of there being a
day equal holiday and that's where we
get this 19 over 24 times 21 over 24
times 8 over 24 times the P of a 24 over
30 divided by the probability of the
discount the free delivery times a day
or 20 over 30 23 over 30 times 11 over
30 and that gives us our
0.986 so what are we going to do with
these two pieces of data we just
generated well let's go ahead and go
over them we have a probability of
purchase equals 0.986 we have a
probability of no purchase equals
0.178 so finally we have a conditional
probabilities of purchase on this day
let us take that we're going to
normalize it and we're going to take
these probabilities and turn them into
percentages this is simply done by
taking these sum of probabilities which
equals
0.98686 plus 0.178 and that equals the
1.164 if we divide each probability by
the sum we get the percentage and so the
likelihood of a purchase is 84.71
percent and the likelihood of no
purchase is 15.29 percent given these
three different variables so it's if
it's on a holiday if it's a with a
discount and has free delivery then
there's an 84.71 percent chance that the
customer is going to come in and make a
purchase hooray they purchased our stuff
we're making money if you're owning a
shop that's like is the bottom line is
you want to make some money so you keep
your shop open and have a living now I
promised you that we were going to be
finishing up the math here with a few
pages so we're going to move on and
we're going to do two steps the first
step is I want you to understand why you
went under why you want to use the naive
Bayes what are the advantages of naive
bays and then once we understand those
advantages we just look at that briefly
then we're going to dive in and do some
python coding advantages of naive Bayes
classifier so let's take a look at the
six advantages of the naive Bayes
classifier and we're going to walk
around this lovely wheel looks like an
origami folded paper the first one is
very simple and easy to implement
certainly you could walk through the
tables and do this by hand you got to be
a little careful because the notations
can get confusing you have all these
different probabilities and I certainly
mess those up as I put them on you know
is it on the top of the bottom got to
really pay close attention to that when
you put it into python it's really nice
because you don't have to worry about
any of that you let the python handle
that the python module but understanding
it you can put it on a table and you can
easily see how it works and it's a
simple algebraic function it needs less
training data so if you have smaller
amounts of data this is great powerful
tool for that handles both continuous
and discrete data it's highly scalable
with number of predictors and data
points so as you can see just keep
multiplying different probabilities in
there and you can cover not just three
different variables or sets you can now
expand this to even more categories
number five it's fast it can be used in
real time predictions this is so
important this is why it's used in a lot
of our predictions on online shopping
carts referrals spam filters is because
there's no time delay as it has to go
through and figure out a neural network
or one of the other mini setups where
you're doing classification and
certainly there's a lot of other tools
out there in the machine learning that
can handle these but most of them are
not as fast as the naive Bayes and then
finally it's not sensitive to irrelevant
features so it picks up on your
different probabilities and if you're
short on date on one probability you can
kind of it automatically adjust for that
those formulas are very automatic and so
you can still get a very solid
predictability even if you're missing
data or you have overlapping data for
two completely different areas we see
that a lot in doing census and studying
of people and habits where they might
have one study that covers one aspect
another one that overlaps and because of
two overlap they can then predict the
unknowns for the group that they haven't
done the second study on or vice versa
so it's very powerful in that it is not
sensitive to the irrelevant features and
in fact you can use it to help predict
features that aren't even in there so
now we're down to my favorite part we're
going to roll up our sleeves and do some
actual programming we're going to do the
use case text classification now I would
challenge you to go back and send us a
note on the notes below underneath the
video and request the data for the
shopping cart so you can plug that into
python code and do that on your own time
so you can walk through it since we walk
through all the information on it but
we're going to do a python code doing
text classification very popular for
doing the naive Bayes so we're going to
use our new tool to perform a text
classification of news headlines and
classify news into different topics for
a News website as you can see here we
have a nice image of the Google news and
then related on the right subgroups I'm
not sure where they actually pulled the
actual data we're going to use from it's
one of the standard sets but certainly
this can be used on any of our news
headlines and classification so let's
see how it can be done using the naive
Bayes classifier now we're at my
favorite part we're actually going to
write some python script roll up our
sleeves and we're going to start by
doing our Imports these are very basic
Imports including our news group and
we'll take a quick glance at the Target
names then we're going to go ahead and
start training our data set and putting
it together we'll put together a nice
graph because it's always good to have a
graph to show what's going on and once
we've trained it and we've shown you a
graphical what's going on then we're
going to explore how to use it and see
what that looks like now I'm going to
open up my favorite editor or inline
editor for python you don't have to use
this you can use whatever your editor
that you like whatever interface IDE you
want this just happens to be the
Anaconda Jupiter notebook and I'm going
to paste that first piece of code in
here so we can walk through it let's
make it a little bigger on the screen so
you have a nice view of what's going on
and we're using Python 3 in this case
3.5 so this would work in any of your 3x
if you have it set up correctly should
also work in a lot of the 2x you just
have to make sure all of the versions of
the modules match your python version
and in here you'll notice the first line
is your percentage matplot library in
line now three of these lines of code
are all about plotting the graph this
one lets the notebook notes and this is
an inline setup that we want the graphs
to show up on this page without it in a
notebook like this which is an Explorer
interface it won't show up now a lot of
Ides don't require that a lot of them
like on if I'm working on one of my
other setups it just has a pop-up and
the graph pops up on there so you have a
that set up also but for this we want
the matplot library in line and then
we're going to import numpy as NP that's
number python which has a lot of
different formulas in it that we use for
both of our sklearn module and we also
use it for any of the upper math
functions in Python and it's very common
to see that as NP numpy is NP the next
two lines are all about our graphing
remember I said three of these were
about graphing well we need our matplot
library.pi plot as PLT and you'll see
that PLT is a very common setup as is
the SNS and just like the NP and we're
going to import Seaborn as S and S and
we're going to do the sns.set now
Seaborn sits on top of Pi plot and it
just makes a really nice heat map it's
really good for heat maps and if you're
not familiar with heat maps that just
means we give it a color scale term
comes from the brighter red it is the
hotter it is in some form of data and
you can set it to whatever you want and
we'll see that later on so those you'll
see that those three lines of code here
are just importing the graph function so
we can graph it and as a data science
test you always want to graph your data
and have some kind of visual it's really
hard just to shove numbers in front of
people and they look at it and it
doesn't mean anything and then from the
sklearn.data sets we're going to import
the fetch 20 news groups very common one
for analyzing tokenizing words and
setting them up and exploring how the
words work and how do you categorize
different things when you're dealing
with documents and then we set our data
equal to fetch 20 news groups so our
data variable will have the data in it
and we're going to go ahead and just
print the target names data.target names
and let's see what that looks like and
you'll see here we have alt atheism comp
Graphics comp osms windows.miscellaneous
and it goes all the way down to talk
politics.miscellaneous talk
religion.miscellane genius these are the
categories we've already assigned to
this news group and it's called fetch20
because you'll see there's I believe
there's 20 different topics in here or
20 different categories as we scroll
down now we've gone through the 20
different categories and we're going to
go ahead and start defining all the
categories and set up our data so we're
actually going to here going to go ahead
and get it get the data all set up and
take a look at our data and let's move
this over to our Jupiter notebook and
let's see what this code does
first we're going to set our categories
now if you noticed up here I could have
just as easily set this equal to
data.target underscore names because
it's the same thing but we want to kind
of spell it out for you so you can see
the different categories it kind of
makes it more visual so you can see what
your data is looking like in the
background once we've created the
categories
we're going to open up a train set so
this training set of data is going to go
into fetch 20 news groups and it's a
subset in there called train and
categories equals categories so we're
pulling out those categories that match
and then if you have a train set you
should also have the testing set we have
test equals fetch 20 News Group subset
equals test and categories equals
categories let's go down one side so it
all fits on my screen there we go and
just so we can really see what's going
on let's see what happens when we print
out one part of that data so it creates
train and under train it creates
train.data and we're just going to look
at data piece number five and let's go
ahead and run that and see what that
looks like and you can see when I print
train dot data number five under train
it prints out one of the Articles this
is article number five you can go
through and read it on there and we can
also go in here and change this to test
which should look identical because it's
splitting the data up into different
groups train and test and we'll see test
number 5 is a different article but it's
another article in here and maybe you're
curious and you want to see just how
many articles are in here we could do
Links of train dot data and if we run
that you'll see that the training data
has 11
314 articles so we're not going to go
through all those articles that's a lot
of articles but we can look at one of
them just you can see what kind of
information is coming out of it and what
we're looking at and we'll just look at
number five for today and here we have
it rewarding the Second Amendment IDs
vtt line 58 lines 58 in article
Etc and you can scroll all the way down
and see all the different parts to there
now we've looked at it and that's pretty
complicated when you look at one of
these articles to try to figure out how
do you weight this if you look down here
we have different words and maybe the
word from well from is probably in all
the Articles so it's not going to have a
lot of meaning as far as trying to
figure out whether this article fits one
of the categories or not so trying to
figure out which category it fits in
based on these words is where the
challenge comes in now that we've viewed
our data we're going to dive in and do
the actual predictions this is the
actual naive Bayes and we're going to
throw another model at you or another
module at you here in just a second we
can't go into too much detail but it
deals specifically working with words
and text and what they call tokenizing
those words so let's take this code and
let's uh skip on over to our Jupiter
notebook and walk through it and here we
are in our jupyter notebook let's paste
that in there and I can run this code
right off the bat it's not actually
going to display anything yet but it has
a lot going on in here so the top we
have the print module from the earlier
one I didn't know why that was in there
so we're going to start by importing our
necessary packages and from the sklearn
features extraction dot text we're going
to import TF IDF vectorizer I told you
we're going to throw a module at you we
can't go too much into the math behind
this or how it works you can look it up
the notation for the math is usually
tf.idf
and that's just a way of weighing the
words and it weighs the words based on
how many times are used in a document
how many times or how many documents are
used in and it's a well used formula
it's been around for a while it's a
little confusing to put this in here but
let's let them know that it just goes in
there and waits the different words in
the document for us that way we don't
have to wait and if you put a weight on
it if you remember I was talking about
that up here earlier if these are all
emails they probably all have the word
from in them from probably has a very
low weight it has very little value in
telling you what this document's about
same with words like in an article in
articles in cost of on maybe cost might
or where words like criminal weapons
destruction these might have a heavier
weight because we describe a little bit
more what the article is doing well how
do you figure out all those weights in
the different articles that's what this
module does that's what the TF IDF
vectorizer is going to do for us and
then we're going to import our
sklearn.naive Bays and that's our
multinomial in B multinomial naive base
pretty easy to understand that where
that comes from and then finally we have
the skylearn pipeline import make
pipeline now the make pipeline is just a
cool piece of code because we're going
to take the information we get from the
TF IDF vectorizer and we're going to
pump that into the multinomial in B so a
pipeline is just a way of organizing how
things flow it's used commonly you
probably already guessed what it is if
you've done any businesses they talk
about the sales pipeline if you're on a
work crew or project manager you have
your pipeline of information that's
going through or your projects and what
has to be done in what order that's all
this pipeline is we're going to take the
tfid vectorizer and then we're going to
push that into the multinomial NB now
we've designated that as the variable
model we have our pipeline model and
we're going to take that model and this
is just so elegant this is done in just
a couple lines of code model dot fit and
we're going to fit the data and first
the train data and then the train Target
now the train data has the different
articles in it you can see the one we
were just looking at and the train dot
Target is what category they already
categorized that that particular article
is and what's Happening Here is the
trained data is going into the tfid
vectorizer so when you have one of these
articles that goes in there it waits all
the words in there so there's thousands
of words with different weights on them
I remember once running a model on this
and I literally had 2.4 million tokens
go into this so when you're dealing like
large document bases you can have a huge
number of different words it then takes
those words gives them a weight and then
based on that weight based on the words
and the weights and then puts that into
the multinomial in B and once we go into
our naive Bayes we want to put the train
Target in there so the train data that's
been mapped to the tfid vectorizer is
now going through the multinomial in B
and then we're telling it well these are
the answers these are the answers to the
different documents so this document
that has all these words with these
different weights from the first part is
going to be whatever category it comes
out of maybe it's the talk show or the
article on religion miscellaneous once
we fit that model we can then take
labels and we're going to set that equal
to model dot predict most of the sklearn
use the term dot predict to let us know
that we've now trained the model and now
we want to get some answers and we're
going to put our test data in there
because our test data is the stuff we
held off to the side we didn't train it
on there and we don't know what's going
to come up out of it and we just want to
find out how good our labels are do they
match what they should be now I've
already read this through there's no
actual output to it to show this is just
setting it all up this is just training
our model creating the labels so we can
see how good it is and then we move on
to the next step to find out what
happened to do this we're going to go
ahead and create a confusion Matrix and
a heat map so the confusion Matrix which
is confusing just by its very name is
basically going to ask how confused is
our answer did it get it correct or did
it Miss some things in there or have
some missed labels and then we're going
to put that on a heat map so we'll have
some nice colors to look at to see how
that plots out let's go ahead and take
this code and see how that take a walk
through it and see what that looks like
so back to our jupyter notebook I'm
going to put the code in there and let's
go ahead and run that code take it just
a moment and remember we had the in line
that way my graph shows up on the inline
here and let's walk through the code and
then we'll look at this and see what
that means so make it a little bit
bigger there we go no reason not to use
the whole screen too big so we have here
from sklearnmetrics import confusion
Matrix
and that's just going to generate a set
of data that says I the prediction was
such the actual truth was either agreed
with it or is something different and
it's going to add up those numbers so we
can take a look and just see how well it
worked and we're going to set a variable
matte equal to confusion Matrix and we
have our test Target our test data that
was not part of the training very
important in data science we always keep
our test data separate otherwise it's
not a valid model if we can't properly
test it with new data and this is the
labels we created from that test data
these are the ones that we predict it's
going to be so we go in we create our SN
heat map the SNS is our Seaborn which
sits on top of the pi plot so we create
a sns.heat map we take our confusion
Matrix and it's going to be matte dot T
and do we have other variables that go
into the sns.heat map we're not going to
go into detail what all the variables
mean The annotation equals true that's
what tells it to put the numbers here so
you have the 166 the one the zero zero
zero one format d and c bar equals false
have to do with the format if you take
those out you'll see that some things
disappear and then the X tick labels and
the y t labels those are our Target
names and you can see right here that's
the alt atheism comp graphics composms
windows.miscellaneous and then finally
we have our plt.x label remember the SNS
or the Seaborn sits on top of our
matplot library our PLT and so we want
to just tell it X label equals a true is
is true the labels are true and then the
Y label is prediction label so when we
say a true this is what it actually is
and the prediction is what we predicted
and let's look at this graph because
that's probably a little confusing the
way we rattled through it and what I'm
going to do is I'm going to go ahead and
flip back to the slides because they
have a black background they put in
there that helps it shine a little bit
better so you can see the graph a little
bit easier so in reading this graph what
we want to look at is how the color
scheme has come out and you'll see a
line right down the middle diagonally
from upper left to bottom right what
that is is if you look at the labels we
have our predicted label on the left and
our true label on the right those are
the numbers where the prediction and the
true come together and this is what we
want to see is we want to see those lit
up that's what that heat map does is you
can see that it did a good job of
finding those data and you'll notice
that there's a couple of red spots on
there where it missed you know it's a
little confused we talk about talk
religion miscellaneous versus talk
politics miscellaneous social religion
Christian versus Alt atheism it
mislabeled some of those and those are
very similar topics you could understand
why it might mislabel them but overall
it did a pretty good job if we're going
to create these models we want to go
ahead and be able to use them so let's
see what that looks like to do this
let's go ahead and create a definition a
function to run and we're going to call
this function let me just expand that
just a notch here there we go I like
mine in big letters predict categories
we want to predict the category we're
going to send it as a string and then
we're sending it train equals train we
have our training model and then we had
our pipeline model equals model this way
we don't have to resend these variables
each time the definition knows that
because I said train equals train and I
put the equal for model and then we're
going to set the prediction equal to the
model dot predict s so it's going to
send whatever string we send to it it's
going to push that string through the
pipeline the model pipeline it's going
to go through and tokenize it and put it
through the TF IDF convert that into
numbers and weights for all the
different documents and words and then
I'll put that through our naive Bayes
and from it we'll go ahead and get our
prediction we're going to predict what
value it is and so we're going to return
train.target namespredict of zero and
remember that the train.target names
that's just categories I could have just
as easily put categories in there dot
predict of zero so we're taking the
prediction which is a number and we're
converting it to an actual category
we're converting it from I don't know
what the actual number numbers are let's
say 0 equals alt atheism so we're going
to convert that 0 to the word or one
maybe it equals comp Graphics so we're
going to convert number one into comp
Graphics that's all that is and then we
got to go ahead and and then we need to
go ahead and run this so I load that up
and then once I run that we can start
doing some predictions I'm going to go
ahead and type in predict category and
let's just do predict category Jesus
Christ and it comes back and says it's
social religion Christian that's pretty
good now note I didn't put print on this
one of the nice things about the Jupiter
notebook editor and a lot of inline
editors is if you just put the name of
the variable out as returning the
variable train.target underscore names
it'll automatically print that for you
in your own IDE you might have to put in
print let's see where else we can take
this and maybe you're a space science
buff so how about sending load to
International
Space Station
and if we run that we get science space
or maybe you're a automobile buff and
let's do um oh they were going to tell
me Audi is better than BMW but I'm going
to do BMW is better than an Audi so
maybe you're a car buff and we run that
and you'll see it says recreational I'm
assuming that's what Rec stands for
Autos so I did a pretty good job
labeling that one how about uh if we
have something like a caption running
through there president of India and if
we run that it comes up and says talk
politics miscellaneous
so when we take our definition or our
function and we run all these things
through Kudos we made it we were able to
correctly classify text into different
groups based on which category they
belong to using the naive Bayes
classifier now we did throw in the
pipeline the TF IDF vectorizer we threw
in the graphs those are all things that
you don't necessarily have to know to
understand the naive Bayes setup or
classifier but they're important to know
one of the main uses for the naive Bayes
is with the TF IDF tokenizer vectorizer
where it tokenizes a word and as labels
and we use the pipeline because you need
to push all that data through and it
makes it really easy and fast you don't
have to know those to understand naive
Bayes but they certainly help for
understanding the industry in data
science and we can see their categorizer
our naive Bayes classifier we were able
to predict the category religion space
motorcycles Autos politics and properly
classify all these different things we
pushed into our prediction and our
trained model before we dive into the
svm let's take a look at applications of
the support Vector machine at least some
general ones that are commonly used with
it face detection text and hypertext
categorization classification of images
and bioinformatics these are only but a
few of those that are used with this svm
as we go through this lesson see if we
can figure out what other ones you could
apply it to and also what you would want
to use some other tools for what's in it
for you today we're going to cover about
six different sections we're going to
start with what is machine learning so
we can see where the vector machine fits
in why the support Vector machine what
is a support Vector machine and
understanding the support Vector machine
once we go through an understanding of
how it works and what it looks like
we're going to look at the advantages of
support Vector machine and finally dive
into a use case in Python where we'll
write some script on it let's start with
what is machine learning an even more
specific where does the support Vector
machine algorithm fit in the biggest
split in machine learning is between the
three categories supervised learning
where we have data and we can train it
unsupervised learning where we're just
guessing at what we can make sense of
the data and reinforcement learning
where we reinforce a good or bad
behavior since the svm is specific to
supervised learning machine learning
model learns from the past input data
and makes future predictions as output
so we teach the model we teach it what a
strawberry is and once the model is
trained it can identify a strawberry
that's what's mean by supervised
learning in the larger picture of the
machine learning model and under
supervised learning you can see that the
support Vector fits in under
classification deciding what yes and no
is and there is also a regression
version but it is primarily used for
classification let's take a detour and
see if we can connect this to The Human
Experience and find out why support
Vector machine so in this example last
week my son and I visited a fruit shop
dad is that an apple or a strawberry so
the question comes up what fruit do they
just pick up from the Fruit Stand after
a couple of seconds you could figure out
that it was a strawberry so let's take
this model a step further and let's uh
why not build a model which can predict
an unknown data and in this we're going
to be looking at some sweet strawberries
or crispy apples we want it to be able
to label those two and decide what the
fruit is and we do that by having data
already put in so we already have a
bunch of strawberries we know our
strawberries and they're already labeled
as such we already have a bunch of
apples we know our apples and are
labeled as such then once we train our
model that model then can be given the
new data and the new data is this image
in this case you can see a question mark
on it and it comes through and goes it's
a strawberry in this case we're using
the support Vector Machine model svm is
a supervised learning method that looks
at data and sorts it into one of two
categories and in this case we're
sorting the strawberry into the
strawberry site at this point you should
be asking the question how does the
prediction work before we dig into an
example with numbers let's let's apply
this to our fruit scenario we have our
support Vector machine we've taken it
and we've taken labeled sample of data
strawberries and apples and we draw on a
line down the middle between the two
groups this split now allows us to take
new data in this case an apple and a
strawberry and place them in the
appropriate group based on which side of
the line they fall in and that way we
can predict the unknown as colorful and
tasty as the food example is let's take
a look at another example with some
numbers involved and we can take a
closer look at how the math Works in
this example we're going to be
classifying men and women and we're
going to start with a set of people with
a different height and a different
weight and to make this work we'll have
to have a sample data set a female where
you have their height and weight 174 65
174 88 and so on and we'll need a sample
data set of the male they have a height
179 90 180 to 80 and so on let's go
ahead and put this on a graph so we have
a nice visual so you can see here we
have two groups based on the height
versus the way weight and on the left
side we're going to have the women on
the right side we're going to have the
men now if we're going to create a
classifier let's add a new data point
and figure out if it's male or female so
before we can do that we need to split
our data first we can split our data by
choosing any of these lines in this case
we draw in two lines through the data in
the middle that separates the men from
the women but to predict the gender of a
new data point we should split the data
in the best possible way and we say the
best possible way because this line has
a maximum space that separates the two
classes here you can see there's a clear
split between the two different classes
and in this one there's not so much a
clear split this doesn't have the
maximum space it separates the two that
is why this line best splits the data we
don't want to just do this by eyeballing
it and before we go further we need to
add some technical terms to this we can
also say that the distance between the
points and the line should be as far as
possible in technical terms we can can
say the distance between the support
vector and the hyperplane should be as
far as possible and this is where the
support vectors are the extreme points
in the data set and if you look at this
data set they have circled two points
which seem to be right on the outskirts
of the woman and one on the outskirts of
the men and hyperplane has a maximum
distance to the support vectors of any
class now you'll see the line down the
middle and we call this the hyperplane
because when you're dealing with
multiple Dimensions it's really not just
a line but a plane of intersections and
you can see here where the support
vectors have been drawn in dashed lines
the math behind this is very simple we
take D plus the shortest distance to the
closest positive point which would be on
the Min side and D minus is the shortest
distance to the closest negative point
which is on the women's side the sum of
D plus and D minus is called the
distance margin or the distance between
the two support vectors that are shown
in the dashed lines and then by finding
the largest distance margin we can get
the optimal hyperplane once we've
created an optimal hyperplane we can
easily see which side the new data fits
in and based on the hyperplane we can
say the new data point belongs to the
male gender hopefully that's clear how
that works on a visual level as a data
scientist you should also be asking what
happens if the hyperplane is not optimal
if we select a hyperplane having low
margin then there is a high chance of
misclassification this particular svm
model the one we discussed so far is
also called referred to as the ls VM so
far so clear but a question should be
coming up we have our sample data set
but instead of looking like this what if
it looked like this where we have two
sets of data but one of them occurs in
the middle of another set you can see
here where we have the blue and the
yellow and then blue again on the other
side of our data line in this data set
we can't use a hyperplane so when you
see data like this it's necessary to
move away from a 1D view of the data to
a two dimensional view of the data and
for the transformation we use what's
called a kernel function the kernel
function will take the 1D input and
transfer it to a two-dimensional output
as you can see in this picture here the
1D when transferred to a two-dimensional
makes it very easy to draw a line
between the two data sets what if we
make it even more complicated how do we
perform an svm for this type of data set
here you can see we have a
two-dimensional data set where the data
is in the middle surrounded by the green
data on the outside in this case we're
going to segregate the two classes we
have our sample data set and if you draw
a line through it's obviously not an
optimal hyperplane in there so to do
that we need to transfer the 2D to a 3D
array and when you translate it into a
three-dimensional array using the kernel
you can see where you can place a
hyperplane right through it and easily
split the data before we start looking
at a programming example and dive into
the script let's look at the advantage
of the support Vector machine we'll
start with high dimensional input space
space or sometimes referred to as the
curse of dimensionality we looked at
earlier one dimension two Dimension
three dimension when you get to a
thousand dimensions a lot of problems
start occurring with most algorithms
that have to be adjusted for the svm
automatically does that in high
dimensional space one of the high
dimensional space one high dimensional
space that we work on is sparse document
vectors this is where we tokenize the
words and documents so we can run our
machine learning algorithms over them
I've seen ones get as high as 2.4
million different tokens that's a lot of
vectors to look at and finally we have
regularization parameter the realization
parameter or Lambda is a parameter that
helps figure out whether we're going to
have a bias or overfitting of the data
whether it's going to be overfitted to a
very specific instance or it's going to
be biased to a high or low value with
the svm it naturally avoids the
overfitting and bias problems that we
see in many other algorithms these three
advantages of the support Vector machine
make it a very powerful tool to add add
to your repertoire of machine learning
tools now we did promise you a used case
study we're actually going to dive into
some Python Programming and so we're
going to go into a problem statement and
start off with the zoo so in the zoo
example we have family members going to
the zoo we have the young child going
dad is that a group of crocodiles or
alligators well that's hard to
differentiate and zoos are a great place
to start looking at science and
understanding how things work especially
as a young child and so we can see the
parents sitting here thinking well what
is the difference between a crocodile
and an alligator well one crocodiles are
larger in size alligators are smaller in
size snout width the crocodiles have a
narrow snout and alligators have a wider
snout and of course in the modern day
and age the father's hitting there is
thinking how can I turn this into a
lesson for my son and he goes let a
support Vector machine segregate the two
groups I don't know if my dad ever told
me that but that would be funny now in
this example we're not going to use
actual measurements in data we're just
using that for imagery and that's very
common in a lot of machine learning
learning algorithms and setting them up
but let's roll up our sleeves and we'll
talk about that more in just a moment as
we break into our python script so here
we arrive in our actual coding and I'm
going to move this into a python editor
in just a moment but let's talk a little
bit about what we're going to cover
first we're going to cover in the code
the setup how to actually create our svm
and you're going to find that there's
only two lines of code that actually
create it and the rest of it is done so
quick and fast that it's all here in the
first page and we'll show you what that
looks like as far as our data because
we're going to create some data I talked
about creating data just a minute ago
and so we'll get into the creating data
here and you'll see this nice correction
of our two blobs and we'll go through
that in just a second and then the
second part is we're going to take this
and we're going to bump it up a notch
we're going to show you what it looks
like behind the scenes but let's start
with actually creating our setup I like
to use the Anaconda Jupiter notebook
because it's very easy to use but you
can use any of your favorite python
editors or setups and go in there but
let's go ahead and switch over there and
see what that looks like so here we are
in the Anaconda python notebook or
anaconda Jupiter notebook with python
we're using python3 I believe this is
3.5 but it should be work in any of your
3x versions and you'd have to look at
the SK learn and make sure if you're
using a 2X version an earlier version so
let's go and put our code in there and
one of the things I like about the
Jupiter notebook is I go up to view and
I'm going to go ahead and toggle the
line numbers on to make it a little bit
easier to talk about and we can even
increase the size because this is edited
in in this case I'm using Google Chrome
Explorer and that's how it opens up for
the editor although anyone any like I
said any editor will work now the first
step is going to be our Imports and
we're going to import four different
parts the first two I want you to look
at are line one and line two are numpy
as NP and
matplotlibrary.pi plot as PLT now these
are very standardized Imports when
you're doing work the first one is the
numbers python we need that because part
of the platform we're using uses that
for the numpy array and I'll talk about
that in a minute so you can understand
why we want to use a numpy array versus
the standard python array and normally
it's pretty standard setup to use NP for
numpy the map plot library is how we're
going to view our data so this has you
do need the NP for the sklearn module
but the matplot library is purely for
our use for visualization and so you
really don't need that for the svm but
we're going to put it there so you have
a nice visual aid and we can show you
what it looks like that's really
important at the end when you finish
everything so you have a nice display
for everybody to look at and then
finally we're going to I'm going to jump
one ahead to line number four that's the
sklearn.datasets.samples generator
import and make blobs and I told you
that we were going to make up data and
this is a tool that's in the SK learning
makeup data I personally don't want to
go to the zoo get in trouble for jumping
over the fence and probably get eaten by
the crocodiles or alligators as I work
on measuring their snouts and width and
length instead we're just going to make
up some data and that's what that make
blobs is It's a Wonderful tool if you're
ready to test your your setup and you're
not sure about what data you're going to
put in there you can create this blob
and it makes it really easy to use and
finally we have our actual svm the
sklearn import svm on line three so that
covers all our Imports we're going to
create remember I used the make blobs to
create data and we're going to create a
capital x and a lowercase y equals make
blobs in samples equals 40. so we're
going to make 40 lines of data it's
going to have two centers with a random
State equals 20s which each each group
is going to have 20 different pieces of
data in it and the way that lux is it
will have under X an X Y plane so I'll
have two numbers under X and Y will be 0
1. that's the two different centers so
we have yes or no in this case alligator
a crocodile that's what that represents
and then I told you that the actual SK
learner the svm is in two lines of code
and we see it right here with clf equals
s
svm.svc kernel equals linear and I set
SQL to 1 although in this example since
we are not regularizing the data because
we want to be very clear and easy to see
I went ahead you can set it to a
thousand a lot of times when you're not
doing that but for this thing linear
because it's a very simple linear
example we only have the two dimensions
and it'll be a nice linear hyper plane
it'll be a nice linear line instead of a
full plane so we're not dealing with a
huge amount of data and then all we have
to do is do clf dot fit X comma Y and
that's it clf has been created and then
we're going to go ahead and display it
and I'm going to talk about this display
here in just a second but let me go
ahead and run this code and this is what
we've done is we've created two blobs
you'll see the blue on the side and then
kind of an orangish on the other side
that's our two sets of data they
represent one represents crocodiles and
one represents alligators and then we
have our measurements in this case we
have like the width and length of the
snout and I did say I was going to come
up here and talk just a little bit about
our plot and you'll see PLT that's what
we imported we're going to do a scatter
plot that means we're just putting dots
on there and then look at this notation
I have the capital x and then in
brackets I have a colon comma 0. that's
from numpy if you did that in a regular
array you'll get an error in a python
array you have to have that in a numpy
array it turns out that our make blobs
returns a numpy array and this notation
is great because what it means is the
first part is the colon means we're
going to do all the rows that's all the
data in our blob we created under
capital x and then the second part has a
comma zero we're only going to take the
first value and then if you notice we do
the same thing but we're going to take
the second value remember we always
start with 0 and then one so we have
column 0 and column one and you can look
at this as our X Y plots the first one
is the X plot and the second one is the
Y plot so the first one is on the bottom
0 2 4 6 8 and 10 and then the second one
X of the one is the 4 five six seven
eight nine ten going up the left hand
side s equals Thirty is just the size of
the dot so we can see them instead of
real tiny dots and then the C map equals
plt.cm dot paired and you'll also see
the C equals y That's the color we're
using two colors zero one and that's why
we get the nice blue and the two
different colors for the alligator and
the crocodile now you can see here that
we did this the actual fit was done in
two lines of code a lot of times
there'll be a third line where we
regularize the data we set it between
like minus one and one and we reshape it
but for this it's not necessary and it's
also kind of nice because you can
actually see what's going on and then if
we wanted to we wanted to actually run a
prediction let's take a look and see
what that looks like and to predict some
new data and we'll show this again as we
get towards the end of digging in deep
you can simply assign your new data in
this case I am giving it a width and
length three four and a with the length
five six and note that I put the data as
a set of brackets and then I have the
brackets inside and the reason I do that
is because when we're looking at data
it's designed to process a large amount
of data coming in we don't want to just
process one line at a time and so in
this case I'm processing two lines and
then I'm just going to print and you'll
see clf.predict new data so the clf and
the dot predict part is going to give us
an answer and let's see what that looks
like and you'll see 0 1 so predicted the
first one the 3 4 is going to be on the
one side and the 5 6 is going to be on
the other side so one came out as an
alligator and one came out as a
crocodile now that's pretty short
explanation for this setup but really we
want to dug in and see what's going on
behind the scenes and let's see what
that looks like so the next step is to
dig in deep and find out what's going on
behind the scenes and also put that in a
nice pretty graph we're going to spend
more work on this and we did actually
generating the original model and you'll
see here that we go through a few steps
and I'll move this over to our editor in
just a a second we come in we create our
original data it's exactly identical to
the first part and I'll explain why we
redid that and show you how not to redo
that and then we're going to go in there
and add in those lines we're going to
see what those lines look like and how
to set those up and finally we're going
to plot all that on here and show it and
you'll get a nice graph with the what we
saw earlier when we were going through
the theory behind this where it shows
the support vectors and the hyper plane
and those are done where you can see the
support vectors as the dashed lines and
the solid line which is the hyperplane
let's get that into our Jupiter notebook
before I scroll down to a new line I
want you to notice line 13 it has Plot
show and we're going to talk about that
here in just a second but let's scroll
down to a new line down here and I'm
going to paste that code in and you'll
see that the plot show has moved down
below let's scroll up a little bit and
if you look at the top here of our new
section one two three and four is the
same code we had before and that's go
back up here and take a look at that
we're going to fit the values on our svm
and then we're going to plot scatter it
and then we're going to do a plot show
so you should be asking why are we
redoing the same code well when you do
the plot show that blanks out what's in
the plot so once I've done this Plot
show I have to reload that data now we
could do this simply by removing it up
here re-running it and then coming down
here and then we wouldn't have to rerun
these first four lines of code now in
this it doesn't matter too much and
you'll see the plot show is down here
and then removed right there on line
five I'll go ahead and just delete that
out of there because we don't want to
blank out our screen we want to move on
to the next setup so we can go ahead and
just skip the first four lines because
we did that before and let's take a look
at the ax equals
plt.gca now right now we're actually
spending a lot of time just graphing
that's all we're doing here okay so this
is how we display a nice graph with our
results and our data ax is very standard
note used variable when you talk about
PLT and it's just setting it to that
axis the last axis in the PLT they can
get very confusing if you're working
with many different layers of data on
the same graph and this makes a very
easy to reference the ax so this
reference is looking at the PLT that we
created and we already mapped out our
two blobs on and then we want to know
the limits so we want to know how big
the graph is we can find out the X limit
and the Y limit simply with the get X
limit and get y limit commands which is
part of our matplot library and then
we're going to create a grid and you'll
see down here we have we've set the
variable XX equal to
mp.linespace X limit 0 x limit 1 comma
30 and we've done the same thing for the
y space and then we're going to go in
here and we create a mesh grid and this
is a numpy command so we're back to our
numbers python let's go through what
these numpy commands mean with the line
space and the mesh grid we've taken XX
small s XX equals NP line space and we
have our X limit zero and our X limit 1
and we're going to create 30 points on
it and we're going to do the same thing
for the y-axis now this has nothing to
do with our evaluation it's all we're
doing is we're creating a grid of data
and so we're creating a set of points
between 0 and the X limit we're creating
30 points and the same thing with the Y
and then the mesh grid Loops those all
together so it forms a nice grid so if
we were going to do this say between the
limit 0 and 10 and do 10 points we would
have a 0 0 1 1 0 1 0 2 0 3 0 4 to 10 and
so on you can just imagine a point at
Each corner of one of those boxes and
the mesh grid combines them all so we
take the YY and the XX we created and
creates the full grid and we've set that
grid into the YY coordinates and the x x
coordinates now remember we're working
with numbi and python we like to
separate those we like to have instead
of it being X comma 1 you know X comma y
I and then X2 comma Y2 and this in the
next set of data it would be a column of
x's and a column of y's and that's what
we have here is we have a column of y's
we put it as a capital y y and a column
of x's capital x x with all those
different points being listed and
finally we get down to the numpy v stack
just as we created those in the mesh
grid we're now going to put them all
into one array X Y array now that we've
created the stack of data points we're
going to do something interesting here
we're going to create a value Z and the
Z equals the clf that's our that's our
support Vector machine we created and
we've already trained and we have a DOT
decision function and we're going to put
the X Y in there so here we have all
this data we're going to put that X Y in
there that data and we're going to
reshape it and you'll see that we have
the XX dot shape in here this literally
takes the XX resets it up connected to
the Y and the Z value lets us know know
whether it is the left hand side it's
going to generate three different values
the Z value does and it'll tell us
whether that data is a support Vector to
the left the hyperplane in the middle or
the support Vector to the right so it
generates three different values for
each of those points and those points
have been reshaped so they're right on a
line on those three different lines so
we've set all of our data up we've
labeled it to three different areas and
we've reshaped it and we've just taken
30 points in each direction if you do
the math you have 30 times 30 so that's
900 points of data and we separate it
between the three lines and reshaped it
to fit those three lines we can then go
back to our matplot library where we've
created the ax and we're going to create
a contour and you'll see here we have
Contour Capital XX capital y y these
have been reshaped to fit those lines Z
is the labels so now we have the three
different points with the labels in
there and we can set the colors equals K
and I told you we had three different
labels but we have three levels of data
the alpha fuzz just makes it kind of
see-through so it's only 0.5 of the
value in there so when we graph it the
data will show up from behind it
wherever the lines go and finally the
line Styles this is where we set the two
support vectors to be Dash dashed lines
and then a single one is just a straight
line that's what all that setup does and
then finally we take our ax dot scatter
we're going to go ahead and plot the
support vectors but we've programmed it
in there so that they look nice like the
dash dashed line and the dashed line on
that grid and you can see here when we
do the clf Dot support vectors we are
looking at column 0 and column one and
then again we have the S equals 100 so
we're going to make them larger and the
line width equals one face colors equals
none let's take a look and see what that
looks like when we show it and you can
see we get down to our end result it
creates a really nice graph we have our
two support vectors and dashed lines and
they have the near data so you can see
those two points or in this case the 4
Four Points where those lines nicely
cleave the data and then you have your
hyperplane down the middle which is as
far from the two different points as
possible creating the maximum distance
so you can see that we have our nice
output for the size of the body and the
width of the snout and we've easily
separated the two groups of crocodile
and alligator congratulations you've
done it we've made it of course these
are pretend data for our crocodiles and
alligators but this Hands-On example
will help you to encounter any support
Vector machine projects in the future
and you can see how easy they are to set
up and look at in depth also data
scientists looking for online training
and certification programs from the best
universities or a professional who else
to switch careers with data science then
try giving simply loans post graduate
program and data science as short the
link in the description box below should
navigate to the home page where you can
find the complete overview of the
program being offered now over to our
training experts let's get started what
is probability
probability is a branch of mathematics
concerning numerical descriptions of How
likely an event is to occur or How
likely it is Data preparation is true
the probability of an event is a number
between 0 and 1.
we're roughly speaking 0 indicates the
impossibility of the event and when
indicates certainty
the higher the probability of an event
the more likely is that the event will
occur
let's look at an example
a simple example is the tossing of a
fair unbiased coin
since the coin is fair the outcome that
is heads and the tails are both equally
probable
the probability of heads equals the
probability of the Tails and since no
other outcomes are possible the
probability of either heads or tails can
be said to be one by two which is also
fifty percent the probability of an
event can be calculated by number of
ways it can happen divided by the total
number of outcomes
now that we know about the probability
let's see if you can answer this
question what is the probability of
drawing a Jack and a queen consecutively
from a deck of 52 cards without
replacement
hair added options
post your answers in the comment section
and let us know
let's move on to conditional probability
let A and B be the two events associated
with a random experiment
then the probability of is occurrence
under the condition that B has already
occurred and probability of B is not
equal to zero is called the conditional
probability
it is denoted by P a slash B
this we can say that p a slash B is
equal to p a intersection B divided by P
of B where p a slash B is the
probability of occurrence of a given
that B has already occurred and PB is
the probability of occurrence of B
to know more about conditional
probability you can check our previous
video which is specifically on condition
probability
now let's move on to base theorem so
the base theorem is a mathematical
formula for calculating condition
probability in probability and
statistics
in other words it is used to figure out
how likely an event is associated on its
proximity to another
base law or base rule are the other
names of this theorem the formula for
the base theorem can be written in a
variety of ways
the most common version is p a slash B
is equal to P of B A into P of a divided
by P of B
where p a slash B is the conditional
probability of event a occurring given
that b is true and P A and B of B are
the probabilities of A and B occurring
independently of one another
let's solve a problem using the base
theorem to understand it better
there is a cricket match tomorrow and in
recent years it has rained only 5 days
each year
unfortunately the meteorologist has
predicted the rain for tomorrow
now when it rains the meteorologists
correctly forecasts rain 90 of the time
and when it doesn't rain he incorrectly
forecast rain 10 of the time let's
calculate what is the probability that
it will rain on the match day
so the two sample spaces here are the
events that it rains and it is not
rained additionally a third event is
also there that meteorologists predicts
the rain so the notation for these
events appear below
event A1 is equal to it rains in the
match Day event A2 that it does not rain
on the match day and event B is the
meteorologist predicting the rain now in
terms of probability means of the
following
probability of A1 is 5 by 365 that it
rains five days in a year which will
come out to be 0.0136
pa2 is 360 by 365. that is no days for
360 days in an year which will come out
to be 0.986
p b slash A1 is 0.9
this signifies when it rains the
meteorologist predicts that a 90 of the
time
in a similar manner p b by A2 is 0.1
that it does not train the meteorologist
predicts the rain 10 percent of the time
combining all this we can calculate p a
1 slash B that is the probability It
Will Rain on the given Match Day given a
forecast of Rain by meteorologist the
answer can be determined using the base
theorem as shown below
so here's the formula of the base
theorem and putting all the values that
we have calculated in the previous Slide
the probability that it will rain on the
match day given a forecast of the rain
by metallurgists will come out to be
0.111
which will be equal to 11.11 percent
so there's an 11 chance that it will
rain on the match day given that the
meteorologist has predicted the rain I
hope this example is clear to you so
what is analysis of variance
to test the differences among the means
of the population by examining the
amount of variation within each sample
related to the amount of variation
between the samples
this technique was invented by r a
Fisher and is thus often referred to as
a fisher's Innova analyzing the variance
tests the hypothesis that a means of two
or more populations are equal
in integration study analysts use the
Anova test to determine the impact of
independent variable onto the dependent
variable
so what are its types
there are basically two types of Anova
which is a one-way Anova and a two-way
Anova
when we are comparing more than three
groups based on one factor variable now
it is said to be a one-way analysis of
variance
for example
if we want to compare whether or not
mean output of three workers is the same
based on the working hours of the three
workers will use the one-way Anova now
let's move on to two-way Anova
when Factor variables are more than two
then it is said to be a two-way analysis
of variance
for example based on working conditions
and working hours we can compare whether
or not the mean output of three workers
is the same
now let's discuss some key terminologies
used in Anova
we'll start with the null hypothesis
the null hypothesis is the assumption
that an event will not occur
a null hypothesis has no bearing on the
study's outcome unless it is rejected
alternative hypothesis
the alternative hypothesis is a logical
opposite of the null hypothesis the
acceptance of the alternative hypothesis
follows the direction of the null
hypothesis
H1 is the symbol for it
p-value
in statistics the p-value also called
probability value is the probability
measure of finding the observed or more
extreme results when the null hypothesis
of a given statistical test is true
the p-value as a primary value used to
quantify the statistical significance of
the result of a hypothesis test
now let's move on to Alpha value
the alpha value is a criteria for
determining whether this test statistics
is a statistically significant or not
this test provides a p-value which is
the probability of observing results as
Extreme as those in the data assuming
the results are truly due to chance
alone
an alpha value of 5 percent or lower is
often considered to be statistically
significant
now let's try to understand its
terminologies by taking an example
let's assume that a new drug is
developed with a goal of lowering the
blood pressure more than the existing
drug
so in this case a null hypothesis will
be that the new drug doesn't lower the
blood pressure more than the existing
drug the alternative hypothesis will be
that a new drug does significantly lower
the blood pressure more than the
existing drug and the p-value will be
the result from evidence like medical
trials showing the positive results
which will reject the null hypothesis
the statistic will be the extent of
difference between the means of
different medical trials the sum of
squares will be the variation from the
mean of different medical trials and the
mean will be average of all the results
from evidence like medical trials
now let's understand the core construct
between how the Anova works
the one-way Anova compares the means
between the groups you are interested in
and determines whether any of those
means a statistically significant
difference from each other
specifically it's just the null
hypothesis if any of the group means is
significantly different from the overall
mean that the null hypothesis in this
case will be rejected
now let's discuss the term F statistics
which is very much used in the
analysis of variance s statistics or F
ratio is a statistical measure that
tells us about the extent of difference
between the means of different samples
lower the F ratio the closer are the
samples to the means if you have three
different data sets
then this part will be the within group
variance that is the variance between
the samples this line will be the
variance between the means of the
different samples which is also called
between group variance
if the between group variance is large
relative to the within group variance
the X statistic will be comparatively
large to the critical value and
therefore will be statistically
significant
now that we have covered almost all the
theory regarding the Anova let's try to
understand how this is used in a real
world example
suppose you are a marketing manager of
the Product Company your team has
recently launched three new
advertisements and you want to know if
the three different types of advertising
effect means sales differently
so how can you do that
you will leave each type of
advertisements at 20 different stores
for one month and then measure the total
sales of each store for a month
to observe if there is statistically
significant differences in the mean
sales between these three types of
advertisements will conduct a one-way
Anova
for calculating the Anova you will use
the type of advertisement as a factor
and the sales is a response variable
now that we have covered all the topics
in Anova and have understood how it is
used in a real world example let's see
how we can run an anova test in Excel
suppose you are a research scientist and
you want to perform clinical trials to
study the effectiveness of three drugs
developed by three Healthcare companies
to cure a certain disease the data below
represents the time taken to cure the
disease for different patients when they
consume either drug a b or c
the time is represented in terms of
total hours and minutes
at the 0.05 level of significance that
is the alpha value we need to test
whether the mean time for the three
drugs to cure the disease are equal so
how you can do that
we'll first move on to a data tab
and then click data analysis
we'll select the Anova single Factor and
click ok
now it is asking for an input range
select the data you have on the table
we have the labels in our first row and
Alpha is 0.05
for the output range
will get the output here on the same
page so select some cell in the same
page and we'll click ok
so you can see we have our analysis of
variance table with the summary
let's understand each term one by one
the average time taken to cure the
disease after consumptions of three
drugs are 107 90 and 96 hours
approximately
the difference between the largest and
the smallest mean is 17.47
here one significant observation is that
the F value is greater than F critical
value
so this means we can reject the null
hypothesis
it also means that the average time
taken to cure the disease is not the
same for all the three drugs
using the paired comparison we can
conclude the time taken by the drug a to
cure the disease is more than time taken
by drug C
and wait further is more time taken by
drug B to cure the disease
what is correlation
correlation refers to a statistical
relationship between the two entities
it measures the extent to which the two
variables are linearly related
for example the price and demand of a
product is in correlation they are
linearly related
the value of correlation always lies
between minus one two plus one
there are mainly two types of
correlation the first one is positive
correlation
a positive correlation means that the
linear relationship is positive
and the two variables increases or
decreases in the same direction
the number of trees cut down and the
probability of erosion are in
correlation when one increases other
also increases and vice versa
negative correlation
a negative correlation is just the
opposite the relationship line has a
negative slope and the variables change
in opposite directions
for example if you decrease the speed of
the car the time taken to reach the
destination increases
this is the negative correlation
correlation coefficient
the correlation coefficient which is
denoted by R gives us a measure of the
strength of the linear relationship
between the two variables
the value is denoted by the letter R and
it ranges from -1 to Plus 1. if it is
less than 0 it implies that there is a
negative correlation the minus 1
indicates there is a strong negative
correlation between the variables
if R is greater than 0 it implies there
is a positive correlation and plus one
is the strongest point of a positive
correlation
when R is equal to 0 we can say that
there is no correlation between the
variables
there are also some limitations of
correlation
the first one is correlation does not
give you every Insight on the data mean
and standard deviation are still the
important parameters to get the Insight
of a data
the second one is the data will not
produce a straight line every time and
it will be difficult to predict the
value of R just with the straight line
or the slope of the graph
now let's see some real life
applications of correlation
if you take any e-commerce company whose
website is accessed by millions of users
all over the world the company can look
at all the data and they can measure how
much time was spent by the customer and
the respective money spent by the
customer
we can also predict the unique users
that visited the website and how it
affected the sales in a day
the third application can be the patient
blood pressure and the medication used
the level of patient blood pressure and
the effect of medication on it can be in
a correlation now let's jump on to excel
workbook to see how you can calculate
the correlation coefficient
so here we are on Excel workbook if you
see we have the temperature in degree
celsius it has 20 25 30 40 50 and 60 and
the corresponding sales of ice cream in
the units to calculate the correlation
we'll use the Corel function now we have
array 1 and array 2. so we'll select the
array1 that is temperature in degree
celsius and the array 2 will be our
sales of ice cream
so you can see there is a very strong
correlation between the temperature of a
day and the sales of ice cream unit
if the temperature increases the sales
of ice cream unit increases and if the
temperature decreases the sales of ice
cream when it decreases
what is correlation
correlation refers to a statistical
relationship between the two entities
it measures the extent to which two
variables are linearly related
for example the increase in the height
of the children
is accompanied often with increase in
weight
the value of the correlation always lies
between minus one two plus one
there are many three categories of
correlation
the first one is positive correlation
and negative correlation
the second one is linear and non-linear
correlation
and the third one is simple multiple and
partial correlation
let's discuss each of them one by one
positive correlation
a positive correlation means that a
linear relationship is positive and the
two variables increases or decreases in
the same direction as you can see from
the graph
for example
the calories you burn is directly
proportional to the amount of time you
run on a treadmill on the other hand a
negative correlation is just the
opposite
the relationship line has a negative
slope and the variable changes in the
opposite direction that is one variable
decreases while the other increases
an example can be a student who has many
absence has a decrease in grades now
let's move on to discuss linear and
nonlinear correlation
when we change the value of a variable
which leads to a constant ratio change
in other variable then that relation is
said to be linear
for example
the factory doubles its output by
doubling the number of workers on the
other hand correlation is said to be
non-linear when the amount of change in
one variable is not in constant ratio to
the change in the other variable
for example
the change in radius of the sphere and
the change in volume of the same sphere
does not happen to be the same ratio now
let's discuss simple multiple and
partial correlation when studying the
relationship between the variables well
only two variables are involved the
correlation is said to be simple
in the multiple correlation we measure
the degree of association between one
variable on the one side and all the
variables together on the other side
and for the partial correlation we study
the relationship of one variable with
one of the other variables presuming
that all the variables remains constant
now that we know all the types of
correlation
let's discuss the two major types to
calculate the correlation coefficient
the first one is Pearson's correlation
PSS correlation coefficient is the test
statistics that measures a statistical
relationship between the two continuous
variables the PSS correlation
coefficient is often denoted by R and
the formula to calculate the Pearson's
relation coefficient is Sigma x i minus
X bar into y Pi minus y bar divided by
under root Sigma x i minus X bar whole
square and y i minus y bar whole Square
but R is the coefficient of correlation
X Y is the mean of X variable and Y Bar
is the mean of Y variable and x i and YY
denotes the samples of variable X and Y
respectively
the PSS coefficient coefficient is the
best method to measure the association
between two variables
of interest because it gives information
about the magnitude of the association
or correlation shape as well as the
direction of the relationship between
the two variables
the second best method is PS1 rank
correlation
this Pearson Ram correlation is used to
discover the strength of a link between
the two sets of data
the formula to calculate this prisoner
rank correlation coefficient is rho is
equal to 1 minus 6 Sigma d i whole
Square divided by n bracket n Square
minus 1. where rho is this P has been
rank correlation coefficient d i has a
difference between the two ranks of each
observations and N is the number of
observations
let's begin what is correlation
a correlation is a statistical
relationship between two entities
it measures the extent to which the two
variables are linearly related
in simple words it is a measurement of
strength of association between the two
variables
for example an increase in the price of
commodity is always accompanied by
decrease in demand
the value of the correlation always lies
between -1 2 plus 1. types of
correlation
there are mainly three categories of
correlation
the first one is positive and negative
correlation the second one is linear
correlation and non-linear correlation
and the third one is simple multiple and
partial correlation
let's discuss positive and negative
correlation
a positive correlation means that a
linear relationship is positive
and the two variables increases or
decreases in the same direction as you
can see from the graph
an example can be as a number of crease
cut down increases the probability of
soil erosion also increases
in the same manner a negative
correlation is just the opposite the
relationship line has a negative slope
as you can see from the graph and the
variables changes in the opposite
direction that is one variable decreases
while the other increases
to understand this an example can be if
a car decreases the speed the time taken
to reach the destination increases
to know more about the types of
correlation you can refer to our
previous videos and you can find the
link in our description
now what is regression regression
analysis like most multivariate
statistics allows you to infer that
there is a relationship between two or
more variables
these relationships are seldom exact
because there's a variation caused by
many variables not just the variables
being studied
in regression analysis there are two
types of variable one is dependent
variable and other is the independent
variable
let's discuss what the boot variable
represents a dependent variable is a
variable whose values influence or to be
predicted the dependent variable is
often denoted by Y and is also known as
a predicted variable
whereas independent variable which is
denoted by X is a variable which
influences the value or is used for
prediction
the independent variable is also known
as a predictive variable
the line of regression the regression
line is a line which is used to describe
the behavior of a set of data in other
words it gives the best trend of the
given data
regression lines are useful in
forecasting procedures its purpose is to
describe their interrelationship between
the dependent variable and independent
variable
the regression equation of Y and X
describes a change in the value of y for
given changes in the value of x and vice
versa
in the regression line equation X and Y
are the variables of interest in our
data with Y the unknown or dependent
variable and X the known or the
independent variable
let's discuss the two key terms in this
graph the first one is slope slope is
the ratio of the vertical and horizontal
distances between the two points on a
line
and you can see y-intercept which is the
coordinate of the point at which the
curve intersects an axis
there are some assumptions we take to
create the regression model the first
one is the dependent variable is assumed
to be normally distributed
the values of the dependent variable are
statistically independent
this means that when we select the
sample of a particular X it does not
depend on any other value of y
and the third one is error values are
statistically independent
now let's discuss a simple regression
model
a simple regression model is used to
depict a relationship between variables
which are proportional to each other
meaning the dependent variable increases
decreases with the independent variable
the equation of a simple regression
model is y is equal to B naught plus b
One X Plus e
where Y is dependent variable X is
independent variable B naught and B1
represents y-intercept and the slope of
the line respectively and E is the error
variable
now let's move on to excel to calculate
the regression coefficient of a given
data
we are on an Excel workbook
in front of us we have the data of
temperature of the day and sales of the
ice cream on that day
let's try to understand this regression
analysis and summary output using this
data
so the first step is to
go to the data Tab and select data
analysis
then select regression and click ok
we'll get this table select the Y range
as the ice cream sales
this y range is predictable variable
also called as dependent variable
for the input X range
we'll select the temperature in degree
Celsius
this is the explanatory variable also
called as the independent variable
one thing you should keep in mind
is that both the X and Y column must be
adjacent to each other
we have the label so we'll check the
label and we'll select the output range
from G5
check the residuals and click ok
Excel produces the following output
let's analyze some key data
we got the asker value to be 0.9676
which is very good this shows that a 96
percent of the variation in ice cream
sales is explained by the temperature of
the day closer to the one the better the
regression line fits the data
now let's move little more and analyze
this coefficient
if you look at the coefficient you'll
get the equation of the regression line
in our case it is minus
176.33 plus 27.68 into temperature of
the day
substituting the temperature of the day
for any given day we can find the sales
of the ice cream
with our discussion we can conclude that
there's a big difference between the
correlation and regression although
these are studied together
correlation is used to study whether the
variables under study are correlated
whereas
regression is used to establish the
functional relationship between the two
variables
the equation of the regression line is a
form Y is equal to f x plus c
correlation is used to stabulate the
strength of the association between the
two variables that are being studied
whereas regression is used to make the
future predictions on the any given
event also data scientists looking for
online training and certification
programs from the best universities or a
professional who elects to switch
careers with data science then try
giving simply learns postgraduate
program in data science as short the
link in the description box below should
navigate to the home page where you can
find the complete overview of the
program being offered now over to our
training experts data science interview
questions my name is Richard kirschner
with the simply learned team that's
www.simplylearn.com get certified get
ahead before we dive in and start going
through the questions one at a time
we're going to start with some of the
logical kind of concept that enters in a
lot of interviews and this one you have
two buckets one of three leaders and the
other are five liters you're expected to
measure exactly four liters how will you
complete the task and note you only have
the two buckets you don't have a third
bucket or anything like that just the
two buckets and the object of the
question like this is to see how well
all you are thinking outside the box in
this case you're in a larger box you
have two buckets and also the pattern
which you go on and what that means is
if you look at the two buckets and we'll
show you their answer in just a second
you have a bucket with three liters and
a bucket with five liters and the first
thought is what happens if you go from
left to right so we have a direction and
what happens if you pour the three
liters into the five liter bucket well
if you pour the three liters into five
liter bucket you have an empty bucket of
three liters and what's really important
here is I was thinking outside the box
you realize that you have a five liter
bucket that has three liters in it and
two empty liters so you have two
additional leaders you can fill up if we
continue that process we can compare
from the left to right from the small
bucket to the large bucket you can now
measure into additional liters into the
five liter bucket and three minus two is
one and you can keep doing that you can
empty the five liter bucket in pour
those three liters in that one liter in
and then you can pour three liters in
what's cool about these questions is you
explore them is you realize there's
multiple ways usually to solve I went
from small bucket to big bucket the
simply learned team their solution that
they pulled out was you fill the five
liter bucket and empty it into the three
liter bucket now you're left with two
liters in the 5 liter bucket so that's
great we can empty the three liter
bucket so now we're going from large to
small remember we went from small to
large so you can go both either way but
you have to go one way or the other it
turns out and you can empty the three
liter bucket and pour the contents of
the 5 liter bucket in it so the three
liter bucket now has two liters and if
it has two liters that means it has an
empty one liter and by now you probably
have guessed that if you have an empty
space you can start using that empty
space of one liter as a measuring so we
fill the five liter bucket again and we
pour the water in the three liter bucket
it already has the two liters and so
we're only pouring one liter in there
and five minus one is four so interview
questions they break up into all kinds
of different patterns we have logic like
this one which is a lot of fun we have
questions that come up that are more
vocabulary list the difference between
supervised and unsupervised learning
probably one of the fundamental
breakdowns in uh data science and
supervised learning uses known and
labeled data as input supervised
learning has a feedback mechanism most
commonly used supervised learning
algorithms are decision tree logistic
regression support Vector machine and
you should know that those are probably
the most common use right now and there
certainly are so many coming out so
that's a very evolving thing and be
aware of a lot of the different
algorithms that are out there outside of
the deep learning because a lot of these
work faster on Raw data numbers than
they do than a deep neural network would
unsupervised learning uses unlabeled
data as input unsupervised learning has
no feedback mechanism most commonly used
unsupervised learning algorithms are
k-means clustering hierarchical
clustering the Aprilia log algorithm and
there certainly are more I'm going to
say k-means definitely is at the top of
the list and the hierarchical clustering
those two are used so many times so
really important to understand what
those are and how they're used and most
important is understand that supervised
learning is you have your data set where
you have training data and you have all
those different pieces moving around but
you you're able to train it you know the
answers and unsupervised we're just
grouping things together that look like
they go together how is logistic
regression done logistic regression
measures the relationship between the
dependent variable our label what we
want to predict and the one or more
independent variables are features by
estimating probability using its
underlying logistic function sigmoid and
whenever I draw these charts I always
end up drawing them the right hand side
first because you want to know what your
output is what is you one out of here in
the left hand side what do you have
going in so you have your in and out you
can see we have a nice labeled image
here to help you remember this we have
our inputs we have our linear model we
have our probabilities what are the
probabilities of it being a certain way
based on these features coming in the
sigmoid function and it's important to
know that the sigmoid function is maybe
the most commonly used but it's only one
of a number of functions that are out
there and the sigmoid function turns our
probabilities into a value between zero
and one or very close to zero very close
to to one between 0.1 and 0.009 and
based on that we generate an answer in
this case is 0 or 1. how is logistic
regression done so last time we talked
about the sigmoid function generally
depending on what your interview and
level of math and what expertise you're
going in for the market you'll have to
understand that formula of the
probability equals one over one plus e
to the negative Y and that's e to the
base two so you have your probability
function or your sigmoid function which
pushes it as you can see we have a nice
visual of that that helps a lot to have
that visual on the sigm weight function
you definitely should know your y equals
m times X plus C your basic lydian
geometry of forming a line and the slope
plus the intercept the y-intercept and
then you have your natural log and the
natural log is to the e as opposed to a
base two or base 10. so your natural log
to the E of the probability over one
minus a probability equals your M times
X plus C or your euclidean line that
helps a lot as far as the graphing and
understanding the sigmoid function so
we'll just keep pushing on to question
number three explain the steps in making
a decision tree and I noticed last time
we brought up the decision tree in the
forest a lot of questions came up what
is the difference so let's go through
that when you make a decision tree
you're going to take the entire data set
as input you're going to calculate
entropy of the target variable as well
as the predictor attributes I remembered
entropy is just how chaotic is it so if
you have like you know banana and grapes
and oranges if you're mixing in fruit
and that's your data coming in you have
all these different objects that are so
separate from each other and the more
they become uniform the lower the
entropy and we call that information
gain so we gain information on sorting
different objects from each other so you
have your entropy you have to calculate
your information gain of all attributes
and then you choose the attribute with
the highest Information Gain as the root
node so if you can separate your group
and each group chaos in each group is
lowered whichever split lowers the chaos
the most that's where you split it and
that's your root node at that point you
repeat the same procedure on every
Branch till the decision node of each
branch finalized so understanding that
setup is pretty important as far as
decision trees and you can see here we
have a nice visual of a decision tree
for example if you want to build a
decision tree to decide whether we
should accept or decline a job offer
since these are interview questions
that's a good one to ask and just as a
tip you should be pretty aware of the
formula for entropy and Information Gain
so you need to look those up if you
don't remember those and the salary if
it's greater than 50 000 no decline the
off yes it's got a good salary the
commute is greater than an hour yes
decline the offer no offers incentives
Yes except the offer no incentives
decline the offer so we use decision
tree pretty much for everything if you
want and if you have a decision tree
then you also should understand how do
you build a random forest model and
remember that a random Forest is built
up of a number of decision trees so if
you split your data up into a lot of
different packages and you do a decision
Tree in each of those different groups
of data the random Forest bringing all
those trees together so how do you build
a random forest model randomly select K
features from a total of M features
where K is less than M among the K
features calculate the node D using the
best split Point split the node into
daughter nodes using the best split
repeat steps two and three steps until
Leaf nodes are finalized build for us by
repeating steps one to four for n number
times to create n number of trees so you
can see it's got the same build pattern
as the tree but instead you're building
a number of different trees little small
trees so it all have an end Leaf node
random Forest has a vote at the end and
whoever gets the most votes wins that's
the answer how can you avoid overfitting
of your model very important question in
any kind of mathematical scientific data
science setup in any of them there are
three main methods to avoid overfitting
and you should really understand
overfitting overfitting means that your
model is only set for a very small
amount of data and ignores the bigger
picture keep the model simple take into
account fewer variables thereby removing
some of the noise in the training data
good advice for any programming at all
use cross-validation techniques such as
k-folds cross validation use
regularization techniques such as lasso
that penalize certain model parameters
if they're likely to cause overfitting
and you should also be well aware that
your cross-validation techniques that's
like a pre-data or your lasso and your
regularization techniques are usually
during the process so when you're
prepping your data that's when you're
going to do a cross validation such as
like splitting your data into three
groups and you train it on two groups
and test it on one and then switch which
two groups you tested on that kind of
thing so can you solve another one of
these I love these things there are nine
balls out of which one ball is heavy in
weight and the rest are of the same
weight in how many minimum weighings
will you find the heavier ball and when
we say weighing think of a scale where
you can put objects on one side and the
other and you can see which side is
heavier and you want to minimize that
you want to split the balls up in such a
way that you're going going to do as few
measurements as you can you will need to
perform two angs so you can get it down
to just two wings and I always think if
there's nine balls I'm going to divide
them into three groups of three Place
three balls on each side so you can just
randomly pick six of the balls and three
on one side three on the other and if
they balance out both sides are equal
then you know the heavy weight isn't in
any of those so out of the remaining
three balls from step one take two balls
and place one ball on each side a little
tricky there because I always want to
put all three I want to put two on one
side and one on the other but no just
take randomly pick two of those put one
on each side if they balance out then
the left out ball the one you didn't
measure will be the heavier one
otherwise you'll see it in the balance
you'll see which one's heavier because
they'll take one of the balls down now
we go to scenario B where they did not
balance out so now we know which side
has a heavier ball in it and it's just
very similar to what we did before if
the balls in Step One do not balance out
then take those three balls that have
the heavier side on them and reproduce
step two to find out the heavier ball
difference between universe Chariot
bivariate and multivariate Analysis and
hopefully if you know a little Latin
it'll kick in there that you have uni
and you have bi and you have multi
because the answer is in the words
themselves so the first one this type of
data contains only one variable so
that's the univariate purpose of the
univariate analysis is to describe the
data and find patterns exist within it
so when you only see one one variable
coming in in this case we're using a
height of students you're limited as far
as what you can do with that data so you
can come up and draw different patterns
and conclusions from those patterns
using the means the median the mode
dispersion range minimum maximum so
we're describing the data so all those
words would describe the data and that's
about all you can do with data like that
there's no correlation there's nothing
to go beyond that as far as guessing or
predicting anything so we move into
bivariate you know uni means one by
means two bivariate this type of data
involves two different variables the
analysis of this type of day data deals
with causes and relationships and the
analysis is done to find out the
relationship among the two variables and
this is always a favorite one because
everybody loves ice cream in the summer
when it's hot and very few people go for
ice cream in the winter when it's really
cold so it's easy to see the correlation
in the data the temperature and ice
cream cells in summer season and you can
see here where the temperature goes from
20 to 35 and as the temperature goes up
so does the cells of ice cream it goes
from 2000 I'm not sure 2 000 Watts I'm
guessing it's a very large chain because
if they're selling 2 000 ice cream cones
and they have a lot of business good for
them a little vendor on the corner
selling 2 000 ice cream cones a day and
3 100 the next day here the relationship
is visible from the table the
temperature in cells are directly
proportional to each other so the hotter
the temperature we can predict an
increase in sales so the word prediction
should come up so we have description
and prediction when the data involves
three or more variables it is
categorized under multivariate it is
similar to bivariate but contains more
than one dependent variable in this
example another really common one the
data for house price prediction the
patterns can be studied by drawing
conclusions using mean median and mode
dispersion or range minimum maximum Etc
and so you can start describing the data
that's what all that was and then using
that description to guess what the price
is going to be so if this is very good
if you're in the market and you have
already looked at the area and you
already know that a two bedroom zero
floor 900 square foot house is usually
runs about forty thousand you can guess
what the next one that looks similar to
it is and I'll just throw in another
word in there I don't see very often
unless you're really a hardcore data
science we talked about describing the
data descriptive we talked about
predictive and there's also
postscriptive postscriptive means we're
going to change the variables to try to
guess what the outcome is if we change
what's going on so that would be the
next step that usually doesn't show up
unless you're dealing with some really
hardcore data science groups what are
the feature selection methods to select
the right variables there are two main
methods for feature selection there's
filter methods and wrapper methods and
when you're filtering your before we
discuss the two methods real quick the
best analogy for selecting features is
bad data in bad answer out so when we're
limiting or selecting our features it's
all about cleaning up the data coming in
so it's cleaner and is more
representative of what we're trying to
predict filter method filter methods as
they come in we have linear
discrimination analysis a Nova
chi-squared chi-square is probably the
most common one and these are all part
of pre-processing we're taking out all
the outliers all the things that have a
difference that is very different from
the data we're looking at the odd ones
and sometimes you take the odd ones out
and then you analyze them separately to
see why they're odd but remember your
filter methods you want to pull all that
weird stuff out wrapper methods on the
other hand are forward selection
backward selection recursive feature
elimination and one of the most
important things remember about wrapper
methods is they're very labor intensive
if you have to have some pretty high-end
computers if you're doing a lot of data
analysis with the wrapper method and
just quickly forward selection means you
have all your different features they're
off to the side and we test just one
feature at a time we keep adding them in
until we get a good fit backwards we
have all the features and we start we
run a test on that to see how well it
does and then we start removing features
to see what works and recursive which is
the most processing hungry algorithm out
there goes through and just recursively
looks through all the different features
and how they pair together but again we
have filter method and wrapper method
and it's important to understand that
we're sorting the data out and finding
out which features are going to
represent the data the best and which
ones are not going to really add any
value to our models let's jump number
eight in your choice of language write a
program that prints the numbers from 1
to 50. but for multiples of three print
Fizz instead of the number and for the
multiples of five print a buzz for
numbers which are multiples of both
three and five print Fizz Buzz and this
really is testing your knowledge in
iterating over data very important my
sister who runs at the university the
data science team is in charge of their
Department it's the first question she
asks in her interview of anybody who
comes in is how do they iterate through
data
so if this question comes up a lot and
it's very important you have an
understanding and there's actually a
slight error on this code which I'll
point out in just a second the concept
is we have Fizz buzz in range and you
have range 51 which in this case goes
from 0 to 51 and I'm going to challenge
you to see if you can catch the error
and I'll tell you at the end of the code
where the error is what that means is
that we're going to go through all the
numbers 0 1 2 3 4 and we're going to
process through this Loop if the
remainder of Fizz Buzz divided by 3
equals zero and fizzbuzz divided by 5
also equals zero then print fizzbuzz
continue and else if fizzbuzz divided by
3 equals 0 in print Fizz print Fizz
continue else if is Buzz divided by five
equals zero print Buzz continue in print
Fizz buzz you fit the print the answer
in this case Fizz Buzz is either going
to be the number we generated which is 0
or it'll be the Fizz Buzz Fizz or Buzz
that's a mouthful now if you didn't
catch the error in the code which is
always a fun game find the error it has
to do with the range and it's important
to remember the range here says range to
51 that's 0 to 51 which is is correct we
want to go to 51 because it stops it
gets to 50 and it stops so that's 0 to
50. but if you remember the question
asked from 1 to 50. so the range should
be 1 comma 51 not just 51 which does 0
to 51. in this particular script in
Python you could leave out the continue
but the continue in the script skips the
next lcf so it doesn't keep processing
it going down and in a programming a lot
of scripts you don't need the
continuance this would depend on what
script you chose and there's probably
some other ways to do this it's a lot of
fun and you can see here from the output
we end up with fizzbuzz for zero which
shouldn't be there one two Fizz four
Buzz Fizz seven eight Fizz Buzz 11 Fizz
and so on sounds like a drinking game
for my college days so long ago many
decades ago you are given a data set
consisting of variables having more than
30 percent missing values how will you
deal with them oh the joy of messy data
coming in ways to handle missing data
values data set is huge we can just
simply remove the rows with missing data
values it is the quickest way I.E we use
the rest of the data to predict the
values you just go in there and say any
row of our data that has a n a in it get
rid of it that doesn't work with smaller
data so a smaller data you start running
into problems because you lose a lot of
data and so we can substitute missing
values with the mean or average of the
rest of the data using Panda's data
frame in Python there's different ways
to do this obviously in different
languages and even in Python there's
different ways to do this but in Python
it's real easy you can do the DF dot
mean so you get the mean value so if you
set mean equal to that then you can do a
df.fill in a with the mean value very
easy to do in a python Panda script and
if you're using python you should really
know pandas and numpy number Python and
pandas data frames for the given points
how will you calculate the euclidean
distance in Python so back to our basic
algebra from high school euclidean
distance is the line on the triangle and
so if we're given the points plot one
equals one comma three plot two equals
two comma five we know that from this we
can take the difference of each one of
those points Square them and then take
the square root of everything so the
euclidean distance equals the square
root of plot one zero minus plot two is
zero squared plus plus one of one minus
plot two of one squared mouthful there
and you can remember if you have
multiple Dimensions that go past two
Dimensions you could have plot three can
simply be the distance from plot one you
only need to do one side of that or plot
two you can do either way and square
that and take the square root of that
another mind Bender how to calculate
some how to figure out the solution to
something what is the angle between the
hour and minute hands of a clock when
the time is half past six so you want to
kind of imagine that clock where the
large hand is pointed down to the 30 and
the other half is going to be right
between between the six and the 7
because it's half past six there's
actually a couple ways to solve this but
let's take a look and see how they did
it note a clock is a complete circle
having 360 degrees in one hour the hour
hand covers 360 over 12. so it equals 30
degrees for each hour in one minute the
minute hand covers 360 degrees over 60
Minutes or 6 degrees per minute the
minute hand has traveled for 30 minutes
so it has covered 30 times 6 which
equals 180 degrees so we know that's 180
degrees from the 12. the hour hand has
traveled for 6.5 hours six and a half
6.5 so it's covered 6.5 times 30 which
equals 195 degrees the difference
between the two will give the angle
between the two hands thus the required
angle equals 195 minus 180 equals 15
degrees and this is nice the way they
solved it because you can now punch in
any kind of time within reason the hard
part is on the hours is you have to be
able to convert the hours into decimals
explain dimensionality reduction and
list its benefits Dimension reduction
refers to the process of converting a
set of data having vast Dimensions into
Data with lesser Dimensions fills to
convey similar information concisely it
helps in data compressing and reducing
the storage space it reduces computation
time as less Dimensions lead to less
computing it removes redundant features
for example there is no point in storing
a value in two different user units
meters and inches and I certainly run to
it a lot with this with text analysis
I've been known to run a text analysis
over a series of documents ends up with
over 1.4 million different features
that's a lot of different words being
used and if you do what they call buy
connect them you connect two words
together now you're up to 4.8 million
different features and you start having
to figure ways to bring that down what
can we get rid of that kind of thing so
you can see where that can get really
high in on processing and learning how
to reduce the list Dimensions is very
important important how will you
calculate eigenvalues and eigenvectors
of a three by three Matrix and what
they're really looking for here is when
you write it out for the eigen is that
you know that you're going to use the
Lambda that's the most common one
obviously you can use any symbol you
want but Lambda is usually what they use
and that you do it down the middle
diagonal and so when you take that
Matrix and you take the characteristic
equation you end up with a determinant
and that's the minus 2 minus Lambda
minus 4 2 minus 2 1 minus Lambda 2 4 2 5
minus Lambda and that's what they're
looking for and you know that's equal to
zero so when you're doing a matrix in
the eigen setup with the eigenvectors
that's all going to come out equal to
zero and then you can go ahead and write
the whole equation out so we can expand
the determinant as you can see right
here the minus 2 minus Lambda times it's
a mouthful I'll leave it up here for a
second so you can look at it when you
break it down into the algebraic
functions you end up with minus Lambda
cubed plus 4 Lambda squared plus 27
Lambda minus 90 equals zero so now we
have a nice algebraic equation built
from the eigenvectors and always
remember you can hit the pause button
and you can also send a note send a note
to Simply learn if you have more
questions on vectors or on this
definitely you have that resource
available to you or post down in below
on the YouTube video comments and so
when we calculate the eigenvalues and
eigenvectors of a three by three Matrix
as we continue on down the math of this
and to be honest I really don't like
working with matrixes like this it's
important to understand the math behind
it and it's important to know the code
just enough so that you're not lost when
someone's explaining it or it comes up
and I'm working on different data
science models of course if you're
dealing with a high-end math side of it
then you better know this first is by
hitting trial so you try in different
variables to solve for zero and you can
come in here and you'll find that if we
put in the 3 in there we end up with a 0
at the end and substitute the three
hence we end up with Lambda minus 3 is
one of the factors and you can do the
math going out on that where we have
Lambda cubed minus four Lambda squared
minus 27 Lambda Plus 90 equals Lambda
minus 3 times Lambda squared minus
Lambda minus 30. So eigenvalues based on
that one are three minus five and six
and then from there we can calculate the
eigenvector for Lambda equals three and
you can see here where the Matrix as we
write it out is the minus five minus
four two minus two minus two minus two
four two two that's from the beginning
put in the X Y and Z equals zero zero
zero and so when we put in those numbers
and we calculate them out we have for x
equals one we have the minus five minus
four y plus two Z equals zero minus two
minus two y plus two Z equals zero and
subtracting the two equations we just
had we get three plus two y equals zero
y equals minus three over two and Z
equals minus one over two that's going
back to the first equation and similarly
we can calculate the eigenvectors for
minus five and six how should you
maintain your deployed model ooh
distribution time my favorite I spent 10
years in software distribution so first
thing and this is true not just of your
data science model but of any computer
code going out there's this basic setup
can work although usually there's a
little added steps in there first we're
going to monitor it so we have a
constant monitoring of all the model is
needed to determine the performance
accuracy of the models so yeah we want
to just keep an eye on it we want to
make sure they're accurate we want to
make sure that whatever they're supposed
to predict or I threw in that bonus word
PostScript where you change something
you want to figure out how your changes
are going to affect things we need to
monitor it and make sure it's doing what
it's supposed to do evaluation metrics
of the current model is calculated to
determine if new algorithm is needed and
then we compare it the new models are
compared against each other to determine
which model performs the best and then
we do a rebuild the best performing
model is rebuilt on the current state of
data this is interesting I found this
out just recently if you're in weather
prediction the really big weather areas
have about seven or eight different
models depending on what's going going
on and so you actually have almost a
little Forest going on there where
they're like which model is going to fit
best and this is what we're going to use
to predict the weather with so not only
do you don't necessarily get rid of the
models but you figure out which models
will fit data of what's going on or the
current state of data what are
recommender systems most commonly used
nowadays in marketing so very big
industry understanding recommender
systems predicts the rating or
preference a user would give to a
product and they they're split into two
different areas one is collaborative
filtering and a good example of that is
the Last.fm recommends tracks that are
often played by other users with similar
interests so people who if you're on
Amazon people who bought this also
bought that this got me a few times and
then there's contact based filtering and
we're looking at content instead of
looking at who else is listening to the
music these example Pandora which uses
the properties of a song to recommend
music with similar properties so you
have collaborative filtering and content
based filtering how to to find rmse and
MSC in linear regression model hopefully
you remember what the two acronyms mean
because that is like half the answer we
have the root mean square error and the
mean square error in linear regression
model so we're looking for error the
rmse and the MSC are the two of the most
common measures of accuracy for a linear
regression model and you can see here we
have the root mean square error rmse
equals and this is the square root of
the sum of the predicted minus the
actual squared over the total number so
we're just looking for the average mean
so we're looking for the average over
the end and the reason you need to know
about the difference between ramse
versus MSC is when you're doing a lot of
these models and you're building your
own model why do you need to take the
square root of it it doesn't change the
value as far as the way you're using it
because you're looking as to see whether
the error is greater or less than so why
add that extra computation in so a lot
of models use the MSC which indicates
the mean square error or the average
error and it's the same formula minus
the square root at the end or across the
whole thing another riddle to solve if
it rains on Saturday with a probability
of 0.6 and it rains on Sunday with a
probability 0.2 what is the probability
that it rains this weekend and the trick
in probabilities on this case is we're
not we need to know what is the
probability of it not raining what is it
not what's the chance of it not raining
on Saturday and if it doesn't rain on
Saturday we want to take that and
combine that with the chance of it not
raining on Sunday the total probability
which in this case we're just going to
use 1 minus the probability that it will
not rain on Saturday so that's 1 minus
0.6 we're going to take that as a union
which we simply just multiply them
together of the probability they will
not rain on Sunday and it's important to
recognize the union here or the and you
can see by the formula down here we end
up with 0.68 or 68 percent chance that
it will rain on the weekend and there
are a couple other ways to solve this
but this is probably the most
traditional way of doing that how can
you select k for K means so first you
better understand that what K means is
and that K is the number of different
groupings and most commonly we use is
the elbow method to select k for K means
the idea of the elbow method is to run
k-means clustering on the data set where
K is the number of clusters within the
sum of squares WSS is defined as the sum
of the squared distance between each
member of the cluster and its centroid
and you should know all the terms for
your K means on there and with the elbow
point and again here's our iteration in
our code we talked about that earlier
you iterate starting with usually you
don't start right at one but you might
start with two three or four and you
just see where it comes out and you can
see the nice elbow there which is easy
to see graphically where the number of K
clusters and the WSS value drops and
then it just kind of flattens out and
there's no reason to take the K means
any further what is the significance of
p-value oh good one especially if you're
dealing with R because that's the first
thing that pops up p-value typically
less than or equal to 0.05 indicates a
strong evidence against the null
hypothesis and you should know what a
difference why we use null hypothesis
instead of the hypothesis so you reject
the null hypothesis very important that
term null hypothesis in any scientific
setup and also in data science it
doesn't mean that it's true it means
that there's a high correlation that
it's true so if your null hypothesis
means it's not true your hypothesis is
has a high correlation that it's
probably true and if the p-value is
typically greater than 0.05 it indicates
a weak evidence against the null
hypothesis so you fail to reject the
whole null hypothesis and if you reject
that then your actual hypothesis is
probably not true the correlation of
your data with what you think it's
saying is is probably Incorrect and if
you're right at the cutoff of 0.05 it's
considered to be marginal could go
either way and again you can use that
p-value on different features to decide
whether you're going to include your
features as far as something worth
exploring in your data science model how
can outlier values be treated oh good
one you can drop outliers only if it is
a garbage value so sometimes you end up
with like one outlier that just is
probably someone's measurements way off
height of an adult equals ABC feet this
cannot be true as height cannot be a
string value in this case outliers can
be removed if the outliers have extreme
values they can be removed for example
if all the data points are clustered
between 0 to 10 but one point lies at
100 then we can remove this point and
again sometimes you just look for the
outliers you can see what's going on if
there's something unusual there so maybe
the equipment's not calibrated correctly
if you cannot drop outliers you can try
the following try a different model data
detected as outliers by linear model can
be fit by non-linear model so we be sure
you are choosing the right model so if
it has like more of a curved look to it
instead of a straight line you might
need to use something other than just a
straight line linear model try
normalizing the data this way the
extreme data points are pulled to a
similar range if you can use algorithms
which are less affected by outliers
example random Forest so there's another
solution is you can come up with the
random Force which a lot of times
completely bypasses your outliers how
can you say that a Time series data is
stationary oh that's an interesting term
stationary meaning it's not moving but
it's a Time series we can say that a
Time series is stationary when the
variance and mean of the series is
constant with time and this the graphic
example is very easy to see we have our
the variance is constant with time so we
have our first variable y and x and x
being the time factor and Y being the
variable as you can see goes through the
same values all the time it's not
changing in the long period of time so
that's stationary and then you can see
in the second example the waves get
bigger and bigger so that's
non-stationary here the variance is
changing with time again we have Y which
stays constant so that if you look at
the bigger picture it's the same wave
over and over again and then of course
we have where the wave is growing in
size going up we can also go down so
it'd also be non-stationary how can you
calculate accuracy using confusion
Matrix oh great one confusion matrixes
are so useful when you're taking that
first look at data and also when you're
showing the sharehold holders and you
want to ask them for money how can you
calculate accuracy using confusion
Matrix so you have your total data that
we're looking at is 650 and you have
your predicted values and your actual
values and you have your predicted p and
your actual p and so when you look at
this you'll note that if the predicted p
and the actual P are 262 but our
predicted P also had 15 that weren't
correct so you can see there's a false
positive there 15. and the same thing
with the N you can see where n predicts
in and it has a false negative of 26 out
of the total number of n values in there
and so we can do an accuracy on there
the true positive plus the true negative
is our total observation stations so you
have a total of 0.93 accuracy or ninety
three percent and just a quick note on
this this is so important because it's
one thing if someone is being diagnosed
with say cancer you know this is life
death or is my nuclear reactor going to
blow up Suddenly if the p is the
probability of it blowing up and this
could say you have 15 that's a lot less
than say the 26 chances of it blowing up
you know so the actual domain of your
data is very important so if you're
non-positive you don't really care about
the predicted value having non-positive
is positive because they're going to go
do a biopsy on the cancer or whatever
anyway but you're very interested if you
have a positive an actual positive value
which is looked at as negative a false
negative that's really important in that
domain depending on what domain you're
in write the equation and calculate
precision and recall rate and so
continuing with our confusion Matrix I
was just talking about the different
domains we have the Precision equals 262
over over 277 so your Precision is the
true positive over the true positive
plus false positive and the recall rate
is your true positive over the total
positive plus false negative and you can
see here we have that 262 over uh 277
equals a 94 percent and the recall over
here is the 262 over 280 which equals
9.9 or 90 percent and oh good we're
going to take a pause for another brain
teaser if a drawer contains 12 Red Socks
16 blue socks and 20 white socks how
many must pull out to be sure of having
a matching pair the last time I went
through these kind of brain teaser
things was like 20 years ago and I had
six people sitting across the table
waiting for my answer that's kind of
mind-numbing when you're in an interview
like that hopefully you're not stuck in
an interview like that but uh on this
you need to ask yourself how many
different colors of socks are there so
they've thrown a lot of extra data in
here that you don't need to solve the
answer her the answer is four an example
your first pick is white your second
pick is red third pick is blue so no
pairs yet and that means when you get to
the fourth pick there's a hundred
percent chance you're gonna have a match
so the most is going to be four that you
ever have to pull out of your drawer if
it was four colors the answer would be
five and so on it doesn't matter how
many white socks you have or how many
red socks or blue socks different pairs
you have it's the different colors a
number of different colors people who
bought this also bought recommendations
seen on Amazon as a result of which
algorithm oh we covered this earlier
recommendation engine is done with
collaborative filtering collaborative
filtering exploits the behavior of other
users in their purchase history in terms
of ratings selection Etc it makes
predictions on what you might interest a
person based on the preference of many
other users and this algorithm features
of the items are not known and we have a
nice example here where they took a
snapshot of a sales page it says for
example suppose X number of people buy a
new phone and then then also by tempered
glass with it next time when a person
buys a phone he'll be recommended to buy
tempered glass along with it and if you
remember the vocabulary words we covered
earlier this is the recommendation this
is collaborative the other word was
content based so looking at things with
similar content versus collaborative
which is similar people I remember you
know you're not going to know every
vocabulary word but it also doesn't hurt
to get your three by five cards out and
make yourself a vocabularies deck of
cards buy an app on your phone for it
SQL query I remember back in the 90s it
was so important to know SQL query and
only a few people got it nowadays it's
just part of your kit you have to know
some basic SQL so write a basic SQL
query to list all orders with customer
information and you can kind of make up
your own name for the database and you
can pause it here if you want to write
that down on a paper and let's go ahead
and look at this we have to list all
orders with customer information and so
usually you have an order table and a
customer table and you have an order ID
a customer ID order number total amount
and then from your customer table you
have ID first name last name City County
and so if we're going to write in SQL
with this we're going to select keyword
there for SQL selecting order number
total amount first name last name City
Country so that's the columns we're
going to look at we're going to do that
from our order where we're going to join
it with our customer and we're going to
join it on the order customer ID equals
the customer ID so very basic SQL query
that's going to return a table of data
for us you are given a data set on
cancer detection you've built a
classification model and achieved an
accuracy of 96 96 percent why shouldn't
you be happy with your model performance
what can you do about it that's an
interesting one because this comes up
that's one of the standard data sets on
there is for cancer detection cancer
detection results in imbalanced data in
an imbalanced data set accuracy should
not be based as a Major Performance
because it is important to focus on the
remaining four percent which are the
people who were wrongly diagnosed we
talked a little Bill about this earlier
you have to know your domain you know
this is the medical cancer domain versus
weather domain you know whether Channel
they get by with 50 wrong in cancer you
don't want four percent of the people
being wrongly diagnosed wrong diagnosis
is of a major concern because there can
be people who have cancer but we're not
predicted so in an imbalanced data set
accuracy should not be used as a
measurement performance which of the
following machine learning algorithm can
be used for inputting missing values of
both categorical and continuous
variables and so we have a couple
choices here we have k-means clustering
we have linear regression we have the K
in N nearest neighbor and decision tree
and which of the following machine
learning algorithms can be used for
inputting missing values of both
categorical and continuous variables now
certainly you can use some
pre-processing to do some of that but
you should have gone with the K nearest
neighbor because it can compute the
nearest neighbor and if it doesn't have
the value it just computes the nearest
neighbor based on all the other features
where when you're dealing with k-means
clustering or linear regression you need
to do that in your pre-processing
otherwise it'll crash decision trees
also although there's some variance on
that too can you solve another riddle
always fun ones given a box of matches
and two ropes not necessarily identical
measure a period of 45 minutes and in
this particular setup the ropes are not
uniform in nature and the Rope takes
exactly 60 Minutes to completely burn
out so each rope takes up to 60 Minutes
to burn out and there's actually a
couple different solutions to this but
let me go ahead and one of the things is
they're not uniform in nature so even
though they take 60 minutes anyways
let's go ahead and see what they did to
solve it and then we can also look at
different options we have two ropes A
and B light a from both ends and B from
one end okay when a is finished burning
we know that 30 minutes have elapsed and
B has 30 minutes remaining now light the
other end of B also so that the
remaining part of B will burn taking 15
minutes to burn this we have gotten 30
plus 15 equals 45 minutes excellent
solution mine which I like was to take
one rope fold it in two so we know it's
a half hour take the other rope fold it
in four places so we know that that
one's 15 minutes and then you can just
connect the two and burn it straight
across I think they're trying to cover
that by saying they're not regular the
ropes are have some irregularities maybe
that's what they meant by that you
couldn't do something like that that's
my solution below are the eight actual
values of Target variable in the train
file so we have a training file and not
to be confused with the Train on the
tracks we have zero zero zero one one
one one one what is the entropy of the
target variable we mentioned earlier
that you should know your entropy and
how to calculate the entropy what is the
entropy of the target variable so we
have a couple options here we have minus
five over eight logarithm of five over
eight plus three over eight logarithm of
three over eight okay let's to see where
they got those numbers from we have one
which is going to be five ones and three
zeros and then we have a total of eight
okay and then we have the option of five
which is number of ones five eight
logarithm of 5 8 plus 3 8 logarithm of 3
8 and we also have three eighths
logarithm of 5 8 plus 5 8 logarithm of 3
8 and then we kind of reverse those
numbers around and let's see what you're
going to get here which one did you
think it was you should have checked the
first one so what is the entropy of the
target variable the key there is a
target variable so we're looking at the
Target in this case is going to be one
usually that's what you're looking for
and so the entropy of that one we want
to subtract out the entropy of the
non-target variable oops I had that
backwards we want to we're looking at
zero so we want to subtract out the 5 8
from there so 5 8 logarithm 5 8 or
negative 5 8 logarithm 5 8 plus 3 8
logarithm three-eighths and they have
the hint on the bottom entropy equals I
of P of n so we have a negative p plus p
and N times the logarithm base 2 of p
over P plus n minus the N over P plus n
times the logarithm two of n over P plus
n we want to predict the probability of
death from heart disease based on three
risk factors age gender and blood
cholesterol level what is the most
appropriate algorithm for this case so
we have three features and we want to
know the predictability of death okay A
little morbid there choose a right
algorithm do we want to use logistic
regression for this linear regression K
means clustering or the Aprilia
algorithm and if you selected logistic
regression then you've probably got the
right answer linear regression remember
deals with like you take your line and
draw a line through the data and of
course you don't necessarily have to use
a straight line there's other means for
that but you're dealing with a lot of
numbers and K means means we're just
going to Cluster objects together with
the logistic regression though you can
mix those things together in buckets so
really the logistic regression is what
you want to use in that model would be
the most app fit after are studying the
behavior of a population you have
identified four specific individual
types who are valuable to your study you
would like to find all users who are
most similar to each individual type
which algorithm is most appropriate for
this study certainly identifying census
in just about a lot of different markets
is common so maybe they have a census or
whatever means but let's take a look at
some of the different algorithms we
might use on this we have k-means
clustering linear regression Association
rules and decision trees and I'll give
you a hint we're looking for grouping
people together by similarities and by
four different similarities so very
specific they gave you one of the values
specifically the K value so K means
clustering would be great for this
particular problem you have run the
association rules algorithm on your data
set and the two rules banana apple is
associated with grape and apple orange
is associated with grape have been found
to be relevant what else must be true so
this would challenge you to understand
Association rules you could picture in
this particular one you're going
shopping and you almost always see
somebody who has bananas they usually
have grapes in their bag also and
somebody who has apples usually has
grapes in their bags and then apples and
oranges is also associated with grapes
and let's go ahead and take a look at
that and we have a couple different
options here first one is banana apple
and grape orange must be a frequent item
set not so much banana apples oranges
must be relevant rule grape is common
with banana apple must be a relevant
Rule and how about grape apple must be a
frequent item set let's go back and take
a look at that and we notice that we
have bananas apples to grapes we have
apple orange to Grape boy there's a lot
of grapes and a lot of apples in there
and so if you said the last one grape
and apple must be a frequent item set
then you got it correct your
organization has a website where
visitors randomly receive one of two
coupons it is also possible that
visitors to the website will not receive
a you have been asked to determine if
offering a coupon to visitors to your
website has any impact on their purchase
decision which analysis method should
you use and so let's go ahead and start
by giving you another hint and give you
some limiting your selection we have a
one-way Anova K means clustering
Association rules and student t-test so
obviously you should know what each one
of these means but let's take a look at
the question again so you want to know
which method should you use to see if
the coupons valid for their purchase
well we're not clustering and we're not
associating things together we want to
know the end result student t-test also
drawing that little T in boxes and
switch them around there's really only
one answer that works in here and that's
a one-way Anova and with that you've
reached the end of this Advanced Data
science full course if you have any
questions please feel free to comment
and we will have it answered as soon as
possible until next time thank you for
watching stay safe keep learning and get
ahead staying ahead in your career
requires continuous learning and
Skilling whether you're a student aiming
to learn today's top skills or a working
professional looking to advance your
career we've got you covered explore our
impressive catalog of certification
programs in Cutting Edge domains
including data science cloud computing
cyber security AI machine learning or
digital marketing designed in
collaboration with leading universities
and top corporations and delivered by
industry experts choose any of our
programs and set yourself on the path to
Career Success click the link in the
description to know more
thank you
hi there if you like this video
subscribe to the simply learned YouTube
channel and click here to watch similar
videos turn it up and get certified
click here