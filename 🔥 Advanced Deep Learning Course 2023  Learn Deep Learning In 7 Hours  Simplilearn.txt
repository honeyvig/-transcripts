hello everyone and welcome to this
Advanced deep learning full course by
simply learn in this comprehensive
program we will dive into the concept of
deep learning deep learning a subfield
of artificial intelligence has made
significant progress in recent years and
profoundly impacted various aspects of
modern world one area where deep
learning has been particularly
transformative is in the advancement of
chatbots and other conversational AI
systems chat Bots are computer programs
designed to stimulate human conversation
and with the help of deep learning
techniques they have become increasingly
intelligent versatile and capable of
engaging in more natural and humanlike
interactions professionals with deep
learning skills can access various
career opportunities including roles
like deep learning engineer machine
learning engineer data scientist
computer vision engineer and NLP
engineer and Based on data from glass
door the average annual salary for deep
learning Professionals in the United
States is around
$19,000 and the average annual salary
for deep learning Professionals in India
is 8 lakhs so if you want to seek
expertise in deep learning skills and
aspire to be a part of Cutting Edge
field of building chat Bots and service
Bots using deep learning our calch
postgraduate program in Ai and machine
learning provides comprehensive coverage
of the latest tools and Technologies
within the AI ecosystem it offers master
classes led by esteemed Caltech faculty
and IBM experts engaging hackathons and
interactive asme anything sessions by
enrolling in this program delivered in
partnership with IBM you will gain the
expertise needed to to become a
professional in Ai and ml this
comprehensive artificial intelligence
course will equip you with the knowledge
about various AI based Technologies
empowering you to excel in this field so
do check out the course Link in the
description for more details let's get
started
now and the 2.0 addressed so many things
out there that the 1.0 really needed so
when we start talking about tensor flow
1.0 versus 2.0 um I guess you would need
to know this for um a legacy programming
job if you're pulling apart somebody
else's code the first thing is that
tensorflow 2.0 supports eager execution
by default it allows you to build your
models and run them instantly and you
can see here from tensorflow one to
tensorflow 2 uh we have um almost double
the code to do the same thing so if I
want to do um with tf. session or
tensorflow session um as a session the
session run you have your variables your
session run you have your tables
initializer and then you do your model
fit
um X train y train and then your
validation data your x value yv value
and your epics and your batch size all
that goes into the fit and you can see
here where that was all just compressed
to make it run easier you can just
create a model and do a Fed on it uh and
you only have like that last set of code
on there so it's automatic that's what
they mean by the eager so if you see the
first part and you're like what the heck
is all this session thing going on
that's tensorflow 1.0 and then when you
get into 2.0 it's just nice and clean if
you if you remember from the beginning I
said coros uh on our list up there and
cross is the high level API in tensor
flow 2.0 cross is the official highlevel
API of tensorflow 2.0 it has
incorporated cross as tf. cross cross
provides a number of model building apis
such as sequential functional and
subclassing so you can choose the right
level of abstraction for your project
and uh we'll hopefully touch base a
little bit more on this sequential being
the most common uh form that is your
your layers are going from one side to
the other so everything's going in a
sequential order
functional is where you can split the
layer so you might have your input
coming on one side it splits into two
completely Mo different models and then
they come back together U and one of
them might be doing classification the
other one might be doing just linear
regression kind of stuff or neural basic
uh reverse propagation neural net networ
and then those all come together into
another layer which is your uh neural
network reverse propagation setup
subclassing is the most complicated as
you're building your own models and you
can subclass your own models into carass
so very powerful tools here this is all
the stuff that's been coming out
currently in the tensorflow cross setup
a third big change we're going to look
at is it in tensor flow
1.0 uh in order to use TF layers as
variables uh you would would have to
write TF variable block so you'd have to
predefine that in tensor flow 2 you just
add your layers in under the sequential
and it automatically defines them as
long as they're flat layers of course
this changes a little bit as the more
complicated um tensor you have coming in
but all of it's very easy to do and
that's what 2.0 does a really good job
of and here we have um a little bit more
on the scope of this and you can see how
tensor flow one asks you to do um these
different layers and value if you look
at the scope and the default name you
start looking at all the different code
in there to create the variable scope
that's not even necessary intenser 2.0
so you'd have to do one before you do do
what you see the code in 2.0 in 2.0 you
just create your model it's a sequential
model and then you can add all your
layers in you don't have to precreate
the um uh variable scope so if you ever
see the variable scope you know that
came from an older version and then we
have the last two which is our API
cleanup and the autograph uh in the API
cleanup tensor flow one you could build
models using TF Gans TF app TF contrib
TF Flags Etc in tensorflow 2 uh a lot of
apis have been removed and this is just
they just cleaned them up because people
weren't using them and they've
simplified them and that's your TF app
your TF flags and your TF logging are
all gone uh so there's those are three
Legacy features that are not in 2.0 and
then we have our TF function and
autograph feature in the old version uh
tensorflow 10 the python functions were
limited and could not be compiled or
exported reimported so you were
continually having to redo your code and
you couldn't very easily just um uh put
a pointer to it and say herey let's
reuse this in tensorflow 2 you can write
a python function using the TF function
to mark it for the jit compilation for
the python jit so that tensorflow runs
it as a single graph autograph feature
of TF function helps to write graph code
using natural python
syntax uh now we just threw in a new
word in you graph uh graph is not a
picture of a person uh you'll hear graph
x and some other things graph is what
are all those lines that are connecting
different objects so if you remember
from before where we had uh the
different layers going through
sequentially each one of those wh lined
arrows would be a graph x that's where
that comp utation taken care of and
that's what they're talking about and so
if you had your own special code or
python way that you're sending that
information forward you can now put your
own function in there instead of using
whatever function they're using in
neural networks this would be your
activation function although it could be
almost anything out there depending on
what you're doing next let's go for
hierarchy and architecture and then
we'll cover three basic Tools in
tensorflow before we roll up our sleeves
and dive into the example so let's just
take a quick look look at tensor flow
tool kits in their hierarchy at the high
level we have our object-oriented API so
this is what you're working with you
have your TF carass you have your
estimators this sits on top of your TF
layers TF losses TF metrics so you have
your reusable libraries for model
building this is really where tins or
flow shines is between the carass uh
running your estimators and then being
able to swap in different layers you can
your losses your metrics all of that is
so built into tensor flow makes it
really easy to use and then you can get
down to your lowlevel TF API um you have
extensive control over this you can put
your own formulas in there your own
procedures or models in there uh you
could have it split we talked about that
earlier so with the 2.0 you can now have
it split One Direction where you do a
linear regression model and then go to
the other where it does a uh neural
network and maybe each neural network
has a different activation set on it and
then it comes together into another
layer which is another neural network so
you can build these really complicated
models and at the low level you can put
in your own apis you can move that stuff
around and most recently we have the TF
code can run on multiple platforms and
so you have your CPU which is uh
basically like on the computer I'm
running on I have uh eight cores and 16
dedicated threads I he they now have one
out there that has over a 100 cores uh
so you have your CPU running and then
you have your G GPU which is your
graphics card and most recently they
also include the TPU setup which is
specifically for tensor flow models uh
neural network kind of set up so now you
can export the TF code and it can run on
all kinds of different platforms for the
most um diverse setup out there and
moving on from the hierarchy to the
architecture in the tensorflow 2.0
architecture uh we have uh you can see
on the left this is usually where you
start out with an 80% your time in data
science is spent pre-processing data
making sure it's loaded correctly and
everything looks right uh so the first
level in tensor flow is going to be your
read and pre-processed data your TF data
feature columns this is going to feed
into your TF caros or your pre-made
estimators and kind of you have your
tensor flow Hub that sits on top of
there so you can see what's going on uh
once you have all that set up you have
your distribution strategy where are you
going to run it are you going to be
running it on just your regular CPU
are you going to be running it uh with
the GPU added in um like I have a pretty
highend graphics card so it actually
grabs that GPU processor and uses it or
do you have a specialized TPU setup in
there that you paid extra money for uh
it could be if you're and later on when
you're Distributing the package you
might need to run this on some really
high processors because you're
processing at a server level for uh
let's say net you might be processing
this at a um a distribute you're
Distributing it not the distribution
strategy but you're Distributing it into
a server where that server might be
analyzing thousands and thousands of
purchases done every minute um and so
you need that higher speed to give them
a um to give them a recommendation or
suggestion so they can buy more stuff
off your website or maybe you're looking
for uh data fraud analysis working with
the banks you want to be able to run
this at a high speed so that when you
have hundreds of people sending their
transactions and it says hey this
doesn't look right someone's scamming
this person and probably has their
credit card so when we're talking about
all those fun things we're talking about
saved model this is we were talking
about that earlier where it used to be
when you did one of these models it
wouldn't truncate the float numbers the
same and so a model going from one you
build the model on your com machine in
the office and then you need to
distribute it and so we have our tensor
flow serving Cloud on premium that's
what I was talking about if you're like
a banking or something like that
now they have tensorflow light so you
can actually run a tensorflow on an
Android or an iOS or Raspberry Pi little
breakout board there in fact they just
came out with a new one that has a
built-in this just a little mini TPU
with the camera on it so it can
pre-process a video so you can load your
tensorflow model onto that um talking
about an affordable way to beta test uh
a new product uh you have the tensorflow
JS which is for browser and node server
so you can get that out on the brows
browser for some simple computations
that don't require a lot of heavy
lifting but you want to distribute to a
lot of end points and now they also have
other language binding so you can now
create your uh tensorflow backend save
it and have it accessed from C Java go C
rust r or from whatever package you're
working on so we kind of have an
overview of the architecture and what's
going on behind the scenes and in this
case what's going on as far as
Distributing it let's go ahead and take
a look at uh three specific pieces of
tensor flow and those are going to be
constants variables and sessions uh so
very basic things you need to know and
understand when you're working with the
tensor flow uh setup so constants in
tensor flow in tensor flow constants are
created using the function constant uh
in other words they're going to stay
static the whole time whatever you're
working with the Syntax for constant uh
value dtype 9 shape equals none name
constant verify shape equals false
that's kind of the syntax you're looking
at and we'll explore this with our hands
on a little more in depth uh and you can
see here we do zal tf. constant 5.2 name
equals x uh dtype is a float that means
that we're never going to change that
5.2 it's going to be a constant value
and then we have our variables and
tensor flow uh variables and tensor flow
are in memory buffers that store tensors
and so we can declare a 2x3 tensor
populated by ones you could also do
constants this way by the way so you can
create a um an array of ones for your
constants I'm not sure why you do that
but you know you might need that for
some reason um in here we have V equals
tf.
variables and then in tensorflow you
have tf. On's and you have the shape
which is 23 which is then going to
create a nice uh 2x3 um array that's
filled with ones and then of course you
can go in there and their variables so
you can change them it's a tensor so you
have full control over that and then you
of course have uh sessions in tensor
flow uh session in tensorflow is used to
run a computational graph to evaluate
the nodes and remember when we're
talking about graph or graph x we're
talking about all that information then
goes through all those arrows and
whatever computations they have that
take it to the next node and you can see
down here uh where we have import tensor
flow as TF if we do x equals a tf.
constant of 10 we do yal a TF constant
of 2.0 of 20.0 and then you can do zal
tf. variable and it's a tf. addx comma y
uh and then once you have that setup in
there you go ahead andit your TF Global
variables initializer with TF session as
session you can do a session run nit and
then you print the session run
y uh and so when you run this you're
going to end up with of course the uh 10
+ 20 is 30 and we'll be looking at this
a lot more closely is we actually roll
up our sleeves and put some code
together so let's go ahead and take a
look at that and for my coding today I'm
going to go ahead and go through
anaconda and then I'll use specifically
the Jupiter Notebook on there and of
course this code is going to work uh
whatever platform you choose whether
you're in a notebook um the Jupiter lab
which is just a Jupiter notebook but
with tabs for larger projects we're
going to stick with Jupiter notebook pie
charm uh whatever it is you're going to
use in here uh you know you have your
spider and your QT console for different
programming environments the thing to
note um is kind of hard to see but I
have my main Pi
36 right now when I was writing this
tensor flow Works in Python version 36
if you have python version 37 or 38
you're probably going to get some errors
in there uh might be that they've
already updated and I don't know it and
I have an older version but you want to
make sure you're in a python version 36
your environment and of course in
Anaconda I can easily set that
environment up make sure you go ahead
and and um pip in your tensor flow or if
you're in Anaconda you can do AA install
tensor flow to make sure it's in your
package so let's just go ahead and dive
in and bring that up this will open up a
nice browser window I just love the fact
I can zoom in and zoom out depending on
what I'm working on making it really
easy to adjust um a demo for the right
size go under new and let's go ahead and
create a new Python and once we're in
our new python window this is is going
to leave it Untitled uh let's go ahead
and import import tensor flow as TF uh
at this point we'll go ahead and just
run it real quick no errors yay no
errors I do that whenever I do my
imports because I I unbearably will have
opened up a new environment and
forgotten to install tensor flow into
that environment uh or something along
those lines so it's always good to
double check
uh and if we're going to double check
that we also it's also good to know uh
what version we're working with and we
can do that simply by um using the
version command in
tensorflow which uh you should know is
is probably intuitively the TF _ uncore
verore
uncore and you know it always confuses
me because sometimes you do tf. version
for one thing you do TF doore verore for
another thing uh this is a double
underscore in tensor flow for pulling
your version out and it's good to know
what you're working with we're going to
be working in tensorflow version
2.1.0 and I did tell you that the um we
were going to dig a little deeper into
our constants and you can do an array of
constants and we'll just create this
nice array um AAL tf. constant and we're
just going to put the array right in
there 43 61 uh we can run this and now
that is what a is equal to and if we
want to just double check that uh
remember we're in Jupiter notebook where
we can just put the letter a and it
knows that that's going to be print um
otherwise you you surround it in print
and you can see it's a TF tensor it has
the shape the type and the and the array
on here it's a 2X two array and just
like we can create a constant we can go
and create a variable and this is also
going to be a 2X two array and if we go
ahead and print the V out we'll run that
uh and sure enough there's our TF
variable in
here uh then we can also let's just go
back up here and add this in here um I
could create another tensor and we'll
make it a constant this
time and we'll go and put that in over
here uh we'll have BTF constant and if
we go and print out uh V and B let me go
and run that and this is an interesting
thing that always that happens in here
uh you'll see right here when I print
them both out what happens it only
prints the last one unless you use print
commands uh so important to remember
that in Jupiter notebooks but we can
easily fix that by go ahead and print
and Surround V with brackets and now we
can see with the two different variables
we have uh we have the 3152 which is a
variable and this is just a flat uh
constant so it comes up as a TF tensor
shape two kind of to and that's
interesting to note that this label is a
tf. tensor and this is a TF variable so
that's how it's looking in the back end
when you're talking about about the
difference between a variable and a
constant the other thing I want you to
notice is that in variable we capitalize
the V and with the constant we have a
lowercase C little things like that can
lose you when you're programming and
you're trying to find out hey why
doesn't this work uh so those are a
couple little things to note in here and
just like any other array in math uh we
can do like a concatenate or concatenate
the different values here uh and you can
see we can take um a B concatenated you
just do a tf. concat values and there's
our AB axis on one hopefully you're
familiar with axes and how that works
when you're dealing with matrixes and if
we go ahead and print this out uh you'll
see right here we end up with a tensor
so let's put it in as a constant not as
a variable and you have your array 4378
and 6145 it's concatenated the two
together and again I want to highlight a
couple things on this our axis equals
one this means we're doing the columns
um so if you had a longer array like
right now we have an array that is like
you know has a shape one Whatever It Is
2 comma 2 um axis zero is going to be
your first one and axis one is going to
be your second one and it translates as
columns and rows if we had a shape let
me just put the word shape here um so
you know what I'm talking about and it's
very clear and this is I'll tell you
what I spent a a lot of time looking at
these shapes and trying to figure out
which direction I'm going in and whether
to flip it or whatever um so you can get
lost in which way your Matrix is going
and which is column which is rows are
you dealing with the third Axis or the
second axis um axis one you know 0 1 2
that's going to be our columns uh and if
you can do columns then we also can do
rows and that is simply just changing
the concatenate uh we'll just grab this
one here and copy it we'll do the whole
thing over um contrl
copy contrl V and changes from axis one
to axis zero and if we run that uh
you'll see that now we can catenate by
row as opposed to column and you have 4
3 6 1 784 7 so it just brings it right
down turns it into rows versus columns
you can see the difference there your
output this really you want to look at
the output sometimes just to make sure
your eyes are looking at it correctly
and it's in the format um I find
visually looking at it is almost more
important than understanding what's
going on uh because conceptually your
mind just just too many dimensions
sometimes the second thing I want you to
notice is it says a numpy array uh so
tensor flow is utilizing numpy as part
of their format as far as Python's
concerned and so you can treat you can
treat this output like a numpy array cuz
it is just that it's going to be a numpy
array another thing that comes up uh
more more than you would think is
filling U one of these with zeros or
ones and so you can see here we just
create a tensor tf.
Z and we give it a shape we tell it what
kind of data type it is in this case
we're doing an integer and then if we um
print out our tensor again we're in
Jupiter so I can just type out tensor
and I run this you can see I have a nice
array of um with shape 3 comma 4 of
zeros one of the things I want to
highlight here is integer 32 if if I go
to the um tensor flow data types I want
you to notice how we have float 16 float
32 float 64 uh complex if we scroll down
you'll see the integer down here of 32
the reason for this is that we want to
control how many bits are used in the
Precision this is for exporting it to
another platform uh so what would happen
is I might run it on this computer where
python goes does a float to indefinite
however long it wants to um and then we
can take it but we want to actually say
hey we don't want that high Precision we
want to be able to run this on any
computer and so we need to control
whether it's a TF float 16 in this case
we did an integer
32 we could also do this as a float so
if I run this as a float 32 uh that
means this has a 32bit Precision you'll
see 0 point whatever and then to go with
uh
zeros we have ones if we're going from
the opposite side and so we can easily
just create a tensor flow with ones you
might ask yourself why would I want
zeros and ones and your first thought
might be to initiate a new tensor
usually we initiate a lot of this stuff
with random numbers because it does a
better job solving it if you start with
a uniform uh set of ones or zeros you're
dealing with a lot of bias so be very
careful about starting a neural network
uh for one of your rows or something
like that with ones and zeros on the
other hand uh I use this for masking you
can do a lot of work with masking you
can also have uh it might be that one t
a row is masked um you know zero is is
false one is true or whatever you want
to do it um and so in that case you do
want to use the zeros and ones and there
are cases where you do want to
initialize it with all zeros or all ones
and then swap in different numbers as
the as the um tensor learns so it's
another form of control but in general
you see zeros and ones you use are
talking about a mask over another array
and just like in uh numpy you can also
uh do reshapes so if we take our
remember this is shaped 3 comma 4 maybe
we want to swap that to 4 comma 3 and if
we print this out you will see let me
just go and do that contrl V let me run
that and you'll see that the the order
of these is now switched instead of uh
four across now we have three across and
four
down and just for fun let's go back up
here where we did the ones and I'm going
to change the ones to um tf. random
uniform uh and we'll go and just take
off well we'll go and leave that we'll
go and run this and you'll see now we
have uh
0441 and this way you can actually see
how the reshape looks a lot different uh
0411 5.71 and then instead of having
this one it rolls down here to the
0.14 and this is what I was talking
about sometimes you fill a lot of times
you fill these with random numbers and
so this is the random. uniform is one of
the ways to do that now I just talked a
little bit about this float 32 and all
these data types uh one of the things
that comes up of course is recasting
your
data um so if we have a dtype float 32
we might want to convert these two
integers because of the project we're
working on um I know one of the projects
I've worked on ended up wanting to do a
lot of roundoff so that it would take a
dollar amount or a float value and then
have to run it off to a dollar amount so
we only wanted two decimal points um and
in which case you have a lot of
different options you can multiply by
100 and then round it off or whatever
you want to do there's a lot of or then
convert it to an integer with one way to
round it off uh kind
of cheap and dirty
trick uh so we can take this and we can
take this same tensor and we go ahead
and create a um as an integer and so
we're going to take this tensor we're
going going to tf. cast it and if we
print tensor uh and then we're going to
go ahead and print our tensor let me
just do a quick copy and paste and when
I'm actually programming I usually type
out a lot of my stuff just to double
check it uh in doing a demo copy and
paste works fine but sometimes be aware
that uh copy and paste can copy the
wrong code over personal choice depends
on what I'm working on and you can see
here we took um a float 32 4.6 4.2 and
so on and it just converts it right down
to a integer value uh so our integer 32
setup and uh remember we talked about um
a little bit about
reshape um as far as flipping it and I
just did uh 4 comma 3 on the reshape up
here and we talked about axis zero axis
one uh one of the things that is
important to be a ble to do is to take
one of these variables we'll just take
this last one ensor as
integer and I want to go ahead and
transpose it and so I can do um we'll do
a equals tf.
transpose and we'll do our tensor
integer in there and then if I print the
A out and we run this you'll see it's
the same array but we've flipped it so
that our columns and rows are flipped
this is is the same as reshaping uh so
when you transpose you're just doing a
reshape what's nice about this is that
if you look at the numbers The Columns
when when we went up here and we did the
reshape they kind of roll down to the
next row so you're not maintaining the
structure of your Matrix so when we do a
reshape up here they're similar but
they're not quite the same and you can
actually go in here and there's settings
in the reshape that would allow you to
turn it into a
transform uh so when we come down here
it's all done for you and so there are
so many times you have to transpose your
digits that this is important to know
that you can just do that you can flip
your rows and columns rather quickly
here and just like numpy you can also do
multip your different math functions
we'll look at multiplication and so
we're going to take matrix
multiplication of tensors uh we'll go
and create a as a constant
5839 and we'll put in a vector v 4 comma
2 and we could have done this where they
match matched where this was a 2X two
array um but instead we're going to do
just a 2x1 array and the code for that
is your tf. matat mole uh so Matrix
multiplier and we have a * V and if we
go ahead and run this
let's make sure we print out our av on
there and if we go ahead and run this uh
you'll see that we end up with 36x 30
and if it's been a while since you seen
The Matrix math uh this is 5 * 4 + 8 * 2
um 3 * 4 + 9 *
2 and that's where we get the 36 and 30
now I know we're covering a lot really
quickly as far as the basic
functionality uh so the Matrix or your
Matrix multiplier is a very commonly
used backend tool as far as Computing um
uh different models or linear regression
stuff like that one of the things is to
note is that just like in um numpy you
have all of your different math so we
have our TF math and if we go in here we
have um functions we have our cosiness
absolute angle all of that's in here so
all of these are available for you to
use in the tensorflow model and if we go
back to our example and let's go ahead
and pull uh um oh let's do some
multiplication that's always good we'll
stick with our um AV our um constant a
and our vector
v and we'll go ahead and do some bitwise
multiplication and we'll create an AV
which is a * V let's go and print that
out and you can see coming across here
uh we have the 42 and the
5839 and it produces uh 2032
618 and that's that's pretty
straightforward if you look at it you
have 4 * uh 5 is 20 4 * 8 is uh 32
that's where those numbers come
from uh we can also quickly create an
identity
Matrix which is basically um your main
values on the diagonal being ones and
zeros across the other side let's go
ahead and take a look and see what that
uh uh looks like and we can do let's do
this uh so we're going to get the shape
um this is a simple way very similar to
your numpy you can do a. shape and it's
going to return a tupal in this case our
rows and columns and so we can do a
quick uh print we'll do rows
oops and we'll do columns
and if we run this uh you can see we
have three rows uh two
columns and then if we go ahead and
create an identity
Matrix uh
SCS the script for that hit a wrong
button there the script for that looks
like
this where we have the number of rows
equals rows the number of columns equals
columns and D type is a 30 2 and then if
we go ahead and just print out our
identity you can see we have a nice
identity column with our ones going
across here now clearly we're not going
to go through every math module um
available but we do want to start
looking at this as a prediction model
and seeing how it functions so we're
going to move on to a more of a um
direct setup where we can actually see
the full tensor flow in use for that
let's go back and create a uh new
setup and we'll go in here new Python 3
module there we
go bring this out so it takes up the
whole window cuz I like to do that
hopefully you made it through that first
part and you have a basic understanding
of tensor flow as far as being uh a
series of numpy arrays you got your math
equations and different things that go
into them
we're going to start building a full um
setup as far as the numpy so you can see
how uh K sits on top of it and the
different aspects of how it works the
first thing we want to do is we're going
to go and do a lot of imports uh date
times warnings CI scipi is your um uh
math so the backend scientific math uh
warnings because whenever we do a lot of
this you have older versions newer
versions um and so sometimes when you
get warnings you want to go ahead and
just suppress them we'll talk about that
if it comes up on this particular setup
and of course date time pandas again is
your data frame think rows and columns
we import it as PD numpy is your uh
numbers array which of course tensor
flow is integrated heavily with caborn
for our graphics and the caborn as SNS
is going to be set on top of our map
plot Library which we import as MPL and
then of course we're going to import our
matplot library pip plot as PLT and
right off the bat we're going to set
some graphic colors um patch Force Edge
color equals true the style we're going
to use the 538 style you can look this
all up there's when you get into map
plot Library into Seaborn there are so
many options in here it's just kind of
nice to make it look pretty when we
start the um when we start up that way
we don't have to think about it later on
uh and then we're going to take we have
our uh mlrc we're going to put a patch
Ed color dim Gray Line width again this
is all part of our Graphics here in our
setup uh we'll go ahead and do an
interactive shell uh node interactivity
equals last expression uh here we are PD
for pandas options display Max columns
so we don't want to display more than 50
um and then our map plot library is
going to be inline this is a Jupiter
notebook thing the map plot library in
line uh then warnings we're going to
filter our warnings and we're just going
to ignore warnings that way when they
come up we don't have to worry about
them not really what you want to do when
you're working on a major project you
want to make sure you know those
warnings and then uh filter them out and
ignore them later on and if we run this
it's just going to be loading all that
into the background uh so that's a
little backend kind of stuff then what
we want to go ahead and do is we want to
go ahead and import our specific
packages U that we're going to be
working with which is under carass now
remember cross kind of sits on tensor
flow so when we're importing carass and
the sequential model we are in effect
importing um tensor flow underneath of
it uh we just brought in the math
probably should have put that up above
and then we have our cross models we're
going to import sequential now if you
remember from our uh slide there was
three different options let me just flip
back over there so we can have a quick
uh recall on that and so in carass uh we
have sequential function
and subclassing so remember those three
different setups in here we talked about
earlier and if you remember from here we
have uh sequential where it's going one
tensor flow layer at a time you go kind
of look at think of it as going from
left to right or top to bottom or
whatever Direction it's going in but it
goes in One Direction all the time where
functional can have a very complicated
graph of directions you can have the
data split into two separate um tensors
and then it comes back together into
another tensor
um all those kinds of things and then
subclassing is really the really
complicated one where now you're adding
your own subclasses into the tensor to
do external computations right in the
middle of like a huge flow of data uh
but we're going to stick with sequential
it's not a big jump to go from
sequential to functional uh but we're
running a sequential tensor flow and
that's what this first import is here we
want to bring in our sequential and then
we have our layers and let's talk a
little little bit about these layers
this is where cross and tensor flow
really are happening this is what makes
them so nice to work with is all these
layers are pre-built uh so from carass
we have layers import dents from carass
uh layers import
lstm when we talk about these layers uh
carass has so many built-in layers you
can do your own layers the dense layer
is your standard neural network um by
default uses Ru for its
activation and then the lstm is a long
shortterm memory layer since we're going
to be looking probably at sequential
data uh we want to go ahead and do the
lstm and if we go
into um carass and we look at their
layers this is the carass website you
can see as we scroll down for the caros
layers that are built in we can get down
here and we can look at let's see here
we have our layer activation our base
layers
um activation layer weight layer weights
there's a lot of stuff in here we had
the railu which is the basic activation
that was listed up here for layer
activations you can change those and
here we have our core layers and our
dense layers you have an input layer a
dense layer um and then we've added more
customized one with the long-term
short-term memory layer and of course
you can even do your own custom layers
in carass there's a whole functionality
in there if you're doing your own thing
what's really nice about this is it's
all built in uh even the convolutional
layers this is for processing Graphics
there's a lot of cool things in here you
can do um this is why cross is so
popular it's open source and you have
all these tools right at your fingertips
so from Cross we're just going to import
a couple layers the dense layer um and
the long short-term memory layer and
then of course from uh sklearn our Cy
kit we want to go ahead and do our
minmax scaler standard scaler for
pre-editing our data and then metrics
just so we can take a look at the errors
and compute those let me go ahead and
run this that just loads it up we're not
expecting anything from the output and
our file coming in is going to be air
quality. CSV let's go ahead and take a
quick look at that this is in Open
Office it's just a standard you know you
do excel whatever you're using for your
spreadsheet and you can see here we have
uh a number of columns uh number of rows
it actually goes down to like 8,000
the first thing we want to notice is
that the first row is kind of just a
random number put in going down probably
not something we're going to work with
the second row um is bandang I'm
guessing that's a reference for the
profile if we scroll to the bottom which
I'm not going to do because it takes
forever to get back up they're all the
same uh the same thing with the status
the status is the same we have a date so
we have a sequential order here um here
is the gam which I'm going to guess is
the time stamp on there so we have a
date and time we have our 03 coo N2
reading s SO2 no CO2 VOC um and then
some other numbers here pm1 pm2.5 pm4
pm1 10 uh without actually looking
through the data um I mean some of this
I can guess is like temperature humidity
I'm not sure what the PMS are uh but we
have a whole slew of data here uh so
we're looking at air quality as far as
an area and a region and what's going on
with our date time stamps on there and
so codewise we're going to read this
into a pandas data frame so our data
frame uh DF is a nice abbreviation
commonly used for data frames equals pd.
read CSV and then our the path to it
just happens to be on my D drive uh
separated by spaces and so if we go
ahead and run this we'll print out the
head of our data and again this looks
very similar to what we were just
looking at
um being in Jupiter I can take this and
go the other way uh make it real small
so you can see all the columns going
across and we get a full view of it um
or we can bring it back up in size
that's pretty small on there overshot um
but you can see it's the same data we
were just looking at uh we're looking at
the number we're looking at the profile
which is the Bandung the um date we have
a time stamp our 03 count Co and so
forth on here uh and this this is just
your basic pandas printing out the top
five rows we could easily have done uh
three rows uh five rows 10 whatever you
want to put in there by default that's
five for pandas now I talk about this
all the time so I know I've already said
it at least once or twice during this
video most of our work is in Pre
formatting data what are we looking at
how do we bring it together uh so we
want to go ahead and start with our date
time uh it's come in in two columns we
have our date here and we have our time
and we want to go ahead and combine that
and then we have uh this is just a
simple script in there that says combine
date time that's our formula we're
building our we're going to submit our
um Panda's data frame and the tab name
when we go ahead and do this uh that's
all of our information that we want to
go ahead and create and then goes for
ION range DF uh shape zero so we're
going to go through um the whole setup
and we're going to list tab a pin DF
location I and here is our date uh going
in there and then return the numpy array
list tab D types date time 64 that's all
we're doing we're just switching this to
a date time stamp and if we go ahead and
do DF date time equals combined date
time and then I always like to uh print
we'll do DF head just so we can see what
that looks like and so when we come out
of this uh we now have our setup on here
and of course just edit it on to the far
right here's our date time you can see
the formats changed uh so there's our
we've added in the date time column and
we've brought the date over and we've
taken this format here and it's an
actual variable with a 0000 on here well
that doesn't look good so we need to
also include the time part of this and
we want to convert it into hourly data
uh so let's go ahead and do that uh to
do that uh to finish combining our date
time let's go ahead and create a a uh a
little script here to combine the time
in there same thing we just did we're
just creating a numpy array returning a
numpy array and C forcing this into a
datetime format and we can actually
spend hours just going through uh these
conversions how do you pull it from the
Panda's data frame how do you set it up
um so I'm kind of skipping through it a
little fast cuz I want to stay focused
on tensor flow and carass keep in mind
this is like 80% of your coding when
you're doing a lot of this stuff is
going to be reformatting these things
resetting them back up uh so that it
looks right on here and uh you know it
just takes time to to get through all
that but that is usually what the
companies are paying you for that's what
the big bucks are
for and we want to go ahead and uh a
couple things going on here is we're
going to go ahead and do our date time
we're going to reorganize some of our
setup in here convert into hourly data
we just put a pause in there um
now remember we can select from DF our
different columns we're going to be
working with and you're going to see
that we actually dropped a couple of the
columns those ones I showed you earlier
they're just repetitive data uh so
there's nothing in there that exciting
and then we want to go ahead and we'll
create a uh second data frame here let
me just get rid of the DF head and df2
was we're going to group by date time
and we're looking at the mean value and
then we'll print that out so you can see
what we're talking about uh we have now
reorganized this so we put in date time
03 Co so now this is in the same order
um as it was before and you'll see the
date time now has our 0 0 uh same date
one 2 three and so on so it's group the
data together so it's a lot more
manageable and in the format we want and
in the right sequential order and if we
go back to um there we go our air
quality uh you can see right here here
we're looking at um these columns going
across we really don't need since we're
going to create our own date time column
we can get rid of those these are the
different Columns of information we want
and that should reflect right here in
the columns we picked coming across so
this is all the same columns on there
that's all we've done is reformatted our
data grouped it together by date and
then you can see the different data
coming out uh set up on there uh and
then as a data s scientist first thing I
want to do is get a description what am
I looking at uh and so we can go ahead
and do the df2 describe and this gives
us our um you know describe gives us our
basic uh data analytics information we
might be looking for like what is the
mean standard
deviation uh minimum amount maximum
amount we have our first quarter second
quarter and third quarter um numbers
also in there uh so you can get a quick
look at a glance describing the data or
descriptive analysis and even though we
have our quantile information in here
we're going to dig a little deeper into
that uh we're going to calculate the
quantile for each variable uh we're
going to look at a number of things for
each variable we'll see right here q1 uh
we can simply do the quantile
25% which should match um our 25% up
here and we're going to be looking at
the Min the max um and we're just going
to do this is basically we're breaking
this down for each uh different variable
in there one of the things is kind of
fun to do uh we're gonna look at that in
just a second let me get put the next
piece of code in here um got clean out
some of our um we're going to drop a
couple thing our um last rows and first
row because those have usually have a
lot of null values and the first row is
just are titles uh so that's important
it's important to drop those rows in
here and so this right here as we look
at our different
quantiles again it's it's the same you
we're still looking at the 25
quantile here we're going to do a little
bit more with this um so now that we've
cleared off our first and last rows
we're going to go ahead and go through
all of our columns and this way we can
look at each uh column individually and
so we'll just create a q1 Q3 min max uh
Min IQR Max IQR and calculate the
quantile of I of
df2 we're basically doing uh the math
that they did up here but we're
splitting it apart um that's all this is
and this happens a lot because you might
want to look at individual uh if this
was my own project I would probably
spend days and days going through what
these different values mean one of the
biggest data science uh things we can
look at that's
important is uh use your use your common
sense you know if you're looking at this
data and it doesn't make sense and you
go back in there and you're like wait a
minute what the heck that I just do at
that point you probably should go back
and double check what you have going on
now uh we're looking at this and you can
see right here here's our attribute for
our 03 so we've broken it down uh we
have our q1 5.88 Q3 10.37 if we go back
up here here's our 58 we've uh rounded
it off 10.37 is in
there so we've basically done the same
math uh just split it up we have our
minimum and our Max IQR and that's
computed uh let's see where is it here
we go uh q1 minus 1.5 * IQR and the IQR
is your Q3 minus q1 so that's the
difference between our two different
quarters and this is all uh data science
um as far as a hard math we're really
not we're actually trying to focus on
carass and tensor flow you still got to
go through all this stuff I told you 80%
of your programming is going through and
understanding what the heck uh happened
here
what's going on what does this data mean
and so when we looking in that we're
going to go ahead and say hey
um we've computed these numbers and the
reason we've computed these numbers is
if you take the minimum value and is
less than your Min minimum
IQR uh that means something's going
wrong there and they usually in this
case is going to show us an outlier uh
so we want to go ahead and find the
minimum value if it's less than the
minimum minimum IQR it's an outlier and
if the max value is greater than the uh
Max IQR uh we have an outlier and that's
all this is doing low outlier is found
uh minimum value High outlier is found
uh really important actually outliers
are almost everything in data sometimes
sometimes you do this project just to
find the outliers CU you want to know uh
crime detection what are we looking for
we're looking for the outliers what
doesn't fit a normal business deal and
then we'll go ahead and throw in um just
threw in a lot of code oh my goodness uh
so we have if your max is greater than
IQR print outlier is found what we want
to do is we want to start cleaning up
these outliers and so we want to convert
uh we'll do create a convert Nan x max
IQR equals maxcore IQR Min IQR equal Min
IQR so this is just saying this is the
data we're going to send that's all that
is in Python and if x is greater than
the max IQR and X is less than the Min
IQR X equals uh null we're going to set
it to null why because we want to clear
these outliers out of the data now again
if you're doing fraud detection you
would do the opposite you would be
cleaning everything else that's not in
that Series so that you can look at just
the outlier uh and then we're going to
convert the Nan hum again we have x uh
Max IQR is 100% Min IQR is min IQR if x
is greater than Max IQR and X is less
than Min IQR again we're going to return
uh null value otherwise it's going to
remain the same value x xal x and you
can see um as we go through the code if
I equals um our hm uh then we go ahead
and do that's the that's a column
specific to humidity that's your hm
column uh then we're going to go ahead
and convert do the run a map on there
and convert the
nonm uh you can see here it's this is
just clean up uh we run we found out
that humidity probably has some weird
values in it uh we have our outliers um
that's all this is and so when we go
ahead and finish this and we take a look
at our outliers and we run this code
here uh we have a low outlier 2.04 we
have a high outlier
99.06 outliers have been
interpolated that means we've given them
a new value uh chances are these days
when you're looking at something like um
these sensors coming in they probably
have a failed sensor in there something
went wrong um that's a kind of thing
that you really don't want to do your
data analysis on so that's what we're
doing is we're pulling that out and then
uh converting it over and setting it up
method linear so we interpolate leth
linears it's going to fill that data in
based on a linear regression model of
similar data
uh same thing with this up here with the
um df2 interpolate that's what we're
doing again this is all data prep uh
we're not actually talking about tensor
flow we're just trying to get all our
data set up correctly so that when we
run it it's not going to cause problems
or have a huge
bias so we've dealt with outliers
specifically in
humidity and again this is one of these
things where when we start running um
we ran through this you can see down
here that we have our um outliers found
high low outliers um we migrated them in
we also know there's other issues going
on with this data uh how do we know that
some of it's just looking at the data
playing with it until you start
understanding what's going on let's take
the temp value and we're going to go
ahead and and use a logarithmic function
on the temp value and uh
it's interesting because it's like how
do you how do you heck do you even know
to use logarithmic on the temp value
that's domain specific we're talking
about being an expert in air care I'm
not an expert in Air Care um you know
it's not what I go look at I don't look
at Air Care data in fact this is
probably the first Air Care data setup
I've looked at but the experts come in
there and they come to you and say hey
in data science um this is a um
exponentially VAR variable on here so we
need to go ahead and do um transform it
and use a logarithmic scale on
that so at that point that would be
coming from your um uh data here we go
data science programmer overview does a
lot of stuff connecting the database and
connecting in with the experts data
analytics a lot of times you're talking
about somebody who is a data analysis
might be all the way usually a PhD level
data science programming level
interfaces database manager that's going
to be the person who's your admin
working on it so when we're looking at
this we're looking at uh something
they've sent to me and they said hey
domain Air Care this needs to be this is
a skew because the data just goes up
exponentially and affects everything
else and we'll go ahead and take that
data um let me just go ahead and run
this just for another quick look at it
um we have our uh uh we'll do a
distribution DF we'll create another
data frame from the temp values and then
from a data set from the um log temp so
we can put them side by side and we'll
just go ahead and do a quick histogram
and this is kind of nice plot of figure
figure size here's our PLT from M plot
Library uh and then we'll just do a
distribution umore DF there's our data
frames this is nice because it just
integrates the histogram right into
pandas love pandas and this is a chart
you would send back to your data
analysis and say hey is this what you
wanted this is how the data is
converting on here and as a data science
scientist the first thing I note is
we've gone from a 10 20 30 scales of 2.5
3.0 3.5 scale um and the data itself has
kind of been um uh adjusted a little bit
based on some kind of a skew on there so
let's jump into uh we're getting a
little closer to actually doing our uh
um carass on here we'll go ahead and
split our data up um and this of course
is any good data scientists you want to
have a training set and a test Set uh
and we'll go ahead and do the train
size we're going to use 75% of the data
make sure it's an integer don't want to
take a slice as a float value give you a
nice error uh and we'll have our train
size is 75% and the test size is going
to be um of course the uh train size
minus the length of the data set and
then we can simply do train comma test
here's our data set which is going to be
the train size the test size uh and then
if we go and print this let me just go a
and run this we can see how these values
um split it's a nice split of 1,298 and
then 433 points of value that are going
to be for our um setup on here and if
you remember we're specifically looking
at the data set where did we create that
data set from um that was from up here
that what we called the uh logarithmic
um value of the temp uh that's where the
data set came from so we're looking at
just that column with this train size in
the test with the train and test data
set here and let's go ahead and do a
convert an array of values into a data
set Matrix we're going to create a
little um setup in here we create our
data set our data set's going to come in
we're going to do a look back of one so
we're going to look back one piece of
data going backward and we have our data
X and our data y for ION range length of
data set look back minus one uh this is
creating let me just go ahead and run
this actually the best way to do this is
to go ahead and create this data and
take a look at the shape of it uh let me
go Ahad and just put that code in here
so we're going to do a look back one
here's our train X our train Y and it's
going to be adding the data on there and
then when we come up here and we take a
look at the shape there we go um and we
run this piece of code here we look at
the shape on this and we have uh a new
slightly different change on here but we
have a shape of X 1296 1 shape of Y
train y Test X test Y and so what we're
looking at is that um the X comes in and
we're only having a single value out uh
we want to predict what the next one is
that's what this little piece of code is
here for what are we looking for well we
want to look back one that's the um what
we're going to train the data with is
yesterday's data yesterday says Hey the
humidity was at 97% what should today's
humidity be at if it's 97% yesterday is
it going to go up or is it going to go
down today if 97 does it go up to 100
what's going on there uh and so our
we're looking forward to the next piece
of data which says Hey tomorrow's is
going to you know today's humidity is
this this is what tomorrow's humidity is
going to be that's all that is all that
is is stacking our data so that um our Y
is basically x + one or X could be Yus
one
and then a couple things to note is our
X data um we're only dealing with the
one column but you need to have it in a
shape that has it by The Columns so you
have the two different numbers and since
we're doing just a single point of data
we have and you'll see with the train y
we don't need to have the extra shape on
here now this is going to run into a
problem uh and the reason is is that we
have what they call a Time step
and the time step is that long-term
short-term memory layer uh so we're
going to add another reshape on here let
me just go down here and put it into the
next cell and so we want to reshape the
input uh array in the form of sample Tim
step features we're only looking at one
feature and I mean this is one of those
things when you're playing with this
you're like why am I getting an error in
the numpy array why is this giving me
something weird going on uh so we're
going to do is we're going to add one
more uh level on here instead of being
being
12991 we want to go one
more and when they put the code together
in the back you can see we kept the same
shape the
1299 uh we added the one dimension and
then we have our trainx shape one um and
this could have depends again on how far
back in the long short-term memory you
want to go that is what that piece of
code is for and that reshape is and you
can see the new shape is now one uh
12991 1 uh versus the 12991 and then the
other part of the shape
43211 again this is our TR our xn and of
course our test X and then our Y is just
a single column because we're just doing
one output that we're looking for so now
we've done our
80% um you know that's all the the
writing all the code reformatting our
data um bringing it in now we want to go
ahead and do the fun part which is we're
going to go ahead and create and fit the
lstm neural network uh and if we're
going to do that the first thing we need
is we're going to need to go ahead and
create a model and we'll do the
sequential model and if you remember
sequential means it just goes in order
uh that means we have if you have two
layers the layers go from layer one to
Layer Two or layer zero to layer one
this is different than functional uh
functional allows you to split the data
and run two completely separate models
and then bring them back together we're
doing just sequential on here and then
we decided to do the long shortterm
memory
uh and we have our input shape uh which
it comes in again this is what all this
switching was we could have easily made
this 1 2 three or four going back as far
as the uh in number on there we just
stuck to going back one and it's always
a good idea when you get to this point
where the heck is this model coming from
um what kind of models do we have
available and uh there's let me go and
put the next model in there uh cuz we're
going to do two models and the next
model is going to go ahead and we're
going to do dents so we have model
equals sequential and then we're going
to add the lstm model and then we're
going to add a dents model and if you
remember from the very top of our
code where we did the Imports oops there
we go our cross this is it right here
here's our importing a dense model and
here's our importing an
lstm now just about every tensor flow
model uses Den uh your DSE model is your
basic forward propagation uh reverse
propagation error or it does reverse
propagation to program the model uh so
any of your neural networks you've
already looked at that uh looks and says
here's the error and sends the error
backwards that's what this is the long
short-term memory is a little different
the real question that we want to look
at right now is where do you find these
models what what kind of models do you
have available and so for that let's go
to the carass website U which is the
cars.io if you go under API layers I
always have to do a search just search
for carass uh API layers it'll open up
and you can see we have um your base
layers right here class trainable
weights all kinds of stuff like there
your activation uh so a lot of your
layers you can switch how it activates
uh railu which is like your smaller
arrays or if you're doing convolutional
neural networks the convolution usually
uses a railu um your sigmoid um all the
way up to soft Max soft plus all these
different choices as far as as how those
are set up and what we want to do is we
want to go ahead and if you scroll down
here you'll see your core layers and
here is your dense layer uh so you have
an input object your dense layer your
activation layer embedding layer this is
your your kind of your one setup on
there that's most common uh
convolutional neural networks or
convolutional layers these are like for
doing uh image categorizing uh so trying
to find objects in a picture that kind
of thing uh we have pooling layer so as
you have the layers come together um
usually bring them down into uh single
layer although you can still do like
Global Max pulling 3D and there is just
I mean this list just goes on and on uh
there's all kinds of different things
hidden in here as far as what you can do
and it changes you know go in here and
you just have to do a search for what
you're looking for uh and figure out
what's going to work best for you as far
as what project you're working on uh
long short-term memory is a big one one
cuz this is when we start talking about
text uh what if someone saysthe what
comes after the uh The Cat in the Hat
little kid's book there um starts
programming it and so you really want to
know not only um what's going on but
it's going to be something that has a
history the history behind it tells you
what the next one coming up is now once
we've built all our different you know
we built our model we've added our
different layers we went in there um
play with remember if you're in
functional you can actually link these
layers together and they Branch out and
come back together if you do a uh um the
sub setup then you can create your own
different model you can embed a model in
there that might be com linear
regression you can embed a linear
regression model uh as part of your
functional split and then have that come
back together with other things so we're
going to go ahe and compile your model
this brings everything together and
we're going to put in what the loss is
which we'll use the mean squared error
uh and we'll go use the atom Optimizer
clearly there's a lot of choices on here
depending on what you're doing and just
like uh any of these uh different
prediction models if you've been doing
any uh um s kit from python uh you'll
recognize that we have to then fit the
model uh so what are we doing in here
we're going to T send in our train X our
train y um we're going to decide how
many epics we're going to run it through
500 is probably a lot for this um I'm
guessing it' probably be about two or
300 probably do just fine our batch size
so how many different uh when you
process it this is the math behind it if
you're in data analytics um you might
try to know what this number is as a
data scientist where I haven't had the
PHD level math that says this is why you
want to use this particular batch size
you kind of play with this number a
little bit um you can dig deeper into
the math see how it affects the results
uh depending on what you're doing and
there's a number of other settings on
here uh we did verbos 2 I'd have to
actually look set up to tell you what
verbos means I think that's actually the
default on there if I remember correctly
uh there's a lot of different settings
when you go to fit it the big ones are
your epic and your batch size those are
what we're looking for and so we're
going to go ahead and run
this and this is going to take a few
minutes to run because it's going
through um 500 times through all the
data so if you have a huge data set this
is the point where you're kind of
wondering oh my gosh is this going to
finish tomorrow tomorrow um if I'm
running this on a single machine and I
have a terap terabyte of data uh going
into
it if this is my personal computer and
I'm running a terabyte of data into this
um you know this is running rather
quickly through all 500 iterations uh
but you know at a terabyte of data we're
talking something closer to days week um
you know even with a uh um a 3.5 ghah
machine in in eight cores it's still
going to take a long time to go through
a full terabyte of data and then we want
to start looking at putting it into some
other framework like spark or something
that will P the process on there more
across multiple um processors and
multiple
computers and if we scroll all the way
down to the bottom you're going to see
here's our square mean error of
0.0088 if we scroll way up you'll see it
kind of oscillates between 888 and
0889 it's right around two 2 250 where
you start seeing that oscillation where
it's really not going anywhere so we
really didn't need to go through a full
500 epics uh you know if you're
retraining the stuff over and over again
it's kind of good to note where that
error zone is so you don't have to do
all the extra processing of course if
you're going to build a model uh we want
to go ahead and run a prediction on it
so let's go ahead and make our
prediction and remember we have our
training test set and our test set or we
have the train X and the train y for
training it or train predict and then we
have our test X and our test y going in
there uh so we can test to see how good
it did uh and when we come in here we
have um uh you'll see right here we go
ahead and do our train predict equals uh
model predict train X and Test predict
model predict test X why would we want
to run the prediction on train X well
it's not 100% on this prediction we know
it has a certain amount of error and we
want to compare the error we have on
what we programmed it with with the
error we get when we run it on new data
that's never seen the model is never
seen before and one of the things we can
do uh we go ahead and invert the
predictions uh this helps us level it
off a little bit more um get rid of some
of our bias we have train predict equals
an NP um exponential M1 the train
predict and then train y equals the
exponential M1 for train Y and then we
do the again that with train test
predict and test
y um again reformatting the data so that
we can it all matches and then we want
to go ahead and calculate the root mean
square error so we have our train score
uh which is your math square root times
the mean square root error train uh Y
and train predict and again we're just
um uh this is just feeding the data
through so we can compare it and the
same thing with the test and take a look
at that cuz really the code makes sense
if you're going through it line by line
you can see what we're doing but the
answer really helps to zoom in uh so we
have a train score which is
2.40 of our root mean square error and
we have a test score of 3.16 of the root
mean square error if these were reversed
if our test score is better than our
training score then we've overtrained
something's really wrong at that point
you got to go back and figure out what
you did wrong uh cuz you should never
have a better result on your test data
than you do on your training data and
that's why we put them both through
that's why we look at the error for both
the training and the testing when you're
going out and quoting your um U
publishing this you're saying hey how
good is my model it's the test score
that you're showing people this is what
it did on my test data that the model
had never seen before this is how good
my model is and a lot of times you
actually want to put together like a
little formal code um where we actually
want to print that out and if we print
that out you can see down here um test
prediction and standard deviation of
data set 3.16 is less than 4 uh 40 I'd
have to go back and we're up here if
you're remember we did the square means
error this is standard deviation that's
why these numbers are different it's
saying the same thing that we just
talked about uh 3.16 is less than 4.40
model is good enough we're saying hey
this is this model is valid we have a
valid model here so we can go ahead and
go with that uh and along with putting a
formal print out of there um we want to
go ahead and plot what's going
on uh and this we just want to pretty
graft here so that people can see what's
going on when I walk into a meeting and
I'm dealing with a number of people they
really don't want these numbers they
don't want to say hey what's I mean
standard deviation unless you know what
statistics are um you might be dealing
with a number of different departments
head of celles might not works with
standard deviation or have any idea what
that really means number-wise and so at
this point we really want to put it in a
graph so we have something to display
and with displaying you got to remember
that we're looking at uh the data today
going into it and what's going to happen
tomorrow so let's take a quick look at
this uh we're going to go ahead and
shift the train predictions for plotting
uh we have our train predict plot uh NP
empty like data set uh train predict
plot uh set it up with uh null
values you know it's just kind of it's
kind of a weird thing where we we're
creating the um the data groups as we
like them and then putting the data in
there is what's going on here uh so we
have our train predict plot uh which is
going to be our look back our length
plus look back we're just is going to
equal train uh train predict so we're
creating this basically we're taking
this and we're dumping the train predict
prict into it so now we have our nice
train predict
plot and then we have the shift test
predictions for the plotting uh we're
going to continue more of that oops
looks like I put it in here double no
it's just a yeah they put it in here
double um didn't mean to do that we
really only need to do it once oh here
we go um this is where the problem was
is because this is the test predict so
we have our training prediction we're
doing the shift on here and then the
test predict we're going to look at that
same thing we're just creating those two
uh data sets uh test predict plot length
prediction set up on there and then
we're going to go through the plotting
the original data set and the
predictions uh so we have a Time axis
always nice to have your time set on
there um set that to the time array time
axes lap all this is setting up the time
variable for the bottom and then we have
a lot of stuff going on here as far as
setting up our figure here let's go
ahead and run that and then we'll break
it
down we have on here uh our main plot we
have two different plots going on here U
the ispu going up and the data and the
ispu here with all these different
settings on
it and so when we look at this we have
our um ax1 that's the main plot I mean
our ax that's the main plot and we have
our ax1 which is the secondary plot over
here so we're doing a figure PLT or pl.
figure and we're going to dump those two
graphs on there um and so we take and if
you go through the code piece by piece
uh which we're not going to do we're
going to do the um the data set here um
exponential reverse exponential so it
looks correctly we're going to label it
the original data set um we're going to
plot the train predict plot that's what
we just created we're going to make that
orange and we'll label it train
prediction uh test predic plot we're
going to make that red and label it test
prediction and so forth um set our ticks
up this actually just put ticks um time
axis gets its ticks the little little
marks they're going along the axes that
kind of thing and let's take a look and
see what these graphs look
like and these are just kind of fun you
know when you show up into a meeting and
this is the final output and you say hey
this is what we're looking at um here's
our original data in blue
here's our training prediction um you
can see that it trains pretty close to
what the data is up there I would also
probably put a um like a little little
Tim stamp and do just right before and
right after where we go from uh train to
test prediction and you can see with the
test prediction the data comes in in
red um and then you can also see what
the original data set looked like behind
it and how it differs and then we can
just isolate that here on the right
that's all this is um is just the test
prediction on the right uh and it's you
know there's you you'll see what the
original data set there's a lot of Peaks
were missing and a lot of lows were
missing but as far as the actual test
prediction it's pretty does pretty good
it's pretty right on you can get a good
idea what to expect for your ispu and so
from this we would probably publish it
and say hey this is um what you expect
and this is our area of this is a range
of error um that's the kind of thing I'd
put out
on a daily basis maybe we predict the
sales are going to be this or maybe
weekly so you kind of get a nice you
kind of flatten the um data coming out
and you say hey this is what we're
looking at the big takeaway from this is
that we're working with let me go back
up here oops over too far there we go um
is this model here this is what this is
all about we worked through all of those
pieces um all the tensor flows and that
is to build this sequential model and
we're only putting in the two layers
this can get pretty complicated if you
get too complicated it never um it never
verges into a usable model uh so if you
have like 30 different layers in here
there's a good chance you might crash it
kind of thing um so don't go too haywire
on that and that you kind of learn as
you go again it's domain knowledge um
and also starting to understand all
these different layers and what they
mean the data analytics behind those
layers
um is something that your data analysis
uh professional would come in and say
this is what we want to try but I tell
you as a data scientist um a lot of
these basic setups are common and I
don't know how many times uh working
with somebody and they're like oh my
gosh if I only did a tangent H instead
of a Ru activation I worked for two
weeks to figure that out well as a data
science I can uh run it through the
model in you know 5 minutes instead of
spending two weeks doing the the math
behind it
um so that's one of the advantages of
data scientists is we do it from
programming side and a data analytics is
going to look for it how does it work in
math and this is really the core right
here of tensor flow and carass is being
able to build your data model quickly
and efficiently and of course uh with
any data science putting out a pretty
graph so that your shareholders um again
we want to take and um reduce the
information down to something people can
look at and say oh that's what's going
on they can see stuff what's going on as
as far as the dates and the change in
the ispu now let's talk a little bit
about recurrent neural networks so
neural networks are of different types
we have CNN we have RNN Ann so on now
RNN is one type of neural network RNN
stands for recurrent neural network the
networks like CNN and Ann are freed
forward Network which means that the
information pretty much only goes from
left to right or from front to back
whichever way you call it whereas in
case of recurrent neural network there
will be some information traveling
backwards as well so that is why it is
known as recurrent neural network and
each of these types have a specific
application so for example convolutional
neural networks or CNN are very good for
doing image processing and uh object
detection using for video and images
whereas recurrent neural networks are
pretty good for doing NLP or speech
recognition and so on on okay so for the
next few minutes we will kind of focus
on recurrent neural networks and we will
see an example of how we can use RNN to
do a Time series analysis so in a
typical neural network this is how it
looks right so where you have a neuron
and then the inputs are coming to the
neuron and uh then you have an output
which goes to other neurons in the other
layers in case of recurrent neural
network what happens is you have the
inputs let's say at a given point in
time but then a part of the previous
output also gets fed in along with the
inputs for the given time now this can
be a little confusing so let's see if we
can take a little expanded view of this
so this is another view of one single
neuron which is what is known as in an
unfolded manner okay so if we are
getting inputs or data over a period of
time then this is how the neuron will
look remember these are not three
neurons they this is one neuron and it
is what is known as it is shown in a
unfolded way okay we are we have
unfolded this single neuron over a
period of time so at a given time let's
say t minus1 an input of XT minus one is
fed to the neuron and it gets a output
of YT minus one then the next instant
which is XT at a time T right there is
an input of XT and then there is an
output of YT so this is a single neuron
but is displayed in an unfolded way so
this is like expanding it over a period
of time so let's start with this part
here this particular neuron gets an
input at at an instant T minus one let's
say time is equal to T minus1 it gets an
input of XT minus1 and it gives an
output of YT minus then when we go to
the instant T here it gets an input of
XT and it also additionally gets an
input from this previous time frame T
minus 1 so this YT minus 1 gets fed here
and that results in YT all right then
when we go to the time t + 1 there is an
input of XT minus one at that given time
plus the input from the previous time
frame which is a time frame of T that
also gets fed in here and then we get an
output of YT + 1 okay so let me explain
once again this is a single neuron this
these are not three different neurons a
single neuron seen over a period of time
from T minus 1 to t + 1 and unlike a
regular feedforward neuron which only
gets X in this case the input is X and
also another input which is coming from
the previous time frame and that is what
this Loop is in this on the left hand
side diagram this Loop is indicating
that okay so on the right hand side it
is represented in an unfolded way okay
so the input if we take a time frame T
for example is not only XT which is the
input which is the normal input at the
time frame frame T but it is also
getting an input from the previous time
frame which is this YT minus one is also
being fed as an input that is the key
differentiator here okay similarly at
the instant t + 1 it is getting a normal
input of XT + 1 plus this YT is actually
being fed here and then the output comes
as YT + 1 okay so this is the construct
of a recurrent neural network now
recurrent neural networks again can be
of different subtypes it can be one to
one one to many many to one and many to
many depending on what kind of
application we want to use one of the
examples is like the stock price so
you're feeding so there is only one
input only thing is it is spread over a
period of time so you're feeding the
stock price input that is what comes
here as an input and you get an output
which is again the stock price which is
probably predicted over the next uh two
days or 3 days or 10 days or whatever so
the number of outputs and inputs is the
same there is only one input there only
one output only thing is it is spread
over a time so variables are not many
then you have one to many so there is
one input but you're expecting multiple
outputs what is an example of this let's
say you want to caption an image so how
do you want to do that what is the input
that will go here is just an image which
is one input but what is output you're
looking for you're looking for a phrase
maybe right not just a word but a phrase
so it's like cat is sitting on a map
right so the images there is only one
image which consists of the cat sitting
on a mat but the output are like maybe
three or four words which is saying the
cat is sitting on a mat so this is one
to many right then you can have many to
one examples of this are like you're
feeding some text and you want to know
the output whether the text is what kind
of sentiment is expressed by the text it
could be positive it could be negative
so that's the output you're looking for
so you feed a large number of words
maybe the text May consist of words or
lines or whatever so that is what is the
multiple inputs but the output all it
says is the sentiment is positive so it
is just one output or the sentiment is
negative just one output right so this
is many to one then we have many to many
so what is an example of this let's say
you want to do some translation so how
do you do a translation you feed in a
sentence maybe in a particular language
English and then you want another a
sentence actually in another language so
that is like there are multiple inputs
one sentence can consist of multiple
words and the output also is a sentence
consisting of multiple words all right
so how do we Implement RNN this is an
example of implementing an RNN for a
particular use case so in our particular
example we have the data about milk
production over a few months and using
RNN we want to predict because this is
time series analysis so RNN is good to
do time series analysis so using RNN we
want to predict what will be the milk
production in the future so let us see
how we can do that I will first take you
through quickly through the slides and
then I will actually run this in in a
Jupiter notebook the code I will run it
in the Jupiter notebook so this is how
it looks the code looks so while this is
tensive flow you still use the standard
python libraries like numpy and pandas
and even matplot li to do some initial
processing getting the data processing
it cleaning it whatever all that can be
still done in within the same program so
that's what we're doing here we're
importing some libraries like numai and
pandas and then we read the data file
and if we plot the data we see here it
is the data for for the years 1962 to 75
and we can see that there is a certain
Trend right so this is how a typical
time series data would look again some
of you if you're not familiar with time
series and uh time series analysis and
so on I would recommend you to go
through some videos around that which
will make it easy to understand this so
this is how typical time series data
would look in this case it is nothing
but the there is only one variable which
is uh milk production and it is spread
across several years so this going from
1962 to 75 and that value has been
plotted so if you see there is a certain
kind of a trend here which is basically
going upwards and there will be some
seasonality so time series data has
these three components right so it will
have a trend it will have a seasonality
it will have seasonality and then it
will have some Randomness so that's what
this graph is uh showing and um now if
we want want to perform analysis on this
time series analysis on this first thing
we need to do is split the data into
train and uh test and in this case we
will just use a straightforward method
which is uh taking the data for the
first 13 years so the idea here is we
need to train our model right this is
time series data so what and we want to
predict for the future so the way we
need to use this is we have to take the
data for a certain period of time and
train our models and then we use a part
of the known data so that we can then
ask our model to predict for that
duration and compare it with the known
information so what do we mean by that
so here if you see this data I will show
you in the notebook as well Jupiter
notebook as well this has 13 years of
data so what we are doing is we are
taking the first 12 years and using that
as our training set right so 12 years we
doing and this has about I think 14
years of data so we are taking 13 years
of data for training purpose and then we
are using the last one year remaining
one year of data for testing purpose
because here what we can do is this onee
data we know the values right because if
we want to compare the accuracy or
anything like that we need to know the
values so in this case we know the
values of this one particular year so we
will use that but at the same time we
train the model for 13 years of data and
for the 14th Year we will ask our model
to predict and then compare it with this
known value so that we know how accurate
our model is I hope I hope that makes
sense okay so that's what we are doing
here 156 is nothing but 12 into 13 on a
monthly basis we get the data the next
step is to scale the data this is all
regular data manipulation data muning
activity and then you split it you are
basically assigning the train and test
data to the to the scaled variables and
uh then you need to read the data in
batches so it is very important as I
mentioned earlier also that instead of
reading the entire data in one shot you
feed the data in batches so in order to
to do that we are writing a a function
for that that's all we are doing here so
that is till here what we have done is
regular Python Programming there is no
tensor flow as of now here okay so so
far what we have done is preparation
data preparation data monging now what
we are doing is actually training our
model so this is where tensorflow now
comes into picture so we are importing
first step is to import the tensorflow
library so this is how you do it import
tensorflow SDF and then you can Define
some variables or constants whichever
way now here of course there are a
couple of ways of doing it you can
create them as tensorflow nodes but that
is a little bit of an overhead we will
just use the regular python variables
are constant so here I'm creating
regular python variables and I'm saying
number of inputs is one so instead of
hard coding I'm just storing them as
variables so this is number of inputs is
one number of time steps is 12 number of
neurons is 100 and so on right and then
learning rate is 03 and number of
iterations we are seeing is 4,000 then
we create placeholders now we will be
storing the independent variables in X
and the dependent variables in y and
this we will read from an external file
remember I told you placeholders are
used for getting data from outside and
then feeding it to our model so that's
the reason we have two placeholders one
for reading the X values and another for
reading the Y values in this step we are
only just creating the nodes right so
this is just creating the graph and
similarly we are mentioning what is the
loss function and what is the optimizer
and uh how to run the optimizer and once
that is done you are initializing the
variables and then you're creating a sa
variable or a saver instance so sa is
basically nothing but in machine
learning you can train your model and
you can save it for later use so that's
where the saving comes into play we will
see how to use that as well and then the
last step is to create your session and
run this graph right so we are
initializing the variables remember this
we run in it so which is nothing but
this one so we are initializing the
variables whatever are there instead of
hardcoding remember in earlier case we
were hardcoding how many iterations so
here we are saying for the given number
of iterations which are stored in this
maybe how many iterations We Said is
What is the value of iterations this is
training iterations is 4,000 so we
specify that based on the number of
training iterations next batch will
basically fetch the data and then we run
the session we are basically saying
train is the node which we will run in
the session and uh this will basically
train our model and this is more for
printing every 100 times you print it
that's all this there's nothing more to
it so for example the output would look
somewhat like this this is for zero this
for 100 next for 200 and so on okay and
then you save the model you remember sa
we created so you save the model for
later use so this is our test data
remember we are doing this on training
data right so once the training data
training is done we then try to create
the inference on the test data so that
we can compare how accurate this
right so this is how the test set would
look and uh then we will basically
restore this model okay because in the
previous slide we created the model and
we stored it so now we have to restore
the model and then run our test data
against it and see what are the values
that are predicted what are the Y values
and then compare with X to see the
accuracy so train seed is what we are
seeing here so that is what we have the
the predicted values and uh these
predicted values what we want to do is
we want to create an additional column
because remember we need to compare
because we want to find out the accuracy
of this module so we need to compare the
predicted values with the actual value
so what we are doing here is adding
another column called generated and uh
assign a value to that all right so this
is the result of the prediction and then
if we need to reshape because we have to
show this in the form of monthly results
so that's what we doing here and um once
we check generate the results and then
display it we can actually see it
month-wise here and uh the actual and
the generated values okay so we create a
data frame which is a combination of the
actual values and the predicted values
from the test set and then we can plot
to see how the trend is as you can see
pretty much the actual value is in blue
color and the generated or predicted
value the curve is in yellow color the
trend is maintained so it will probably
it's not 100% accurate but the trend is
maintained all right so let's do one
thing let's go into the Jupiter notebook
and take a look at how this works in
tsor flow environment actually the code
I will walk you through the code this is
my Jupiter notebook and uh the data is
taken from this particular Link in case
you're interested you can uh download it
from there and this is the data for the
years 1962 to 75 the first thing we do
is import these libraries because before
we start with the actual tensorflow
activity we need to prepare the data and
so on so for that you can use your
regular python libraries which are like
numpy pandas and so on so that's what we
doing here and then we read the data
using pandas into a data frame so this
is a data frame milk is a data frame and
we will do some quick exploratory
analysis just to see how our data is
looking initial five records that we'll
get 1 2 3 4 5 that is head so it goes
from 1962 J January to 1962 May once we
rechange or basically we need to split
this into month specifically separately
so that's what we are doing here date to
time so we kind of do a little bit of a
reindexing and then if we plot this this
is how the data look as you can see
there is a clear upward Trend and um
there is some seasonality and so on but
anyway we will not break that up into
those components we will just use RNN to
predict and test okay so the next thing
is to check some if we can do run like a
info it will tell us what is the
information about this data set so let
us just run that okay so it is just
telling us how many total columns and
what is the size of the file and so on
and so forth again in case you're
interested in doing some exploratory
analysis so what we'll do next is we
will take the 13 years of data for our
training data set so what are we going
to do now let me step back so what have
we seen here we have seen that there are
168 records so which is nothing but 14
years of data we now have to split this
into our training and test data set so
how do we do that I think in case of
Time series we cannot do it like 80/20
like we do in normal splitting in normal
machine learning process here it is time
series data so what we are going to do
is we will take out of this 14 years we
will take the first 13 years of data and
we will use that for training and then
we will test it with the last which is
the 14th year data we will use it for
testing okay so that's what we are going
to do here here so let's uh split that
so training set will consist of my 156
records which is nothing but 12 13 into
12 right my 13 years of data I'm taking
for training and then my test set will
consist of the bottom 12 observations
which is last one year of data which is
the 14th year okay so that's now done
training and test splitting is done the
next step is to do some normalizing
which is basically we'll use or scaling
rather we will use minmax Scala and uh
we will just scale the data and now we
do it for both test as well as train now
we are ready to create our RNN model but
before that one quick thing in order to
fetch the data we have to create a
function so that's what we are doing
here we create a function called Next
batch and uh how you want to fetch the
steps they are all defined as our
constants if you remember and uh that's
what this uh function is all about so we
Define that function we will be calling
that in our Training Method All right so
from here on onwards up to there we are
done with uh the preparation of the data
and whatever functions or whatever are
required from here onwards is where the
tensorflow part starts so the first
thing we do is import the tensorflow
library typically this is a very
standard way of importing the library we
say import tens of flow as TF now TF is
nothing but a name so you can change it
to tf1 or ABC or XY Z whatever so this
part you can change but by and large
everybody uses this so we I would rather
recommend you also use the same syntax
so you say importance of low stf so it's
very easy for others to understand as
well and then you declare or Define a
bunch of constants uh that's what we are
doing here like for example the number
of uh time steps in in this case it is
12 number of neurons there are 100
number of outputs it is only one and so
on right and then the learning rate and
all that we are declaring or defining
those variables in this particular block
next is to create placeholders you
remember the placeholders are used for
feeding the data so we have X and Y X is
for the input which is the independent
variable and Y is the output which is
the dependent variable and in our case
these are not different characteristics
or features but it is the same one
feature or one variable but it is over a
period of time so that's the only
difference so that's what we are doing
here and now we need to create our
Network right the neural network so in
our case we said we will create a RNN
layer now there are different ways or
different format of RNN which is
probably not in scope of this module so
for now we will just assume we are
creating one RNN layer and one type of
RNN is Gru cell so we will use Gru cell
and we use this apis for creating our
layer so each cell will be with an
output projection wrapper there is a
need for doing that and again the
details of this we will probably do in
another video where we talk in detail
about RNN so for now we are creating a
GRU cell with the wrapper and the the
Syntax for creating the gru cell is uh
like this the number of units which we
we said there should be 100 neurons what
will be the activation function in this
case it is reu and then number of
outputs in our case it is only one okay
so we create the cell here okay then we
got the gru cell and then we say what is
our output which is nothing but we get
the outputs and the states and their
states and which is uh the way we get
that is tf. nnn do dynamicore RNN so if
we call this and we pass the cell and
the data which is basically in
placeholder again remember all we are
doing here is we are creating just the
nodes for the graph so nothing is
actually getting executed from a
tensorflow perspective okay all right so
once that is done now we need to pass
this calculate outputs and the states
using the dynamicore AR method and we
pass the cell as a parameter and then
the X values as a parameter and if we
run that we will have the outputs and
the states and this is where we actually
run the Training Method so or create not
really run the Training Method but we
create the nodes for the training and
the optimizer then we have the
initialization of the variables and we
save this model we create a saver object
just to save the model because we will
then restore it and run run it to do our
predictions so this is what we will do
here so that is another node and this is
where we actually create a session and
run the training okay so let's go ahead
and do that that will take a little
while because we said 4,000 iterations
so we will allow it to finish we will
probably come back once the training is
completed all right so the training is
done now let's go and U run this on the
test data so let's just quickly take a
look at the test data set so this is our
one-year data for the year of 1975 so if
we see this this from January February
and so on and we will pass this to our
model so what are we doing here we are
restoring the model here for example
right this is the path that we have
given when we were saving the model
let's go back once and show you let me
show you where we did the saving of the
model yes so we saved the model here so
that again we will restore that and we
will use that to run it on our test data
and see how well it predicts so let's go
ahead and run this and this is just 12
records so this will not take time
remember I told you training is what
takes a lot of time the regular
inference this is called inference
doesn't take much time right so it just
depends on how much data there were only
12 records here so it was very quick but
in training what we do we pass this
multiple times there are iterations
4,000 iterations we did for example so
that takes long longer and in general in
machine learning deep learning training
is what takes the maximum amount of time
all right so let's go ahead and see the
results we will in order to plot it we
will have to kind of adjust the format
of the results otherwise we will not be
able to see it in a proper way and then
we will so this is our so what we have
done is we created a data frame which
consists of the predicted value and the
and the actual value so this is the data
frame so this is the actual value this 8
34 782 and so on this is what our model
has predicted so it may not be so easy
to see in a Tabler format so let's go
ahead and plot it so that we can compare
these two so when we plot it you will
see that the trend is more or less
maintained right so we go from the blue
color is the actual values and the
yellow color or the orange color
whatever is it is the one which is
generated by our model so it pretty much
is following the actual Trend so not bad
for such a quick iteration and uh
training this tutorial is about object
detection we will walk you through a
tensorflow code using which we will do
object detection in images we will tell
you what are the libraries that are
required a little bit about the Coco
data set and then we will show you the
implementation code itself a demo of the
code all right so let's get started so
what is the denlow object detection API
this is an open- Source framework which
is actually provided by the tensorflow
team and there are train models
available and the sample code is also
available which we can use in order to
easily detect objects in images and
videos this is pretty robust and can
detect objects fairly quickly and this
is very easy for people to use as well
people with very less or no knowledge of
machine learning or deep learning can
also with a little bit of Python
Programming knowledge can actually use
this API this library to build object
detection applications this is a list of
libraries that are required and they
have been shown in the code as well the
exact purpose of each of this Library
why it is required is out of the scope
of this tutorial but we will see in the
code as we walk through some of these
libraries how and why they are used the
Coco data set Coco stands for common
objects in context so this data set
consists of 300,000 images of uh 90 most
commonly found objects like chairs and
tables and so on and so forth so this
model has been trained or in fact a set
of models have been trained on this data
set and this is pretty good to detect
the most common object objects in the
images and videos so with that let's get
into the code all right so the first
part is to import all these libraries
and this we have shown you in uh the
slides as well again large part of this
will be for doing some helper functions
and maybe for visualizing the images and
so on and so forth so that's the reason
they are required as I said the exact
details of each and every Library
probably is out of scope but these are
needed so as a first step maybe you just
go ahead and include these libraries and
run the code and maybe at later point we
can discuss what each of these libraries
does now this will work with tensorflow
version higher than 1.4 so if you are
having tensorflow version below 1.4 you
may have to upgrade to a higher version
so let me go ahead and execute the cell
and
um we also need this line of code to
make sure that once we run this object
detection the labeled images are
displayed within this uh notebook and
many of you by now must be familiar with
this and we will import a few utility
libraries and you will see that we will
be using some of these for visualization
purpose so once the objects are detected
we need to display the information what
that object is and then what percentage
of confidence the model has so all these
details that's the utility functions
that stored here and then the next part
is to prepare the model as I mentioned
we will be using an existing trained
model the tens ofl team has actually
provided these models the one that we
will be using is SSD with the mobile net
but you can actually use any one of the
ones that are listed in this URL let me
just quickly take you through this URL
these are a bunch of models trained
models that are readily available for
anybody to use it is open source and let
me scroll down the only thing is that if
there are some of them with with
accuracy is much higher but they take
longer and there are some where the
accuracy is not so high and they are
much faster so they are faster but the
accuracy may not be that very high so
you can play around with some of these
and we in this particular exercise or in
this particular tutorial we are using
this SSD model which is SSD mobilet
version one so that's the model that
we'll be using so in this cell we are
primarily creating a bunch of variables
with the various for example the name of
the model the path and so on and so
forth so that we will be using these
names in The Next Step which is to
download this model and install it
locally these are also referred to as
Frozen models so once they are trained
and then you kind of extract or you you
freeze the model so that's the reason
they are called Frozen model so that
others can just use this without any
further training so this is where we
download and extract our model locally
so this will take a little while let me
see if I can wait or maybe pause the
video and come back once it is done
might take a little while while let's
see if it uh completes I have a pretty
highp Speed network but even then it
takes some time so that's good but this
part is over now let us see this part
and yes both are done so once that is
done we need to load some label mapping
uh basically what this will do is your
model as you may be aware by now if you
do some classification the model will
actually not give any output as a text
it will give some numbers so if there
are five classes it will say okay this
belongs to mod class one or two or three
or five and so on the numbers now each
of these will obviously the numbers will
not make any sense to the outside world
so we need to do some small mapping so
in this case let's say one may be a
chair two may be a table three may be a
balloon and so on so that kind of number
to text mapping we need to do and that
is what is being done in this particular
cell and then we have a helper code
which will load the image and convert it
into a num by array so that the num by
array is what gets processed and used by
the model to do the detection part of it
so that is what this uh method is all
about so we will be that later on we
will be calling that uh function and
next is preparation for detection so
here we are basically telling where the
images are stored and how many images or
what is the naming convention or format
of the images now if you want you can
modify this code for example currently I
have testore images as my folder so let
me go and show this to you so this is
under my object detection folder I have
another subfolder where I'm storing my
images which is textor images now you
can rename this folder and give some
other name and then in your code you can
probably give that particular name for
the Suba similarly the format of these
files what is the name and format of
this files here it is in a very simple
format which is the names of the files
are like beach one beach 2 Beach 3 and
so on so I have taken Beach as the theme
so I have images which are related to
beaches so this is beach one beach 2 and
then Beach three a few others but we
will use these three for our demo and so
that's what I'm saying here the name of
the images will be Beach something. jpeg
which is uh jpeg format and in this
curly braces basically we will will be
filled with either one 2 or three based
on in this particular for Loop okay so
that is what this is doing all right so
the next step is to run inference on
these images in a loop and what we are
basically doing here is um getting these
images one by one and then running
through the model to find out what are
the objects that can be detected and
then against each of the object a box
will be drawn and it will be labeled
with the name and the percentage of
accuracy or confidence that the model
detects these objects okay so that is U
the function here and so let me just run
that piece of code and here is basically
where we are calling this function so we
are loading this images and then we are
calling this function for each image and
then we are displaying this using the
Mac plot liet
so let's run this it will take one image
at a time and then detect the images now
the beauty is that the same format of
the code can be used for doing object
detection in a video so we have another
video for doing object dection in a
video so most of the code out there will
be reused from here and only thing is
that instead of reading the images from
the local storage we read the frames
from the video and there is a neat
little video reader that is available
and it will be shown in the other video
and frame by frame we read the video and
then pass on to this function and it
will act as if each of these frames is
an image and then it will do the same
object detection on the entire video so
that's in a separate video just uh look
out for that and actually the
information about that is provided uh in
the cards the I symbol so that's the the
video object detection in video that's
the separate uh tutorial all right so
now that we have all the pieces together
this the last cell is where the whole
action takes place so let's run this and
see how it looks so it will take
probably a little while and there are
about three images let's see what it
detects there we go so good so the first
one it has detected a person and that to
with 97% accuracy which is uh I think
pretty good okay and then the next image
it detects umbrella and chair there are
a few other objects but it's not able to
detect it has detected umbrella with 63%
accuracy or confidence rather and uh the
chair with 58% again not bad then let's
see the next image so here these are
actually
balloons hot air balloons but the model
thinks it is a kite which is uh probably
not that bad it sees there's something
in the sky and therefore probably it
thinks it is a kite and it detects that
with 65% uh confidence okay so that was
pretty much all I wanted to show you in
this particular tutorial about uh object
detection in images and in this video I
will walk you through the tensorflow
code to perform object detection in a
video so let's get started this part is
basically you're importing all the
libraries we need a lot of these
libraries for example numpy we need
image IO datetime and pill and so on and
so forth and of course mat plot lib so
we import all these libraries and then
there are a bunch of variables which
have some paths for the files and
folders so this is regular stuff let's
keep moving then we import the ma plot
lib and make it in line and uh a few
more Imports all right and then these
are some warnings we can just ignore
them so if I run this code once again it
will go away all right and then here
onwards we do the model preparation and
what we're going to do is we're going to
use an existing neural network model so
we are not going to train a new one
because that really will uh take a long
time and uh it needs a lot of uh
computation resources and so on and it
is really not required there are already
models that have been trained and in
this case it is the SSD with mobile net
that's the model that we are going to
use and uh this model is trained to
detect objects and uh it is readily
available as open source so we can
actually use this and if you want to use
other models there are a few more models
available so you can click on this link
here and uh let me just take you there
there are a few more models but we have
chosen this particular one because this
is faster it may not be very accurate
but that is one of the faster models but
on this link you will see a lot of other
models that are readily available these
are trained models some of them would
take a little longer but they may be
more accurate and so on so you can
probably play around with these other
models okay so we will be using that
model so this piece of code this line is
basically importing that model and this
is also known as uh Frozen model the
term we use is frozen model so we import
download and import that and then we
will actually use that model in our code
all right so these two cells we have
downloaded and imported the model and
then once it is available locally we
will then load this into our program all
right so we are loading this into memory
and uh you need to perform a couple of
additional steps which is basically we
need to to map the numbers to text as
you may be aware when we actually build
the model and when we run predictions
the model will not give a text the
output of the model is usually a number
so we need to map that to a text so for
example if the network predicts that the
output is five we know know that five
means it is an aeroplane things like
that so this mapping is done in this
next cell all right so let's keep moving
and then we have a helper code which
will basically load the data or load the
images and transform into numpy array
this is also used in doing object
detection in images so we are actually
going to reuse because video is nothing
but it consists of frames which in turn
are images so we are going to pretty
much use reuse the same code which we
used for doing object detection in
images so this is where the actual
detection starts so here this is the
path for where the images are stored so
this is here once again we are reusing
the code which we wrote for detecting
objects in an image so this is the path
where the images were stored and this is
the extension and this was done for
about two or three images so we will
continue to use this and uh we go down
down I'll skip this section so this is
the cell where we are actually loading
the video and converting it into frames
and then using frame by frame we are
detecting the objects in the image so in
this code what we are doing basically is
there are a few lines of code what they
do is basically once they find an object
a box will be drawn around those uh each
of those objects and the input file the
name of the input video file is uh
traffic it is the extension is mty
before and uh we have this video reader
it's a excellent object which is
basically part of this class called
image iio so we can read and write
videos using that and uh the video that
we are going to use is traffic. MP4 you
can use any mp4 file but in our case I
picked up video which has uh like car so
let me just show you so this is in this
object detection folder I have this mp4
file I'll just quickly play this video
it's a little slow yeah okay so here we
go this is the video it's a short one
relatively small video so that for this
particular demo and what it will do is
once we run our code it will detect each
of these cars and it will annotate them
as cars so in this particular video we
only have cars we can later on see with
another video I think I have cat here so
we can also try with that but let's
first check with this uh traffic video
so let me go go back so we will be
reading this frame by frame and um no
actually we will be reading the video
file but then we will be analyzing it
frame by frame and we will be reading
them at 10 frames per second that is the
rate we are mentioning here and
analyzing it and then annotating and
then writing it back so you will see
that we will have a video file named
something like this traffic undor
annotated and um we will see the
annotated video so let's go back and run
through this piece of code and then we
will come back and see the annotated uh
video this might take a little while so
I will pause the video after running
this particular cell and then come back
to show you the results all right so
let's go ahead and run it so it is
running now and it is also important
that at the end you close the video
writer so that it is similar to a file
pointer when you open a file you should
also make sure you close it so that it
doesn't hog the resources so it's very
similar at the end of it the last piece
or last line of code should be video
writer. close all right so I'll pause
and then I'll come back okay so I will
see you in a little bit all right so now
as you can see here the processing is
done the r Glass has disappeared that
means the video has been processed so
let's go back and check the annotated
video we'll go back to my file manager
so this was the original traffic
traffic. MP4 and now you have here
traffic uncore annotated MP4 so let's go
and run this and see how it
looks you see here it has got each of
these cars are getting detected let me
pause and show you so we pause here it
says car 70% let us allow it to go a
little further it detect something on
top what is that truck okay so I think
because of the board on top it's somehow
thinks there is a truck let's play some
more and see if it detects anything else
so this is again a car looks like so let
us yeah so this is a car and it has
confidence level of 69% okay this is
again a car all right so basically till
the end it goes and detects each and
every car that is passing by now we can
quickly repeat this process for another
video let me just show you the other
video which is a cat again there is uh
this cat is not really moving or
anything but it is just standing there
staring and moving a little slowly and
uh our application will our network will
detect that this is a cat and uh even
when the cat moves a little bit in the
other direction it'll continue to detect
and show that it is a cat Okay so yeah
so this is our the original video is
let's go ahead and change our code to
analyze this one and see if it detects
our Network detects the Cat close this
here we go and I'll go back to my code
all we need to do is change this traffic
to cat the extension it will
automatically pick up because it is
given here and then it will run through
so very quickly once again what it is
doing is this video reader video reader
has a a neat little feature or interface
whereby you can say for frame in video
uncore reader so it will basically
provide frame by frame so you are in a
loop frame by frame and then you take
that each frame that is given to you you
take it and analyze it as if it is an
image individual image so that's the way
it works so it is very easy to handle
this all right so now let's once again
run just this cell the rest of the stuff
Remains the Same so I will run this cell
again it will take a little while so the
our glasses come back I will pause and
then come back in a little while all
right so the processing is done let's go
and check the annotated video go here so
we have cat annotated MP4 let's play
this all right so you can see here it is
detecting the cat and in the beginning
you also saw it detected something else
here there looks like it detected one
more object so let's just go back and
see what it has detected here let's see
yes so what is it trying to show here
it's too small not able to see but uh it
is trying to detect something I think it
is saying it is a car I don't know all
right okay so in this video there's only
pretty much only one object which is the
cat and uh let's wait for some time and
see if it continues to detect it when
the cat turns around and moves as well
just in a little bit that's going to
happen and we will see there we go and
in spite of turning the other way I
think our network is able to detect that
it is a cat so let me freeze and then
show here it is actually still continues
to detect it as a cat all right so
that's pretty much it I think that's the
only object that it detects in this
particular video okay so close this so
that's pretty much it thank you very
much for watching this video hey there
learner simply learn brings you master's
program in artificial intelligence
created in collaboration with IBM to
learn more about this course you can
find the course Link in the description
box below in this tutorial we will take
the use case of recognizing handwritten
digits this is like a hollow world of
deep learning and this is a nice little
Ms database is a nice little database
that has images of handwritten digits
nicely formatted because very often in
deep learning and neural networks we end
up spending a lot of time in preparing
the data for training and with amness
database we can avoid that
you already have the data in the right
format which can be directly used for
training and mnist also offers a bunch
of built-in utility functions that we
can straight away use and call those
functions without worrying about writing
our own functions and that's one of the
reasons why amness database is very
popular for training purposes initially
when people want to learn about deep
learning and tensor flow this is the
database that is used used and it has a
collection of 70,000 handwritten digits
and a large part of them are for
training then you have test just like in
any machine learning process and then
you have validation and all of them are
labeled so you have the images and their
label and these images they look
somewhat like this so they are
handwritten images collected from a lot
of individuals people have these are
samples written by by human beings they
have handwritten these numbers these
numbers going from 0 to 9 so people have
written these numbers and then the
images of those have been taken and
formatted in such a way that it is very
easy to handle so that is amness
database and the way we are going to
implement this in our tensor flow is we
will feed this data especially the
training data along with the label
information and uh the data is basically
these images are stored in the form of
the pixel information as we have seen in
one of the previous slides all the
images are nothing but these are pixels
so an image is nothing but an
arrangement of pixels and the value of
the pixel either it is lit up or it is
not or in somewhere in between that's
how the images are stored and that is
how they are fed into the neural network
and for training once the network is
trained when you provide a new image it
will be able to identify within a
certain error of course and for this we
will use one of the simpler neural
network configurations called softmax
and for Simplicity what we will do is we
will flatten these pixels so instead of
taking them in a two-dimensional
arrangement we just flatten them out so
for example it starts from here it is at
28 by 28 so there are 700 for 84 pixels
so pixel number one starts here it goes
all the way up to 28 then 29 starts here
and goes up to 56 and so on and the
pixel number 784 is here so we take all
these pixels flatten them out and feed
them like one single line into our
neural network and this is a what is
known as a softmax layer what it does is
once it is trained it will be able to
identify what what digit this is so
there are in this output layer there are
10 neurons each signifying a digit and
at any given point of time when you feed
an image only one of these 10 neurons
gets activated so for example if this is
strained properly and if you feed a
number nine like this then this
particular neuron gets activated so you
get an output from this neuron let me
just use
uh a pen or a laser to show you here
okay so you're feeding the number nine
let's say this is been trained and now
if you're feeding a number nine this
will get activated now let's say you
feed one to the trained Network then
this neuron will get activated if you
feed two this neuron will get activated
and so on I hope you get the idea so
this is one type of a neural network or
an activation function known as soft Max
layer so that's what we will be using
here this is one of the simpler ones for
quick and easy understanding so this is
how the code would look we will go into
our lab environment in the cloud and uh
we will show you there directly but very
quickly this is how the code looks and
uh let me run you through briefly here
and then we will go into the Jupiter
notebook where the actual code is and we
will run that as well so as a first step
first of all we are using using python
here and that's why the syntax of the
language is Python and the first step is
to import the tensor flow Library so and
we do this by using this line of code
saying import tens of flow as TF TF is
just for convenience so you can name
give any name and once you do this TF is
tens flow is available as an object in
the name of TF and then you can run its
uh methods and accesses its attributes
and so on and so forth and and mless
database is actually an integral part of
tensorflow and that's again another
reason why we as a first step we always
use this example mnist database example
so you just simply import mnist database
as well using this line of code and you
slightly modify this so that the labels
are in this format what is known as one
hot true which means that the label
information is stored like an array and
uh let me just uh use the pen to show
what exactly it is so when you do this
one hot true what happens is each label
is stored in the form of an array of 10
digits and let's say the number is uh 8
okay so in this case all the remaining
values there will be a bunch of zeros so
this is like array at position zero this
is at position one position two and so
on and so forth let's say this is
position seven then this is position 8
that will be one because our input is
eight and again position 9 will be zero
okay so one hot encoding this one hot
encoding true will kind of load the data
in such a way that the labels are in
such a way that only one of the digits
has a value of one and that indicat so
based on which digit is one we know what
is the label so in this case the eighth
position is one therefore we know this
sample data the value is eight similarly
if you have a two here let's say then
the labeled information will be somewhat
like this so you have your labels so you
have this as zero the zeroth position
the first position is also zero the
second position is one because this
indicates number two and then you have
third as zero and so on okay so that is
the significance of this one hot true
all right and then we can check how the
data is uh looking by displaying the the
data and as I mentioned earlier this is
pretty much in the form of digital form
like numbers so all these are like pixel
value so you will not really see an
image in this format but there is a way
to visualize that image I will show you
in a bit and uh this tells you how many
images are there in each set so the
training there are 55,000 images in
training and in the test set there are
10,000 and then validation there are
5,000 so altogether there are 70,000
images all right so let's uh move on and
we can view the actual image by uh using
the matplot flip library and this is how
you can view this is the code for
viewing the images and you can view them
in color or you can view them in Gray
scale so the cmap is what tells in what
way we want to view it and what are the
maximum values and the minimum values of
the pixel values so these are the Max
and minimum values so of the pixel
values so maximum is one because this is
a scaled value so one means it is uh
White and zero means it is black and in
between is it can be anywhere in between
black and white and the way to train the
model there is a certain way in which
you write your Tor flow code and um the
first step is to create some
placeholders and then you create a model
in this case we will use the softmax
model one of the simplest ones and um
placeholders are primarily to get the
data from outside into the neural
network so this is a very common
mechanism that is used and uh then of
course you will have variables which are
your remember these are your weights and
biases so for in our case there are 10
neurons and each neuron actually has
784 because each neuron takes all the
inputs if we go back to our slide here
actually every neuron takes all the 784
inputs right this is the first neuron it
has it receives all the 784 this is the
second neuron this also receives all the
78 so each of these inputs needs to be
multiplied with the weight and that's
what we are talking about here so these
are this is a a matrix of
784 values for each of the neurons and
uh so it is like a 10x 784 Matrix
because there are 10 neurons and uh
similarly there are biases now remember
I mentioned bias is only one per neuron
so it is not one per input unlike the
weights so therefore there are only 10
biases because there are only 10 neurons
in this case so that is what we are
creating a variable for biases so this
is uh something little new in tens oflow
you will see unlike our regular
programming languages where everything
is a variable here the variables can be
of three different types you have
placeholders which are primarily used
for feeding data you have variables
which can change during the course of
computation and then a third type which
is not shown here are constants so these
are like fixed numbers all right so in a
regular programming language you may
have everything as variables or at the
most variables and constants but in tens
oflow you have three different types
placeholders variables and constants and
then you create what is known as a graph
so tensorflow programming consists of
graphs and tensors as I mentioned
earlier so this can be considered
ultimately as a tensor and then the
graph tells how to execute the whole
implementation so that the execution is
stored in the form of a graph and in
this case what we are doing is we are
doing a multiplication TF you remember
this TF was created as a tensorflow
object here one more level one more so
TF is available here now tensor flow has
what is known as a matrix multiplication
or matal function so that is what is
being used here in this case so we are
using the matrix multiplication of
tensor flow so that you multiply your
input values x with W right this is what
we were doing x w + B you're just adding
B and this is in very similar to one of
the earlier slides where we saw Sigma XI
wi so that's what we are doing here
matrix multiplication is multiplying all
the input values with the corresponding
weights and then adding the bias so that
is the graph we created and then we need
to Define what is our loss function and
what is our Optimizer so in this case we
we again use the tensor flows apis so
tf. NN softmax cross entropy with logits
is the uh API that we will use and
reduce mean is what is like the
mechanism whereby which says that you
reduce the error and Optimizer for doing
deduction of the error what Optimizer
are we using so we are using gradient
descent Optimizer we discussed about
this in couple of slides uh earlier and
for that you need to specify the
learning rate you remember we saw that
there was a a slide somewhat like this
and then you define what should be the
learning rate how fast you need to come
down that is the learning rate and this
again needs to be tested and tried and
to find out the optimum level of this
learning rate it shouldn't be very high
in which case it will not converge or
shouldn't be very low because it will in
that case it will take very long so you
find the optimizer and then you call the
method minimize for that Optimizer and
that will Kickstart the training process
and so far we've been creating the graph
and in order to actually execute that
graph we create what is known as a
session and then we run that session and
once the training is completed we
specify how many times how many
iterations we wanted to run so for
example in this case we are saying
Thousand Steps so that is a exit
strategy in a way so you specify the
exit condition so training will run for
th000 iterations and once that is done
we can then evaluate the model using
some of the techniques shown here so let
us get into the code quickly and see how
it works so this is our Cloud
environment now you can install
tensorflow on your local machine as well
I'm showing this demo on our existing
Cloud but you can also install Tor flow
on your local machine and uh there is a
separate video on how to set up your
torf flow environment you can watch that
if you want to install your local
environment or you can go for other any
cloud service like for example Google
Cloud Amazon or Cloud Labs any of these
you can use and uhu run and try the code
okay so it has got
started we will log in
all right so this is our deep learning
tutorial uh code and uh this is our
tensorflow
environment and uh so let's get started
the first we have seen a little bit of a
code walk through uh in the slides as
well now you will see the actual code in
action so the first thing we need to do
is import tensor flow and then we will
import the data data and we need to
adjust the data in such a way that the
one hot is encoding is set to True one
hot encoding right as I explained
earlier so in this case the label values
will be shown appropriately and if we
just check what is a type of the data so
you can see that this is a data sets
python data sets and if we check the
number of images the way it looks so
this is how it looks it is an array of
type float 32
similarly the number if you want to see
what is the number
of training images there are 55,000 then
there are test images 10,000 and then
validation images 5,000 now let's take a
quick look at the data itself
visualization so we will use um matte
plot lip for this and um if we take a
look at the shape now shape gives us
like dimension of the tensors or or or
the arrays if you will so in this case
the training data set if we sees the
size of the training data set using the
method shape it says there are 55,000
and 55,000 by 784 so remember the 784 is
nothing but the 28 by 28 28 into 28 so
that is equal to 784 so that's what it
is uh showing now we can take just uh
one image and just see what is the the
first image and see what is the shape so
again size obviously it is only 784
similarly you can look at the image
itself the data of the first image
itself so this is how it it shows so
large part of it will probably be zeros
because as you can imagine in the image
only certain areas are written rest is U
blank so that's why you will mostly see
Zero say it is black or white but then
there are these values are so the values
are actually they are scaled so the
values are between 0o and one okay so
this is what you're seeing so certain
locations there are some values and then
other locations there are zeros so that
is how the data is stored and loaded if
we want to actually see what is the
value of the handwritten image if you
want to view it this is how you view it
so you create like do this reshape and
um matplot lib has this um feature to
show you these images so we will
actually use the function called um IM
show and then if you pass this
parameters appropriately you will be
able to see the different images now I
can change the values in this position
so which image we are looking at right
so we can say if I want to see what is
there in maybe
5,000 right so
5,000 has three similarly you can just
say five what is in five five has eight
what is in
50 again eight so basically by the way
if you're wondering uh how I'm executing
this code shift enter in case you're not
familiar with Jupiter notebooks shift
enter is how you execute each cell
individual cell and if you want to
execute the entire program you can go
here and say run all so that is
how this code gets executed and um here
again we can check what is the maximum
value and what is the minimum value of
this pixel values as I mentioned this is
it is scaled so therefore it is between
the values lie between 1 and zero now
this is where we create our
model the first thing is to create the
required placeholders and variables and
that's what we are doing here as we have
seen in the slide
so we create one placeholder and we
create two variables which is for the
weights and biases these two variables
are actually matrices so each variable
has 784 by 10 values okay so one for
this 10 is for each neuron there are 10
neurons and 784 is for the pixel values
inputs that are given which is 28 into
28 and the the biases as I mentioned one
for each neuron so there will be 10
biases they are stored in a variable by
the name b and this is the graph which
is basically the multiplication of these
matrix multiplication of X into W and
then the bias is added for each of the
neurons and the whole idea is to
minimize the error so let me just
execute I think this code is executed
then we Define what is our the Y value
is basically the label value so this is
another placeholder we had X as one
placeholder and Yore true as a second
placeholder and this will have values in
the form of uh 10 digigit 10 digit uh
arrays and uh since we said one hot
encoded the position which has a one
value indicates what is the label for
that particular number all right then we
have entropy which is nothing but the
loss loss function and we have the
optimizer we have chosen gradient
descent as our Optimizer then the
training process itself so the training
process is nothing but to minimize the
cross entropy which is again nothing but
the loss function so we Define all of
this in the form of a graph so the up to
here remember what we have done is we
have not exactly executed any tens oflow
code till now we are just preparing the
graph the execution plan that's how the
tensorflow code works so the whole
structure and format of this code will
be completely different from how we
normally do programming so even with
people with programming experience may
find this a little difficult to
understand it and it needs quite a bit
of practice so you may want to view this
uh video also maybe a couple of times to
understand this flow because the way
tensorflow programming is done is
slightly different from the normal
programming some of you who let's say
have done uh maybe spark programming to
some extent we'll be able to easily
understand this uh but even in spark the
the programming the code itself is
pretty straightforward behind the scenes
the execution happens slightly
differently but in 10 flow even the code
has to be written in a completely
different way so the code doesn't get
executed uh in the same way as you have
written so that that's something you
need to understand and a little bit of
practice is needed for this so so far
what we have done up to here is creating
the variables and feeding the variables
and um or rather not feeding but setting
up the variables and uh the graph that's
all defining maybe the uh what kind of a
network you want to use for example we
want to use soft Max and so on so you
have created the variables how to load
the data loaded the data viewed the data
and prepared everything but you have not
yet executed anything in tens of flow
now the next step is the execution in
tensor flow so the first step for doing
any execution in tens of flow is to
initialize the variables so anytime you
have any variables defined in your code
you have to run this piece of code
always so you need to basically create
what is known as a a node for
initializing so this is a node you still
are not yet executing anything here you
just created a node for the
initialization so let us go ahead and
create that and here onwards is where
you will actually execute your code uh
in tensive flow and in order to execute
the code what you will need is a session
tensor flow session so tf. session will
give you a session and there are a
couple of different ways in which you
can do this but one of the most common
methods of doing this is with what is
known as a width Loop so you have a
width tf. session as SS and with a uh
colon here and this is like a block
starting of the block and these
indentations tell how far this block
goes and this session is valid till this
block gets executed so that is the
purpose of creating this width block
this is known as a with block so with
tf. session as cess you say cs. run in
it now cs. run will execute a node that
is specified here so for example here we
are saying cs. run sess is basically an
instance of the session right so here we
are saying f. session so an instance of
the session gets created and we are
calling that sess and then we run a node
within that one of the nodes in the
graph so one of the nodes here is in it
so we say run that particular node and
that is when the initialization of the
variables happens now what this does is
if you have any variables in your code
in our case we have W is a variable and
B is a variable so any variables that we
created you have to run this code you
have to run the initialization of these
variables otherwise you will get an
error okay so that is the that's what
this is doing then we within this width
block we specify a for Loop and we are
saying we want the system to iterate for
thousand steps and perform the training
that's what this for Loop does run train
training for thousand
iterations and what it is doing
basically is it is fetching the data or
these images remember there are about
50,000 images but it cannot get all the
images in one shot because it will take
up a lot of memory and performance
issues will be there so this is a very
common way of Performing deep learning
training you always do in batches so we
have maybe 50,000 images but you always
do it in in batches of 100 or maybe 500
depending on the size of your system and
so on and so forth so in this case we
are saying okay get me 100 uh images at
a time and get me only the training
images remember we use only the training
data for training purpose and then we
use test data for test purpose you must
be familiar with machine learning so you
must be aware of this but in case you
are not in machine learning also not
this is not specific to deep learning
but in machine learning in general you
have what is known as training data set
and test data set your available data
typically you will be splitting into two
parts and using the training data set
for training purpose and then to see how
well the model has been trained you use
the test data set to check or test the
validity or the accuracy of the model so
that's what we are doing here and You
observe here that we are actually
calling an mest function here so we are
saying mnist train. nextt batch right so
this is the advantage of using mnist
database because they have provided some
very nice helper functions which are
readily available otherwise this
activity itself we would have had to
write a piece of code to fetch this data
in batches that itself is a a lengthy
exercise so we can avoid all that if we
are using amness database and that's why
we use this for the initial learning
phase okay so when we say fetch what it
will do is it will fetch the images into
X and the labels into Y and then you use
this batch of 100 images and you run the
training so cs. run basically what we
are doing here is we are running the
training mechanism which is nothing but
it passes this through the neural
network passes the images through the
neural network finds out what is the
output and if the output obviously the
initially it will be wrong so all that
feedback is given back to the neural
network and thereby all the W's and Bs
get updated till it reaches th000
iterations in this case the exit
criteria is th000 but you can also
specify probably accuracy rate or
something like that for the as an exit
criteria so here it is it just says that
okay this particular image was wrongly
predicted so you need to update your
weights and biases that's the feedback
given given to each neuron and that is
run for thousand iterations and
typically by the end of this thousand
iterations the model would have learned
to recognize these handwritten images
obviously it will not be 100% accurate
okay so once that is
done after so this happens for thousand
iterations once that is done you then
test the accuracy of these models by
using the test data set that right so
this is what we are trying to do here
the code may appear a little complicated
because if you're seeing this for the
first time you need to understand uh the
various methods of tensor flow and so on
but it is basically comparing the output
with the what has been what is actually
there that's all it is doing so you have
your test data and uh you're trying to
find out what is the actual value and
what is the predicted value and seeing
whether they are equal or not TF do
equal right and how many of them are
correct and so on and so forth and based
on that the accuracy is calculated as
well so this is the accuracy and uh that
is what we are trying to see how
accurate the model is in predicting
these uh numbers or these digits okay so
let us run this this entire thing is in
one cell so we will have to just run it
in one shot it may take a little while
let us see and uh not bad so it has
finished the thousand iterations
and what we see here as an output is the
accuracy so we see that the accuracy of
this model is around
91% okay now which is pretty good for
such a short exercise within such a
short time we got 90% accuracy however
in real life this is probably not
sufficient so there are other ways in to
increase the accuracy we will see
probably in some of the later tutorials
how to improve this accuracy how to
change maybe the hyper parameters like
number of neurons or number of layers
and so on and so forth and uh so that
this accuracy can be increased Beyond
90% hey there learner simply learn
brings you master's program in
artificial intelligence created in
collaboration with IBM to learn more
about this course you can find the
course Link in the description box below
we're going to dive right into what is
carass we also uh go all the way through
this into a couple of tutorials CU
that's where you really learn a lot is
when you roll up your sleeves so we talk
about what is carass carass is a
highlevel deep learning API written in
Python for easy imple implementation of
neural networks uses deep learning
Frameworks such as tensor flow pie torch
Etc is backend to make computation
faster and this is really nice because
as a programmer there is so much stuff
out there and it's evolving so fast
it can get confusing and having some
kind of high level order in there we can
actually view it and easily program
these different neural networks uh is
really powerful it's really powerful to
to um uh have something out really quick
and also be able to start testing your
models and seeing where you're going so
cross works by using complex deep
learning Frameworks such as tensorflow
pytorch um ml play Etc as a backend for
fast computation while providing a user
friendly and easy to learn frontend and
you can see here we have the cross API
uh specifications and under that you'd
have like TF carass for tensor flow
thano coros and so on and then you have
your tensor flow workflow that this is
all sitting on top
of and this is like I said it organizes
everything the heavy lifting is still
done by tensor flow or whatever you know
underlying package you put in there and
this is really nice because you don't
have to um dig as deeply into the
heavy-end stuff while still having a
very robust package you can get up and
running rather quickly and it doesn't
distract from the processing time
because all the heavy lifting is done by
packages like tensor flow this is the
organization on top of it so the working
principle of
carass uh the working principle of
carass is carass uses computational
graphs to express and evaluate
mathematical
Expressions you can see here we put them
in blue blue they have the expression um
expressing complex problems as a
combination of simple mathematical
operators uh where we have like the
percentage or in this case in Python
that's usually your uh left your um
remainder or multiplication uh you might
have the operator of x uh to the power
of3 and it us is useful for calculating
derivatives by using uh back propagation
so if we're doing with neural networks
when we send the error back up to figure
out how to change it uh this makes it
really easy to do that without really
having not banging your head and having
to hand write everything it's easier to
implement distributed computation and
for solving complex problems uh specify
input and outputs and make sure all
nodes are
connected and so this is really nice as
you come in through is that um as your
layers are going in there you can get
some very complicated uh different
setups nowadays which we'll look at in
just a second and this just makes it
really easy to start spinning this stuff
up and trying out the different models
so when we look at carass models uh
carass model we have a sequential model
sequential model is a linear stack of
layers where the previous layer leads
into the next
layer and this if you've done anything
else even like the sklearn with their
neural networks and propagation and any
of these setups this should look
familiar you should have your input
layer it goes into your layer one layer
two and then to the output layer and
it's useful for simple classifier decod
Cod or models and you can see down here
we have the model equals a coros
sequential and this is the actual code
you can see how easy it is uh we have a
layer that's dense your layer one has an
activation they're using the ru in this
particular example and then you have
your name layer one layer Den railu name
Layer Two and so forth uh and they just
feed right into each other so it's
really easy just to stack them as you
can see here and it automatically takes
care of everything else for you and then
there's a functional model
and this is really where things are at
this is new make sure you update your
carass or you'll find yourself running
this um doing the functional model
you'll run into an error code because
this is a fairly new release and he uses
multi-input and multi-output model the
complex model which Forks into two or
more branches and you can see here we
have our image inputs equals your caros
input shape equals 32x 32x 3 you have
your dense layers dense 64 activation
railu this should look similar to what
you already saw before uh but if you
look at the graph on the right it's
going to be a lot easier to see what's
going on you have two different
inputs uh and one way you could think of
this is maybe one of those is a small
image and one of those is a full-sized
image and that feedback goes into you
might feed both of them into one Noe
because it's looking for one thing and
then only into one node for the other
one and so you can start to get kind of
of an idea that there's a lot of use for
this kind of split and this kind of
setup uh we have multiple information
coming in but the information's very
different even though it overlaps and
you don't want it to send it through the
same neural network um and they're
finding that this trains faster and is
also has a better result depending on
how you split the date up and and how
you Fork the models coming
down and so in here we do have the two
complex uh models coming in uh we have
our image inputs which is a 32x 32 by3
your three channels or four if you're
having an alpha channel uh you have your
dense your layers dense is 64 activation
using the railu very common u x equals
dense inputs X layers dense x64
activation equals Ru X outputs equals
layers dense 10 X model equals coros
model inputs equals inputs outputs
equals outputs name equals n
model uh so we add a little name on
there and again this is this kind of
split here this is setting us up to um
have the input go into different areas
so if you're already looking AC cross
you probably already have this answer
what are neural networks uh but it's
always good to get on the same page and
for those people who don't fully
understand neural networks to dive into
them a little bit or do a quick overview
neural networks are deep learning
algorithms modeled after the human brain
they use multiple neurons which are
mathematical operations to break down
and solve complex maical
problems and so just like the neuron one
neuron fires in and it fires out to all
these other neurons or nodes as we call
them and eventually they all come down
to your output layer and you can see
here we have the really standard graph
input layer a hidden layer and an output
layer one of the biggest parts of any
data processing is your data
pre-processing uh so we always have to
touch base on that with a neural network
like many of these mod models they're
kind of uh when you first start using
them they're like a black box you put
your data in you train it and you test
it and see how good it was and you have
to pre-process that data because bad
data in is uh bad outputs so in data
pre-processing we will create our own
data examples set with carass the data
consists of a clinical trial conducted
on 2100 patients ranging from ages 13 to
100 with a the patients under 65 and the
other half over 65 years of age we want
to find the possibility of a patient
experiencing side effects due to their
age and you can think of this in today's
world with uh covid uh what's going to
happen on there and we're going to go
ahead and do an example of that in our
uh life Hands-On like I said most of
this you really need to have handson to
understand so let's go ahead and bring
up our anaconda and uh open that up and
open up a Jupiter notebook for doing the
PIP python code in now if you're not
familiar with those you can use pretty
much any of your uh setups I just like
those for doing demos and uh showing
people especially shareholders it really
helps CU it's a nice visual so let me go
and flip over to our anaconda and the
Anaconda has a lot of cool to tools they
just added datal lore and IBM Watson
Studio Cloud into the Anaconda framework
but we'll be in the Jupiter lab or
Jupiter notebook um I'm going to do
Jupiter notebook for this because I use
the lab for like large projects with
multiple pieces because it has multiple
tabs where the notebook will work fine
for what we're doing and this opens up
in our browser window because that's how
Jupiter notebook soorry Jupiter notebook
is set to run and we'll go under new
create a new Python 3 and uh it creates
an Untitled python we'll go ahead and
give this a title and we'll just call
this uh
coros tutorial
and let's change that to a capital there
we go we'll go and just rename that and
the first thing we want to go ahead and
do is uh get some pre-processing tools
involved and so we need to go ahead and
import some stuff for that like our
numpy do some random number
Generation Um I mentioned sklearn or
your s kit if you're installing sklearn
the SK learn stuff it's a site kit you
want to look
up that should be a tool of anybody who
is um doing data science if if you're
not if you're not familiar with the
sklearn
toolkit it's huge uh but there's so many
things in there that we always go back
to and we want to go ahead and create
some train labels and train samples uh
for training our
data and then just a note of what we're
we're actually doing in here uh let me
go ahead and change this this is kind of
a fun thing you can do we can change the
code to markdown
and then markdown code is nice for doing
examples once you've already built this
uh our example data we're going to do
experimental there we go experimental
drug was tested on 2100 individuals
between 13 to 100 years of age half the
participants are under 65 and 95% of
participants are under 65 experience no
side effects well 95% of participants
over 65 um experience side effects
so that's kind of where we're starting
at um and this is just a real quick
example because we're going to do
another one with a little bit more uh
complicated
information uh and so we want to go
ahead and
generate our setup uh so we want to do
for I in range and we want to go ahead
and create if you look here we have
random
integers train the labels a pen so we're
just creating some random
data uh let me go ahead and just run
that
and so once we've created our random
data and if you if I mean you can
certainly ask for a copy of the code
from Simply learn they'll send you a
copy of this or you can zoom in on the
video and see how we went ahead and did
our train samples a pin um and we're
just using this I do this kind of stuff
all the time I was running a thing on uh
that had to do with errors following a
bell-shaped curve on uh a standard
distribution error and so what do I do I
generate the data on
standard distribution error to see what
it looks like and how my code processes
it since that was the Baseline I was
looking for and this we're just doing uh
uh generating random data for our setup
on
here and uh we could actually go in um
print some of the data up let's just do
this
print um we'll do
train
samples and we'll just do the first um
five pieces of data in there to see what
that looks like
and you can see the first five pieces of
data in our train samples is 49 85 41 68
19 just random numbers generated in
there that's all that is uh and we
generated significantly more than that
um let's see 50 up here 1,000 yeah so
there's 1,000 here 1,000 numbers we
generated and we could also if we wanted
to find that out we could do a quick uh
print the length of
it and so or you could do a shape kind
of thing and if you're using
numpy although the link for this is just
fine and there we go it's actually 2100
like we said in the demo setup in
there and then we want to go ahead and
take our labels oh that was our train
labels we also did samples didn't
we uh so we could also print do the same
thing oh
labels um and let's change this to
to
labels and
[Music]
labels and run that just to double check
and sure enough we have 2100 and they're
labeled one0 one 0 one0 I guess that's
if they have symptoms or not one
symptoms uh Zer none and so we want to
go ahead and take our train labels and
we'll convert it into a numpy array and
the same thing with our samples and
let's go ahead and run that and we also
Shuffle uh this is just a neat feature
you can do in uh numpy right here put my
drawing thing on which I didn't have on
earlier um I can take the data and I can
Shuffle it uh so we have our so it's it
just randomizes it that's all that's
doing um we've already randomized it so
it's kind of an Overkill it's not really
necessary but if you're doing uh a
larger package where the data is coming
in in and a lot of times it's organized
somehow and you want to randomize it
just to make sure that that you know the
input doesn't follow a certain pattern
uh that might create a bias in your
model and we go ahead and create a
scaler uh the scaler range uh minimum
Max scaler feature range 0 to
one uh then we go ahead and scale the uh
scaled train samples we're going to go
ahead and fit and transform the data uh
so it's nice and scaled and that is the
age uh so you can see up here we have 49
85 41 we're just moving that so it's
going to be between zero and one and so
this is true with any of your neural
networks you really want to convert the
data uh to zero and one otherwise you
create a bias uh so if you have like a
100 creates a bias
versus the math behind it gets really
complicated um if you actually start
multiplying stuff because a lot of
multiplication addition going on in
there that higher end value will
eventually multiply down and it will
have a huge bias as to how the model
fits it and then it will not fit as well
and then one of the fun things we can do
in Jupiter notebook is that if you have
a variable and you're not doing anything
with it it's the last one on the line it
will automatically
print um and we're just going to look at
the first five samples on here it's just
going to print the first five
samples and you can see here we go uh 9
95.7 91 so everything's between zero and
one and that just shows us that we
scaled it properly and it looks good uh
it really helps a lot to do these kind
of print UPS halfway through uh you
never know what's going to go on
there I don't know how many times I've
gotten down and found out that the data
sent to me that I thought was scaled was
not and then I have to go back and track
it down and figure it out on
there uh so let's go ahead and create
our artificial neural
network and for doing that this is where
we start diving into tensor flow and
carass uh tensor
flow if you don't know the history of
tensor
flow it helps to uh jump into we'll just
use
Wikipedia careful don't quote Wikipedia
on these things because you get in
trouble uh but it's a good place to
start uh back in 2011 Google brain built
disbelief as a proprietary m learning
setup tensor flow became the open source
for it uh so tensorflow was a Google
product and then it became uh open
sourced and now it's just become
probably one of the def factos when it
comes for neural networks as far as
where we're at uh so when you see the
tensor flow
setup it it's got like a huge following
there are some other setups like um the
S kit under the sklearn has our own the
neural network but the tensor flow is
the most robust one out there right now
and carass sitting on top of it makes it
a very powerful tool so we can leverage
both the carass uh easiness in which we
can build a sequential setup on top of
tensor
flow and so in here we're going to go
ahead and do our input of tensor flow uh
and then we have the rest of this is all
carass here from number two down uh
we're going to import from tensor flow
the coros uh connection and then you
have your tensor flow cross models
import sequential it's a specific kind
of model we'll look at that in just a
second if you remember from the files
that means it goes from one layer to the
next layer to the next layer there's no
funky splits or anything like
that uh and then we have from tensorflow
Cross layers we're going to import our
activation and our dense
layer and we have our Optimizer atom um
this is a big thing to be aware of how
you optimize uh your data when you first
do it atom's as good as any atom is
usually uh there's a number of Optimizer
out there there's about uh there's a
couple main ons but atom is usually
assigned to bigger data uh it works fine
usually the lower data does it just fine
but atom is probably the mostly used but
there are some more out there and
depending on what you're doing with your
layers your different layers might have
different activations on them and then
finally down here you'll see
um our setup where we want to go ahead
and use the metrics and we're going to
use the tensorflow cross metrics um for
categorical cross entropy uh so we can
see how everything performs when we're
done that's all that is um a lot of
times you'll see us go back and forth
between tensor flow and then pyit has a
lot of really good metrics also for
measuring these things um again it's the
end of the you know at the end of the
story how good does your model do and
we'll go ahead and load all that and
then comes the fun part um I actually
like to spend hours messing with these
things and uh four lines of code you're
like ah you're going to spend hours on
four lines of code um no we don't spend
hours on four lines of code that's not
what we're talking about when I say
spend hours on four lines of code uh
what we have here I'm going to explain
that in just a second we have a model
and it's a sequential model if you
remember correctly we mentioned the
sequential up here where it goes from
one layer to to the next and our first
layer is going to be our
input it's going to be uh what they
called D which is um usually it's just D
and then you have your input and your
activation um how many units are coming
in we have 16 uh what's the shape What's
the activation and this is where it gets
interesting um because we have in here
uh
Ru on two of these and soft Max activ on
one of these there are so many different
options for what these mean um and how
they function how does the ru how does
the softmax
function and they do a lot of different
things um we're not going to go into the
activations in here that is what really
you spend hours doing is looking at
these different
activations um and just some of it is
just U um almost like you're playing
with it
like an artist you start getting a fill
for like a uh inverse tangent activation
or the tan
activation takes up a huge processing
amount uh so you don't see it a lot yet
it comes up with a better solution
especially when you're doing uh when
you're analyzing Word documents and
you're tokenizing the words and so
you'll see this shift from one to the
other because you're both trying to
build a better model and if you're
working on a huge data set um it'll
crash the system it'll just take too
long to process um and then you see
things like soft Max uh soft Max
generates an interesting um
setup where a lot of these when you talk
about rilu oops let me do this uh railu
there we go railu has um a setup where
if it's less than zero it's zero and
then it goes up um and then you might
have what they call lazy uh setup where
it has a slight negative to it so that
the errors can translate better same
thing with softmax it has a slight
laziness to it so that errors translate
better all these little
details make a huge different on your
model um so one of the really cool
things about data science that I like is
you build your uh what they call you
build defil and it's an interesting uh
design setup oops I forgot the um end of
my code here
the concept to build a fail is you want
the model as a whole to work so you can
test your model
out so you can do uh you can get to the
end and you can do your let's see where
was it overshot down here you can test
your test out the the quality of your
setup on there and see where did I do my
tensor flow oh here we go I it was right
above me here we go we start doing your
cross entropy and stuff like that is you
need a full functional set of code so
that when you run
it you can then test your model out and
say hey it's either this model works
better than this model and this is why
um and then you can start swapping in
these models and so when I say I spend a
huge amount of time on pre-processing
data is probably 80% of your programming
time um well between those two it's like
8020 you'll spend a lot of time on the
models once you get the model down once
you get the whole code and the flow down
uh set depending on your data your
models get more and more robust as you
start experimenting with different
inputs different data streams and all
kinds of things and we can do a simple
model summary
here uh here's our sequential here's a
layer output our
parameter this is one of the nice things
about coros is you just you can see
right here here's our sequential one
model boom boom boom boom everything's
set and clear and easy to read so one
once we have our model built uh the next
thing we're going to want to do is we're
want to go ahead and train that
model and so the next step is of course
model training and when we come in here
this a lot of times it's just paired
with the model because it's so
straightforward it's nice to print out
the model setup so you can have a
tracking but here's our model uh the
keyword in Cross is compile
Optimizer atom learning rate another
term right there that we're just
skipping right over that really becomes
the meat of um the setup is your
learning rate uh so whoops I forgot that
I had an arrow but I'll just underline
it a lot of times the learning rate set
to 0.0 uh set to 0.01 uh depending on
what you're doing this learning rate um
can overfit and underfit uh see want to
look up I know we have a number of
tutorials out on overfitting and
underfitting that are really worth
reading once you get to that point in
understanding and we have our loss um
sparse categorical cross entropy so this
is going to tell carass how far to go
until it stops and then we're looking
for metrics of accuracy so we'll go
ahead and run that and now that we've
compiled our model we want to go ahead
and um run it fit it so here's our model
fit um we have our scaled train
samples our train labels our validation
split um in this case we're going to use
10% of the data for
validation uh batch size another number
you kind of play with not a huge
difference as far as how it works but it
does affect how long it takes to run and
it can also affect the bias a little bit
uh most of the time though a batch size
is between 10 to 100 um depending on
just how much data you're processing in
there we want to go ahead and Shuffle it
uh we're going to go through 30 epics
and put a verbose of two and let me just
go a and run this and you can see right
here here's our epic here's our training
um here's our loss now if you remember
correctly up here we set the loss let's
see where was it um compiled our
data there we go loss uh so it's looking
at the sparse categorical cross entropy
this tells us that as it goes how how
how much um how how much does the um
error go down uh is the best way to look
at that and you can see here the lower
the number the better it just keeps
going down and vice versa accuracy we
want let's see where's my
accuracy value accuracy at the end uh
and you can see 619 69. 74 it's going up
we want the accuracy would be ideal if
it made it all the way to one but we
also the loss is more important because
it's a balance um you can have 100%
accuracy and your model doesn't work
because it's overfitted uh again you
won't look up overfitting and
underfitting
models and we went ahead and went
through uh 30 epics it's always fun to
kind of watch your code going um to be
honest I usually uh um the first time I
run it I'm like oh that's cool I get to
see what it does and after the second
time of running it I'm like I like to
just not see that and you can repress
those of course in your code uh repress
the warnings in the
printing and so the next step is going
to be building a test set and predicting
it now uh so here we go we want to go
ahead and build our test set and we have
uh just like we did our training
set a lot of times you just split your
your initial setup uh but we'll go ahead
and do a separate set on here
and this is just what we did above uh
there's no difference as far as
um the randomness that we're using to
build this set on here uh the only
difference is
that we already um did our scaler up
here well it doesn't matter because the
the data is going to be across the same
thing but this should just be just
transform down here instead of fit
transform uh because you don't want to
refit your data um on your testing data
there we go now we're just transforming
it because you never want to transform
the test data um easy mistake to make
especially on an example like this where
we're not
doing um you know we're randomizing the
data anyway so it doesn't matter too
much because we're not expecting
something
weird and then we went ahead and do our
predictions the whole reason we built
the model is we take our model we
predict and we're going to do here's our
xcal data batch size 10 verbose and now
we have our predictions in here and we
could go ahead and do a um oh we'll
print
predictions and then I guess I could
just put down predictions and five so we
can look at the first five of the
predictions and what we have here is we
have our age and uh the prediction on
this age versus on what what we think
it's going to be what what we think is
going to going to have uh symptoms or
not and the first thing we notice is
that's hard to read because we really
want a yes no answer uh so we'll go
ahead and just uh round off the
predictions using the argmax um the
numpy argmax uh for prediction so it
just goes to
a01 and if you remember this is a
Jupiter notebook so I don't have to put
the print I can just put in uh rounded
predictions and we'll just do the first
5 and you can see here 0 1 0 0 0 so
that's what the predictions are that we
have coming out of this um is no
symptoms symptoms no symptoms symptoms
no symptoms and just as uh we were
talking about at the beginning we want
to go ahead and um take a look at this
there we go confusion matrixes for
accuracy check um most important part
when you get down to the end of the
story how accurate is your model before
you go and play with the model and see
if you can get a better accuracy out of
it and for this we'll go ahead and use
the S kit um the SK learn metrics uh py
kit being where that comes from import
confusion Matrix uh some iteration tools
and of course a nice map plot library
that makes a big difference so it's
always nice to
um have a nice graph to look at um
pictures worth a thousand
words um and then we'll go ahead and do
call it CM for confusion Matrix y true
equals test labels y predict rounded
predictions and we'll go ahead and load
in our
cm and I'm not going to spend too much
time on the plotting um going over the
different plotting
code um you can spend uh like whole we
have whole tutorials on how to do your
different plotting on there uh but we do
have here is we're going to do a plot
confusion Matrix there's our c M our
classes normalized false title confusion
Matrix cmap is going to be in
blues and you can see here we have uh to
the nearest cmap titles all the
different pieces whether you put tick
marks or not the marks the classes a
color bar um so a lot of different
information on here as far as how we're
doing the printing of the of the
confusion Matrix you can also just dump
the confusion Matrix um into a caborn
and real quick get an output it's worth
knowing how to do all this uh when
you're doing a presentation to the
shareholders you don't want to do this
on the Fly you want to take the time to
make it look really nice uh like our
guys in the back did and uh let's go
ahead and do this forgot to put together
our CM plot labels we'll go and run
that and then we'll go ahead and call
the little the
definition for our
mapping and you can see here plot
confusion Matrix that's our the the
little script we just wrote and we're
going to dump our data into it um so our
confusion Matrix our classes um title
confusion Matrix and let's just go ahead
and run
that and you can see here we have our
basic setup uh no side effects
195 had side effects uh 200 no side
effects that had side effects so we
predicted the 10 of them who actually
had side effects and that's pretty good
I mean I I don't know about too but you
know that's 5% error on this and this is
because there's 200 here that's where I
get 5% is uh divide these both by by two
and you get five out of a 100 uh you can
do the same kind of math up here not as
quick on the flly it's 15 or 195 not an
easily rounded number but you can see
here where they have 15
people who predicted to have no uh with
the no side effects but had side effects
kind of setup on there and these
confusion Matrix are so important at the
end of the day this is really where
where you show uh whatever you're
working on comes up and you can actually
show them hey this is how good we are or
not how messed up it
is so this was a uh I spent a lot of
time on some of the parts uh but you can
see here is really simple uh we did the
random generation of data but when we
actually built the model coming up here
uh here's our model summary
and we just have the layers on here that
we built with our model on this and then
we went ahead and trained it and ran the
prediction now we can get a lot more
complicated uh let me flip back on over
here because we're going to do another
uh demo so that was our basic
introduction to it we talked about the
uh oops here we go okay so implementing
a neural network with carass after
creating our samples and labels we need
to create our carass neural network
model we will be working with a
sequential model which has three layers
and this is what we did we had our input
layer or hidden layers and our output
layers and you can see the input layer
uh coming in uh was the age Factor we
had our hidden layer and then we had the
output are you going to have symptoms or
not so we're going to go ahead and go
with something a little bit more
complicated um training our model is a
two-step process we first compile our
model and then we train it in our
training data set uh so we have
compiling compiling converts the code
into a form of understandable by
Machine we use the atom in the last
example a gradient descent algorithm to
optimize a model and then we trained our
model which means it let it uh learn on
training data uh and I actually had a
little backwards there but this is what
we just did is we if you remember from
our code we just had o let me go back
here
um here's our model that we
created
summarized uh we come down here and we
compile it so it tells it hey we're
ready to build this model and use it uh
and then we train it and this is the
part where we go ahead and fit our model
and and put that information in here and
it goes through the training on there
and of course we scaled the data which
was really important to do and then you
saw we did the creating a confusion
Matrix with carass um as we are
performing classifications on our data
we need a confusion Matrix to check the
results a confusion Matrix breaks down
the various
misclassifications as well as correct
classifications to get the accurac
see um and so you can see here this is
what we did with the true positive false
positive true negative false negative
and that is what we went over let me
just scroll down
here on the end we printed it out and
you can see we have a nice print out of
our confusion Matrix uh with the true
positive false positive false negative
true negative and so the blue ones uh we
want those to be the biggest numbers
because those are the better side and
then uh we have our false predictions on
here uh as far as this one so it had no
side effects but we predicted let's see
no side effects predicting side effects
and vice versa if getting your learning
started is half the battle what if you
could do that for free visit scaleup by
simply learn click on the link in the
description to know more now uh saving
and loading models with carass we're
going to dive into a more complicated
demo um and you're going to say oh that
was a lot of complication before four
well if you broke it down we randomized
some data we created the um carass setup
we compiled it we trained it we
predicted and we ran our
Matrix uh so we're going to dive into
something a lot a little bit more fun is
we're going to do a face mask detection
with carass uh so we're going to build a
carass model to check if a person is
wearing a mask or not in real time and
this might be important if you're at the
front of a store this is something today
which is um might be very useful as far
as some of our you know making sure
people are
safe uh and so we're going to look at
mask and no mask and let's start with a
little bit on the
data and so in my data I have with a
mask you can see they just have a number
of images showing the people in masks
and again if you want some of this
information uh contact simply learn and
they can send you some of the
information as far as people with and
without masks so you can try it on your
own and this is just such a wonderful
example of this setup on here
so before I dive into the mass detection
uh talking about being in the current
with uh covid and seeing that people are
wearing masks this particular example I
had to go ahead and update to a python
3.8 version uh it might run in a 37 I'm
not sure I I kind of skipped 37 and
installed
38 uh so I'll be running in a 3 python
38 um and then you also want to make
sure your tensor flow is up to date
because the um they call
functional uh layers that's where they
split if you remember correctly from
back uh oh let's take a look at this
remember from here the functional model
and a functional layer allows us to feed
in the different layers into different
you know different nodes into different
layers and split them uh very powerful
tool very popular right now in the edge
of where things are with neural networks
and creating a better model so upgraded
to python 3.8 and let's go ahead and
open that up and go through uh our next
example which includes uh multiple
layers um programming it to recognize
whether someone wears a mask or not and
then uh saving that model so we can use
it in real time so we're actually almost
a full um end to end development of a
product here uh course this is a very
simplified version and there'd be a lot
more to it you'd also have to do like uh
recognizing whether it's someone's face
or not all kinds of other things go into
this so let's go ahead and jump into
that code and we'll open up a new Python
3 oops Python 3 it's working on it there
we
go um and then we want to go ahead and
train our mask we'll just call this
train
mask and we want to go ahead and train
mask and save it uh so it's it's uh save
mask train mask detection not to be
confused with masking data a little bit
different we're actually talking about a
physical mask on your
face and then from the cross standpoint
we got a lot of imports to do here and
I'm not going to dig too deep on the
Imports uh we're just going to go ahead
and notice a few of them uh so we have
in here go alt D there we go have
something to draw with a little bit here
we have our uh image
processing and the image processing
right here let me underline that uh
deals with how do we bring images in
because most images are like a a square
grid and then each value in there has
three values for the three different
colors uh cross and tensorflow do a
really good job of uh working with that
so you don't have to do all the heavy
listing and figuring out what's going to
go on uh and we have the mobile net
average pooling 2D um this again is how
do we deal with the images and pulling
them uh dropout's a cool thing worth
looking up if you haven't when as you
get more and more into carass and tensor
flow uh it'll Auto drop out certain
notes that way you'll get a better um
the notes just kind of die uh they find
that they actually create more of a bias
than help and they also add processing
time so they remove them um and then we
have our flatten that's where you take
that huge array with the three different
colors and you find a way to flatten it
so it's just a one-dimensional array
instead of a twox two
by3 uh D input we did that in the other
one so that should look a little
familiar oops there we go our input um
our model again these are things we had
on the last one here's our Optimizer
with our
atom um we have some pre-processing on
the input that goes along with bringing
in the data in
uh more pre-processing with image to
array loading the image um this stuff is
so nice it looks like a lot of works you
have to import all these different
modules in here but the truth is is it
does everything for you you're not doing
a lot of pre-processing you're letting
the software do the
pre-processing um and we're going to be
working with the setting something to
categorical again that's just a
conversion from a number to a category
uh 0 one doesn't really mean anything
it's like true false
um label binarized the same thing uh
we're changing our labels around and
then there's our train test split
classification report um our IM
utilities let me just go ahead and
scroll down here Notch for these this is
something a little different going on
down here this is not part of the uh
tensor flow or the SK learn this is the
S kit setup and tensor flow above uh the
path this is part of um open CV and
we'll actually have another tutorial
going out with the open CV so if you
want to know more about Open CV you'll
get a glance on it in uh this software
especially the ne the second piece when
we reload up the data and hook it up to
a video camera we're going to do that on
this round um but this is part of the
open CV thing and you'll see CV2 is
usually how that's referenced um but the
IM utilities has to do with how do you
rotate pictures around and stuff like
that uh and resize them and then the map
plot library for plotting because it's
nice to have a graph tells us how good
we're doing and then of course our numpy
numbers array and just a straight OS
access wow so that was a lot of imports
uh like I said I'm not going to spend I
spent a little time going through them
uh but we didn't want to go too much
into
them and then I'm going to create um
some variables that we need to go ahead
and initialize we have the learning rate
number of epics to train for and the
batch size and if you remember correctly
we talked about the learning rate uh to
the4
001 um a lot of times it's 0.001 or
0.001 usually it's in that uh variation
depending on what you're doing and how
many epics and they kind of play with
the epics the epics is how many times
are we going to go through all the
data now I have it as two um the actual
setup is for 20 and 20 works great the
reason I have it for two is it takes a
long time to process one of the
downsides of
Jupiter is that Jupiter isolates it to a
single kernel so even though I'm on an
eight core processor uh with 16
dedicated threads only one thread is
running on this no matter what so it
doesn't matter uh so it takes a lot
longer to run even though um tensor flow
really scales up nicely and the batch
size is how many pictures do we load at
once in process again those are numbers
you have to learn to play with depending
on your data and what's coming coming in
and the last thing we want to go ahead
and do is there's a directory with a
data set we're going to
run uh and this just has images of mass
and not
mkks and if we go in here you'll see
data
set um and you have pictures with mass
they're just images of people with mass
on their face uh and then we have the
opposite let me go back up here without
masks so it's pretty straightforward
they look kind of a skew because they
tried to format them into very similar
uh setup on there so they're they're
mostly squares you'll see some that are
slightly different on here and that's
kind of an important thing to do on a
lot of these data sets get them as close
as you can to each other and we'll we
actually will run in the in this
processing of images up here and the
cross uh layers and importing and and
dealing with images it does such a
wonderful job of converting these and a
lot of it we don't have to do a whole
lot with uh so you have a couple things
going on there and so uh we're now going
to be this is now loading the um images
and let me see and we'll go ahead and
create data and labels here's our um uh
here's the features going in which is
going to be our pictures and our labels
going out and then for categories in our
list directory directory and if you
remember I just flashed that at you it
had uh uh face mask or or no face mask
those are the two options and we're just
going to load into that we're going to
pin the image itself and the labels so
we're just create a huge array uh and
you can see right now this could be an
issue if you had more data at some point
um thankfully I have a a 32 gig hard
drive or U
Ram even that you could do with a lot
less of that probably under 16 or even
eight gigs would easily load all this
stuff um and there's a conversion going
on in here I told you about how we are
going to convert the size of the image
so it resizes all the image
that way our data is all identical the
way it comes
in and you can see here with our labels
we have without mask without mask
without mask uh the other one would be
with mask those are the two that we have
going in
there uh and then we need to change it
to the one not hot
encoding and this is going to take our
um um up here we had what was it labels
and data uh we want the labels uh to be
categorical so we're we're going to take
labels and change it to categorical and
our labels then equal a categorical list
uh we'll run that and again if we do uh
labels and we just do the last or the
first 10 let's do the last 10 just
because um minus 10 to the end there we
go just so we can see what the other
side looks like we now have one that
means they have a mask one Zer one Zer
so
on uh one being they have a mask and
zero no mask and if we did this in
Reverse I just realized that this might
not make sense if you've never done this
before let me run this
01 so zero is uh do they have a mask on
zero do they not have a mask on one so
this is the same as what we saw up here
without mask one equals um the second
value is without mask so with mask
without mask
uh and that's just a with any of your
data
processing we can't really a zero if you
have a 01
output uh it causes issues as far as
training and setting it up so we always
want to use a one hot encoder if the
values are not actual uh linear Val or
regression values they not actual
numbers if they represent a
thing and so now we need to go ahead and
do our train X test X train y test y um
train split test data and we'll go ahead
and make sure it's going to be uh random
and we'll take 20% of it for testing and
the rest for um setting it up as far as
training their model this is something
that's become so cool when they're
training these Set uh they realize we
can augment the data what does augment
mean well if I rotate the data around
and I zoom in I zoom out I rotate it um
share it a little bit it flip it
horizontally um fill mode as I do all
these different things to the data it um
is able to it's kind of like increasing
the number of samples I have uh so if I
have all these perfect samples what
happens when we only have part of the
face or the face is tilted sideways or
all those little shifts cause a problem
if you're doing just a standard set of
data so we're going to create an augment
in our image data generator um which was
going to rotate zoom and do all kinds of
cool thing and this is worth looking up
this image data generator and all the
different features it has um a lot of
times I'll the first time through my
models I'll leave that out because I
want to make sure there's a thing we
call build def fail which is just cool
to know you build the whole process and
then you start adding these different
things in uh so you can better train
your model and so we go and run this and
then we're going to load um and then we
need to go ahead and you probably would
have gotten an error if you had and put
this piece in right here um I haven't
run it myself cuz the guys in the back
did this uh we take our base model and
one of the things we want to do is we
want to do a mobile net
V2 um and this what we this is a big
thing right here include the top equals
false a lot of data comes in with a
label on the top row uh so we want to
make sure that that is not the case uh
and then the construction of the head of
the model that will be placed on the top
of the base model um we want to go ahead
and set that
up and you'll see a warning here I'm
kind of ignoring the warning because it
has to do with the uh size of the
pictures and the weights for input shape
um so they'll it'll switch things to
defaults just saying hey we're going to
Auto shape some of this stuff for you
you should be aware of that with this
kind of imagery we're already augmenting
it by moving it around and flipping it
and doing all kinds of things to it uh
so that's not a bad thing in this but
another data it might be if you're
working in a different
domain and so we're going to go back
here and we're going to have we have our
base model we're going to do our head
model equals our base model
output um and what we got here is we
have an average pooling 2D pool size 77
head model um head model flatten so
we're flattening the data uh so this is
all processing and flattening the images
and the pooling has to do with with some
of the ways that can process some of the
data we'll look at that a little bit
when we get down to the lower level in
this processing it um and then we have
our D we've already talked a little bit
about a d just what you think about and
then the head model has a Dropout of
0.5 uh what we can do with a
Dropout the Dropout says that we're
going to drop out a certain amount of
nodes while training uh so when you
actually use the model it will use all
the notes but this drops certain runs
out and it helps stop biases from up
forming uh so it's really a cool feature
in here they discovered this a while
back uh we have another dense mode and
this time we're using soft Max
activation lots of different activation
options here soft Max is a real popular
one for a lot of things U so is
Ru and you know there we could do a
whole talk on activation formulas uh and
why what their different uses are and
how they work when you first start out
you'll you'll use mostly the ru and the
softmax for a lot of them uh just
because they're they're some of the
basic setups it's a good place to
start uh and then we have our model
equals model inputs equals base model.
input outputs equals head model so again
we're still building our model here
we'll go ahead and run
that and then we're going to Loop over
all the layers in the base model and
freeze them so they will not be updated
during the first training
process uh so for layer and base model
layers layers. trable equals False A lot
of times when you go through your data
um you want to kind of jump in partway
through um I I'm not sure why in the
back they did this for this particular
example um but I do this a lot when I'm
working with series and and specifically
in stock data I wanted to iterate
through the first set of 30 Data before
it does
anything um I would have to look deeper
to see why they froze it on this
particular one
and then we're going to compile our
model uh so compiling the model
atom a nit layer
Decay um initial learning rate over
epics and we go ahead and compile our
loss is going to be the binary cross
entropy which will have that print out
Optimizer for opt metrics is accuracy
same thing we had before not a huge jump
as far as um the previous code
and then we go ahead and we've gone
through all this and now we need to go
ahead and fit our model uh so train the
head of the network print info training
head
run now I skipped a little time because
it you'll see the run time here is um at
80 seconds per epic takes a couple
minutes for it to get through on a
single
kernel one of the things I want you to
notice on here while we're while it's
finishing the processing
is that we have up here our augment
going on so anytime the train X and
train y go in there's some Randomness
going on there and is jiggling it around
what's going into our setup uh of course
we're batch sizing it uh so it's going
through whatever we set for the batch
values how many we process at a time and
then we have the steps per epic uh the
train X the batch size validation data
here's our test X and Test Y where we're
sending that in
uh and this again it's validation one of
the important things to know about
validation is our um when both our
training data and our test data have
about the same accuracy that's when you
want to stop that means that our model
isn't biased if you have a higher
accuracy on your uh testing you know
you've trained it and your accuracy is
higher on your actual test data then
something in there is probably uh has a
bias and it's overfitted uh so that's
what this is really about right here
with the validation data and validation
steps so it looks like it's let me go
ahead and see if it's done processing
looks like we've gone ahead and gone
through two epics again you could run
this through about 20 with this amount
of data and it would give you a nice
refined uh model at the end we're going
to stop at two because I really don't
want to sit around all afternoon and I'm
running this on a single thread so now
that we've done this we're going to need
to evaluate our model and see how good
it is and to do that we need to go ahead
and make our
predictions um these are predictions on
our test X to see what it thinks are
going to be uh so now it's going to be
evaluating the network and then we'll go
ahead and go down here and we want need
to uh turn the index in because remember
it's it's either zero or one it's uh 0
one 01 so you have two outputs uh not
wearing uh wearing a not wearing a mask
and so we need to go ahead and take that
argument at the end and change those
predictions to a zero or one coming out
uh and then to finish that off we want
to go ahead and let me just put this
right in here and do it all in one shot
we want to show a nicely formatted
classification report so we can see what
that looks like on here and there we
have it we have our Precision uh it's
97% with the mask there's our F1 score
support without a mask
97% um so that's pretty high high uh
setup on there you know three people are
going to sneak into the store who are
without a mask and that thinks they have
a mask and there's going to be three
people with a mask that's going to flag
the person at the front to go oh hey
look at this person you might not have a
mask um if I guess it's a set up in
front of a store um so there there you
have it and of course one of the other
cool things about this is if someone's
walking into the store and you take
multiple pictures of them
um you know this is just an it it would
be a way of flagging and then you can
take that average of those pictures and
make sure they match or don't match if
you're on the back end and this is an
important step because we're going to
this is just cool I love doing this
stuff uh so we're going to go ahead and
take our model and we're going to save
it uh so model save Mass detector. model
we're going to give it a name uh we're
going to save the format um in this case
we're going to use the H5
format and so this model we just
programmed has just been saved uh so now
I can load it up into say another
program what's cool about this is let's
say I want to have somebody work on the
other part of the program well I just
saved the model they upload the model
and now they can use it for whatever and
then if I get more information uh and we
start working with that at some
point I might want to update this model
um make a better model and this is true
of so many things where I take this
model and maybe I'm uh running a
prediction on uh making money for a
company and as my model gets
better I want to keep updating it and
then it's really easy just to push that
out to the actual end user uh and here
we have a nice graph you can see the
training loss and accuracy as we go
through the epics uh we only did the you
know only shows just the one Epic coming
in here but you can see right here as
the uh um value loss train accuracy and
value accuracy
starts switching and they start
converging and you'll hear converging
this is a convergence they're talking
about when they say you're you're um I
know when I work in the S kit with
sklearn neural networks this is what
they're talking about a convergence is
our loss and our accuracy come together
and also up here and this is why I'd run
it more than just two epics as you can
see they still haven't converged all the
way uh so that would be a cue for me to
keep going
but what we want to do is we want to go
ahead and create a new Python
3
program and we just did our train mask
so now we're going to go ahead and
import that and use it and show you in a
live action um get a view of uh both
myself in the afternoon along with my
background of an office which is in the
middle still of reconstruction for
another
month and we'll call this uh
mask
detector and then we're going to grab a
bunch of um a few items coming
in uh we have our um mobet V2 import
pre-processing input so we're still
going to need that um we still have our
tensor floral image to array we have our
load model that's where most of the
stuff's going on this is our CV2 or open
CV again I'm not going to dig too deep
into that we're going to flash a little
open CV code at you uh and we actually
have a tutorial on that coming out um
our numpy array our IM utilities which
is part of the open CV or CV2
setup uh and then we have of course time
and just our operating system so those
are the things we're going to go ahead
and set up on here and then we're going
to create this takes just a moment
our module here which is going to do all
the heavy lifting uh so we're going to
detect and predict a mask we have frame
face net Mass net these are going to be
generated by our open CV we have our
frame coming in and then we want to go
ahead and and create a mask around the
face it's going to try to detect the
face and then set that up so we know
what we're going to be processing
through our model
um and then there's a frame shape here
this is just our height versus width
that's all HW stands for um they've
called it blob which is a CV2 DNN blob
form image frame so this is reformatting
this Frame that's going to be coming in
literally from my camera and we'll show
you that in a minute that little piece
of code that shoots that in here uh and
we're going to pass the blob through the
network and obtain the face
detections uh so faet do set in import
blob detections face net forward print
detections
shape uh so these is this is what's
going on here this is that model we just
created we're going to send that in
there and I'll show you in a second
where that is but it's going to be under
face
net uh and then we go ahead and
initialize our list of faces their
corresponding locations and the list of
predictions from our face mask
Network we're going to Loop over the
detections and this is a little bit more
work than you think um as far as looking
for different faces what happens if you
have a fa a crowd of faces um so We're
looping through the detections and the
shapes going through here and
probability associated with the
detection uh here's our confidence of
detections we're going to filter out
weak detection by ensuring the
confidence is greater than the minimum
confidence uh so we said it remember
zero to one so 0 five would be our
minimal confidence probably is pretty
good
um and then we're going to put in
compute bounding boxes for the object if
I'm zipping through this it's because
we're going to do an open CV and I
really want to stick to just the carass
part and so I'm I'm just kind of jumping
through all this code you can get a copy
of this code from Simply learn and take
it apart or look for the open CV coming
out and we'll create a box uh the box
sets it around the
image ensure the bounding boxes fall
within dimensions of the
frame uh so we create a box around
what's going to we hope it's going to be
the face extract the face Roi convert it
from BGR to RGB
Channel again this is an open CV issue
not really an issue but it has to do
with the order um I don't know how many
times I've forgotten to check the order
colors when working with open CV because
it's all kinds of fun things when red
becomes blue and blue becomes red uh
then we're going to go ahead and resize
it process it frame it uh face frame
setup again the face the CBT color we're
going to convert it uh we're going to
resize it image to array pre-process the
input uh pin the face locate face. x.y
and x boy that was just a huge amount
and I skipped over a ton of it but the
bottom line is we're building a box
around the face and that box because the
open CV does a decent job of finding the
face and that box is is going to go in
there and see hey does this person have
a mask on
it uh and so that's what that's what all
this is doing on here and then finally
we get down to this where it says
predictions equals mass net. predict
faces batch size
32 uh so these different images of where
we're guessing where the face is are
then going to go through an generate an
array of faces if you will and we're
going to look through and say does this
face have a mask on it and that's what's
going right here is our prediction
that's the big thing that we're working
for and then we return the locations and
the predictions the locations just tells
where on the picture it is and then the
um prediction tells us what it is is it
a mask or is it not a mask all right so
we've loaded that all up so we're going
to load our serialized face detector
model from dis um and we have our the
path that it was saved in obviously
you're going to put it in a different
path depending on where you have it or
however you want do it and how you saved
it on the last one where we trained it
uh then we have our weights path um and
so finally our faet here it is equals
CB2 dn. read net U Proto text path
weights path and we're going to load
that up on here so let me go ahead and
run
that and then we also need to I'll just
put it right down here I always hate
separating these things in there um and
then we're going to load the actual mass
detector model from disk this is the the
the model that we saved so let's go
ahead and run that on there also so this
is pulling in all the different pieces
we need for our model and then the next
part is we're going to create open up
our video uh and this is just kind of
fun because it's all part of the open
CV video
setup and me just put this all in as one
there we go uh so we're going to go
ahead and open up our video we're going
to start it and we're going to run it
until we're done
and this is where we get some real like
kind of live action stuff which is fun
this is what I like working about with
images and videos is that when you start
working with images and videos it's all
like right there in front of you it's
Visual and you can see what's going on
uh so we're going to start our video
streaming this is grabbing our video
stream Source zero
start uh that means it's C grabbing my
main camera I have hooked up um and then
you know starting video you're going to
print it out here's video Source equals
zero start Loop over the frames from the
video
stream oops a little redundancy there um
let me
close I'll just leave it that's how they
had it in the code so uh so while true
we're going to grab the frame from the
threaded video stream and resize it to
have the maximum width of 400 pixels so
here's our frame we're going to read it
uh from our visual uh stream we're going
to resize it
and then we have a returning remember we
returned from the our procedure the
location and the prediction so detect
and predict mask we're sending it the
frame we're sending it the face net and
the mass net so we're sending all the
different pieces that say this is what's
going through on
here and then it returns our location
and predictions and then for our box and
predictions in the location and
predictions um and the box is is again
again this is an open CV set that says
hey this is a box coming in from the
location um because you have the two
different points on there and then we're
going to unpack the box and predictions
and we're going to go ahead and do mask
without a mask equals
prediction we're going to create our
label no mask we create color if the
label equals mask
l225 and you know this is going to make
a lot more sense when I hit the Run
button here uh but we have the
probability of the label we're going to
displ the label and bounding box
rectangle on the output
frame uh and then we're going to go
ahead and show the output from the frame
CV2 IM show frame frame and then the key
equals CV2 wait key one we're just going
to wait till the next one comes through
from our
feed and we're going to do this until we
hit the stop button pretty much so are
you ready for this let's see if it works
we've distributed our uh our model we've
loaded it up into our distributed uh
code here we've got it hooked into our
camera and we're going to go ahead and
run it and there it goes it's going to
be running and we can see the data
coming down here and we're waiting for
the
popup and there I am in my office with
my funky headset
on uh and you can see in the background
my unfinished wall and it says up here
no mask oh no I don't have a mask on uh
I wonder if I cover my mouth what would
happen uh you can see my no
mask goes down a little bit I wish I'd
brought a mask into my office it's up at
the house but you can see here that this
says you know there's a 95 98% chance
that I don't have a mask on and it's
true I don't have a mask on right now
and this could be distributed this is
actually an excellent little piece of
script that you could start you know you
install somewhere on a a video feed on a
on a security camera or something and
then you'd have this really neat uh
setup saying hey do you have a mask on
when you enter a store or a public
transportation or whatever it is where
they're required to wear a mask uh let
me goe and stop
that now if you want a copy of this uh
code definitely give us a hauler we will
be going into open CV in another one so
I skipped a lot of the open CV um code
in here as far as going into detail
really focusing on the carass uh saving
the model uploading the model and then
processing a streaming video through it
so you can see that the model works we
actually have this working model that
hooks into the video
camera which is just pretty cool and a
lot of
fun so I told you we're going to dive in
and really Roll Up Our Sleeve and do a
lot of coating today uh we did the basic
uh demo up above for just pulling in a
carass and then we went into a cross
model uh where we pulled in data to see
whether someone was wearing a mask or
not so very useful in today's world as
far as a fully running application so if
you want to seek expertise in deep
learning skills and aspire to be a part
of Cutting Edge field of building chat
Bots and service Bots using deep
learning our CTIC postgraduate program
in Ai and machine learning provides
comprehensive coverage of the latest
tools and Technologies within the AI
ecosystem it offers master classes led
by esteemed calch faculty and IBM
experts engaging hackathons and
interactive ask me anything sessions by
enrolling in this program delivered in
partnership with IBM you will gain the
expertise needed to become a
professional in Ai and ml this
comprehensive artificial intelligence
course will equip you with the knowledge
about various AI based Technologies
empowering you to excel in this field so
do check out the course Link in the
description for more details let's get
started
now now let's move on to the differences
between tensorflow kasas and py the
first difference that we'll be looking
at is called level of API there are two
main types of apis a lowlevel API and a
high level API API stands for
application programming
interface a low-level application
programming interface is generally more
detailed and allows you to have more
detailed control to manipulate functions
within them on how to use and Implement
them while a high level API is more
generic and simple and provides more
functionality with one command
statements than a lower level API high
level interfaces are comparatively
easier to learn and to implement the
models using them they allow you to
write code in a short amount of time and
to be less involved with the details in
this case tensor flow is a high and
lowlevel API pure tensorflow is a
lowlevel API while tensor flow wrapped
in kasas is a high level
API Keras in itself is a high level API
which uses multiple low-level apis as a
backend and simplifies the operation of
these low-level apis py torch is a
low-level API the next criteria that
we'll be looking at is speed tensor flow
is very fast and is used for high
performances caras is slower as it works
on top of tensor flow not only does it
have to wait for tensor flow to finish
implementation it then starts its own
implementation meanwhile py toor works
at the same speed as tensorflow as both
of them are both low-level apis now
kasas is a rapper class for tensor flow
and has added abstraction
functionalities on top of tensor flow
which make it slower than t tensor flow
and py to in computation speed both
tensor flow and py toch are almost equal
and in development speed caras is faster
as it has built-in functionalities which
can significantly reduce your
development time the next difference is
on the
architecture tensor flow is not very
easy to use and even though it provides
caras as a framework that makes it work
easier tensorflow still has a very
complex architecture which is how hard
to use meanwhile kasas has a simpler
architecture and is easier to use it
provides a high level of abstraction
which makes implementation of programs
in Keras significantly easier pych on
the other hand also has a complex
architecture and the readability is less
when compared to kasas tensorflow uses
computational graphs which makes it very
complex and hard to interpret but it has
amazing computational ability across
platforms py is a little hard for
beginners but is really good for
computer vision and deep learning
purposes data sets and debugging tensor
flow works with large data sets due to
its high execution speed and debugging
is really hard in tens of flow due to
its complex nature meanwhile kasas only
works with very small data sets as its
speed of execution is low programs do
not require frequent debugging in kasas
as they are relatively simpler and P
toor can manage high level tasks in
higher Dimension data sets and is easier
to debug than both Keras and tens oflow
next we'll be looking at ease of
development as we said before tensorflow
works with many hard Concepts such as
computational graphs and tensors which
means that writing code in tensorflow is
very hard it is generally used by people
when they are doing research work and
really need very specific
functionalities there are on the other
hand provides a high level of
abstraction which makes it very easy to
use it is best for people who are just
starting out with python and machine
learning py to is easier than tens oflow
but is still comparatively hard than
Keras it is not very easy to learn for
beginners but is significantly more
powerful than just plain caras ease of
deployment tens of flow is very easy to
deploy as it uses tensorflow serving
tensorflow serving is a flexible high
performance serving system for machine
learning models designed for production
environments tensorflow serving makes it
easy to deploy new algorithms and
experiments while keeping the same
server architecture and apis tensorflow
serving provides outof the Box
integration with tensorflow models but
can be easily extended to serve other
types of models and data in kasas model
deployment can be done with either
tensorflow serving or flask which makes
it relatively easy but not as easy as
you as it would be with tensorflow and
py py uses py mobile which makes
deployment easy but again for tensorflow
deployment is way easier as tensorflow
serving can update your machine learning
back end on the fly without the user
even realizing there's a growing need to
execute ml models on edge devices to
reduce latency preserve privacy and
enable new interactive use cases in the
past Engineers used to to train models
separately they would then go through a
multi-step error prone and often complex
process to train the models for
execution on a mobile device the mobile
run time was often significantly
different from the operations available
during training leading to inconsistent
developer and eventually user experience
all of these frictions have been removed
by pyo Mobile by allowing a seamless
process to go from training to
deployment by staying entirely within
the py to ecosystem it provides an
endtoend workflow that simplifies the
research to production environment for
mobile devices in addition it paves the
way for privacy preserving features via
Federated learning
techniques at the end of the day the
question that really matters is which
framework should you use Keras
tensorflow or py
to now tens oflow has implemented
various levels of abstraction to make
implementation of deep learning and Ural
networks easy this has also made
debugging easier Keras is simple and
easy but not as fast as tensor flow it
is more user friendly than any other
deep learning API however and is easier
to learn for beginners py on the other
hand is the preferred deep learning API
for teachers but it is not as widely
used in production as tensor flow is it
is faster but it has lower GPU
utilization at the end of the day the
framework that we would suggest that you
use is tensor
flow why while py Tor may have been the
prefer deep learning library for
researchers denor flow is much more
widely used in day-to-day production P's
ease of use combined with the default
eego execution mode for easier debugging
predestines it to be used for fast hacky
Solutions and smaller scale models but
tensor flows extensions for deployment
on both servers and mobile devices
combined with the lack of python
overhead makes it the preferred option
for companies that work with deep
learning models in addition the tensor
FL board visualization features offers a
nice way of showing the inner workings
of your model to say your customers
meanwhile between tensor flow and kasas
the main difference isn't in performance
tensor flow is a bit faster due to less
overhead but also the level of control
you would like kasas is much easier to
start with than plain tens oflow but if
you want to do something with kasas that
doesn't come out of the box it'll be
harder to implement that tens of flow on
the other hand allows you to create any
arbitrary computational graph providing
much more flexibility so if you're doing
more research type of work tensor flow
is the sure route to go due to the
flexibility that it provides so if you
want to seek expertise in deep learning
skills and aspire to be a part of
Cutting Edge field of building chat Bots
and service Bots using deep learning our
c postgraduate program in Ai and machine
learning provides comprehensive coverage
of the latest tools and Technologies
within the AI ecosystem it offers master
classes led by esteemed Caltech faculty
and IBM experts engaging hackathons and
interactive ask me anything sessions by
enrolling in this program delivered in
partnership with IBM you will gain the
expertise needed to become a
professional in Ai and ml this
comprehensive artificial intelligence
course will equip you with the knowledge
about various AI based Technologies
empowering you to excel in this field so
do check out the course Link in the
description for more details let's get
started
now so what are generative adversarial
networks generative adversarial networks
or Gans introduced in 2014 by ianj
Goodfellow and co-authors became very
popular in the field of machine learning
Gan is an unsupervised learning task in
machine learning it consists of two
models that automatically discover and
learn the patterns in input data the two
models called generator and
discriminator
compete with each other to analyze
capture and copy the variations within a
data set Gans can be used to generate
new examples that possibly could have
been drawn from the original data
set in the image below you can see that
there is a database that has real 100
rupee notes the generator which is
basically a neural network generates
fake 100 rupees notes the discriminator
network will identify if the notes are
real or fake let us now understand in
brief about what is a generator a
generator in Gans is a neural network
that creates fake data to be trained on
the discriminator it learns to generate
plausible data the generated instances
become negative training examples for
the discriminator it takes a fixed
length random Vector carrying noise as
input and generates a sample now the
main aim of the generator is to make the
discriminator classify its output as
real the portion of the Gan that trains
a generator
includes a noisy input Vector the the
generator Network which transforms the
random input into a data instance a
discriminator network which classifies
the generator data and a generator loss
which penalizes the generator for
failing to do the discriminator the back
propagation method is used to adjust
each weight in the right direction by
calculating the weight's impact on the
output the back propagation method is
used to obtain gradients and these
gradients can help change the generator
weights now let us understand in brief
what a descript minator is a
discriminator is a neural network model
that identifies real data from the fake
data generated by the generator the
discriminator training data comes from
two
sources the real data instances such as
real pictures of birds humans currency
notes Etc are used by the discriminator
as positive samples during the training
the fake data instances created by the
generator are used as negative examples
during the training process while
training the discriminator it connects
with two laws L functions during
discriminator training the discriminator
ignores the generator LW and just uses
the discriminator LW in the process of
training the discriminator the
discriminator classifies both real data
and fake data from the generator the
discriminator laws penalizes the
discriminator from misclassifying a real
data instance as fake or a fake data
instance as real now moving ahead let's
understand how Gans work now Gans
consists of two networks a generator
which is represented as G ofx and a
discriminator which is represented as D
ofx they both play an adversarial game
where the generator tries to fool the
discriminator by generating data similar
to those in the training set the
discriminator tries not to be fooled by
identifying fake data from the real data
they both work simultaneously to learn
and train complex data like audio video
or image files now you are aware that
Gans consists of two networks a
generator G ofx and discriminator D ofx
now the generat Network takes a sample
and generates a fake sample of data the
generator is trained to increase the
probability of the discriminator network
to make mistakes on the other hand the
discriminator network decides whether
the data is generated or taken from the
real sample using a binary
classification problem with the help of
a sigmoid function that gives the output
in the range 0o and 1 here is an example
of a generative adversarial Network
trying to identify if the 100 rupe notes
are real or fake so first a noise vector
or the input Vector is fed to the
generator Network the generator creates
fake 100 rupee nodes the real images of
100 rupee notes stored in a database are
passed to the discriminator along with
the fake notes the discriminator then
identifies the notes and classifies them
as real or fake we train the model
calculate the loss function at the end
of the discriminator network and back
propagate the loss into both
discriminator and Generator now the
mathematical equation of training again
can be represented as you can see see
here now this is the
equation and these are the parameters
here G represents generator D represents
the discriminator now P data of X is the
probability distribution of real data P
of Z is the distribution of Generator X
is the sample of probability data of X
Zed is the sample size from P of z d of
X is the discriminator network and G of
Z is the generator Network now the
discriminator focuses to maximize the
objective function such that D of X is
close to 1 and Z of Z is close to zero
it simply means that the discriminator
should identify all the images from the
training set as real that is one and all
the generated images as fake that is
zero the generator wants to minimize the
objective function such that D of Z of Z
is 1 this means that the generator tries
to generate images that are classified
as real that is one by the discriminator
network next let's see the steps for
training a neural network so we have to
first Define the problem and collect the
data then we'll choose the architecture
of Gan now depending on your problem
choose how your Gan should look like
then we need to train the discriminator
in real data that will help us predict
them as real for n number of times next
you need to generate fake inputs for the
generator after that you need to train
the discriminator on fake data to
predict the generator data is fake
finally train the generator on the
output of
discriminator with the discriminator
predictions available train the
generator to fool the discriminator let
us now look at the different types of
Gans so first we have vanilla Gans now
vanilla Gans have minmax optimization
formula that we saw earlier where the
discriminator is a binary classifier and
is using sigmoid cross entropy loss
during
optimization in vanilla Gans the
generator and the discriminator are
simple multi-layer percept droms the
algorithm tries to optimize the
mathematical equation using stochastic
gradient decent up next we have deep
convolutional Gans or DC Gans now DC
Gans support convolutional neural
networks instead of vanilla neural
networks at both discriminator and
Generator they are more stable and
generate higher quality images the
generator is a set of convolutional
layers with fractional strided
convolutions or transpose convolutions
so it unsampled samples the input image
at every convolutional layer the
discriminator is a set of convolutional
layers with strided convolutions so it
down samples the input image at every
convolutional lay moving ahead the third
type you have is conditional Gans or C
Gans vanilla Gans can be extended into
conditional models by using an extra
label information to generate better
results in C Gan an additional parameter
called Y is added to the generator for
generating the corresponding data labels
are fed as input to the discriminator to
help distinguish the real data from fake
data generated finally we have super
resolution Gans now Sr Gans use deep
neural networks along with adversarial
neural network to produce higher
resolution images super resolution Gans
generate a photo realistic high
resolution image when given a low
resolution image let's look at some of
the important applications of
Gans so with the help of DC Gans you can
train images of cartoon characters for
generating faces of anime characters and
Pokmon characters as well next Gans can
be used on the images of humans to
generate realistic faces the faces that
you see on your screens have been
generated using Gans and do not exist in
reality third application we have is
Gans can be used to build realistic
images from textual descriptions of
objects like birds humans and other
animals we input a sentence and generate
multiple images fitting the description
here is an example of a text to image
translation using Gans for a bird with a
black head yellow body and a short beak
the final application we have is
creating 3D objects so Gans can generate
3D models using 2D pictures of objects
from multiple perspectives Gans are very
popular in the gaming industry Gans can
help automate the task of creating 3D
characters and backgrounds to give them
a realistic feel so if you want to seek
expertise in deep learning skills and
aspire to be a part of Cutting Edge
field of building chat Bots and service
BS using deep learning our CTIC
postgraduate program in Ai and machine
learning provides comprehensive coverage
of the latest tools and Technologies
within the AI ecosystem it offers master
classes led by esteemed calch faculty
and IBM experts engaging hackathons and
interactive as anything sessions by
enrolling in this program delivered in
partnership with IBM you will gain the
expertise needed to become a
professional in Ai and ml this
comprehensive artificial intelligence
course will equip you with the knowledge
about various AI based Technologies
empowering you to excel in this field so
do check out the course Link in the
description for more details let's get
started
now so computational graphs are really
the heart and soul of neural networks uh
we talk about a computational graph they
are a visual representation of
expressing and evaluating mathematical
equations the nodes and data flow in a
graph correspond to mathematical
operations and variables you'll hear a
lot uh some of the terms you might hear
on are node and Edge The Edge being the
data flow in this case um it could also
represent an actual value they have um
oh I think in spark they have a graph x
which works just on Computing edges
there's all kinds of stuff that has
evolved from computational graphs we're
focusing just on carass and on uh neural
networks so we're not going to go into
great detail on everything a
computational graph does it is a core
component of a neural network is what's
important to know on this so carass
offers a python userfriendly front end
while maintaining a strong computation
Power by using a low level API like
tensor flow pie torch Etc which use
computational grph as a
backend so one this allows for
abstraction of complex problems while
specifying control flow if you've ever
looked at some of the backend or the
original versions of tensor flow uh it's
really a nightmare you have all these
different settings you have to put in
there and create
uh it's a lot of a lot of back-end
programming this is like the old
computers when you had to uh tell it how
to dispose of a variable and how to
properly reallocate the memory for use
all that is covered nowadays in our
higher level programming well this is
the same thing with carass is it covers
a lot of this stuff and does things for
you that you would could spend hours on
just trying to figure
out it's useful for calculating
derivatives by using back propagation we
definitely not going to teach a class on
derivatives uh in this little video but
understanding uh a derivative is the
rate of change so if you have a
particular function you're using in your
neural network a lot of them is just
simple uh uh y equals MX plus b um your
ukian geometry where you just have a
simple slope times the intercept and
they get very complicated they have the
inverse tangent function for Activation
as opposed to just a linear ukian model
and you can think about this as you have
your data coming in and you have to
alter it somehow well you alter it going
down to get an answer you end up with an
error and that error goes back up and
you have to have that back propagation
with the derivative you want to know how
it changed so that you can figure out
how to adjust it for the
error a lot of that's hidden so you
don't even have to worry about it with
carass and in today's carass it'll even
if you create your own um uh formula for
computing an answer it will autom Ally
compute the back prop the the derivative
for you in a lot of
cases it's easier to implement
distributed computation uh so cross is
really nice way to package it and get it
off on different computers and share it
and it allows parallelism which means
that two operations can run
simultaneously so as we start developing
these backends it can do all kinds of
cool things and utilize multiple cores
gpus on a computer uh to get that
parallel processing up
what are neural networks well like I
said there already uh we talked about in
computational edges you have a node and
you have a connection or your Edge so
neural networks are algorithms fashioned
after the human brain which contain
multiple layers each layer contains a
node called a neuron which performs a
mathematical operation they break down
complex problems into simple
operations so one an input layer takes
in our data and pre-processes it when we
talk about pre-processing when you're
dealing with neural networks uh you
usually have to pre-process your data so
that it's between minus one and one or
zero and one um into some kind of value
that's usable that occurs before it gets
to the neural network in fact 80% of
data science is usually impr prepping
that data and getting it ready for your
different
models two you have hidden layer
performs a nonlinear transformation of
input now it can do a hidden a linear
transformation it can use just a basic
um ukian geometry and you could think of
a node adding all the different
connections coming in uh so each
connection would have a weight and then
it would add to that weight plus an
intercept um in the node itself so you
can actually use ukan geometry but a lot
of these get really complicated they
have all these different formulas and
they're really cool to look at but when
you start looking at them look at how
they work uh you really don't need to
know the high math behind it
um to figure them out and figure out
what they're doing which is really cool
that means a lot of people can use this
without having to go get a PhD in
mathematics number three the output
layer takes the results from hidden
layer transform them and gives a final
output so sequential models uh so what
makes this a sequential model sequential
models are linear stacks of layers where
one layer leads to the next it is simple
and easy to implement and you just have
to make sure that the previous layer is
the input to the next layer so uh you
have used for plain stack of layers
where each layer has one input and one
output tensor and this is what tensor
flow is named after is um each one of
these layers is like a tensor each each
node is a tensor and then the layer is
also considered a tensor of
values and it's used for simple
classifier declassify models you can
it's also used for regression models too
so it's not just about uh this is
something this is a teapot this is a cat
this is a dog um it's also used for
generating um uh reget the actual values
you know this is worth $10 that's worth
$30 uh the weather's going to be 90 out
or whatever it is so you can use it for
both classifier and declassify uh
models and one more note when we talk
about sequential models the term
sequential is used a lot and it's used
in different areas and different
notations when you're in data science so
when we talk about time series we'll
talk about sequential that is something
very different uh sequential in this
case means it goes from the input to
layer one to Layer Two to the output so
it's very directional it's important to
note this because if you have a
sequential model can you have a
non-sequential model and the answer is
yes uh if you master the basics of a
sequential model you can just as easily
have another model that shares layers um
you can have another model where the
have an input coming in and it splits
and then you have one set that's doing
one set of uh nodes maybe they're doing
a yes no kind of node where it's either
putting out a zero or a one a classifier
and the other one might be regression
it's just processing numbers and then
you recombine them for the output um
that's what they call a cross uh the
cross
API so there's a lot of different
availabilities in here and all kinds of
cool things you can do as far as
encoding and decoding and all kinds of
things and you can share layers and
things like that we're just focusing on
the basic cross model with the
sequential
model so let's dive into the meat of the
matter let's do a and do a demo on here
uh today's demo in this demo we will be
performing flower classification using
sequential model and carass and we'll
use our model to classify between five
different types of flowers
now for this demo and you can do this
demo on whatever platform you want or
whatever um user interface for
developing python um I'm actually using
anaconda and then I'm using Jupiter
notebooks to develop in and if you're
not familiar with this um you can go
under environment once you've created
environment you can come in here to open
a terminal window and if you don't have
the different modules in here you can do
your cond install or whatever module it
is um just Happ in that this particular
setup didn't have a Seaborn in it which
I already installed
uh so here's our anaconda and then I'm
going to go
back and start up my Jupiter
notebook where I already created a uh
new uh python project Python 3 I'm in
Python 3.8 on this particular one um
sequential model for flowers so lots of
fun there uh so we're going to jump
right into this the first thing is to
make sure you have all your modules
installed so if you don't have uh numpy
pandas matplot library and Seaborn and
the carass um and sklearn or sidekit
it's not actually sklearn you'll need to
go ahead and install all of those now
having done this for years and having
switched environments and doing
different things um I get all my imports
done and then we just run it and if we
get an error we know we have to go back
and install something um right off the
bat though we have numpy pandas matplot
Library Seaborn these are built on top
of each other pandas the data frame and
built on top of numpy the uh um data
array and then we bring in our SK learn
or scit this is the S kit setup SCI uh
kit even though you use sklearn to bring
it in it's a s kit and then our carass
we have our pre-processing the images
image data generator um or model this is
our basic model or sequential
model uh and then we bring in from coros
layers uh import dents um
optimizers these optimizers a lot of
them already come in these are your
different optimizers and it's almost a
lot of this is so automatic now um atom
is the a lot of times the default
because you're dealing with a large data
uh and then we get our SGD which is uh
smaller data does better on smaller
pieces of data and I'm not going to go
into all of these uh different
optimizers we didn't even use these in
the actual demo you just have to be
aware that they are different optimizers
and the Digger the more you dig into
these models um you'll hit a point where
you do need to play with these a little
bit but for the most part leave it at
the default when you're first starting
out and we're doing just the sequential
you'll see here layers
dense and then if we come down a little
bit more uh when they put this together
and they're running the dense layers
you'll also see they have Dropout they
have flatten they have
activation uh they have the uh
convolutional layer 2D Max pooling 2D
batch
normalization what are all these layers
uh and when we get to the model we're
going to talk about them uh a lot of
times when you're just starting you can
just uh uh import cross. layers and then
you have your Dropout your flatten uh
your convolutional uh neural network 2D
and we'll we'll cover what these do in
the actual example when we get down
there uh what I want you to take from
here though is you need to run your
Imports um and load your different
aspects of this and of course your
tensorflow TF because this is all built
on tensor flow and then finally uh
import random is RN just for random
generation and then we get down here we
have our uh
CV2 that is your um open image or your
open CV they call it for processing in
images that's what the CV2
is uh we have our
tqdm the tqdm is for um is a progress
bar just a fancy way um of adding when
you're running a process you can view
the bar going across in the Jupiter uh
setup not really necessary but it's kind
of fun to have um we want to be able to
shuffle some files uh again these are
all different things pill is another
um image processor it goes with the CV2
a lot of times you'll see both of those
and so we run those we got to bring them
all
in and the next thing is to set up our
directories and so when we come into the
directories there's an important thing
to note on here other than we're looking
at a lot of flowers which is
fun uh is we get down here we have our
directory archive flowers that just
happens to be where the different uh
files for different flowers are put in
we're noting an X and a z and the x is
the data of the image and the Z is the
tag for it what kind of flower is this
uh and the image size is really
important because we have to resize
everything if you have a neural network
and if you remember from our neural
networks uh let me flip back to that
slide we look at this slide we have two
input nodes here uh with an image you
have an input
node depending on how you set it up for
each pixel and that pixel has three
different color schemes usually in it
sometimes four so if you have a picture
that's 150 by
150 uh you multiply 150 * 150 * 3 that's
how many nodes input layers coming in I
mean so this is a massive input a lot of
times you think oh yeah it's just a a
small amount of data or something like
that uh no it's a full image coming in
then you have your hidden layers A lot
of times they match with the image size
size is coming in so each one of those
is also just as big and then we get down
to just a single output so that's kind
of a a thing to note in here what's
going on behind the scenes and of course
each one of these layers has a lot of
processes and stuff going
on and then we have our our different uh
directories on here let me go and run
that so I'm just setting the directories
that's all this is um archive flowers
Daisy sunflower tulip dandelion Rose uh
just our different directories that
we're going to be looking
at uh and then we want to go ahead and
we're need to assign labels remember we
defined x and
z so we're just going to create a uh uh
definition here um and the first thing
is a return flower type
okay just returns it what kind of flower
it is I guess assign label to it uh but
we're going to go ahead and make our
train data and when you look at this
there's a couple things to take away
from here uh the first one is we're just
appending right onto our numpy array the
image we're going to let numpy handle
all that different aspects as far as 150
by 150 by 3 uh we just dump it right
into the numpy which makes it really
easy we don't have to do anything funky
on the processing and we want to leave
it like that and I'm going to talk about
that in a minute uh and then of course
of course we have to have the string a
pin the label on there and I want you to
notice right here uh we're going to read
the image
in and then we're going to size it and
this is important because we're just
changing this to 150 by 150 we're
resizing the image so it's uniform every
image comes in identical to the other
ones uh this is something that's so
important is um when you're resizing or
reformatting your data you really have
to be aware of what's going on with IM
is it's not a big deal because with an
image you just resize it so it looks
squishy or spread out or stretched um
the neural network picks up on that and
it doesn't really change how it
processes
it so let's go ahead and run that uh and
now we've got our definition set up on
there and then we want to go ahead and
make our
uh training data uh so make the train
data uh daisy flower daisy directory uh
print length of X so here we go let's go
and run that and we're just loading up
the flower daisy uh so this is going all
in there and it's setting um it's adding
it in to the our setup on there to our x
and z setup and we see we have
769 um and then of course you can see
this nice bar here this is the bar going
across is that little added uh code in
there that just makes it really cool for
doing demos uh not necessarily when
you're building your own model or
something like that but if you're going
to display this to other people adding
that little what was it called
um tqdm I can never remember that uh but
the tqdm module in there is really nice
and we'll go ahead and do sunflowers and
of course you could have just uh created
an array of these um but this has an
interesting problem that's going to come
up and I want to show you something it
doesn't matter how good the people in
the back are or how good you are at
programming errors are going to come up
and you got to figure out how to handle
them uh and so when we get all the way
down
to the um where is it dandelion here's
our dandelion directory we're going to
build
um Jupiter has some cool things it does
which makes this really easy to deal
with but at the same time you would want
to go back in there depending on how
many times you rerun this how many times
you pull this so when you're finding
errors uh I'm going in here there's a
couple things you can do and we're just
going to oh it wasn't there it is
there's our error I knew there was an
error this processed
1,62 out of
1,65 now I can do a couple things one I
can go back into our definition and I
can just put in here try and so if it
has a bad conversion because this is
where the errors is coming from uh just
skip it that's one way to do it um when
you're doing a lot of work in data
science and you look at something like
this where you're losing three points of
uh data at the end you just say okay I
lost three points who cares um or you
can go in there and try to delete it um
it really doesn't matter for this
particular demo and so we're just going
to leave that error right alone and skip
over because it's already added all the
other files in there and this is
wonderful thing about Jupiter notebook
is that I can just continue on there and
the x and z which we're creating is
still uh running and we'll just go right
into the next flower row so all these
flowers are in there
um that's just a cool thing about
Jupiter
notebook uh and then we can go ahead and
just take a quick look and
see what we're dealing with and this is
of course really when you're dealing
with other people and showing them stuff
this is just kind of fun where we can
display it on the plot Library here and
we're just going to go through and um
let's see what we got here uh looks like
we're going to do like five of each of
them I think is that how they set this
up um plot Library five by two okay oh I
see how they did it okay so two each so
we have 5 by two set up on our axes and
we're just going to go in and look at a
couple of these
flowers it's always a good thing to look
at some of your data uh no matter what
you're doing we've reformatted this to
15 by 150 you can see how it really
blurs this one up here on the Tulip that
that is that resize to 150 by 150 um and
these are what's actually going in these
are all 150 by 150 images you can check
the dimensions on the side and you can
see uh just a quick sampling of the
flowers we're actually going to process
on here and again like I said at the
beginning most of your work in data
science is
reprocessing this different uh
information so we need to go ahead and
take our
labels uh and run a label encoder on
there and and we're just going to Le as
a label encoder one of the things we
imported and then we always use the fit
um to categorical y comma 5 uh X here's
our array um X so if you look at this
here's our fit we're going to transform
Z that's our Z array we
created um and then we have Y which
equals that and then we go ahead and do
uh to categorical we want five different
categories and then we create our x uh
inpay of x x = x over
255 so what's going on here there's two
different Transformations one we've
turned our categories into 0 1 2 3 4 5
as the output and we have taken our X
array and remember the X array is three
values of your different
colors this is so important to
understand when we do this across a
numpy array this takes every one of
those three colors so we have 150 by 150
pixels out of those 150 by 150 pixels
they each have three um color arrays and
those color arrays ra range from 0 to
250 so when we take the xal X over
255 I'm sorry range from 0 to 255 this
converts all those pixels to a number
between zero and one and you really want
to do that when you're working with
neural networks uh now if you do a
linear regression model um it doesn't
affect it as much and so you don't have
do that conversion if you're doing
straight numbers but when you're running
neural networks if you don't do this
you're going to create a huge bias and
that means they'll do really good on
predicting one or two things and they'll
just totally die on a lot of other
predictions so now we have our um X and
Y values uh X being the data in y being
our no one
output and with any good setup we want
to divide this data into our training so
we have X train uh we have our X test
this is the data we're not going to
program the model with and of course
your y train corresponds to your X train
and your y test corresponds to your X
test the outputs and this is uh when we
do the train test split this was from
the S kit sklearn we imported train test
split and we're just going to go ahead
and do the test size at about a quarter
of the data 0.25 and of course random is
always good this is such a good tool I
mean certainly you can do your own
division um you know you could just take
the first you know 0.25 of the data or
whatever do the length of the data not
real hard to do but this is randomized
so that if you're running this test a
few times you can kind of get an idea
whether it's going to work or
not sometimes what I will do is um I'll
just split the data into three parts and
then I'll test it on two with one being
the uh or train it on two of those parts
with one being the test and I rotate it
so I come up with three different
answers which is a good way of finding
out just how good your model is uh but
for setting up let's stick with the XT
train X test and the SK learn
package and then we're going to go ahead
and uh do a random
seed uh now a lot of times the cross
actually does this automatically but
we're going to go ahead and set it up on
here and you can see we do an NP random
seed um from 42 and we get a nice RN
number um and then we do TF random we
set the seed so you can set your
Randomness at the beginning of your
tensor 4
and that's what the tf. random. set
is so that's a lot of prep um all this
prep and then we finally get to the
exciting part um this is where you
probably spend once you have the data
prepped and you have your pipeline going
and you have everything set up on there
this is the part that's exciting is
building these
models and so we look at this model one
we're going to designate it sequential
um they have the API which is a cross
the cross tensorflow API versus
sequential sequential means we're going
one layer to the next so we're not going
to split the layer and bring it back
together it looks almost the same with
the exception of um bringing it back
together so it's not a huge step to go
from this to an
API and the first thing we're going to
look at is um our convolutional neural
network in 2D uh so what's going on here
there's a lot of stuff that's going on
here um the default for well let's start
with the beginning what is a
convolutional 2d
Network well convolutional 2D Network
creates a number of small windows and
those small Windows float over the
picture and each one of them is their
own neural network and it's basically um
becomes like a uh um a categorization
and then it looks at that and it says oh
if we add these numbers up a certain way
uh we can find out whether this is the
right flower based on this this little
window floating around which looks at
different things and we have filters 32
so this is actually creating 32 Windows
is what that's
doing and the kernel size is 5x5 so
we're looking at a 5x5 Square remember
it's 150 by 150 so this narrows it down
to a 5x5 it's a 2d so it has your XY
coordinates um and when we look at this
5x5 remember each one of these is it's
actually looking at 5x
5x3 uh so we're actually looking at 15x
15 different um
pixels and padding is just um uh usually
I just ignore that activation by default
is railu we went ahead and put the rayu
in
there there's a lot of different
activations Ru is for your smaller uh
when you remember I mentioned atom when
you have a lot of data data use an atom
kind of activation or using atom
processing we're using the railu here uh
it kind of gives you a yes or no but it
it doesn't give you a full yes or no it
has a um a zero and then it kind of
shoots off at an angle very common this
the most common one and then of course
here's our input shape 150 by 150 by 3
pixels and then we have to pull it so
whenever you have a two convolutional 2D
um uh layer we have to bring this back
together and pull this into uh neural
network and then we're going to go ahead
and repeat
this uh so we're going to add another
Network here one of the cool things if
you look at this is that it as it comes
in it just kind of automatically assumes
you're going down to the next layer and
so we have another convolutional null
network uh 2D here's our Max pooling
again we're going to do that again Max
pooling uh and we're just going to
filter on down now one of the things
they did on this one is they changed the
kernel size they changed the number of
filters and so each one of these steps
kind of looks at the data a little bit
differently and that's kind of cool
because then you get a little added
filtering on there this is where you
start playing with the model you might
be looking at a convolutional no network
which is great for image
classifications um we get down to here
one of the things we see is flatten so
we add we just flatten it remember this
is 150 by 15 50 by3 well and actually
the pool size changes so it's actually
smaller than that flatten just puts that
into a 1D array uh so instead of being
you know a tensor of this really
complexity with the the pixels and
everything it's just flat and then the
DSE is just another activation on there
um by default it is probably railu as
far as this
activation and then oh yeah here we go
in sequential they actually added the
activation as railu so this just because
this is sequential this activation is
attached to the dents uh and there's a
lot of different activations but Ru is
the most common one and then we also see
a soft Max uh soft Max is similar but it
has its own kind of variation and one of
the cool things you know what let me
bring this up because if we if you don't
know about these activations this
doesn't make
sense and I just did a quick Google
search on images of tensor flow
activations um I should probably look at
which website this is but this is the
output of the values uh so as your X as
it adds in all those uh weighted X
values going into the node it's going to
activate it a certain way and that's a
sigmoid activation and you can see it
goes between zero and one and has a nice
curve there this also shows the
derivatives um and if we come down the
seven popular activation functions
nonlinear activations there's a lot of
different options on this let me see if
I can find the
o let me see we can find this specific
to
Rao so this is a leaky Ru and you can
see instead of it just being zero and
then a value between uh going up it has
a little leaky there otherwise your Ru
loses some nodes they just become
inactive um but you can see there's a
lot of different options here here's a
good one right here with the railu you
can see the railo function on the upper
on the upper left here and then the
Leaky raayo over here on the right which
is where commonly used
also one of the things I use with
processing um language is the S is the
exponential one or the tangent H the
hyperbolic tangent because they have
that nice uh funky curve that comes in
that um has a whole different meaning
and captures word use better again these
are very specific to domain and you can
spend a lot of time playing with
different models for our basic model uh
we'll stick to the ra and the softmax on
here and we'll go and run and build this
model so now that we've had fun playing
with all these different models that we
can add in there uh we need to go ahead
and have a batch size on here uh
128 epic
10 this means that we're going to send
128 uh rows of data or flowers at a time
to be processed and the Epic 10 that's
how many times we're going to Loop
through all the data reduce um the
values and verbose verbose equals 1
means that we're going to show what's
going on um value monitor what we're
monitoring we'll see that as we actually
train the model this is what's what's
going to come out of there if you set
the verbos equal to zero um you don't
have to watch it train the model
although it is kind of nice to actually
know what's going on
sometimes and since we're still working
on uh bringing the data in here's our
batch site here's our epics we need to
go ahead and create a data generator uh
this is our image data
generator and it has all the different
settings in here almost all of these are
defaults uh so if you're looking at this
going oh my gosh this is confusing most
of the time you can actually just ignore
most of this um vertical flip so you can
randomly flip pictures you can randomly
horizontally flip them um you can shift
the picture around this kind of helps
gives you multiple data off of them uh
zooming rotation there's all kinds of
different things you can do with images
most of these we're just going to leave
as false we don't really need to do all
that um um setup because we already have
a huge amount of data if you're short
data you can start flipping like a
horizontal picture and it will generate
it's like doubling your data almost um
so the upside is you double your data
the downside is that if you already have
a bias in your data you already have um
5,000 sunflowers and only Two Roses
that's a huge bias it's also going to
double that bias uh that is the downside
of
that and so we have our model comp model
compile and this you're going to see in
all the carass we're going to take this
model here we're going to take all this
information as far as how we want it to
go and we're going to compile
it this actually builds the model and so
we're going to run that and I want you
to notice uh learning
rate very important this is the default
001 um there's there you really don't
this is how slowly it adjusts to find
the right answer and the more data you
have you might actually make this a
smaller number um with larger with you
have a very small sample of data you
might go even larger than that and then
we're going to look at the loss
categorically categorical cross entropy
most commonly used and this is uh how
how much it improves the model is
improving is what this number means or
yeah that's that's important on there
and then the accuracy we want to know
just how good our model is on the
accuracy and then uh one of the cool
things to do is if you're in a group of
people who are studying the model if
you're in shareholders you don't want to
do this is you can run the model summary
I do this by default and you can see the
different layers that you built into
this model just a quick summary on there
so we went ahead and we're going to go
ahead and create a um
we'll call it history but we want to do
a model fit
generator and so what this history is
doing is this is tracking what's going
on as while it fits the
model now there's a lot of new setups in
here where they just use fit and then
you put the generator in here um we're
going to leave it like this even though
the new default um is a little different
on that doesn't really matter it does
the same thing and we'll go ahead and
just run
that and you can see while it's running
right here uh we're going through the
epics we have one of 10 now we're going
through 6 to 25 here's our loss we're
printing that out so you can see how
it's improving and our accuracy the
accuracy gets better and better and this
is 6 out of 25 this is going to take a
couple minutes to process uh because we
are training 150 by 150 by 3 pixels
across uh six layers or eight layers
whatever it was that is a huge amount of
processing so this will take a few
minutes to process this is when we talk
about the hardware and the problems that
come up in data science and why it's
only now just exploding being able to do
neural networks this is why this process
takes a long
time now you should have seen a jump on
the screen here because I did uh uh
pause the recorder to let this go ahead
and run all the way through its epics
let's go ahead and take a look and see
what these epics are and um if you set
the verbos to zero instead of one it
won't show what's going on in the behind
the scenes as it's training it so when
we look at this epic 10 epics we went
through all the data 10 times uh if I
remember correctly there's roughly a gig
of data there so that's a lot of data
the first thing you're going to notice
is the 270 seconds um that's how much
each of those epics took to run and so
if you divide 60 in there you roughly
get about five minutes worth of each
epic so if I have 10 epics that's 50
minutes almost an hour of
runtime that's a big deal when we talk
about processing uh in on this
particular computer um I actually have
what is it uh uh eight cores with 16
dedicated threads so it runs like a 16
core computer it alternates the threads
going in and it still takes it 5 minutes
for each one of these epics so you start
to see that if you have a lot of data
this is going to be a problem if you
have a number of models you want to Fe
find out how good the models are doing
and what model to use and so each of
those models could take all night to run
in fact I have a model I'm running now
that takes over uh takes about a day and
a half to test each model um it takes
four days to do with the whole data uh
so what I do is I actually take a small
piece of the
data test it out to find out uh get an
idea of of how the different setups are
going to do and then I increase that
size of the data and then increase it
again and I can just take that that
curve and kind of say okay if U the data
is doing this then I need to add in more
dense layers or whatever uh so you can
do a small chunks of data then figure
out what it cost to do a large set of
data and what kind of model you
want the loss as we see here continues
to go down uh this is the error this is
how much errors in there it really isn't
a um userfriendly number other than
the more it Trends down the better and
so if you continue to see the loss going
down eventually it'll get to the point
where it stops going down and it goes up
and down and kind of waivers a little
bit that point you know you've run too
many epics you're you're starting to get
a bias in there and it's not going to
give you a good model fit the accuracy
just turns us into something that uh we
can use and so the accuracy is what
percentage of guesses in this case it's
categorical so this is the percentage of
guesses are correct um value loss is
similar you know it's a minus a value
loss and then you have the value
accuracy and you'll see the value
accuracy is pretty similar to the
accuracy um just rounds it off basically
and so a lot of times you come down here
and you go okay we're doing 0.5
[Music]
6.7 and that is 70% accuracy or in this
case 68 uh 59% accuracy that's a very
usable number and it's very important to
have if you're identifying uh flowers
that's probably good enough if you can
get within a close distance and knowing
what flower you're you're identifying uh
if you're trying to figure out whether
someone's going to die from a heart
attack or not might want to rethink it a
little bit or uh rekey how you're
building your model so if I'm working
with a uh uh a group of um clients um
shareholders in a company or something
like that you don't really want to show
them this um you don't want to show them
hey you know this is what's going on
with the accuracy these are just numbers
and so we want to go and put the
finishing touches just like when you are
building a house and you put in the
frame and the trim on the house it's
nice to have something a nice view of
what's going on and so we'll go ahead
and do a pie plot and we'll just plot
the history of the loss uh the history
of the value
loss over here um epics train and test
and so we're just going to compute these
this is really important uh and what I
want you to notice right here is when we
get to about oh five epics a little more
than five six epics you see a cross over
here and it starts Crossing as far as
the um uh value loss and what's going on
here is you have the loss in your actual
model in your actual data and you have
the value loss where it's testing it
against the the test data the the data
wasn't used to program your model wasn't
used to train your model on and so when
we see this crossing over this is where
the bias is coming in this is becoming
overfitted
and so when you put these two together
uh right around five and six you start
to see how it does this this switch over
here and that's really where you need to
stop right around five yeah six um it's
always hard to guess because at this
point the model is kind of a black box
uh see but you know that right around
here if you're saving your model after
each run you want to use the one that's
right around five epics cuz that's the
one that's going to have the least
amount of bias so this is really
important as far as guessing what's
going on with your model and its
accuracy and when to stop uh it also is
you know I don't show people this mess
up here um I show somebody this kind of
model and I say this is where the
training and the testing comes in on
this model uh it just makes it easier to
see and people can understand what's
going
on so that completes our demo and you
can see we did what we were set out to
do we took our flowers and we're able to
classify them uh Within about you 68 70%
accuracy whether it's going to be a
dollia sunflower cherry blossom Rose um
a lot of other things you can do with
your output as far as a different tables
to see where the errors are coming from
and what problems are coming
up and we're going to take a look at
image classification using carass and
the basic setup we'll actually look at
two different demos on
here uh what's in it for you today what
is image
classification Intel image
classification data creating neural
networks with carass and the vgg16
model what is image
classification the process of image
classification refers to assigning
classes to an entire image images can be
classified based on different categories
like whether it is a nighttime or
daytime shot what the image represents
Etc and see here we have uh mountains
looking for mountains we'll actually be
doing some uh uh pictures of scenery and
stuff like that in deep learning we
perform image classification by using
neural networks to extract features from
images and classifies them based on
these
features and you can see here where it
says like what computer sees and this
says oh yeah we see mostly Forest maybe
a little bit of mountains because the
way the image is um and this is really
where one of the areas that neural
networks really shine
um if you try to run this stuff through
uh more like a linear regression model
you'll still get results uh but the
results kind of miss a lot of things as
they as the neural networks get better
and better at what they do with
different tools we have out
there uh so Intel image classification
data the data being used is the Intel
image classification data set which
consists of images of six types of land
areas and so we have Forest building
glaciers and mountains sea and Street uh
and you can see here there's a couple of
the images out of there as a setup in in
the um uh Intel image classification
data that they
use and then we're going to go into
creating a neural networks with
carass the convolutional neural network
that we are creating from scratch looks
uh as showing
below you see here we have our input
layer uh
they have a listed Max pulling uh so you
have as you're coming in with the input
layer and this the input layer is
actually um before this but the first
layer that it's going to go into is
going to be a convolutional neural
network uh then you have a Max pooling
that pulls those the the convolutional
neural network returns uh in this case
they have two of those that is very
standard with convolutional neural
networks uh one of the ones that I was
looking at earlier that was standard
being being used by um I want one of the
larger companies I can't remember which
one for doing a large amount of
identification had two convolutional
neural networks each with their Max
pooling and then about 17 dense layers
after it we're not going to do that
heavy duty of a of a code but we'll get
you head in the right direction and that
gives you an idea of what you're
actually going to be looking at when you
look at the flattened part and then the
dense we're talking like 17 dense layers
afterwards uh I find that a lot of the
stuff I've been working on I end up
maxing it out right around nine dense
layers it really depends on what you
have going in and what you're working
with and the vgg16
model uh vgg16 is a pre-trained CNN
model which is used for image
classification it is trained on a large
varied data set and fine-tune to fit
image classification data sets with
ease and you can see down here we have
the input coming in uh the convolutional
neural network 1: one 1:2 and then
pooling and then we do 2: one 2:2
convolutional neural network then
pooling 3 to2 and you can see there's
just this huge layering of convolutional
neural networks and in this case they
have five such layers going in and then
you three dents going out or uh more now
when they took this setup this actually
won an award uh back in 2019 for this
particular
setup uh and it does it does really good
except that again um we only show the
three dense layers here and if you find
out depending on your data going in and
what you have set up uh that really
isn't enough on one of these setups and
I'm going to show you why we restricted
it because it does take up a lot of
processing power in some of these things
so let's go ahead and roll up our
sleeves and we're going to look at both
the setups we're going to start with the
um uh the first
classification um and then we'll go into
the vgg 16 and show you how that's set
up now I'm going to be using anaconda
and let me flip over to my anaconda so
you can see what that looks like now I'm
running in the Anaconda here uh you'll
see that I've set up a main python uh 38
I always put that in there this is where
I'm doing like most of my kind of
playing around uh this is done in Python
version 3.8 we're not going to dig too
much into versions uh at this point you
should already have carass installed on
there usually carass takes a number of
extra
steps and then our usual um uh setup is
the numpy the pandas uh your SK your s
kit which is going to be the SK learn
your caborn and I'll I'll show you those
in just a minute um and then I'm just
going to be in the Jupiter lab where
I've created a new um notebook in here
and let's flip on over there to my blank
notebook now I'm there's a couple couple
cool things to note in here is that um
one I used the the um Anaconda Jupiter
notebook setup because it keeps
everything separate uh except for carass
uh carass is actually running separately
in the back I believe it's a a c program
uh what's nice about that is that it
utilizes the multi-processors on the
computer and I'll mention that just in a
little bit when we actually get down to
running the
code and when we look in here here uh a
couple things to note is here's our uh
um oops I thought I grabbed the other
drawing thing uh but here's our numpy
and our pandas right here and our
operating system this is our s kit you
always import it is sklearn for the
classification report uh we're going to
be using well usually import like
Seaborn brings in all of your pip plot
Library
also kind of nice to throw that in there
I can't remember if we're actually using
C born if they just the people in the
back just threw that together um and
then we have the sklearn shuffle for
shuffling data here's our map plot
library that the Seaborn is pretty much
built on um CV2 if you're not familiar
with that that is our image um module
for importing the image and then of
course we have our ttin or flow down
here which is what we're really working
with and then the last thing is just for
visual effect while we're running this
um if you're doing a demo and you're
working with the partners or the
shareholders uh this tqdm is really kind
of cool it's an extensible progress bar
for Python and I I'll show you that too
remember data science is not I mean you
know most of this code when I'm looking
through this code I'm not going to show
half of this stuff to the shareholders
or anybody I'm working with they don't
really care about pandas and all that we
do because we want to understand how it
works uh so we need to go ahead and
import those different um uh setup on
there and then the next thing is we're
going to go ahead and set up our
classes uh now we remember if we had
Mountain streak Glacier building sea and
Forest those were the different images
that we have coming
in and we're going to go ahead and just
do class name labels and we're going to
kind of match that class name of I for I
class name uh equals the class names so
our labels are going to match the names
up here
uh and then we have the number of uh
classes and print the class names and
the labels and we'll go ahead and set
the image size this is important that we
resize everything because if you
remember with neural
networks they take one siiz data coming
in and so when you're working with
images you really want to make sure
they're all resized to the same uh setup
it might squish them it might stretch
them that generally does not cause a
problem in these uh and some of the
other tricks you can do with if you if
you need more data um and this is one
that's used regularly we're not going to
do it in here is you can also take these
images and not only resize them but you
can tilt them one way or the other crop
parts of them um so they process
slightly differently and it'll actually
increase your accuracy of some of these
predictions uh and so you can see here
we have Mountain equals zero that's what
this class name label is Street equals 1
Glacier equals 2 buildings equals 3 C4
Forest equals
5 now we did this as an enumerator so
each one is 0 through five uh a lot of
times we do this instead as um uh uh 0
one 0 1 01 you have five outputs and
each one's a zero or a one coming out so
the next thing we really want to do is
we want to go ahead and load the data up
and just put a label in there loading
data just just so you know what we're
doing we going to put in the loading
data down here uh make sure it's well
labeled uh and we'll create a definition
for
this and this is all part of your
preprocessing at this point you could
replace this with all kinds of different
things depending on what you're working
on and if you once you download you can
go download this data set uh send a note
to the simply learn team here in YouTube
um and they'll be happy to direct you in
the right direction and make sure you
get this path here um so you have the
right whatever wherever you save it a
lot of times I'll just abbreviate the
path or put it as a sub thing and just
get rid of the directory um but again
double check your paths um we're going
to separate this into a segment for
training and a segment for testing and
that's actually how it is in the folder
let me just show you what that looks
like so when I have my uh lengthy path
here where I keep all my programming
simply learn this particular setup we're
working on image classification and
image classification clearly you
probably wouldn't have that lengthy of a
list and when we go in here uh you'll
see sequence train sequence test they've
already split this up this is what we're
going to train the data in and again you
can see buildings Forest Glacier
Mountain Sea Street uh and if we double
click let's go under Forest you can see
all these different Forest uh images and
and there's a lot of variety here I mean
we have wintertime we have summer time
um so it's kind of interesting you know
here's like a Fallen Tree versus
um a road going down the middle that's
really hard to train and if you look at
the
buildings A lot of these buildings
you're looking up a skyscraper we're
looking down the
setup here's some trees with one I want
to highlight this one it has trees in it
uh let me just open that up so you can
see it a little
closer the reason I want to highlight
this is I want you to think about this
we have trees growing is this the city
or a
forest um so this kind of imagery makes
it really hard for a classifier and if
you start looking at these you'll see a
lot of these images do have trees and
other things in the foreground weird
angles really a hard thing for a
computer to sort out and figure out
whether it's going to be a forest or a
um
city and so in our loading of data uh
one we have to have the path of
directory we're going to come in here we
have our images and our labels so we're
going to load the images in one section
the labels in another
um and if you look through here it just
goes through the different folders uh in
fact let me do this let
me there we go uh as we look at this
we're just going to Loop through the
three the six different folders that
have the different Landscapes and then
we're going to go through and pull each
file
out and each label uh so we set the
label we set the folder for file and
list uh here's our image path join the
paths this is all kind of General stuff
um so I'm kind of skipping through it
really
quick and here's our image setup uh if
you remember we're talking about the
images we have our CV2 reader so it
reads the the image in uh it's going to
go ahead and take the image and convert
it to uh from blue green red to red
green green blue this is is a CV2 thing
um almost all the time it Imports it and
instead of importing it as a standard
that's used just about everywhere it
Imports it with the BGR versus RGB um
RGB is pretty much a standard in here
you have to remember that was CV2 uh and
then we're going to go ahead and resize
it this is the important part right here
we've set a we've decided what the size
is and we want to make sure all the
images have the same size on
them and then we just take our images
and we're just going to impend the image
pin the label um and then the images
it's going to turn into a numpy array
this just makes it easier to process and
manipulate and then the labels is also a
numpy array and then we just return the
output pend images and labels and we
return the output down
here so we've loaded these all into
memory uh we haven't talked to much
there'd be a different setup in there
because there is ways to feed the files
directly into your cross model uh but we
want to go ahead and just load them all
it's
really for today's processing and that
what our computers can handle that's not
a big deal and then we go ahead and set
the uh train images train labels test
images test labels and that's going to
be returned in our output app pinned and
you can see here we did um uh images and
labels set up in there and it just loads
them in there so we'll have these four
different categories let me just go
ahead and run
that
uh so now we've gone ahead and loaded
everything on
there and then if you remember from
before uh we imported we just go back up
there Shuffle here we go here's our
sklearn utilities import Shuffle and so
we want to take these labels and shuffle
them around a little bit um just mix
them up so it's not having the same if
you run the same process over and over
uh then you might run into some problems
on
there and just real quick let's go ahead
and do uh um a plot so we just you know
we've looked at them as far as from
outside of our code we pulled up the
files and I showed you what that was
going on we can go and just display them
here too and tell you when you're
working with different
people this should be highlighted right
here um this thing is like when I'm
working on code and I'm looking at this
data and I'm trying to figure out what
I'm doing I skip this process the second
I get into a meeting and I'm showing
what's going on to other people I skip
everything we just did so and go right
to here where we want to go ahead and
display some images and take a look at
it and in this display um I've taken
them and I've resized the images to 20
by 20
that's pretty small uh so we're going to
lose just a massive amount of detail and
you can see here these nice pixelated
images um I might even just stick with
the folder showing them what images
we're
processing uh again this is you got to
be a little careful this maybe resizing
it was a bad idea um in fact let me try
it without resizing it and see what
happens oops so I took out the image
size and then we put this straight in
here one of the things again this is
um put the D there we go one of the
things again that we want to
know whenever we're working on these
things uh is the
CV2 there are so many different uh image
classification setups it's really a
powerful package when you're doing
images but you do need to switch it
around so that it works with the pi plot
and so make sure you take your numpy
array and change it to a u integer a
format uh because it comes in as a float
otherwise you'll get some weird images
down there um and so this is just
basically we split up our we've created
a plot we went ahead and did the plot 20
by 20 um or the plot figure size is 20
by 20 um and then we're doing 25 so a
5x5 subplot um nothing really going on
here too exciting but you can see here
where we get the images and really when
you're showing people what's going on
this is what they want to see uh so you
skip over all the code and you have your
meeting you say okay here's our images
of the
building um don't get caught up in how
much work you do get caught up in what
they want to see so if you want to work
in data science that's really important
to
know and this is where we're going to
start uh having fun uh here's our model
this is where it gets exciting when
you're digging into these models and you
have here here uh let me
get there we
go when you have here if you look here
here's our convolutional neural network
uh
2D and uh 2D is an image you have two
different dimensions x y and even though
there's three colors is still considered
2D if you're running a video you'd be
convolutional neural network 3D if
you're doing a series going across um a
Time series it might be
1D and on these you need to go ahead and
have your convolutional n network if you
look here there's a lot of really cool
settings going on to dig into um we have
our input shape so everything's been set
to 150 by 150 uh and it has of course
three different color schemes in it
that's important to notice um
activation default is Ru uh this is
small amounts of data being processed on
a bunch of little um neural
networks and right here is the 32 that's
how many of these convolutional Nal
networks are being strung up on here and
then the
3X3 uh when it's doing its steps it's
actually looking at uh a little 3x3
Square on each image and so that's
what's going on here and with
convolutional noral networks the window
floats across and adds up all these
numbers going across on this data and
then eventually it comes up with 30 in
this case 32 different feature options
uh that it's looking for and of course
you can change that 32 you can change
the 3X3 so you might have a larger setup
and you know if you're going across
150 um by 150 that's a lot of steps so
we might run this as 15 by 15 uh there's
all kinds of different things you can do
here we're just putting this together
again that would be something you would
play with to find out which ones are
going to work better on this setup um
and there's a lot of play
involved that's really where it becomes
an art form is guessing at what that's
going to be the second part I mentioned
earlier and I I can only begin to
highlight this um when you get to these
dense layers one is the activation is a
railu they use a railu and a softmax
here um it's a whole whole uh setup just
explaining why these are different um
and how they're different because
there's also an exponential there's a
tangent in fact uh there's just a ton of
these and you can build your own custom
activations depending on what you're
doing a lot of different things go into
these activations uh there are two or
three major thoughts on these
activations and Ru and softmax are uh U
well Ru uh you're really looking at just
the number you're adding all the numbers
together and you're looking at ukian
geometry um ax Plus
bx2 plus
cx3 plus a bias with softmax this
belongs to the party of um it's
activated or it's not except it's they
call it softmax because when you get to
the to zero instead of it just being
zero uh it's actually slightly a little
bit less than zero so that when it
trains it doesn't get lost um there's a
whole series of these activations
another activation is the
tangent um where it just drops off and
you have like a very narrow area where
you have from minus one: one or
exponential which is 0 to one so there's
a lot of different ways to do the
activation again we can do that'd be a
whole separate lesson on here we're
looking at the convolutional neural
network um and we're doing the two pools
this is so common you'll see two two
convolutional n networks stacked on top
of each other each with with its own Max
pull underneath and let's go ahead and
run that so we built our model there and
then we need to go ahead and
um compile the model so let's go ahead
and do
that uh we are going to use the atom uh
Optimizer the bigger the data the atom
fits better on there there's some other
Optimizer but I think atom is a default
um I don't really play with the
optimizer too much that's like the if
once you get a model that works really
good you might try some different
optimizers uh but atom's usually the
most and then we're looking at a
loss pretty standard we want to minimize
our LW we want
to maximize the loss of error and then
we're going to look at accuracy um
everybody likes the accuracy I'm going
to tell you right
now I start talking to people and like
okay what's what's the loss on this and
that and as a data science yeah I want
to know how the law what what's going on
with that and we'll show you why in a
minute
but everybody wants to see accuracy they
want to know how accurate this is uh and
then we're going to run the fit and I
wanted to do this just so I can show you
even though we're in a python setup in
here where jupyter notebook is using
only a single processor I'm going to
bring over my little CPU Tool uh this is
eight cores on 16 dedicated threads so
it shows up as 16
processors and actually I got to run
this and then move it over so we're
going to run this and hopefully it
doesn't destroy my
mic uh and as it comes in you can see
it's starting to do go through the epics
we said I set it for five epics and then
this is really nice because carass uses
all the different uh threads available
so it does a really good job of doing
that uh this is going to take a while if
you look at here it's uh uh ETA 2
minutes and 25 seconds 24 seconds so
this is roughly 2 and 1/2 minutes per
epic uh and we're doing five epics so
this is going to be done in roughly 15
minutes I don't know about you but I
don't think you want to sit here for 15
minutes watching The Green bars go
across so we'll go ahead and let that
run and there we go uh there was our 15
minutes it's actually less than that uh
because I did when I went in here
realized that uh uh where was it
here we go here's our model compile
here's our model FL uh fit and here's
our epics uh so I did four epics so a
little bit better more like 10 to 11
minutes instead of uh uh doing the full
uh 15 and when we look at this here's
our model we did talked about the
compiler uh here's our history we're
going to um history equals the model fit
we'll go into that in just a
minute and we're looking at that is we
have our epics um here's our validation
split so as we train it uh we're
weighing the accuracy versus you kind of
pull some data off to the side uh while
you're training it and the reason we do
that is that um you don't want to
overfit and we'll look at that chart in
just a
minute uh here's batch
size this is just how many images you're
sending through at a time the larger the
bat it actually increases the processing
speed um and there's reasons to go up or
down on the batch size because of the um
the the smaller of the batch there's a
certain point where um you get too large
of a batch and it's trying to fit
everything at once uh so I 128 is kind
of big U depends on the computer you're
on what it can handle and then of course
we have our train images and our train
labels going in telling it what we're
going to train on
and then we look at our four epics here
uh here's our accuracy we want the
accuracy to go up and we get all the way
up to 83 or
83% now this is actual percentage based
pretty much and we can see over here our
loss we want our loss to go down really
fluctuates uh 55 1.2
7748 uh so we have a lot of things going
on on there let's go ahead and graph
those turn that off and our our team in
the back did a wonderful job of putting
together um this basic plot setup um
here's our subplot coming in we're going
to be looking at um uh from the history
we're going to send it the accuracy and
the value
accuracy labels and setup on there um
and we're going to also look at loss and
value Lo so you can see what this looks
like what's really interesting about
this setup and let me let me just go
ahead and show you cuz uh without
actually seeing the plots it doesn't
make a whole lot of sense uh it's just
basic plotting of uh of the data using
the PIP plot library and I want you to
look at this this is really interesting
uh when I ran this the first time I had
very different
results um and they they vary greatly
and you can see here our accurac see
continues to
climb um and there's a crossover
here put it in here right here's our
crossover and I point that out because
as we get to the right of that crossover
where our
accuracy U and we're like oh yeah I got
8% we're starting to get an overfit here
that's what this this switch over means
um as our value U as a training set
versus a value um accuracy stays the
same and so that this is the one we're
actually really want to be aware of and
where it
crosses is kind of where you want to
stop at um and we can see that also with
the train loss versus the value loss
right here we did one Epic and look how
it just flatlines right there with our
loss so really one Epic is probably
enough and you're going to say wow okay
8%
um certainly if I was working with the
shareholders um telling them that it has
an 80% accuracy isn't quite true and and
we'll look at that a little deeper it
really comes out here that the accuracy
of our actual values is closer to0 41%
right here um even after running it this
number of times and so you really want
to stop right here at that crossover one
Epic would have been enough um so the
data is a little overfitted on this when
we do four
epics and uh oops there we are
okay my drawing won't go away um let me
see if I can get there we
go uh for some reason I've killed my
drawing ability on my
recorder all right took a couple extra
clicks uh so let's go ahead and take a
look at our actual test loss um so you
see where a cross is over that's where
I'm looking at that's where we start
overfitting the model and this is where
if uh we were going to go back and
continually upgrade the model we would
start taking a look at the images and
start rotating them uh we might start
playing with the convolutional neural
network instead of doing the 3X3 window
um we might expand that or you know find
different things that might make a big
difference as far as the way it
processes these things um so let's go
ahead take a click at our uh our test
loss now remember we had our training
data now we're going to look at our test
images and our test
labels for our test loss here and this
is just model evaluate uh just like we
did fit up here where was it um one more
model fit with our trending data going
in now we're going to evaluate it on the
and and this data has not been touched
yet so this model's never seen this data
this is on uh completely new information
as far as as the model is concerned of
course we already know what it is from
the labels we
have and this is what I was talking
about here's the actual accuracy right
here 0
48 uh or 4847 so this 49% of the Time
guesses what the image
is uh and I mean really that's a bottom
dollar uh does this work for what you're
needing does 49% work do we need to
upgrade the model more um in some cases
this might be uh oh what was I doing I
was working on uh stock
evaluations and we were looking at what
stocks were the top
performers well if I get that 50%
correct on top
performers uh I'm good with that um
that's actually pretty good for stock
evaluation in fact the number I had for
stock was more like U um 30 something
per as far as being a top performer
stock much harder to predict um but at
that point you're like well I'm you'll
make money off of that so again this
number right here depends a lot on the
domain you're working
on and then we want to go ahead and
bring this home a little bit more uh as
far as looking at the different setup in
here and one of the uh from SK learn if
you remember actually let's go back to
the top uh we had the classification
report and this came in from our sklearn
or S kit setup and that's right here you
can see it right here on the
um see there we go uh classification
report right here uh sklearn metrics
import classification report that's what
we're going to look at
next a lot of this stuff uh depends on
who you're working with so when we start
looking at um precision
you know this is like for each value I
can't remember what 111 was probably
mountains so if 44% is not good enough
if if you're doing like um you're in the
medical department and you're doing
cancer is it is this cancerous or not
and I'm only 44% accurate not a good
deal you know I would not go with that
um so it depends on what you're working
with on the different labels and what
they're used for Facebook you know 44%
I'm guessing the right person
I would hope it does a little bit better
than that um but here's our main
accuracy this is what almost everybody
looks at they say oh 48% that's what's
important um again it depends on what
domain you're in and what you're working
with and now we're going to do the same
model somehow I got my there it goes I
thought I was going to get stuck in
there again uh this time we're going to
be using the vgg16
and remember this one has uh all those
layers going into it so it's basically a
bunch of convolutional n networks
getting smaller and smaller on here uh
and so we need to go ahead and um import
all our different stuff from carass uh
we're importing the main one is the V
g16 setup on there just aim that there
we go
um there's kind of a pre-processing
images um applications pre-process input
this is all part of the VG g16 setup on
there uh and once we have all those we
need to go ahead and create our
model and we're just going to create a
vgg16 model in here um inputs model
inputs outputs model inputs I'm not
going to spend as much time as I did on
the other one uh we're going to go
through it really quickly one of the
first things I would do is if you
remember in carass you can trate treat a
model like you would a
layer um and so at this point I would
probably add a lot of dense layers on
after the VG g16 model and create a new
model with all those things in there and
we'll go ahead and uh run this uh cuz
here's our model coming in and our setup
and it'll take it just a moment to
compile that what's funny about this is
I'm I'm waiting for it to download the
um package since I pre- ran this um it
takes it a couple minutes to downlo
download the vgg16 model into here um
and so we want to go ahead and train
features for the model we're going to
proct that we're going to predict the
train images and we're going to test
features on the predict test images on
here and then I told you I was going to
create another model too and the people
in the back uh did not disappoint me
they went ahead and did just that and
this is really an important part um this
is worth stopping for I told you I was
going to go through this really quick so
here's our
uh we we have our model
two um coming in and we we've created a
model up here with the vgg16 model
equals model inputs model inputs and so
we have our
vgg16 this has already been
pre-programmed uh and then we come down
here I want you to notice on this um
right here layer model two layers minus
4 to One X layer X
um we're basically taking this model and
we're adding stuff onto it and so uh
we've taken we've just basically
duplicated this model we could have done
the same thing by using model up here as
a layer um we could have had the input
go to this model and then have that go
down here so we've added on this whole
setup this whole block of code from 13
to 17 has been added on to our vgg16
model and we have a new model uh with
the layer input and X down here let's go
ahead and run that and compile it and
that was a lot to go through right there
uh when you're building these models
this is the part that get so
complicated that you get stuck playing
in and yet it's so fun uh it's like a
puzzle how can I Loop these models
together and in this case you can see
right here that the layers uh we're just
copying layers over and adding each
layer in um this is one way to build a
new model and we'll go ahead and run
that like I said the other way is you
can actually use the model as a layer I
had a little trouble playing with it uh
sometimes when you're using the Straight
model
over you run into issues
um it seems like it's going to work and
then you mess up on uh the input and the
output layers there's all kinds of
things that come
up let's go ahead and do the new model
we're compile it uh again here's our
metrics accuracy sparse categorical loss
uh pretty straightforward just like we
did before you got to compile the
model and just like before we're going
to take our create a history uh the
history is going to be uh new model fit
train
128 and just like before if you remember
when we started running this stuff we're
going to have to go ahead and it's going
to light up our uh setup on here and
this is going to take a little bit to
get us all up uh it's not going to just
happen in in a couple minutes so let me
go ahe and pause the video and run it
and then we'll talk about what
happened okay now when I ran that these
actually took about six minutes each um
so it's a good thing I put it on ho we
did four epics uh actually had to stop
ex say at 10 and switch it to four
because I didn't want to wait an
hour and you can see here our
accuracy um and our loss numberers going
down and just a
glance it actually performed if you look
at the
accuracy.
2658 um so our accuracy is going down or
you know
26% um 34%
35% and you can see here at some point
it just kind of kicks a bucket again
this is
overfitting that's always an issue when
you're running on uh programming these
different neural networks
and then we're going to go ahead and
plot the accuracy um history we built
that nice little sub routine up above so
we might as well use it and you can see
it right
here um there's that crossover
again and if you look at this look how
the how the um uh the red shifts up how
the uh our loss functions and everything
crosses over we're over fitting after
one Epic um we were
clearly not helping the problem or doing
better um we're just going to it's just
going to Baseline this one actually
shows what the training versus the
loss um value loss maybe second epic so
on here we're now talking more between
the first and the SE second epic and and
that also shows kind of here so
somewhere in here it starts
overfitting and right about now you
should be saying uhoh uh something went
wrong there I thought that um when we
went up here and ran this look at this
we have the accuracy up here is hitting
that
48% and we're down here
um you look at the score down here that
looks closer to 20% not nearly anywhere
in the ballpark of what we're looking
for and we'll go ahead and run it
through the uh the actual test features
here and and there it is um we actually
run this on the Unseen data and
everything point
uh8 or
18% um I don't know about you but I
wouldn't want you know at 18% this did a
lot worse than the other one I thought
this is supposed to be the super model
the model that beats all models the
vgg16 that won the awards and everything
well the reason is is that um one we're
not pre-processing the data uh so it
needs to be more there needs to be more
um as far as like rotating the data at
you know 45 angle taking partials of it
so you can create a lot more data to go
through here um and that would actually
greatly change the outcome on here and
then we went up here we only added a
couple dense layers uh we added a couple
convolutional neural
networks this huge pre-trained setup is
looking for a lot more information
coming in as far as how it's going to
train and so uh this is one of those
things where I thought it would have
done better and I had to go back and
research it and look at it and say why
didn't this work why am I getting only
uh 18% here instead of 44% or better and
that would be wise it doesn't have
enough training data coming in uh and
again you can make your own training
data so it's not that we have a shortage
of data it's that that some of that has
to be switched around and moved around a
little bit and this is interesting right
here too if you look at the
Precision we're getting it on number two
and yet we have zero on everything else
so for some reason is not
seeing uh the different variables in
here so it'd be something else to look
in and try to find track down um and
that probably has to do with the input
but you can see right here we have a
really good solid 0 48 up here uh and
that's where I'd really go with is
starting with this model and then we
look at this model and find out why are
these numbers not coming up better is it
the data coming in um where's the setup
on there and that is the art of data
science right there is finding out which
models work better and why so if you
want to seek expertise in deep learning
skills and aspir to be a part of Cutting
Edge field of building chat Bots and
service Bots using deep learning our
calch postgraduate program in Ai and
machine learning provides comprehensive
coverage of the latest tools and
Technologies within the AI ecosystem it
offers master classes led by esteemed
Caltech faculty and IBM experts engaging
hackathons and interactive ask me
anything sessions by enrolling in this
program Del delivered in partnership
with IBM you will gain the expertise
needed to become a professional in AI ml
this comprehensive artificial
intelligence course will equip you with
the knowledge about various AI based
Technologies empowering you to excel in
this field so do check out the course
Link in the description for more details
let's get started
now now let's talk about our first deep
learning project we have image
classification using the cifar 10 data
set now this data set was created by the
Canadian Institute for advanced research
the cifar 10 data set contains 6,32
cross 32 color images in 10 different
classes the 10 different classes are
airplanes cars Birds cats deer and
others which you can see on the screen
there are 6,000 images of each class now
there are 50,000 training images and
10,000 test images you can build a
convolutional neural network model using
the Caris library to classify each image
into a category so is more of an image
recognition kind of a project that is
recommended for beginners who are new to
deep learning the next project in our
list is brain tumor detection such
projects are heavily used in the
healthcare Industries for detecting
diseases before they occur brain tumors
are the consequence of abnormal growths
and uncontrolled cell division in the
brain they can lead to death if they are
not detected early and accurately brain
tumors can be classified into two types
benign and
malignant deep learning can help
Radiology ol IST in tumor Diagnostics
without invasive measures a deep
learning algorithm that has achieved
significant results in image
segmentation and classification is the
convolutional neural network it can be
used to detect a tumor through brain
using magnetic resonance imaging or M
image pixels are First Fed as input to
the CNN soft Mac's fully connected
layers are used to classify the images
the accuracy of the convolutional neural
network can be obtained with the radial
basis function classifier next we have
Anna chatbot now this chatbot is the
world's first open source conversation
platform which comes with a graphical
chart flow designer and chat simulator
it's supported channels of web chat
Android iOS and Facebook Messenger the
Anna chat board is available to answer
your questions 24 hours a day 365 days a
year Anna is free for personal and
commercial use with the Ana Studio
server simulator and SDK which is
software development kit your
development time is cut from from days
to hours using Anna you can create chat
Bots to suit your needs chat Bots play
an important role in customer support
for an e-commerce website you can seeare
realtime order and sipping updates
personalized product recommendations and
targeted offers within the conversation
similarly automobile Brands showrooms
and service centers can use a chatbot
for lead generation scheduling test
drives and roadside assistance you can
check the GitHub repository that's on
the screen to know more about working on
this project the next project we have in
deep learning is image captioning now
image captioning is the process of
generating textual description of an
image it uses both methods from computer
vision to understand the content of the
image and a language model from the
field of natural language processing to
turn the understanding of the image into
words in the right order Microsoft has
built its own caption bot where you can
upload an image or the URL of an image
and it will display the text ual
description of the image on the screen
you can see we have the picture of Alon
musk and the caption boot has generated
a description that says I think it's
Alon musk wearing a shoot and a tie and
he seems it ends with an
emoticon another such application that
suggests a perfect caption and best
hashtags for a picture is caption AI
automatic image caption generation
software can be built using recurr
neural networks and long shortterm
memory networks or
lstms now we have image colorization as
our next project image colorization has
seen significant advancements using deep
learning image colorization is the
process of taking an input of a
grayscale image and then producing an
output of a colored image or a colorized
image colorization is a highly
undetermined problem that requires
mapping a real valued luminance image to
a three-dimensional colored value one
that has not a unique solution chroman
is an example of a picture colorization
model a generative network is framed in
an adversarial model that learns to
colorize by incorporating perceptual and
semantic understanding of color and
class distributions the model is trained
using a fully supervised strategy if you
want to implement chromag Gan check the
GitHub link that's on the screen below
next in our list of projects we have
open nmt machine translation open nmt is
an open- Source ecosystem for neural
machine translation and neural sequence
learning it started in December 2016 by
the Harvard NLP group an cist Tran the
project has since been used in several
research and Industry applications
neural machine translation or nmt is a
new methodology for machine translation
that has led to significant improvements
particularly in terms of human
evaluation compared to rule-based and
statistical machine translation systems
open and empty provides implementations
in two popular deep learning Frameworks
P torch and tensorflow please feel free
to refer to the below GitHub link to
learn more about neural machine
translation now we have music generation
using deep learning it is possible for a
machine to learn the notes structures
and patterns of Music and start
producing music automatically music 21
is a python toolkit used for computer
readed musicology it allows us to teach
the fundamentals of Music Theory
generate music examples and study music
the toolkit provides a simple interface
to acquire the music notation of mid
files which stands for musical
instrument digital interface using webet
architecture and long shortterm memory
networks you can generate music without
human intervention Amper music mubert
and Juke deck produce smart music
powered by Deep learning
algorithms then we have Alpha go now
Alpha go was created by Google Deep Mind
and is the first computer program to
defeat a professional human go player
Alpha go algorithm uses a combination of
machine learning and tree search
techniques combined with extensive
training both from human and computer
play it uses Monte Carlo tree search
Guided by a value Network and a policy
Network both implemented using deep
neural network technology alphao was
initially trained to mimic human play by
attempting to match the moves of expert
players from recorded historical games
it was done using a database of around
30 million moves once it had reached a
certain degree of proficiency by being
set to play large number of games
against other instances of itself using
reinforcement learning to improve its
play next in our list of projects we
have deep dream deep dream is a computer
vision program which uses a
convolutional neural network to find and
enhance patterns in images via
algorithmic paradia it then creates a
dream like helicogenic
appearance in the overprocessed images
deep dream is an experiment that
visualizes the patterns learned by a
neural network
similar to when a child watches clouds
and interprets and tries to interpret
random shapes deep dream over interprets
and enhances the patterns it sees in an
image do check the given GitHub link to
install dependencies listed in the
notebook and play with the code locally
then we have deep voice deep voice was
developed by BYU which is an AI system
that can clone an individual's voice the
train model takes just 3 seconds to
replicate the output of a person's voice
deep voice 3 is a fully convolutional
attention based neural text to speech
system that converts text to
spectrograms or other acostic parameters
to be used with an audio waveform
synthesis method below you can find the
GitHub link for pytorch implementation
of convolutional networks based text to
speech synthesis
models now we have IBM Watson IBM Watson
helps run machine learning models
anywhere across any Cloud using IBM
Watson machine learning you can build
analy iCal models and neural networks
trained with your own data that you can
deploy for use in
applications with its open extensible
model operation Watson machine learning
helps businesses simplify and harness AI
at scale across any Cloud it is used in
healthare teaching assistant chatbot as
well as for weather forecasting and
finally in our list of deep learning
projects we have the YOLO realtime
object detection you only look once or
YOLO is a state-of-the-art real-time
object detection system it frames object
detection in images as a regression
problem to spatially separated bounding
boxes and Associated class
probabilities using this approach a
single neural network divides the image
into regions and predicts bounding boxes
and probabilities for each region the
neural network predicts bounding boxes
and class probabilities directly from
full images in one evaluation the base
YOLO model processes images in real time
at 45 frames per second you can use the
Coco data set and tensorflow library to
train and test the model to learn more
about yolu object detection check the
GitHub link that's shown on the screen
below so if you want to seek expertise
in deep learning skills and aspire to be
a part of Cutting Edge field of building
chat Bots and service Bots using deep
learning our calch postgraduate program
in Ai and machine learning provides
comprehensive coverage of the latest
tools and Technologies within the AI
ecosystem it offers master classes led
by esteemed calch faculty and IBM
experts engaging hackathons and
interactive ask me anything sessions by
enrolling in this program delivered in
partnership with IBM you will gain the
expertise needed to become a
professional in Ai and ml this
comprehensive artificial intelligence
course will equip you with the knowledge
about various AI based Technologies
empowering you to excel in this field so
do check out the course Link in the
description for more details let's get
started
now deep learning interview questions
and we're going to go from the very
basics of neural networks and deep
learning into some of the more commonly
used models so you can have an
understanding of what kind of questions
are going to come up and what you need
to know in interview questions we'll
start with a very general concept of
what is deep learning this is where we
take large volumes of data in this case
on cats and dogs or whatever A lot of
times you use um a training setup to
train your model remember it's kind of
like a magic Black Box going on there
then we use that to extract features or
extract information and in this case
classify the image of a cat and a dog so
the primary takeaway when we're talking
about deep learning is it learns from
large volumes of structured and even
unstructured data and uses complex
algorithms to train neural network it
also performs complex operations to
extract hidden patterns and features and
if we're going to discuss deep learning
in this very uh simplified overview then
we also have to go over what is a neural
network this is a common image you'll
see of a drawing of a forward
propagation neural network and it's it's
a human brain inspired system which
replicate the way humans learn so this
is inspired how our own neurons in our
brain fire but at a much simplified
level obviously it's not ready to take
over the human uh population and and be
our leader yet not for many years it's
very much in its infant stage but it's
inspired by how our brains work um and
they use a lot of other Inspirations you
can study brains of moths and other
animals that they've used to figure out
how to improve these neural networks the
most common one consists of three layers
of network and this is generally how you
view these networks is you have an input
you have a hidden layer and an output
and the neural network is uh broken up
into many pieces but when we focus just
on the neural network it's always on the
hidden layers that we're making all the
adjustments and figuring out how to best
set up those hidden layers for their
functions to both train faster and to
function better when we look at this of
course we have our input hidden and
output each layer contains neurons
called as nodes perform various
operations and you can see here we have
the list of the nodes we have both our
input nodes and our output nodes and
then our hidden layer nodes and it's
used in deep learning algorithm like CNN
RNN GN Etc we'll address some of these
models a little closer at least the most
common models as we go down the list and
we study the Deep learning and the
neural network framework let's start
with what is a multi-layer patron or MLP
a lot of time is a referred to and
you'll see these abbreviations I'll be
honest I have to write them down on a
piece of paper and go through them cuz
never remember what they all mean even
though I play with them all the time
what is a multi-layer patron well if you
look at the image on the right it's very
similar to what we just looked at you
have your input layer your hidden layer
and your output layer and that's exactly
what this is it has the same structure
of a single layer Perron with one or
more hidden layers except the input
layer each node in the other layers uses
a nonlinear activation function what
that means is your input layers your
data coming in and then your activation
function is based upon all those nodes
and weights being added together and
then it has the output MLP uses
supervised learning method called back
propagation for training the model very
key word there is back propagation
single layer Perron can classify only
linear separable classes with binary
output 01 but the MLP can classify
nonlinear classes so let's break this
down just a little bit the multi-layer
patron with an input layer and a hidden
layer and an output layer as you see
that it comes in there it has adds up
all the numbers and weight
depending on how your setup is that then
goes to the next layer that then goes to
the next hidden layer if you have
multiple hidden layers and finally to
the output layer the back propagation
takes the error that it sees so whatever
the output is it says hey this has an
error to it it's wrong and then sends
that error backwards from where it came
from and there's a lot of different
functions used to uh train this based on
that error and how that error goes
backwards in the notes uh so forward is
you get your answers backward is for
training you see this every day even my
uh uh Google pixel phone has this it
they train the neural network which
takes a lot more data to train than it
does to use and then they load up that
neural network into in this case I have
a pixel 2 which actually has a built-in
neural network for processing pictures
and so it's just the forward propagation
I use when it processes my photos but
when they were training it you used a
back propagation to train it with the
errors they had we'll be coming back to
different models that are used for right
now though multi-layer percept Patron
MLP put that down as your vocabulary
word and of course back propagation what
is data normalization and why do we need
it this is so important we spend so much
time in normalizing our data and getting
our data clean and setting it up uh so
we talk about data there's a
pre-processing step to standardize the
data so whatever we have coming in we
don't want it to be a uh you know 1
Gigabyte file here a 2 GB picture here
and a 3 kiloby text there even as a
human and I can't process those all in
the same group I have to reformat them
in some way that Loops them together so
a standardized format we use this uh
data normalization in pre-processing to
reduce and eliminate data redundancy a
lot of times the data comes in and you
end up with two of the same images or um
uh the same information in different
formats then we want to rescale values
to fit into a particular range for
achieving better convergence what this
means is with most neural networks they
form a bias we've seen this in recently
in attacks on neural networks where they
light up one pixel or one piece of the
view and it skews the whole answer so
suddenly um because one pixel is really
bright uh it doesn't know what to do
well when we start rescaling it we put
all the values between say minus one and
one and we change them and refit them to
those values it helps get rid of that
bias helps fix for some of those
problems and then finally we restructure
the data and improve the Integrity we
want to make sure that we're not missing
values um or we don't have partial data
coming in one way to look at this is uh
bad data in bad data out so we want
clean data in and you want good answers
coming out one of the most basic models
used is a bolts machine so let's address
what is a boltzman machine and if you
know we just did the MLP multi-layer
prron so now we're going to come into
almost a simplified version of that and
in this we have our visible input layer
and we have our hidden layer the Bol
spin machines are almost always shallow
they're usually just two layer neural
Nets that make stochastic decisions
whether a neuron should be on or off
true false yes no first layer is the
visible layer and second layer is the
hidden layer nodes are connected to each
other across layers but no two nodes of
the same layer are connected hence it is
also known as restricted boltzman
machine now that we've covered a basic
MLP or multi-layer Perron and we've gone
over the boltzman machine also known as
the restricted boltzman machine let's
talk a little bit about activation
formulas and this is a huge topic that
can get really complicated but it also
is automated so it's very simple so you
have both a complicated and a simple at
the same time so what is the role of
activation functions in a neural network
activation function decides whether a
neuron should be fired or not that's the
most basic one and that actually changes
a little bit because it's either whether
fired or not in this case activation
function or what value should come out
when it's fired but in these models
we're looking at just the boltzman
restricted layers so this is what causes
them to fire either they don't or they
do it's a yes or no true false All or
Nothing it accepts a weighted sum of the
inputs the bias as input to any
activation function so whatever our
activation function is it needs to have
the sum of the weights times the input
so each input if you remember on that
model and let's just go back to that
model real quick and then you always
have to add a bias and you can look at
the bias if you remember from your ukian
geometry you draw a straight line
formula for that line has a y-coordinate
at the end it's always um CX plus M or
something like that where m is where it
crosses the Y coordinates if you're
doing a straight line with these weights
it's very similar but a lot of times we
just add it in as its own weight we take
it as a node of a one value coming in
and then we compute its new weight and
that's how we compute that bias just
like we compute all the other weights
coming in the node which gets fires
depends on the Y value and then we have
a step function and the step function
this is where remember I said it's going
to get complicated and simple all at the
same time we have a lot of different
step functions we have the sigmoid
function we have just a standard step
function we have the um rayu it's
pronounced like Ray the ray of from the
Sun and Lou like a name so Ru function
and we have the tangent H function and
if you look at these they all have
something similar they all either force
it to be um one value or the other they
force it to be in the case of the first
three is zero or one and in the last
last one it's either a minus one or one
and you can easily convert that to a 0
one yes no true false and on this one of
the most common ones is the step
function itself because there is no
middle value there is no um uh
discrepancy that says well I'm not quite
sure but as you get into different
models probably the most commonly used
used to be the sigmoid was most commonly
used but I see the ru used more often
really depending on what you're doing
you just have to play with these and
find out which one works best depending
on the data in your output the reason to
have a non Z1 answer or something kind
of in the middle is when you're looking
at this and it's coming out you can
actually process that middle ground as
part of the answer into another neural
network so it might be that the ru
function says hey this is only a 6 not a
one and uh even though the one is what's
going into the next neural network or
the next hidden layer as an input the 6
value might also be going in there to
let you know hey this is not a straight
up one or straight up zero it's
someplace in the middle this is a little
uncertain what's coming out here so it's
a very powerful tool but in the basic
neural network you usually just use the
step function it's yes or no let's take
a um a big step back and take a kind of
an overview the next function is what is
a cost function that we're going to
cover this is so important because this
is your end result that you're going to
do over and over again and use to decide
whether the model is working or not
whether whether you need to try a
different step function whether you need
to try a different activation whether
you need to try a fully different model
used uh so what is the cost function
cost function is a measure to evaluate
how good your model's performance is it
is also referred as loss or error used
to compute the error of the output layer
during back propagation there's our back
propagation where we're training our
model that's one of our key words mean
squared error is an example of a popular
cost function and so here we have the
cost function C equals half of Yus y
predicted um and then you square that so
the first thing is um you know real
quick if you haven't done statistics
this is not a percentage it's not a
percentage of how accurate it is it's
just a measurement of the error and we
take that error for training it and we
push that error backwards through the
neural network and we use that through
the different training functions
depending on what model you're using to
train the neural network so when you
deploy the network you're usually done
training it because it takes a lot of
computational force to train it um this
is a very simple model and so you deploy
the trained one uh but we want to know
how your error is and so how do we do
that well you split your data part of
your data is for training and part of
your data is for testing and then we can
also test the error on there so it's
very important and then we're going to
go one more step on this we got to look
at both the local and the global setup
it might work great to test your data on
what you have on your computer but
that's different than in the field so
when we're talking about all these
different tests and the error test as
far as your loss you don't you want to
make sure that you're in a closed
environment when you do initial testing
but you also want to open that up and
make sure you follow up with the testing
on the larger scale of data because it
will change it might not fit the larger
scale there might be something in there
in the way you brought the data in
specifically or the data group you used
or um any of those could cause an error
so it's very important to remember that
we're looking at both the local and the
global context of our error and just one
other side note on a lot of the newer
models of neural networks by comparing
the error we get on the data our
training data with a portion of the test
data we can actually figure out how good
the model is whether it's overfitted or
not we'll go into that a little bit more
as we go into some of the different
models so we have our output we're able
to um figure out the error on it based
on the Square means usually although
there's other uh functions used so we
want to talk about what is gradient
descent another vocabulary word gradient
descent is an optimation algorithm to
minimize the cost function uh or to
minimize the error aim is to find the
local or Global Minima of a function
determine the direction the model should
take to reduce the error so as we're
looking at this we have our uh squared
error that we just figured out the co
based on the cost function it says how
bad is my model fitting the data I just
put through it and then we want to
reduce that error so how do you figure
out what direction to do that in well
you could be that you're looking at just
that line of that line of data coming in
so that would be a local Minima we want
to know the error of that particular
setup up coming in and then you have
your Global your Global Minima we want
to minimize it based on the overall data
we're putting through it and with this
we can figure out the global minimum
cost we want to take all those local
minimum costs of each piece of data
coming in and figure out the global one
how are we going to adjust this model to
fit all the data we don't want it to be
biased just on three or four lines of
data coming in we want it to kind of
extrapolate a general answer for all the
data coming in at this of course uh we
mentioned it briefly about back
propagation this is where really comes
in handy is training our model neural
network technique to minimize the cost
function helps to improve the
performance of the network back
propagates the error and updates the
weights to reduce the error so as you
can see here is a very nice depiction of
a back propagation we have our U
predicted y coming out and then we have
since it's a training set we already
know the answer and the answer comes
back and based on case of the square
means was one of the functions we looked
at uh one of the activation functions
based on cost function that cost
function then depending on what you
choose for your back propagation method
and there's a number of them will change
the weights it will change the weight
going to each of one of those nodes in
the hidden layer and then based upon the
error that's still being carried back
it'll change the weights going to the
next hidden layer and then it computes
an error level on that and sends that
back up and you're going to say well if
it computes the error into the first
hidden layer and fixes it why would it
stop there well remember we don't want
to create a biased neural network so we
only make small adjustments on these
weights we don't make a big adjustment
that changes everything right off the
bat so no matter how far back you go
you're always going to have a small
amount of error and that's still going
to continue to go all the way back up
the hidden layers for right now focus on
the back propagation is taking that
error and moving it backwards on the
neural network to change the weights and
help program it so that it'll have the
correct answers so far we've been
talking about forward propagation Nal
networks everything goes forwards goes
left to right uh but let's let's take a
little detour and let's see what is the
difference between a feed forward neural
network and a recurrent neural network
now this is in the function not when
we're training it using the back
propagation so you've got new
information coming in and you want to
get the answer and there's a couple
different networks out there and we want
to know we have a feed forward neural
network and we have a new uh vocabulary
term recurrent neural network a feed
forward neural network signals travel in
one direction from from input to Output
no feedback loops considers only the
current input cannot memorize previous
inputs one example of one of these feed
forward neural networks and we've
covered a number of them but one of the
ones this has a big highlight nowadays
is the CNN a convolutional neural
network tensorflow the one put out by
Google is probably most known for their
CNN where the information goes forward
it uh first takes a picture splits it
apart goes through the individual pixels
on the picture so it picks up up a
different reading then calculates based
on that goes into a regular feed forward
neural network and then gives your
categorization on there now we're not
covering the CNN today but we do have a
video out that you can look up on
YouTube put out by simply learn the
convolutional neural network wonderful
tutorial check that out and learn a lot
more about the convolutional neural
network but you do need to know that the
CNN is a forward propagation neural
network only so it's only moving in One
Direction so we want to look at a
recurrent neural network signals travel
in both directions making it a looped
Network considers the current input
along with the previous received inputs
for generating the output of a layer has
the ability to memorize past data due to
its internal memory and you can see they
have a nice uh image here we have our um
input and for some reason they always do
the recurrent neural network um in
Reverse from bottom up in the images
kind of a standard although I'm not sure
why your X goes into your hidden layer
and your hidden layer the answer for
part of the answer from it generates
feeds back into the hidden layer so now
you have an input of both X and part of
the Hidden layer and then that feeds
into your output now if we go back to
the forward let me just go back a slide
and we're looking at uh our forward
propagation Network one of the tricks
you can do to use just a forward
propagation network is if you're in a
what they call a Time sequence that's a
good uh term to remember or a Time
series meaning that it's sequential data
each term comes after the other you can
trick this by creating your input nodes
as with the history so if you know that
you have values one five and seven going
in and you know what the output is from
one what those outputs are you can
expand the input to include the history
input that's one of the ways to trick a
forward propagation Network into looking
at that but when you do with the
recurrent neural network you let the
hidden layer do that for you it sends
that data and reprocesses it back into
itself what are some of the applications
of recurrent neural network the RNN can
be used for sentiment analysis and text
mining getting up early in the morning
is good for health it's a positive
sentiment one of the catches you really
want to look at this when you're looking
at the language is that I could switch
this around and totally negate the
meaning of what I'm doing so would no
longer be positive so when you're
looking at a sentence knowing the order
of the words is as important as the
meaning of the words you can't just
count how many good words there are
versus bad words to get positive
sentiment you now have to know what
they're addressing and there's lots of
other different uses uh kids are playing
football or soccer as we call it in the
US RN can help you caption an image So
based on previous information coming in
it refeeds that back in and you have a
uh image Setter and then time series
problems like predicting the prices of
stocks in a month or quarter or sale of
products can be solved using an RNN and
this is a really good example you have
whatever your stocks were doing earlier
this month will have a huge effect on
what they're doing today if you're
investing so having an RNN model a
recurrent neural network feeding into
itself what was happening previously
allows it to take that model and program
in that whole series without having to
put in the whole a month at a time of
data you can only put in one day at a
time but if you keep them in order it
will look back and say oh this because
of what happened yesterday I need some
information from that and I'm going to
use that to help predict today and so on
and so on we're going to go back to our
activation functions remember I told you
uh Ru is one of the most common
functions used uh so let's talk a little
bit more about Ru and also soft Max
softmax is an activation function that
generates the output between 0er and one
it divides each output such that the
total sum of the outputs is equal to one
it is often used in the output layers
softmax L of the N equals e to L of the
N over the absolute value of e to the L
so what does this function mean I mean
what is actually going on here so we
have our output nodes and our output
nodes are giving us let's say they gave
us
1.2.9 and4 as a human being I look at
that and I say well the greatest value
is 1.2 so whatever category that is if
you have three different categories
maybe you're not just doing if it's a
cat or it's a dog or um oh let's say
it's a cow we had cats and dogs earlier
why the cats and dogs are hanging out
with a cow I don't know but we have a
value and it might say 1.2 is a cat 0.9
is a dog and 04 is a cow uh for some
reason it thinks that there's a chance
of it being any one of these three items
and that's how it comes out of the
output layer well as a human I can look
at 1.2 and say this is definitely what
it it is it's definitely a cat or
whatever it is uh maybe it's looking at
different kinds of cars might be a
better whether it's a car truck or
motorcycle maybe that'd be a better
example well from a computer standpoint
that may be a little confusing because
they're just numbers waving at us and so
with the softmax we want all those
numbers to always add up to one so when
I add three numbers together I want the
final output to be one on there and so
goes through this formula changes each
of these numbers in this case it changes
them to
0463 4 and20 they all add up to one and
that's a lot easier to register cuz it's
very set it's a set output it's never
going to be more than one it's never
going to be less than zero and so you
can see here that there's probably a
pretty high chance that it's the first
one so you're a human being we have no
problem knowing that but this output can
then also go into say another input so
it might be an automated car that's
picking up images and it says that image
in front of us is probably a big truck
we should deal with it like it's a big
truck it's probably not a motorcycle um
or whatever those categories are that's
the softmax part of it but now we have
the railu well what where's the ru
coming from well the railu is what's
generating the 1.2 and the 0.9 and the
04 and so if you remember our Ru stands
for rectified linear unit and is the
most widely used activation function we
looked at a number of different
activation functions including tangent H
the step function I remember I said the
step function is really used if that's
what your actual output is because then
you know it's a zero or one but the
raylo if you have that as your output
but you know have a discrepancy in there
and if that's going into another neural
network or another process having that
discrepancy is really important and it
gives an output of x if x is positive
and zero otherwise so it says my x value
is going to be somewhere between zero or
one and then the uh usually unless it's
really uncertain the output's usually a
one or zeros and then you have that
little piece of uncertainty there that
you can send forward to another Network
or you can look at to know that there's
uncertainty involved and is often used
in the hidden layers this is what's
coming out of the Hidden layers into the
output layer usually or as we reference
the convolution neural network the CNN
you'd have to go to another video to
review the railu is the most common used
for convolutional part of that Network
it has a bunch of little pieces that are
very simplified looking at all the
different images or different sections
of the map and the rayu works really
good for that like I said there's other
formulas used but that this is the most
common one and you'll see that in the
hidden layers going maybe between one
layer and the next layer so just a quick
recap we have our soft Max which means
that if you have uh numerous categories
only one of them is going to be picked
but you also want to have some value
attached to it how well it picked it and
you put that between zero one so it's
very uh standardized so we have our soft
Max we looked at that let's go back one
we looked at that here where it
transforms in numbers and then we have
our railu function which takes the
information and the summation and puts
it between a zero and a one where it's
either clearly a zero or depending on
how confident our model is it'll go
between the zero and one value what are
hyper parameters oh this is a great
interview question hyperparameters when
you are doing neural networks this is
what you're playing with most of the
time once you gotten the data formatted
correctly a hyperparameter is a
parameter whose value is set before the
learning process begins determines how a
network is trained and the structure of
the network this includes things like
the number of hidden units how many
layers are you going to have and how
many nodes in each layer learning rate
learning rate is usually multiplied once
you figured out the error and how much
you want to change the weights we talked
about or I mentioned it early just
briefly you don't want to just make a
huge change otherwise you're going to
have a biased model so you only take
little incremental changes and that's
what the learning rate is is those small
incremental changes epics how many times
are you going to go through all the data
in your training set so one Epic is one
trip through all the data and there's a
lot of other things depending on what
model you're working with and which
programming script you're working with
like the python SK learn package will
have it slightly different than say
Google's tensorflow package which will
be a little bit different than the spark
machine learning package so these are
just some examples of the
hyperparameters and so you see in here
we have a nice image of our data coming
in and we train our model then we do a
comparison to see how good our model is
and then we go back and we say hey this
this model is pretty good but it's
biased so then we send it back and we
change our hyperparameters to see if we
can get an unbiased model or we can have
a better prediction on it that matches
our data closer what will happen if
learning rate is set too low or too high
we have a nice couple graphs here we
have one over here says the learning
rate set too low and you can see that it
slowly Works its way down the curve and
on the right you can see a learning rate
set too high it's just bouncing back and
forth when your learning rate is too low
that's what we studied at two slides of
they asked what the learning rate was
training of the model will progress very
slowly as we are are making very tiny
updates to the weights we'll take many
updates before reaching the minimum
point so I just mentioned epic going
through all the data you might have to
go through all the data a thousand times
instead of 500 times for it to train
learning rate too high causes
undesirable Divergent Behavior to the
loss function due to drastic updates and
weights at times it may fail to converge
or even diverge so if you have your
learning rate set too high and it's
training too quickly maybe you'll get
lucky and it trains after one epic run
but a lot of times it might never be
able to train because the weights are
changing too fast they they flip back
and forth too easy and you see down here
we've introduced uh two new terms
converge and diverge a converge means
that our model has reached a point where
it's able to give a fairly good answer
for all the data we put in all those
weights have adjusted and it's minimized
the error diverg means that the data is
so chaotic that it can never manage to
to train to that data the data is just
too chaotic for it to train so we have
two new words there converge and diverge
are important to know also what is
Dropout and batch normalization Dropout
is a technique of dropping out hidden
and visible units of a network randomly
to prevent overfitting of data it
doubles the number of iterations needed
to converge the network so here we have
our standard neural network and then
after applying Dropout now it doesn't
mean we actually delete the node the
node is still there and we're still
going to use that node what it means is
that we're only going to work with a few
of the nodes um a lot of times I think
the most common one right now used is
20% uh so you'll drop out 20% of the
nodes when you do your training you
reverse propagate your data and then
you'll randomly pick another 20 nodes
the next time you go through an epic
data training so each time you go
through one Epic you will randomly pick
20 of those nodes not to not to mess
with and this allows for less
overfitting of the data so by randomly
doing this you create some I guess it
just kind of pull some nodes off to the
Sid it says we're going to handle the
data later on so we don't overfit batch
normalization is a technique to improve
the performance and stability of neural
network the idea is to normalize the
inputs in every layer so that they have
mean output and activation of zero and
standard deviation of one this question
covers a lot of different things which
is great it's a great uh interview
question because it pulls in that you
have to understand what the mean value
is so a mean output activation of zero
that means our average activation is
zero so when you normalize it remember
usually we're going between minus one
and one on a lot of these it's a very
standard setup so you have to be very
aware that this is your mean output
activation of zero and then we have our
standard deviation of one so we want to
keep our error down to just a one value
the benefits of this doing a batch
normalization is it provides
regularization it trains faster Higher
Learning rates and weights are easier to
initialize what is the difference
between batch gradient descent and
stochastic gradient descent batch
gradient descent batch gradient computes
the gradient using the entire data set
it takes time to converge because the
volume of data is huge and weights
update slowly so you can look at the
batches a lot of times if you're using
big data batch the data in but you still
go through a full epic you still go
through all the data on there so bash
gradient descent means you're going to
use it to fit all the data and look for
a convergence there stochastic gradient
descent stochastic gradient computes the
gradient using a single sample it
converges much faster than batch
gradient because it updates weight more
frequently explain overfitting and
underfitting and how to combat them
overfitting happens when a model learns
the details and noise and the training
data to the degree that it adversely
impacts the execution of the model on
the new information it is more likely to
occur with nonlinear models that have
more flexibility when learning a Target
function an example of this would be um
if you're looking at say cars and trucks
and motorcycles it might only recognize
trucks that have a certain box like
shape it might not be able to notice a
flatbed truck unless it's only a
specific kind of flatbed truck or only
Ford trucks because that's what it saw
on the training set this means that your
model performs great on your train data
and great on maybe a small test amount
of data but when you go to use it in the
real world it leaves out a lot and start
is not very functional outside of your
small area your small labatory data
coming in underfitting doing the
opposite when you underfit your data
underfitting alludes to a model that is
neither well trained on training data
nor can generalize to new information
usually happens when there is less and
improper data to train a model has a
performance and accuracy so if you're
using underfitted data and you generate
a model and you distribute that in a
commercial Zone you'll have a lot of
people unhappy with you because it's not
going to give them very good answers so
we've explained overfitting and
underfitting so now we want to ask how
to combat them combating overfitting and
underfitting resampling the data to
estimate the model accuracy k cross
validation having a validation data set
to evaluate the model so when we do the
resampling we're randomly going to be
picking out data and we'll run it a few
times to see how that works depending on
our random data and how we U sample the
data to generate our model and then we
want to go ahead and validate the data
set by having our training data and then
keeping some data on the side uh testing
data to validate it how are weights
initialized in a network initializing
all weights to zero all the weights are
set to zero this makes your model
similar to a linear model so if you have
linear data coming in doing a basic
setup like that might work all the
neurons in every layer perform the same
operation given the same output and
making the Deep net useless right there
is a key word it's going to be useless
if you initialize everything to zero at
that point be looking into some other uh
machine learning tools initializing all
weights randomly here the weights are
assigned randomly by initializing them
very close to zero it gives better
accuracy to the model since every neuron
performs different comp mutations and
here we have the weights are set
randomly we have our input layer the
hidden layers and the output layer and W
equals NP random random in layer size L
layer size L minus one this is the most
commonly used is to randomly generate
your weights what are the different
layers in CNN convolutional neural
network first is the convolutional layer
that performs a convolutional operation
we have our other video out if you want
to explore that more so you go into
detail exactly how the C the
convolutional layer works in the CNN as
far as creating a number of smaller uh
picture windows that go over the data uh
the second step is has a railu layer Ru
brings nonlinearity to the network and
converts all the negative pixels to zero
output is rectified feature map so goes
into a mapping feature there pooling
layer pooling is a down sampling
operation that reduces the
dimensionality of the feature map so we
have all our railu layer which is
pulling all these little Maps out of our
convolutional layer it's taking that
picture and little creating little tiny
neural networks to look at different
parts of the picture uh then we need to
pull it together and then finally the
fully connected layer so we flatten our
pooling layer out and we have a fully
connected layer recognizes and
classifies the objects in the image and
that's actually your Ford propagation
reverse propagation training model
usually I mean there's a number of
different models out there of course
what is pooling in CNN and how does it
work pooling used to reduce the spatial
dimensions of a CNN performs down
sampling on operation to reduce the
dimensionality creates a pulled feature
map by sliding a filter Matrix over the
input Matrix I mentioned that briefly on
the previous slide um it's important to
know that you have if you can see here
they have a rectified feature map and so
each one of those colors like the yellow
color that might be one of the a smaller
little neural network using the ru
you'll look at it'll just kind of um go
over the main picture and look at all
the different areas on the main picture
so you might step one two three four
spaces um and then you have another one
it's also looking at features and it has
a 2785 each one of those is a map so it
might be the first one might be a map
looking for cat ears and the second one
looking for human eyes when it does this
you then have this rectified feature map
looking at these different features and
the max pooling with a 2 by two filters
and a stride of two stride means instead
of skipping every pixel you're going to
go every two pixels you take the maximum
values and you can see over here when we
look at a pulled feature map one of the
features says hey I had a max value of
eight so somewhere in here we saw a
human eye labeled as eight pretty high
label and maybe seven was a human hand
and maybe four was cat whiskers or
something that we thought might be cat
whiskers four is kind of a low number in
this particular case compared to the
other ones so you have your full pulled
feature map you can see the process here
is we have our stepping we look for the
max value and then we create a pulled
feature map of the maxed values how does
a lstm network work that's long
short-term memory so the first thing to
know is that an lstm M are a special
kind of recurrent neural network capable
of learning long-term dependencies
remembering information for long periods
of time is their default Behavior we did
look at the RNN briefly talked about how
the hidden layer feeds back into itself
with the lstm has a much more
complicated feedback and you can see
here we have um the hidden layer of T
minus one and the hidden layer that's
what the H stands for hidden layer of T
and the formula is going in as we can
see here we have the hidden layers we
have have T minus one and then h of T
where T stands for time so this is a
series remember working with series and
we want to remember the past and you can
see you have your your input of T and
that might be a frame in a video as a
frame comes in they usually use in this
one the tangent H activation formula but
you also see that it goes through a
couple other formulas the Omega formula
and so when it combines these that thing
goes into the next layer your next
hidden layer that thing goes into the
data that's submitted to the next input
so you have your X of t + one so when
you have that coming in then you have
your H value that's coming forward from
the last process and depending on how
many of these um Omega structures you
put in there depends on how longterm the
memory gets so it's important remember
this is more for your long-term
recurrent neural networks the three
steps in an lstm step one decides what
to forget and what to remember step two
selectively update cell State values So
based on we want to remember and forget
we want to update those cell values and
then decides what part of the current
state make it to the output so now we
have to also have an output on there
what are Vanishing and exploding
gradients this is a great question that
affects all our neural networks while
training an RNN your slope can become
either too small or too large and this
makes the training difficult when the
slope is too small the problem is known
as Vanishing gradient so our slope we
have our change in X and our change in y
when the slope decreases gradually to
very small value sometimes negative and
makes training difficult when the slope
tends to grow exponentially instead of
decaying this problem is called
exploding gradient the slope grows
exponentially you can see a nice graph
of that here issues in gradient problem
Long training time poor performance and
low accuracy what is the difference
between Epic bat and iteration in deep
learning epic an epic represents one
iteration over the entire data set so
that's everything you're going to go
ahead and put into that training model
bat we cannot pass the entire data set
into the neural network at once so we
divide the data set into a number of
batches and then iteration if we have
10,000 images as data and a batch size
of 200 then the Epic should run 10,000
times over 200 so that means we have our
total number over the 200 equals 50
iterations so in each epic we're running
over all the data set we're going to
have 50 iterations and each of those
iterations includes a batch of 200
images in this case one tensorflow is
the most preferred library in deep
learning uh well first tensorflow
provides both C++ and python apis that
makes it easier to work on has a faster
compilation time than other deep
learning libraries like carass and torch
tensorflow supports both CPUs and gpus
Computing devices so right now
tensorflow is at the top of the market
because it's so easy to use for both
programmer side and for Hardware side
and for the speed of getting something
up and running what do you mean by
tensor and T flow tensor is a
mathematical object represented as
arrays of higher Dimension these arrays
of data with different dimensions and
ranks that are fed as input to the
neural network are called tensors and
you can see here we have a tensor of
Dimensions 5 comma 4 so it's a
two-dimensional tensor coming in um you
can look at an image like this that each
one of those pixels is a different value
if it's a black and white so it might be
zero and ones and then each one
represents a black and white image in a
color photo you might um either find a
different value system or you might have
a tensor value that has the XY
coordinates as we see here plus the
colors so you might have three more
different dimensions for the three
different images the red the blue and
the yellow coming in and even as you go
from one layer or one tensor to the next
these layers might change we might
flatten them might bring in numerous in
the case of the convergence neural
network we have all those smaller
different mappings of features that come
in so each one of those layers coming
through is a tensor if it has multiple
Dimensions coming in and weights
attached to it what are the programming
elements in tensor flow well we have our
constants constants are parameters whose
value does not change to define a
constant we use tf. constant command
example a equal tf. constant 2.0 TF
float 32 so it's a tensor float value of
32 b equals TF constant 3.0 print AB if
we did a print of ab we'd have um tf.
constant and then of course uh B is that
instance of it variables variables allow
us to add new trainable parameters to
graph to Define a variable we use tf.
variable command and initialize them
before running the graph in session
example W equal TF variable. 3 dtype TF
float 32 or Bal a TF variable minus 3
comma dtype float 32 placeholders
placeholders allow us to feed data to a
tensor flow model from outside a model
it permits a value to be assigned later
to define a placeholder we use TF
placeholder command example AAL TF
placeholder B = A * 2 with the TF
session as sess result equals session
run B comma feed dictionary equals a 3.0
print result uh so we have a nice
example there a placeholder session a
session is run to evaluate the nodes
this is called as the tensor flow
runtime so for example you have a equals
TF constant 2.0 Bal TF constant 4.0 Cal
a plus b and at this point you'd go
ahead and create a session equals TF
session and then you could evaluate the
tensor C print session run C that would
input c as a input into your session
what do you understand by a
computational graph everything in
tensorflow is based on creating a
computational graph it has a network of
nodes where each node performs an
operation nodes represent mathematical
operation and edges represent tensors
since data flows in a form of a graph it
is also called a data flow graph and we
have a nice visual of this graph or
graphic image of a computational graph
and you can see here we have our input
nodes our add multiply nodes and our
multiply node at the end and then we
have the edges where the data flows so
we have from a going to C A going to D
you can see we have a two- flowing a
four flowing explain generative
adversarial Network along with an
example suppose there's a wine shop that
purchases wine from dealers which they
will resell later so we have our dealer
point to the wine or shop owner that
then sells it for a profit but there are
some malactor dealers who sell fake wine
in this case the shop owner should be
able to distinguish between fake and
authentic wine the forger will try to
different techniques to sell fake wine
and make sure certain techniques go past
the shop owner's check so here's our
forager fake wine shop owner the shop
owner would probably get some feedback
from the wine experts that some of the
wine is not original the owner would
have to improve how he determines
whether a wine is fake or authentic goal
of forger to create wines that are
indistinguishable from the authentic
ones goal of shop owner to accurately
tell if the wine is real or not there
are two main components of generative
adversarial Network and we refer it to
as a noise Vector coming in where we
have our forger who's going to generate
fake wine and then we have our real
authentic wine and of course our shop
owner who has to figure out whether it's
real or fake the generator is a CNN that
keeps producing images that are closer
in appearance to the real images while
the discriminator tries to determine the
difference between real and fake images
the ultimate aim is to make the
discriminator learn to identify real and
fake images what is an auto encoder the
network is trained to reconstruct conu
its inputs it is a neural network that
has three layers here the input neurons
are equal to the output neuron the
Network's Target outside is same as the
input it uses dimensionality reduction
to restructure the input input image
comes in we have our Lattin space
representation and then it goes back out
reconstructing the image it works by
compressing the input to a Latin space
representation and then reconstructing
the output from this representation what
is bagging and boosting bagging and
boosting are on sample techniques where
the idea is to train multiple models
using the same learning algorithm and
then take a call so we have in here
where we're bagging we take a data set
and we split it we're going to have our
training data and our test data very
standard thing to do then we're going to
randomly select data into the bags and
train your model separately so we might
have bag one model one bag two model two
bag three model 3 and so on in boosting
the infinis is to select the data points
which give wrong output in order to
improve the accuracy so in boosting we
have our data set again we split it to
test data and train data and we'll take
a bag one and we'll train the model data
points with wrong predictions then go
into Bag two and we then train that
model and repeat and with this we have
come to the end of this Advanced deep
learning full course if you like this
video subscribe to our YouTube channel
and hit the Bell icon to never miss any
updates from Simply learn please let us
know in the comment section below if you
need help opting for this Caltech postr
program in a machine learning course we
will get back to you as soon as possible
until next time thank you and and keep
learning staying ahead in your career
requires continuous learning and
upskilling whether you're a student
aiming to learn today's top skills or a
working professional looking to advance
your career we've got you covered
explore our impressive catalog of
certification programs in cuttingedge
domains including data science cloud
computing cyber security AI machine
learning or digital marketing Des
designed in collaboration with leading
universities and top corporations and
delivered by industry experts choose any
of our programs and set yourself on the
path to Career Success click the link in
the description to know
more hi there if you like this video
subscribe to the simply learn YouTube
channel and click here to watch similar
videos to nerd up and get certified
click here