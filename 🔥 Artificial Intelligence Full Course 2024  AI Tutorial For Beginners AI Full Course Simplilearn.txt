hi everyone and welcome to this
fantastic video on eii full course by
simply learn but before we begin if you
enjoy watching these videos and find
them interesting subscribe to our
Channel as we bring you the best videos
daily also hit the Bell icon to never
miss any updates from Simply learn so
let's go through the agenda of this
videos real quick we'll brief you with
the detail introduction to artificial
intelligence after which we will see the
artificial intelligence road map
followed by top skills required by AI
engineer we will walk you through some
fantastic Concepts like generative ai ai
for Edge Computing and many more
Concepts after which we will see some AI
algorithms like linear regression and
logistic regression finally we will
cover some essential projects like fake
news detection object detection that you
can mention on your CV to make it stand
out from the crowd in your upcoming
interviews speaking of interviews we
have covered you along with the most
frequently Asked interview questions to
help you crack the most challenging
interviews but before we begin to
understand what AI is if you want to
become an AI expert and gain handsome
salary packages look at the wide range
of eiml courses by simply learn in
collaboration with top universities
across the globe by enrolling in any of
these certification programs you will
gain expertise in the skills like
generative AI prompt engineer chat GPT
explainable AI machine learning
algorithm supervised and unsupervised
learning model training and optimization
and there is much more on the list with
hands-on experience in the tools like
chat GPT d python open CV and tensorflow
you will catch the eye of top recruiters
so what are you waiting for hurry up and
enroll now an year of experience is
preferred to enroll in these courses
find the cost Link in the description
box so without any further Ado over to
our training experts it's a weekend and
John decided to watch the latest movie
recommended by Netflix at his friend's
place before heading out he asked sir
about the weather and realized it would
rain so he decided to take his Tesla for
the long journey and switched to
autopilot on the highway after coming
home from the eventful day he started
wondering how technology has made his
life easy he did some research on the
internet and found out that Netflix Siri
and Tesla are all using AI so what is is
ai ai or artificial intelligence is
nothing but making computers based
machines think and act like humans
artificial intelligence is not a new
term John McCarthy a computer scientist
coined the term artificial intelligence
back in
1956 but it took time to evolve as it
demanded heavy computing power
artificial intelligence is not confined
to just movie recommendations and
virtual assistance broadly classifying
there are three types of a a i
artificial narrow intelligence also
called weak AI is the stage where
machines can perform a specific task
Netflix Siri chatbots spatial
recommendation systems are all examples
of artificial narrow intelligence next
up we have artificial general
intelligence referred to as an
intelligent agent's capacity to
comprehend or pick up any intellectual
skill that a human camp we are halfway
into successfully implementing this
space IBM's Watson super super computer
and gpt3 fall under this category and
lastly artificial super intelligence it
is the stage where machines surpass
human intelligence you might have seen
this in movies and imagined how the
world would be if machines occupy it
fascinated by this John did more
research and found out that machine
learning deep learning and natural
language processing are all connected
with artificial intelligence machine
learning a subset of AI is the process
of automating and enhancing ing how
computers learn from their experiences
without human Heth machine learning can
be used in email spam detection medical
diagnosis Etc deep learning can be
considered a subset of machine learning
it is a field that is based on learning
and improving on its own by examining
computer algorithms while machine
learning uses simpler Concepts deep
learning works with artificial neural
networks which are designed to imitate
the human brain this technology can be
applied in face recognition speech
recognition and many more
applications natural language processing
popularly known as NLP can be defined as
the ability of machines to learn human
language and translate it chatbots fall
under this category artificial
intelligence is advancing in every
crucial field like healthcare education
robotics banking e-commerce and the list
goes on like in healthcare AI is used to
identify diseases helping healthcare
service providers and their Pat patients
make better treatment and lifestyle
decisions coming to the education sector
AI is helping teachers automate grading
organizing and facilitating parent
Guardian
conversations in robotics AI powered
robots employ realtime updates to detect
obstructions in their path and
instantaneously design their routes
artificial intelligence provides
Advanced data analytics that is
transforming banking by reducing fraud
and enhancing compliance with this
growing demand for AI more and more
Industries are looking for AI Engineers
who can help them develop intelligent
systems and offer them lucrative
salaries going north of
$120,000 the future of AI looks
promising with the AI Market expected to
reach $190
billion artificial intelligence is
reshipping Industries and transforming
the way we leave and work it encompasses
a wide range of Technologies including
machine learning deep learning natural
language processing computer vision and
many more with its ability to analyze
vast amount of data and make Intelligent
Decisions AI has become a game changer
across various domains hi everyone
welcome to Simply learns YouTube channel
this AI road map guides you in
navigating the path towards building a
successful career in artificial
intelligence the ultimate goal of
artificial intelligence is to create
intelligent missions that can perform
complex task exhibit human like
intelligence and contribute positivity
to society Yi presents exciting career
opportunity in various Industries and
sectors roles like AI engineer data
scientist NLP Engineers computer vision
Engineers AI research scientists robotic
engineers and many more offer exciting
prospects for working with cutting ITS
Technologies and making an impact
through artificial intelligence
Innovation according to glass door the
average reported salary of an AI
engineer in the United State is around
$15,000 per year however in India it is
10 lakh perom leading companies
worldwide are fully aware of the immense
value of AI and are actively pursuing
skilled AI Engineers to contribute to
their research development and
implementations of AI technology among
this top companies are Google Microsoft
Amazon Goldman saat apple and JP Morgan
Chase therefore top companies hiring AI
engineer provide excellent opportunity
for aspiring professionals on that note
elevate your career with our Ai and ml
course developed in partnership with P
University and IBM gain expertise in
high demand skills such as machine
learning deep learning NLP computer
vision reinforcement learning chat GPT
generative Ai explainable Ai and more
Unleash Your Potential and unlock
exciting opportunity in the world of AI
and ml this course cover tools like
python tensor flow kage CH GPT open AI
gy matplot lab and many more so hurry up
and find the course Link in the
description box for more details our
Learners have experienced huge success
in their careers listen to their
experience find the simply learn Course
Review Link in the description box
without any further delay let's dive
into the topic now if you desire to
become an AI engineer here are the steps
you can follow to achieve your goal
firstly obtain a strong foundation in
mathematics and programming gain a
strong foundation in creating mathematic
Concepts like linear algebra calculus
and probability theory in addition to
that it is crucial to acur Mastery of a
programming languages like python
commonly used in Ai and become
proficient in coding then earn a degree
in a relevant field that means earn a
bachelor's or master degree in computer
science data science AI or a related
field to gain a comprehensive
understanding of AI principles and
techniques next gain knowledge in
machine learning and deep learning to
become a AI engineer develop similarity
with ML algorithms neural networks and
deep learning Frameworks example tensor
Flow by touch to train and optimize
models using real world data set
followed by that work on practical
project s gain hands-on experience and
showcase your skills by working on AI
projects moreover build a portfolio of
projects demonstrating your ability to
solve eii problems impressing potential
employers then stay updated with the
latest advancements stay updated with
the latest trends and research in AI a
rapidly involving field and expanding
your knowledge by reading research
papers participating in online courses
and workshops and joining AI communities
next collaborate and network engage with
AI communities attend conference
participate in alline forums to connect
with Professionals in this field
collaborating with others can enhance
your learning experience and open new
opportunities later seek internships or
entry-level positions gain practical
experience through AI internships or
entry-level roles in industry or
research institutions this provides
valuable exposures and help develop your
skills finally continuously learn and
adapt in the rapidly involving field of
AI stay updated on new development
explore specialized areas and embrace
emerging Technologies and tools to
pursue a career as an AI engineer now
that we have covered the essential steps
to become an AI engineer let's explore
the necessary programming languages and
algorithms for aspiring eii Engineers so
mastering programming languages like
python R Java and C++ is vital for
acquiring Proficiency in AI this
languages enables you to construct and
deploy models effectively additionally
it will help if you gain a thorough
understanding of machine learning
algorithms such as linear regressions K
nearest neighbors na Bas and support
Vector missions these languages and
algorithms are fundamental tools in the
field of AI they will enable you to
develop and Implement effective
artificial intelligence hello everyone
and Welcome to our video on skills
required for an AI engineer with the
Advent of artificial intelligence and
its Superior abilities to improve human
life the need and demand for expert AI
professional is at an all-time high this
expanding demand has led to a lot of
people applying for AI jobs and
upskilling themselves in the field of AI
but doing this is just like playing a
game but not knowing its rules for
someone to become an AI professional and
learn a job like an AI engineer they
first have to understand the demand from
this role and what skills are expected
of an AI engineer so if you're someone
who wishes to become an AI engineer or
if you wish to upgrade your skills in Ai
and get promoted as an AI engineer then
make sure you watch this amazing video
till the very end and this this video we
will be breaking down in complete detail
each and every skill that you would need
in order to crack an AI engineer job
interview but why should you consider a
career in ai ai is not just a passing
Trend it's a sesmic ship that is
reshaping our world and creating new
avenues for Innovation and Discovery by
embracing a career in AI you become a
part of dynamic field that thrives on
solving complex problems pushing
boundaries and making a profound impact
on society the demand for AI
professional is scating across
industries from Healthcare and finance
to entertainment and transportation
organization are actively seeking
talented individuals who can harness the
power of AI to drive their business
forward but what skills does it take to
become an AI engineer how can you embark
on the thrilling Journey we have
answered to all of your questions also
accelerate your career in a ml with our
comprehensive post-graduate program in
Ai and machine learning gain expertise
in machine learning deep learning NLP
computer vision and reinforcement
learning you will receive a prestigious
certificate exclusive alumini membership
and ask me anything session by ABIA with
three Capstone project and 25 plus
industry projects using real data set
from Twitter Uber and many more you will
gain practical experience master classes
by ctech faculty and IBM expert ensure
topnotch education simply learn job
assist help you to get notied by Leading
companies this program covers statistic
python supervised and unsupervised
learning NLP neural network computer
vision G caras tensorflow and many more
skills so enroll now and unlock exciting
AI ml opportunities the link is in the
description box below so without any
further delay let's get started some
steps are crucial to master in the field
of AI and becoming an AI engineer let's
go through them real quick first
establish a strong foundation in
mathematics and programming start by
gaining a solid understanding of
critical Concepts like linear algebra
calculus and probability Theory
additionally it is crucial to become
proficient in programming language like
python which is commonly used in Ai and
developed coding skills and the next one
is pursue a degree in relevant field
earn a bachelor or master degree in
computer science data science or AI or
related discipline to acquire
comprehensive understanding of AI
principles and techniques then acquire
knowledge in machine learning and deep
learning familiarize yourself with ML
algorithms neural networks and deep
learning Frameworks that is tensor flow
and pytorch to train and optimize models
using real world data sets afterward
engage in Practical projects gain
hands-on experience and demonstrate your
skills by working on AI projects
building a portfolio of projects that
showcase your ability to solve AI
problems can make a strong impression on
potential employers and the fifth one is
collaborate and network engage with
community attend conferences and
participate in the online forums to
connect with the Professionals in the
field collaborating with others can
enhance your learning experience and
open up new opportunities and the sixth
one is seek internship or entry-level
positions gain practical experience
through AI internship or entry-level
roles in Industries or research
institution this will provide valuable
exposure and help you further develop
your skill and at the last continuously
learning and adapt in the fast-paced
world of eii it it is crucial to stay
updated on new developments explore
specialized areas and embrace emerging
Technologies and tools continuous
learning and adaptability are essential
for pursuing a successful career as a AI
engineer so now that you are familiar
with the steps involved in the Journey
of AI engineer let's have a look at the
salary of an AI
engineer so according to glass door the
average reported salary of an AI
engineer in the United States is
$115,000 per year however in India it is
10 lakh perom so these figures are way
better than the average selling figures
of any job road so let's discuss the
skills you need to become an AI engineer
you should have a strong background in
data science and machine learning here
is a breakdown of your skills number one
strong programming abilities they
typically refers to expertise in one or
more programming languages commonly used
in data science and machine learning
such as Python and R Proficiency in
programming allow allows you to write
efficient and scalable code for data
analysis modeling and algorithm
implementation and the second one is
knowledge of machine learning algorithms
this involves understanding and
familiarity with a wide range of machine
learning algorithms including both
supervised and and supervised learning
you should be able to select and apply
appropriate algorithm for specific
problem as well as evaluate and optimize
their performance and the third one is
Proficiency in statistic and Mathematics
sound knowledge of of statistics and
Mathematics is fundamental for data
analysis and machine learning you should
be comfortable with statical Concept
hypothesis testing regression analysis
probability Theory L algebra and
calculus and the fourth one
is familiarity with deep learning
Frameworks deep learning has gained
significant popularity in recent years
and familiarity with deep learning
Frameworks like tensorflow py torch or
kiras is valuable these Frameworks
provide two tools and libraries for
building training and deploying deep
neural networks for task such as image
recognition natural language processing
and time series analysis and the fifth
one is experience with big data
Technologies dealing with large scale
data set requires knowledge of Big Data
Technologies such as Apache Hadoop or
Apache spark or distributed computing
Frameworks understanding how to process
store and analyze data efficiently in
distributed environment and the sixth
one is excellent problem solving and
analytical skills these skills enable
you to break down complex problem
identify key factors and develop
effective Solutions you should be adapt
at critical thinking troubleshooting and
debugging to handle real world
challenges in the data science and
machine learning remember to stay
updated with the latest advancement in
the field and continuing learning to
stay at the Forefront of data science
and machine learning first let's
understand what really is artificial
intelligence artificial intelligence is
the science of of building intelligent
machines from vast volumes of
data this data can be structured
semi-structured or unstructured in
nature AI systems learn from past
experiences and perform humanlike tasks
artificial intelligence enhances the
speed precision and effectiveness of
human efforts AI uses sophisticated
algorithms and methods to build machines
that can make decisions on their own
deep learning and machine learning are
the two subsets of artificial
intelligence so so you need both machine
learning algorithms and deep learning
networks to build intelligence systems
AI is now being widely used in almost
every sector of business such as
Transportation Health Care banking
retail entertainment and
e-commerce now let's look at the
different types of artificial
intelligence so AI can be classified
based on capabilities and
functionalities under capabilities there
are three types types of artificial
intelligence they are narrow AI General
Ai and super AI under functionalities we
have four types of artificial
intelligence reactive machine limited
memory theory of mind and self-awareness
let's look at them one by
one first we will look at the different
types of artificial intelligence based
on
capabilities so what is narrow AI narrow
AI also known as as we AI focuses on one
narrow task and cannot perform Beyond
its limitations it aims at a single
subset of cognitive abilities and
advances in that Spectrum applications
of narrow AI are becoming increasingly
common in our day-to-day lives as
machine learning and deep learning
methods continue to
evolve Apple Siri is a simple example of
a narrow AI that operates with a limited
predefined range of functions Siri often
has challenges with tasks outside its
range of abilities
IBM Watson super computer is another
example of narrow AI which applies
cognitive Computing machine learning and
natural language processing to process
information and answer your
questions IBM Watson wants outperformed
human contestant Ken Jenkins to become
the champion on the popular game show
Jeopardy other examples of narrow AI
include Google Translate image
recognition software recommendation
systems spam filtering and Google's page
ranking
algorithm
next we have General artificial
intelligence or general AI General AI
also known as strong AI has the ability
to understand and learn any intellectual
task that a human can General artificial
intelligence has received A1 billion
investment from Microsoft through open
AI it allows the machine to apply
Knowledge and Skills in different
contexts AI researchers and scientists
have not achieved strong AI so far to
succeed they would need to find a way to
make machines conscious programming a
full set of cognitive
abilities Fujitsu built the K computer
which is one of the fastest computers in
the world it is one of the most notable
attempts at achieving strong AI it took
40 minutes to simulate a single second
of neural activity so it is difficult to
determine whether or not strong AI will
be achieved in the near
future tan 2 is a super computer created
by China's national university of
Defense technology
it currently holds the record for CPS at
33.86 petaflops although it sounds
exciting the human brain is estimated to
be capable of one
exaflop now CPS means characters per
second that A system can
process third in the list of AI that is
based on capabilities we have super AI
super AI exceeds human intelligence and
can perform any task better than a human
the concept concept of artificial super
intelligence sees AI evolved to be so
akin to human emotions and experiences
that it doesn't just understand them it
evokes emotions needs beliefs and
desires of its own its existence is
still hypothetical some of the key
characteristics of super AI include the
ability to think solve puzzles make
judgments and decisions on its
own now if you have enjoyed watching
this video so far please make sure to
subscribe to our YouTube channel and hit
the Bell icon to stay updated with all
the latest Technologies also if you have
any questions related to this video
please put it in the chat section our
team of experts will help you address
your
questions moving ahead now we will see
the different types of artificial
intelligence based on
functionalities in this category first
we have reactive machine a reactive
machine is the basic form of AI that
does not store memories or use past
experience experiences to determine
future actions it works only with
present data they simply perceive the
world and react to it reactive machines
are given certain tasks and don't have
capabilities Beyond those
duties IBM's deep blue which defeated
chess Grandmaster Gary casprov is a
reactive machine that sees the pieces on
a chess board and reacts to them it
cannot refer to any of its prior
experiences and cannot improve with
practice deep blue can identify the
pieces on a chessboard and know how each
moves it can make predictions about what
moves might be next for it and its
opponent it can choose the most optimal
moves from among the possibilities deep
blue ignores everything before the
present moment all it does is look at
the pieces on the chest board as it
stands right now and choose from
possible next
moves up next we have limited memory
limited memory AI learn learns from past
data to make decisions the memory of
such systems is
shortlived while they can use this data
for a specific period of time they
cannot add it to a library of their
experiences this kind of technology is
used for self-driving Vehicles they
observe how other vehicles are moving
around them in the present and as time
passes that ongoing collected data gets
added to the static data within the AI
machine such as Lane markers and traffic
lights
they're included when the vehicle
decides to change lanes to avoid cutting
of another driver or being hit by a
nearby vehicle Mitsubishi Electric is a
company that has been figuring out how
to improve such technology for
applications like self-driving
cars then we have theory of Mind theory
of Mind represents a very advanced class
of technology and exists as a concept
this kind of AI requires a thorough
understanding that the people and the
things within an environment can alter
feelings and
behaviors it should be able to
understand people's emotions sentiment
and
thoughts even though a lot of
improvements are there in this field
this kind of AI is not complete yet one
real world example of theory of Mind AI
is Kismet a robot head made in the late
'90s by a Massachusetts Institute of
Technology researcher Kismet can mimic
human emotions and recognize them both
abilities are key advancements in theory
of Mind AI but Kismet can't follow gazes
or convey attention to humans Sophia
from Hansen robotics is another example
where the theory of Mind AI was
implemented cameras within sopia's eyes
combined with computer algorithms allow
her to see she can follow faces sustain
eye contact and recognize individuals
she is able to process speech and have
conversations using natural language
subsystem finally we have
self-awareness self-awareness AI only
exists
hypothetically such systems understand
that internal traits States and
conditions and perceive human
emotions these machines will be smarter
than the human
mind this type of AI will not only be
able to understand and evoke emotions in
those it interacts with but also have
emotions needs and beliefs of its own
while we are probably far away from
creating machines that are self-aware we
should focus our efforts towards
understanding memory learning and the
ability to base decisions on past
experiences let's look at the definition
of each of these learning
techniques supervised learning uses
label data to train machine learning
models label data means that the output
is already known to you the model just
needs to map the inputs to the outputs
an example of supervised learning can be
to train a machine that identifies the
image of an animal below you can see we
have a trained model that identifies the
picture of a cat unsupervised learning
uses unlabeled data to train machines
unlabeled data means there is no fixed
output variable the model learns from
the data discovers patterns and features
in the data and Returns the output here
is an example of an unsupervised
learning technique that uses the images
of vehicles to classify if it's a bus or
a truck so the model learns by
identifying the paths of of a vehicle
such as the length and width of the
vehicle the front and rear end covers
roof hoods the types of Wheels used Etc
based on these features the model
classifies if the vehicle is a bus or a
truck reinforcement learning trains a
machine to take suitable actions and
maximize reward in a particular
situation it uses an agent and an
environment to produce actions and
rewards the agent has a start and an end
state but there might be different parts
for reaching the end State like a maze
in this learning technique there is no
predefined target variable an example of
reinforcement learning is to train a
machine that can identify the shape of
an object given a list of different
objects such as square triangle
rectangle or a circle in the example
shown the model tries to predict the
shape of the object which is a square
here now let's look at the different
machine learning algorithms that come
under these learning techniques
some of the commonly used supervised
learning algorithms are linear
regression logistic regression support
Vector machines K nearest neighbors
decision tree random forest and knife
base examples of unsupervised learning
algorithms are key means clustering
hierarchical clustering DB scan
principal component analysis and others
choosing the right algorithm depends on
the type of problem you're trying to
solve some of the important
reinforcement learning algorithms are Q
learning
Monte Carlo sarsa and deep Q Network now
let's look at the approach in which
these machine learning techniques
work so supervised learning takes
labeled inputs and Maps it to known
outputs which means you already know the
target
variable unsupervised learning finds
patterns and understands the trends in
the data to discover the output so the
model tries to label the data based on
the features of the input
data while reinforcement learning
follows trial and error method to get
the desired solution after accomplishing
a task the agent receives an award an
example could be to train a dog to catch
the ball if the dog learns to catch a
ball you give it a reward such as a
biscuit now let's discuss the training
process for each of these learning
methods so supervised learning methods
need external supervision to train
machine learning models and hence the
name supervised they need guidance and
additional information to return the
result unsupervised learning techniques
do not need any supervision to train
models they learn on their own and
predict the output similarly
reinforcement learning methods do not
need any supervision to train machine
learning models and with that let's
focus on the types of problems that can
be solved using these three types of
machine learning techniques so superise
learning is generally used for
classification and regression problems
we'll see the examples in the next slide
and unsupervised learning is used for
clustering and Association problems
while reinforcement learning is
reward-based so for every task or for
every step completed there will be a
reward received by the agent and if the
task is not achieved correctly there
will be some penalty
used now let's look at a few
applications of supervised unsupervised
and reinforcement
learning as we saw earlier supervised
learning are used to solve
classification and regression problems
for example You can predict the weather
for a particular day based on humidity
precipitation wind speed and pressure
values you can use supervised learning
algorithms to forecast sales for the
next month or the next quarter for
different products similarly you can use
it for stock price analysis or
identifying if a cancer cell is
malignant or
benign now talking about the
applications of unsupervised learning we
have customer segmentation So based on
of behavior likes dislikes and interests
you can segment and cluster similar
customers into a group another example
where unsupervised learning algorithms
are used as customer churn
analysis now let's see what applications
we have in reinforcement learning so
reinforcement learning algorithms are
widely used in the gaming Industries to
build games it is also used to train
robots to perform human tasks profit
estimation of a company if I was going
to invest in a company I would like to
know how much money I could expect to
make so we'll take a look at a venture
capitalist firm and try to understand
which companies they should invest in so
we'll take the idea that we need to
decide the companies to invest in we
need to predict the profit the company
makes and we're going to do it based on
the company's expenses and even just a
specific expense in this case we have
our company we have the different
expenses so we have our R&D which is
your research and development we have
our marketing uh we might have the
location
we might have what kind of
administration it's going through based
on all this different information we
would like to calculate the profit now
in actuality there's usually about 23 to
27 different markers that they look at
if they're a heavy duty investor we're
only going to take a look at one basic
one we're going to come in and for
Simplicity let's consider a single
variable R&D and find out which
companies to invest in based on that so
when we take a R&D and we're plotting
The Profit based on the R&D expenditure
how much money they put into the
research and development and then we
look at the profit that goes with that
we can predict a line to estimate the
profit so we draw a line right through
the data when you look at that you can
see how much they invest in the R&D is a
good marker as to how much profit
they're going to have we can also note
that companies spending more on R&D make
good profit so let's invest in the ones
that spend a higher rate in their R&D
what's in it for you first we'll have an
introduction to machine learning
followed by Machine learning algorithms
these will be specific to linear
regression and where it fits into the
larger model then we'll take a look at
applications of linear regression
understanding linear regression and
multiple linear regression finally we'll
roll up our sleeves and do a little
programming in use case profit
estimation of companies let's go ahead
and jump in let's start with our
introduction to machine learning along
with some machine learning algorithms
and where that fits in with learning
your regression let's look at another
example of machine learning based on the
amount of rainfall how much would be the
crop yield so we here here we have our
crops we have our rainfall and we want
to know how much we're going to get from
our crops this year so we're going to
introduce two variables independent and
dependent the independent variable is a
variable whose value does not change by
the effect of other variables and is
used to manipulate the dependent
variable it is often denoted as X in our
example rainfall is the independent
variable this is a wonderful example cuz
you can easily see that we can't control
the rain but the rain does control the
crop so we talk about the independent
variable controlling the dependent
variable let's Define dependent variable
as a variable whose value change when
there is any manipulation the values of
the independent variables it is often
denoted as why and you can see here our
crop yield is dependent variable and it
is dependent on the amount of rainfall
received now that we've taken a look at
a real life example let's go a little
bit into the theory and some definitions
on machine learning and see how that
fits together with linear regression
numerical and categorical values use
let's take our data coming in and this
is kind of random data from any kind of
project we want to divide it up into
numerical and categorical so numerical
is numbers age salary height where
categorical would be a description the
color a dog's breed gender categorical
is limited to very specific items where
numerical is a range of information now
that you've seen the difference between
numerical and categorical data let's
take a look at some different machine
learning definitions when we look at our
different machine learning algorithms we
can divide them into three areas
supervised
unsupervised reinforcement we're only
going to look at supervised today
unsupervised means we don't have the
answers and we're just grouping things
reinforcement is where we give positive
and negative feedback to our algorithm
to program it and it doesn't have the
information till after the fact but
today we're just looking at supervised
cuz that's where linear regression fits
in in supervised data we have our data
already there and our answers for a
group and then we use that to program
our model and come up with an answer the
two most common uses for that is through
the regression and classification now
we're doing linear regression so we're
just going to focus on the regression
side and in the regression we have
SIMPLE linear regression we have
multiple linear regression and we have
polom linear regression now on these
three simple linear regression is the
examples we've looked at so far where we
have a lot of data and we draw a
straight line through it multiple linear
regression means we have multiple
variables remember where we had the
rainfall and the crops we might add
additional variables in there like how
much food do we give our crops when do
we Harvest them those would be
additional information add into our
model and that's why it' be multiple
linear regression and finally we have
polinomial linear regression that is
instead of drawing a line we can draw a
curved line through it now that you see
where regression model fits into the
machine learning algorithms and we're
specifically looking at linear
regression let's go ahead and take a
look at applications for linear
regression let's look at a few
applications of linear regression
economic growth used to determine the
economic growth of a country or a state
in the coming quarter can also be used
to predict the GDP of a country product
price can be used to predict what would
be the price of a product in the future
we can guess whether it's going to go up
or down or should I buy today housing
sales to estimate the number of houses a
builder would sell and what price in the
coming months score predictions Cricut
fever to predict the number of runs a
player would score in the coming matches
based on the previous performance I'm
sure you can figure out other
applications you could use linear
regression for so let's jump in and
let's understand linear regression and
dig into the theory understanding linear
regression linear regression is the
statistical model used to predict the
relationship ship between independent
and dependent variables by examining two
factors the first important one is which
variables in particular are significant
predictors of the outcome variable and
the second one that we need to look at
closely is how significant is the
regression line to make predictions with
the highest possible accuracy if it's
inaccurate we can't use it so it's very
important we find out the most accurate
line we can get since linear regression
is based on drawing a line through data
we're going to jump back and take a look
at some ukian geometry the simplest form
of a simple linear regression equation
with one dependent and one independent
variable is represented by y = m * x + C
and if you look at our model here we
plotted two points on here uh X1 and y1
X2 and Y2 y being the dependent variable
remember that from before and X being
the independent variable so y depends on
whatever AIS m in this case is the slope
of the line where m equals the
difference in the Y 2us y1 and X2 - X1
and finally we have C which is the
coefficient of the line or where happens
to cross the zero axis let's go back and
look at an example we used earlier of
linear regression we're going to go back
to plotting the amount of crop yield
based on the amount of rainfall and here
we have our rainfall remember we can
cannot change rainfall and we have our
crop yield which is dependent on the
rainfall so we have our independent and
our dependent variables we're going to
take this and draw a line through it as
best we can through the middle of the
data and then we look at that we put the
red point on the y axis is the amount of
crop yield you can expect for the amount
of rainfall represented by the Green Dot
so if we have an idea what the rainfall
is for this here and what's going on
then we can guess how good our crops are
going to be and we've created a nice
line right through the middle to give us
a nice mathematical formula let's take a
look and see what the math looks like
behind this let's look at the intuition
behind the regression line now before we
dive into the math and the formulas that
go behind this and what's going on
behind the
scenes I want you to note that when we
get into the case study and we actually
apply some python script that this math
that you're going to see here is already
done automatically for you you don't
have to have it memorized it is however
good to have an idea what's going on so
if people reference the different terms
you'll know what they're talking about
let's consider a sample data set with
five rows and find out how to draw the
regression line we're only going to do
five rows because if we did like the
rainfall with hundreds of points of data
that would be very hard to see what's
going on with the mathematics so we'll
go ahead and create our own two sets of
data and we have our independent
variable X and our dependent variable Y
and when X was 1 we got yal 2 when X was
uh 2 y was 4 and so on and so on if we
go ahead and plot this data on a graph
we can see how it forms a nice line
through the middle you can see where
it's kind of grouped going upwards to
the right the next thing we want to know
is what the means is of each of the data
coming in the X and the Y the means
doesn't mean anything other than the
average so we add up all the numbers and
divide by the total so 1 + 2 + 3 + 4 + 5
over 5 = 3 and the same for y we get
four if we go ahead and plot the means
on the graph we'll see we get 3 comma
four which draws a nice line down the
middle a good estimate here we're going
to dig deeper into the math behind the
regression line now remember before I
said you don't have to have all these
formulas memorized or fully understand
them even though we're going to go into
a little more detail of how it works and
if you're not a math whz and you don't
know if you've never seen the sigma
character before which looks a little
bit like an e that's opened up that just
means summation that's all that is so
when you see the sigma character it just
means we're adding everything in that
row and for computers this is great
because as a programmer you can easily
iterate through each of the XY points
and create all the information you need
so in the top half you can see where
we've broken that down into pieces and
as it goes through the first two points
it computes the squared value of x the
squared value of y and x * Y and then it
takes all of X and adds them up all of Y
adds them up all of x^2 adds them up and
so on and so on and you can see we have
the sum of equal to 15 the sum is equal
to 20 all the way up to x * Y where the
sum equals 66 this all comes from our
formula for calculating a straight line
where y equals the slope * X plus the
coefficient C so we go down below and
we're going to compute more like the
averages of these and we're going to
explain exactly what that is in just a
minute and where that information comes
from is called the square means error
but we'll go into that in detail in a
few minutes all you need to do is look
at the formula and see how we've gone
about Computing it line by line instead
of trying to have a huge set of numbers
pushed into it and down here you'll see
where the slope m equals and on the top
part if you read through the brackets
you have the number of data points times
the sum of x * Y which we computed one
line at a time there and that's just the
60 six and take all that and you
subtract it from the sum of x times the
sum of Y and those have both been
computed so you have 15 * 20 and on the
bottom we have the number of lines times
the sum of X squar easily computed as 86
for the sum minus I'll take all that and
subtract the sum of X2 and we end up as
we come across with our formula you can
plug in all those numbers which is very
easy to do on the computer you don't
have to do the math on a piece of paper
or calculator and you'll get a slope of
6 and you'll get your C coefficient if
you continue to follow through that
formula you'll see it comes out as equal
to 2.2 continuing deeper into what's
going behind the scenes let's find out
the predicted values of Y for
corresponding values of X using the
linear equation where M = 6 and C = 2.2
we're going to take these values and
we're going to go ahead and plot them
we're going to predict them so y = 6 *
or x = 1 + 2.2 = 2.8 so on and so on and
here the Blue Points represent the
actual y values and the brown points
represent the predicted y values based
on the model we created the distance
between the actual and predicted values
is known as residuals or errors the best
fit line should have the least sum of
squares of these errors also known as e
square if we put these into a nice chart
where you can see X and you can see Y
where we actual values were and you can
see y predicted you can easily see where
we take Yus y predicted and we get an
answer what is the difference between
those two and if we square that Yus y
prediction squared we can then sum those
squared values that's where we get the
64 plus the 36 + 1 all the way down
until we have a summation equals 2.4 so
the sum of squared errors for this
regression line is 2.4 we check this
error for each line and conclude the
best fit line having the least e Square
value in a nice graphical representation
we can see here where we keep moving
this line through the data points to
make sure the best fit line has the
least Square distance between the data
points and the regression line now we
only looked at the most commonly used
formula for minimizing the distance
there are lots of ways to minimize the
distance between the line and the data
points like sum of squared errors sum of
absolute errors root mean square error
Etc what you want to take away from this
is whatever form formula is being used
you can easily using a computer
programming and iterating through the
data calculate the different parts of it
that way these complicated formulas you
see with the different summations and
absolute values are easily computed one
piece at a time up until this point
we've only been looking at two values X
and Y well in the real world it's very
rare that you only have two values when
you're figuring out a solution so let's
move on to the next topic multiple
linear regression let let's take a brief
look at what happens when you have
multiple inputs so in multiple linear
regression we have uh well we'll start
with the simple linear regression where
we had y = m + x + C and we're trying to
find the value of y now with multiple
linear regression we have multiple
variables coming in so instead of having
just X we have X1 X2 X3 and instead of
having just one slope each variable has
its own slope attached to it as you can
see here we have M1 m M2 M3 and we still
just have the single coefficient so when
you're dealing with multiple linear
regression you basically take your
single linear regression and you spread
it out so you have y = M1 * X1 + M2 * X2
so on all the way to m to the nth x to
the nth and then you add your
coefficient on there implementation of
linear regression now we get into my
favorite part let's understand how
multiple linear regression works by
implementing it in Python if you
remember before we were looking at a
company and just based on its R&D trying
to figure out its profit we're going to
start looking at the expenditure of the
company we're going to go back to that
we're going to predict his profit but
instead of predicting it just on the R&D
we're going to look at other factors
like Administration costs marketing
costs and so on and from there we're
going to see if we can figure out what
the profit of that company is going to
be to start our coding we're going to
begin by importing some basic libraries
and we're going to be looking through
the data before we do any kind of linear
regression we're going to take a look at
the data see what we're playing with
then we'll go ahead and format the data
to the format we need to be able to run
it in the linear regression model and
then from there we'll go ahead and solve
it and just see how valid our solution
is so let's start with Import in the
basic libraries now I'm going to be
doing this in Anaconda Jupiter notebook
a very popular IDE I enjoy it CU it's
such a visual to look at and so easy to
use um just any ID for python will work
just fine for this so break out your
favorite python IDE so here we are in
our Jupiter notebook let me go ahead and
paste our first piece of code in there
and let's walk through what libraries
we're importing first we're going to
import numpy as NP and then I want you
to skip one line and look at import
pandas as PD these are very common tools
that you need with most of your linear
regression the numpy which stands for
number python is usually denoted as NP
and you have to almost have that for
your SK learn toolbox you always import
that right off the beginning pandas
although you don't have to have it for
your sklearn libraries it does such a
wonderful job of importing data setting
it up into a data frame so we can
manipulate it rather easily and it has a
lot of tools also in addition to that so
we usually like to use the pandas when
we can and I'll show you what that looks
like the other three lines are for us to
get a visual of this data and take a
look at it so we're going to import
matplot library. pyplot as PLT and then
caborn as SNS caborn works with the
matap plot Library so you have to always
import matplot library and then caborn
sits on top of it and we'll take a look
at what that looks like you could use
any of your own plotting libraries you
want there's all kinds of ways to look
at the data these are just very common
ones and the caborn is so easy to use it
just looks beautiful it's a nice
representation that you can actually
take and show somebody and the final
line is the Amber sign matap plot
library in line that is only because I'm
doing an inline IDE my interface in the
Anaconda Jupiter notebook requires I put
that in there or you're not going to see
the graph when it comes up let's go
ahead and run this it's not going to be
that interesting because we're just
setting up variables in fact it's not
going to do anything that we can see but
it is importing these different
libraries and setup the next step is
load the data set and extract
independent and dependent variables now
here in the slide you'll see companies
equals pd. read CSV and it has a long
line there with the file at the end
1,000 companies. CSV you're going to
have to change to fit whatever setup you
have and the file itself you can request
just go down to the commentary below
this video and put a note in there and
simply learn we'll try to get in contact
with you and Supply you with that file
so you can try this coding yourself so
we're going to add this code in here and
we're going to see that I have companies
equals pd. reader CSV and I've changed
this path to match my computer c/s
simplylearn
sl1000 companies. CSV and then below
there we're going to set the x x equals
to companies under the ication and
because this is companies is a PD data
set I can use this nice notation that
says take every row that's what the
colon the first colon is comma except
for the last column that's what the
second part is where we have a colon
minus one and we want the values set
into there so X is no longer a data set
a pandis data set but we can easily
extract the data from our Panda's data
set with this notation and then y we're
going to set equal to the last row well
the question is going to be what are we
actually looking at so let's go ahead
and take a look at that and we're going
to look at the companies. head which
lists the first five rows of data and
I'll open up the file in just a second
so you can see where that's coming from
but let's look at the data in here as
far as the way the pandas sees it when I
hit run you'll see it breaks it out into
a nice setup this is what pandas one of
the things pandas is really good about
is it looks just like an Excel
spreadsheet you have your rows and
remember when we're programming we
always start with zero we don't start
with one so it shows the first five rows
0 1 2 3 4 and then it shows your
different columns R&D spend
Administration marketing spend State
profit it even notes that the top are
column names it was never told that but
pandas is able to recognize a lot of
things that they're not the same as the
data rows why don't we go ahead and open
this file up in a CSV so you can
actually see the raw data so here I've
opened it up as a text editor and you
can see at the top we have R&D spin
comma administr ation comma marketing
spin comma State comma profit carriage
return I don't know about you but I'd go
crazy trying to read files like this
that's why we use the pandas you could
also open this up in an Excel and it
would separate it since it is a comma
separated variable file but we don't
want to look at this one we want to look
at something we can read rather easily
so let's flip back and take a look at
that top part the first five row now as
nice as this format is or I can see the
data to me it doesn't mean a whole lot
maybe you're an expert in business and
Investments and you understand what
$165,300
compared to the administration cost of
$136,800 so on so on helps to create the
profit of
$2,261 83 that makes no sense to me
whatsoever no pun intended so let's flip
back here and take a look at our next
set of code where we're going to graph
it so we can get a better understanding
of our data and what it means so at this
point we're going to use a single line
of code to get a lot of information so
we can see where we're going with this
let's go ahead and paste that into our
notebook and see what we got going and
so we have the visualization and again
we're using SNS which is pandas as you
can see we imported the map plot
library. pyplot is PLT which then the
caborn uses and we imported the caborn
as SNS and then that final line of code
helps us show this in our um inline
coding without this it wouldn't display
and you can display it to a file and
other means and that's the matap plot
library in line with the Amber sign at
the beginning so here we come down to
the single line of code caborn is great
because it actually recognizes the panda
data frame so I can just take the
companies. core for coordinates and I
can put that right into the Seaborn and
when we run this we get this beautiful
plot and let's just take a look at what
this plot means if you look at this plot
on mine the colors are probably a little
bit more purplish and blue than the
original one uh we have the column in
the rows we have R and D spending we
have Administration we have marketing
spending and profit and if you cross
index any two of these since we're
interested in profit if you cross index
profit with profit it's going to show up
if you look at the scale on the right
way up in the dark why because those are
the same data they have an exact
correspondence so R&D spending is going
to be the same as R&D spending and the
same thing with Administration cost so
right down the middle you get this dark
row or dark um diagonal row that shows
that this is the highest corresponding
data that's exactly the same and as it
becomes lighter there's less connections
between the data so we can see with
profit obviously profit is the same as
profit and next it has a very high
correlation with R&D spending which we
looked at earlier and it has a slightly
less connection to marketing spending
and even less to how much money we put
into the administration so now that we
have a nice look at the data let's go
ahead and dig in and create some actual
useful linear regression model so that
we can predict values and have a better
profit now that we've taken a look at
the visualization of this data we're
going to move on to the next step
instead of just having a pretty picture
we need to generate some hard data some
hard values so let's see what that looks
like we're going to set up our linear
regression model in two steps the first
one is we need to prepare some of our
data so it fits correctly and let's go
ahead and paste this code into our
Jupiter notebook and what we're bringing
in is we're going to bring in the
sklearn pre-processing where we're going
to import the label encoder and the one
hot encoder to use the label encoder
we're going to create a variable called
label encoder and set it equal to
capital L label capital E encoder this
creates a class that we can reuse for
transferring the labels back and forth
now about now you should ask what labels
are we talking about let's go take a
look at the data we processed before and
see what I'm talking about here if you
remember when we did the companies. head
and we printed the top five rows of data
we have our columns going across we have
column zero which is R&D spending column
one which is Administration column two
which is marketing spending and column
three is State and you'll see under
State we have New York California
Florida now to do a linear regression
model it doesn't know how to process New
York it knows how to process a number so
the first thing we're going to do is
we're going to change that New York
California and Florida and we're going
to change those to numbers that's what
this line of code does here x equals and
then it has the colon comma 3 in
Brackets the first part the colon comma
means that we're going to look at all
the different rows so we're going to
keep them all together but the only row
we're going to edit is the third row and
in there we're going to take the label
coder and we're going to fit and
transform the X also the third row so
we're going to take that third row we're
going to set it equal to a
transformation and that transformation
basically tells it that instead of
having a uh New York it has a zero or
one or a two and then finally we need to
do a one hot encoder which equals one
hot encoder categorical features equals
three and then we take the X and we go
ahead and do that equal to one hot
encoder fit transform X to array this
final transformation preps our data for
us so it's completely set the way we
need it as just a row of numbers even
though it's not in here let's go ahead
and print X and just take a look what
this data is doing you'll see I have an
array of arrays and then each array is a
row of numbers and if I go ahead and
just do row zero you'll see I have a
nice organized row of numbers that the
computer now understands we'll go ahead
and take this out there because it
doesn't mean a whole lot to us it's just
a row of numbers next on setting up our
data we have avoiding dummy variable
trap this is very important why because
the computer's automatically transformed
our header into the setup and it's
automatically transformed all these
different variables so when we did the
encoder the encoder created two columns
and what we need to do is just have the
one because it has both the variable and
the name that's what this piece of code
does here let's go ahead and paste this
in here and we have xal X colon comma 1
colon all this is doing is removing that
one extra column we put in there when we
did our one hot encoder and our label en
coding let's go ahead and run that and
now we get to create our linear
regression model and let's see what that
looks like here and we're going to do
that in two steps the first first step
is going to be in splitting the data now
whenever we create a uh predictive model
of data we always want to split it up so
we have a training set and we have a
testing set that's very important
otherwise we'd be very unethical without
testing it to see how good our fit is
and then we'll go ahead and create our
multiple linear regression model and
train it and set it up let's go ahead
and paste this next piece of code in
here and I'll go ahead and shrink it
down a size or two so it all fits on one
line so from the sklearn module
selection we're going to import train
test split and you'll see that we've
created four completely different
variables we have capital x train
capital X test smaller case y train
smaller case y test that is the standard
way that they usually referen these when
we're doing different uh models usually
see that a capital x and you see the
train and the test and the lowercase Y
what this is is X is our data going in
that's our R&D spin our Administration
our Market marketing and then Y which
we're training is the answer that's the
profit because we want to know the
profit of an unknown entity so that's
what we're going to shoot for in this
tutorial the next part train test split
we take X and we take y we've already
created those X has the columns with the
data in it and Y has a column with
profit in it and then we're going to set
the test size equals 0 2 that basically
means 20% So 20% of the rows are going
to be tested we're going to put them off
to the side so since we're using a th000
lines of data that means that 200 of
those lines we're going to hold off to
the side to test for later and then the
random State equals zero we're going to
randomize which ones it picks to hold
off to the side we'll go ahead and run
this it's not overly exciting it setting
up our variables but the next step is
the next step we actually create our
linear regression model now that we got
to the linear regression model we get
that next piece of the puzzle let's go
ahead and put that code in there and
walk through it so here we go we're
going to paste it in there and let's
let's go ahead and since this is a
shorter line of code let's zoom up there
so we can get a good look and we have
from the SK learn. linear model we're
going to import linear regression now I
don't know if you recall from earlier
when we were doing all the math let's go
ahead and flip back there and take a
look at that do you remember this where
we had this long formula on the bottom
and we were doing all this suiz and then
we also looked at setting it up with the
different lines and then we also looked
all the way down to multiple linear
regression where we're adding all those
formulas together all of that is wrapped
up in this one section so what's going
on here is I'm going to create a
variable called regressor and the
regressor equals the linear regression
that's a linear regression model that
has all that math built in so we don't
have to have it all memorized or have to
computed individually and then we do the
regressor do fet in this case we do XT
train and Y train because we're using
the training data X being the data in
and Y being profit what we're looking at
and this does all that math for us so
Within one click and one line we've
created the whole linear regression
model and we fit the data to the linear
regression model and you can see that
when I run the regressor it gives an
output linear regression it says copy x
equals True Fit intercept equals true in
jobs equal one normalize equals false
it's just giving you some general
information on what's going on with that
regressor model now that we've created
our linear regression model let's go
ahead and use it and if you remember we
kept a bunch of data aside so we're
going to do a y predict variable and
we're going to put in the X test and
let's see what that looks like scroll up
a little bit paste that in here
predicting the test set results so here
we have y predict equals regressor do
predict X test going in and this gives
us y predict now because I'm in Jupiter
in line I can just put the variable up
there and when I hit the Run button
it'll print that array out I could have
just as easily done print y predict so
if you're in a different IDE this not an
inline setup like the Jupiter notebook
you can do it this way print why predict
and you'll see that for the 200
different test variables we kept off to
the side it's going to produce 200
answers this is what it says the profit
are for those 200 predictions but let's
don't stop there let's keep going and
take a couple look we're going to take
just a short detail here and calculating
the coefficients and the intercepts this
gives us a quick flash at what's going
on behind the line we're going to take a
short detour here and we're going to be
calculating the coefficient and
intercepts so you can see what those
look like what's really nice about our
regressor we created is it already has a
coefficients for us and we can simply
just print regressor do coefficient
uncore when I run this you'll see our
coefficients here and if we can do the
regressor coefficient we can also do the
regressor intercept and let's run that
and take a look at that this all came
from the multiple regression model and
we'll flip over so you can remember
where this is going into and where it's
coming from you can see the formula down
here where y = M1 * X1 + M2 * X2 and so
on and so on plus C the coefficient so
these variables fit right into this
formula y = slope 1 * column 1 variable
plus slope 2 * column 2 variable all the
way to the m into the n and x to the N
plus C the coefficient or in this case
you have minus
8.89 to the power of 2 etc etc times the
First Column and the second column and
the third column column and then our
intercept is the minus
10309 Point boy it gets kind of
complicated when you look at it this is
why we don't do this by hand anymore
this is why we have the computer to make
these calculations easy to understand
and calculate now I told you that was a
short detour and we're coming towards
the end of our script as you remember
from the beginning I said if we're going
to divide this information we have to
make sure it's a valid model that this
model works and understand how good it
works so calculating the r s value
that's where we're we're going to use to
predict how good our prediction is and
let's take a look at what that looks
like in code and so we're going to use
this from SK learn. metrics we're going
to import R2 score that's the R squar
value we're looking at the error so in
the R2 score we take our y test versus
our y predict y test is the actual
values we're testing that was the one
that was given to us so we know are true
the Y predict of those 200 values is
what we think it was true and when we go
ahead run this we see we get a
9352 that's the R2 score now it's not
exactly a straight percentage so it's
not saying it's 93% correct but you do
want that in the upper 90s oh and higher
shows that this is a very valid
prediction based on the R2 score and if
r squar value of 91 or 92 as we got on
our model remember it does have a random
generation involved this proves the
model is a good model which means
success yay we successfully trained our
model with certain predictors and
estimated the profit of the companies
using linear regression why linear
regression now that we have come this
far I'm planning to take a closer look
at our sales so that we can estimate
sales in the future how about we hire a
data scientist you can see our two uh
corporate individuals looks more like
they should be agents or secret agents
someplace discussing how to better
forward their company and so they're
going to come in and ask the data
scientist to come in good idea this will
help us to keep a constant track of our
CS so why do we need linear regression
let's assume that we need to predict the
number of skiers based on snowfall so
this happens to be uh we've kind of
jumped one business to the next we're
looking at the skiing business very
popular in a lot of areas and it's based
on snowfall a lot of times you figure
you don't have snow you don't have
skiers but can we actually use something
more specific instead of look it's
snowing and instead of saying Hey look
it's snowing we can actually start
drawing graphs and the graph shows that
with an increased in snowfall the
frequency of skiers also increases so
and there's a pretty direct correlation
if you've ever been up to ski areas when
there's a lot of snowfalls the skiers
all show up because they know it's going
to be better skiing right afterward so
it's kind of easy to see why skiers and
snowfall would go together and usually
draws a nice straight line and you can
easily predict how many skiers and that
way you can also predict how many people
you need to service them how many lifts
to have up and running and all the stuff
that goes with running a ski area thus
we see that the number of skiers are
directly proportional to the mount of
snowfall regression models a relation
between a dependent Y and an independent
X variable this is real important to
understand regression because when we
talk about linear regression it's the
basis of almost all our machine learning
algorithms out there and it's usually an
underlying part of the math in our deep
learning so it all starts here with
linear regression and we talk about a
dependent Y and an independent X
variable these are numbers we're usually
talking about floats so we want to have
an actual number value value coming out
and that's different than something
that's categorical where we want to know
yes or no true false so regression means
we're looking for a number the
independent variable is known as
predictor variable and dependent
variable is known as the respon variable
so we have one predictor variable the
amount of snowfall and one response
variable how many skiers are going to
show up and then we can take this
relationship and it can be expressed as
y equals beta we're going to use the
Greek um letter beta and we have beta KN
plus beta 1 X1 and if you continue out
it'd be plus beta 2 X2 and so on till
the nth degree in this case snowfall is
an independent variable and skars is a
dependent variable so we kind of had a
little quick overview let's go ahead and
talk a little bit more about what is
linear regression so we're going to go
from y predicting number of skiers for
snowfall and uh we'd looked at the
formula a little bit but let's look a
bit closer at what exactly is going on
with linear regression and the question
is going to come up at an interview what
is linear regression linear regression
is a type of statistical analysis that
attempts to show the relationship
between two variables linear regression
creates a predictive model on any data
showing Trends in data the model is
found by using the least Square method
there are other methods the least Square
method happens to be the most commonly
used out there and we usually start with
the least Square method since it's the
most common and usually works the best
on most models and we're already looking
at how linear regression works but let's
dig deeper into it and let's take a
closer look how linear regression works
I will provide you with a data set which
has rent area other information in it
looks like our secret agents have uh put
on their casual wear for this one you
need to predict rent accordingly we are
given area and rent here so you can do
linear regression with more variables
but we're just going to look at the area
and then from that try to figure out
what the rent should be we plot the
graph and if you look at it here you can
see that the graph kind of a linear
pattern with a little dip in it so the
area seems the rent seems to be based on
the area in most of our work as data
scientists we usually try to start with
a physical graph if we can so we can
actually look at it and say hey does
what I'm fitting to this graph look
right you can solve a lot of problems
that arise by looking at the graph and
saying no that doesn't look right at all
or oh I should be looking over here for
this then we find the mean of area and
rent so the mean just means average and
if you take 1 2 3 4 5 and add them all
together and divide by in this case
there's five areas you get three and the
rent is 2 4 6 5 and eight if you add
them all together and divide by five you
get five and we plot the mean on the
graph so you can see right here here's
the mean of the rent and the area it's
kind of a very Central Point which is
what we're looking for the average of
everything in there the best fit line
passes through the mean this is real
important when you're um eyeballing it
as humans we can do this fairly easy if
it's a straight line through a bunch of
data it get more complicated
mathematically when you start adding
multiple variables and instead of a line
you have a curve but for basic linear
regression we're going to draw a line
through the data and the line should go
through the means that's going to be
part of the best fit for that line but
we see there are multiple lines that can
pass through the means so depending on
how you look at it you can kind of
Wiggle the line around back and forth so
we keep moving the line to ensure that
the best fit line has the least Square
distance from the data points and this
is you can see right here residual we
have have our data point and then we
look at this distance between them and
we square that distance and that's what
they mean by the least squared distance
we want all those distances to add up to
the smallest amount we can when we talk
about that distance we call it the
residual it equals the Y actual minus
the Y predicted very straightforward
just looking at the distance you can
accidentally switch these and it'll come
out the same because we're going to
square it but when you're working with
other sets of linear regression you want
to make sure you do the Y actual minus
the Y predicted the value of M and C for
the best fit line y = mx + C can be
calculated using the mentioned formula
and you can see here we have m equals
the number of points that's what n
stands for then we have the sigma the
Greek symbol is there which is a
summation of x * y minus the summation
of x * the summation of Y over the
number of the summation of x^2 minus the
summation of X2 of X all of it squared
that can be a little confusing trying to
say that and then of course your C value
is going to be the summation of y * the
summation of x^2 minus the summation of
x * the summation of x * y over the
total count of the summation x^2 minus
the summation of x^ squar that's a
mouthful normally we don't worry too
much about these formulas other than to
know they're there now if you're a
mathematician you might go back in there
and work on those and those people who
originally started to put together the
different models in R of course had to
know all this math to build them and had
to test it out that's one of the nice
things about R what is important is that
you know about these formulas so that
you know where it's coming from so if
you're using one linear regression model
this is how this one works there's other
linear regression models based on
different math and so you'd have to be
aware when that math changes and how
that changes the model we find the
corresponding values so in this case if
you're going to break this up and you're
building your own model instead of using
the one in R you would have your rent
you would have your area you would find
your rent squared your area squared and
your rent times area and if we go back
one we can see that that's all the
different little pieces in this model we
have x * y we have X2 we have the sum of
X which is going to be squared we have
the sum of X the sum of Y this is the
same these are all the little pieces in
here that we see for m equals and C
equals and so once we compute all these
we have 15 25 55 145 88 very easy to do
on a computer thankfully you could have
millions of these points and it would do
it in a very short period we can then
plug in those those values very easily
into this formula and get a final answer
and we'll see that m equal 1.3 and C =
1.1 and then when we take this for the Y
predicted equals m of i x of I + C we
can take these and actually run the data
we have so we're going to see how it
predicts so now we find the Y the value
of y predicted so we have our xn and we
want to know what Y what we think y will
be based on our formula we generated the
linear regression formula and so we come
in here we have our X we have our actual
y then we have our y predict what we
think it's going to be and then we have
uh we take y minus y predict and we get
those values minus point4 it's off by3
this one's off by one this one's on off
by minus 1.3 by point4 and we go ahead
and square that and then we can sum that
square and average it out and we'll get
a value this is just a summation down
here of y - y predicted square is 3.1
and we call that the least Square value
for this line is 3.1 and we go ahead and
divide that by five so when we plot the
Y predict and this is the best fit line
and you can see that does a pretty good
job going right through the middle of
lines and in something like rent versus
area if you're trying to figure out how
much rent to charge or how many people
are allowed to be in the area that's
going to work but if you're looking for
the rent value compared to the area this
gives you a good idea based on the area
of what you should rent the place for
it's close enough that in the business
bus world this would work you're not
Computing something down to the
millimeters or micrometers or Nuclear
Physics we're just charging people and
so if you're off by a couple dollars
it's not a big deal you'll be pretty
close to what you think it's worth and
get the right value for your property
use case predicting the revenue using
linear regression now that you have a
good idea of how linear regression works
and we're kicking back on our lounging
sofa today let's work on a real life
scenario where you have to predict the
revenue so we're going to take a data
set and we'll pull this up in just a
minute we'll have uh paid organic social
and revenue there are three attributes
we'll be working on the first one is our
paid traffic so all the traffic through
the advertisement that comes in the
non-paid traffic from search engines so
these are people coming to the website
and are doing so because they did a
search for something specific so it's
pulling it from your website all the
traffic coming from social networking
sites so we want to know what's coming
from Twitter and Facebook and somehow
they've linked into your website and
coming into your marketing area to buy
something and we'll use this to predict
the revenue so we want to know how all
this traffic comes together and how it's
going to affect our cells if the
traffic's coming in we're going to make
use of the multiple linear regression
model so you'll see that this is very
similar to we had before where we had y
= m * x + C but instead we have y = M1
X1 + M2 X2 plus M3 X3 plus c m is the
slope so you have M1 M2 M3 the three
different slopes of three different
lines Y is the revenue so we're only
looking for one variable and remember
this is regression so we're looking for
a number X1 is going to be the paid
traffic and of course M1 the
corresponding slope X2 the organic
traffic and X3 the social traffic we
will follow these steps to create the
regression model we'll start by
generating inputs using the CSV files
we'll then import the libraries
splitting the data into train and test
very important whenever you're doing any
kind of data science applying regression
on paid traffic organic traffic social
traffic so we're going to apply our
regression model on them and then we're
going to validate the model that's the
whole reason we split it to a training
and testing is so we can now validate to
see just how good our model is now
before we open up our I always like to
just take a quick look at the data
itself and we're using the rev. CSV file
and this is a CS file or comma separated
variable file we'll just open that with
word pad any kind of text editor would
be nice to show you what's going on and
you can see we have paid organic social
and revenue across the top says your
your titles for your columns and the
values going down and this is the same
as what we saw on the slide just a
minute ago paid organic social and then
revenue from them they probably have it
in more of a spreadsheet than just a
word pad and I'm using the newer version
of our studio which already has a nice
layout automatically opens up with your
console and terminal window down and
left you have your script and we'll run
it in script and then when I run it line
by line in script you'll see it show up
on the console bottom left I could type
it straight into the console and it
would do the same thing and then on the
right we have our environment and under
the environment we'll see uh plots if
you're in an older version the plots
will pop up and you usually are running
just in the console and then you have to
open up a different window to run the
script and you can run the script from
there this opens up everything at the
same time when you use the new R Studio
setup and the first thing we want to do
is we want to go ahead and generate a
variable we'll call it cells and we want
to load this with our data set and so uh
in R we use the less than minus sign
that's the same as assigned to it's like
an arrow pointing at cells and it says
whatever it goes past here is going to
be assigned to cells and we want to read
in our CSV file and then I went ahead
and uh fixed the CSV file up a little
bit when I say the CSV file I mean the
path to it um and we'll go ahead and
just paste that in there and this is my
edited version and you'll see I took
wherever we had the backward slash I
switched it for a forward slash because
a backward slash is an escape character
and whenever you're programming in just
about any scripting language you want to
do the forward slash instead so uh like
a good Chef I prepped this and copied it
into my clipboard and we can see the
full path here and the full path ends in
our rev. CSV file so that's the path on
my machine and then I'm going to run
that and now we have down here you see
it appear right here uh oh must have an
error in my path it says object rev not
found let me fix that real quick and run
it again and this time it does assign
the cells the uh data set and then we'll
just type in cells and this is the same
as printing it so if you work in other
languages you'll do like print and then
cells or maybe print cells. head if
you're in pandas and python or something
like that but in R you just type in the
variable we'll run this and you can see
it printed out all those column so
that's the same thing we just looked at
and it says uh Max print so it emitted
750 rows so it has a total of a th000
rows and we can scroll up here on our
console where the answer came in and if
I scroll up to the top whoop wait up to
the top there we go you'll see paid
organic social and revenue so everything
looks good on this and then there's a
lot of cool things we can do with it one
of the things we can do is I can summary
it so let's do a summary of sales and
we'll run that and the summary of sales
comes in it tells you your Min first
quarter median mean third quarter Max
for paid organic social breaks it up and
you can actually get kind of a quick
view as to what the data is doing you
can take more time to study this a lot
of people look at this and you can see
from the median the mean that tells you
a lot and is divided into quarters so
you have your first quarter value and
your third quarter setup on there and
Max it just gives you a quick idea of
what's going on in the data summary is a
really wonderful tool one of the reasons
I like to jump into R before I might do
a major project in something else or you
can run everything in our and let's take
a look at the head head of cells and let
me run this and you can see it just
shows the head of our data the first six
rows in this case and it starts with one
important to know other different
systems start with zero and go up R is
one that starts with the first as one
and then if we want to do a plot I'll go
ahead and do a plot cells another really
powerful tool that you can just jump
right into with r once you have your
data in here and you'll see that the
plot comes over on the right hand side
and what it does is it Compares them so
if I look at my paid and my organic and
you cross it two you'll see there's a
nice line down the middle big black line
where it plots the two together let me
just widen this a little bit so we can
see more of it there we go the same
thing with with paid and social paid and
revenue so each one of these has a clear
connection to each other so that's a
good sign we have a nice correlation
between the data and you can just
eyeball that that's a good way to jump
in and explore your data rather quickly
and see what's going on so we'll go
ahead and I'll put a um pound sign there
hashtag and uh splitting the data into
training and test data and we want to
split it so that we have something to
work with to train our dat data set and
when we do our data set you do this with
any data set because you want to make
sure you have a good model and then once
we've trained it we're going to take the
other set of data we pulled off to the
side and we'll test it to see how good
it is if you test it with the training
data it's already got that memorized and
should give you a really good answer in
fact when you get into more advanced
machine learning tools you watch those
very closely you watch your training set
and how it does versus your testing set
and if your training set starts doing
better than your testing set then you're
starting to overtrain it now that's more
with deep learning and some other
packages that are beyond what we're
working with today but you know this is
important it's very important to
understand that that test and that
training correlate with each other and
gives you an idea what's going on and
make sure that you're set up correctly
and we'll go ahead and set a seed of two
and that's for our random generators so
that when we run them you could see them
with just a Rand there's ways to
randomize a number you could do like
date and stuff like that but that way we
always use the same random numbers so
we'll seat it with two and then we're
going to need a library in this case
we're going to import the library and we
want to import CA tools and CA tools has
our split function so we're going to be
able to split our data along with many
of the other tools we're going to need
for this example so they're all built
into the ca tools that's why we need to
import that a lot of times they call
them library or sometimes you hear them
referred to as packages so let's go
ahead and run that line and that brings
the library in so now we can use all
those different tools that are in there
and then I'm going to do a split and
we're going to assign to the split
that's going to be our variable sample
split okay so that's one of the tools we
just brought in is to sample is the
actual keyword or the function word
sample dosit and we're going to take our
cells and we're going to go ahead and a
split ratio equals to7 and let's do 0.7
so we make sure it knows it's a decimal
point or float and what this is saying
is that we're going to take our um cells
data the variable we created called
cells that has the data in there and I
want to split it and I'm going to split
it so that 70% of it goes in one side
we'll use that for training and 30% will
go into the other side which we'll use
then for testing out our model Let Me Go
a and run that ands hold on one second
got an error there spit is definitely
very different than split I don't think
we want to spit our data out we which is
actually kind of what we're doing is
spitting it out but we want to split we
want to split the data let me get that
spelled correctly and then when I type
in Split we can just run that cuz that's
now a variable you'll see that it has
true false false true so it generates a
number of interesting different
statistics going across there as far as
the way it splits the data and right
here if you have not used R in a while
or um if you're new to R completely that
line one that's what the little one
means down there and then true false
false true that means that's how we're
looking at the data we're splitting all
those different pieces of data in line
one different directions and so we now
want to create our train set and we're
going to assign that and when we take
and we assign to the train set a subset
of cells and then the subset of cells is
going to be based on split equal and put
this in Brackets true so you can see the
values down here is capital T capital r
capital u capital E so I just want to
reflect that on the train and this is
going to take everything where our split
variable is equal to true and it's going
to set cells equal to that and we'll go
ahead and run that and then we're going
to set our test variable as a subset of
cells and if we assign the true to the
train set then we want to assign the
false to our test set so now we'll have
uh this will have 30% of the variables
in it and train will have 70% we'll go
ahead and run that so we've now created
both our training set and our test set
and we can just real quickly type in
train hit the run on there and you can
see our train set if we scroll way up to
the top I'll have the column names on
there which should match what we had
before paid organic social and revenue
and then we'll type in test when I hit
run on there that's going to do the same
thing it'll sped out all the test
variables going out so now that we have
the test and train variables we want to
go ahead and create the model and you'll
see this in any of the machine learning
tutorials they always refer to them as
models we're modeling a function of some
kind on the data to fit the data and
then we're going to put the the test
data through there to see how well it
does and so we're going to create the
variable called models and I'm going to
assign that LM that is our linear
regression model I love how simplified R
makes it just LM linear regression and
then model M um I guess it dates back
because the linear regression model is
like one of the first major models used
so they kept it easy on there and uh
Revenue happens to be the main variable
that we're want to track so if you
remember correctly from our formula Y is
the revenue and then we have X is our
paid traffic X2 organic traffic and X3
social traffic so Y is what we want to
predict in here and then you'll see this
notation where we have our squiggle and
the period which means we're going to
match up the revenue lines with the
lines of the data we're putting in here
and then I'll put comma and then our
data equals train and of course that's
the training data that we created so
when I hit the Run remember we did all
that discussion about all those
different functions and formulas to
compute our model and how that's set up
when it comes down to it we spend all
our time setting up our data and then I
hit the Run button on the single line
and our model's been trained so we now
have a trained model here and we can do
a summary of the model summary is such a
wonderful command because you can do
that on r that works on all kinds of
different things on there oops and of
course it does help if I remember that
my model has a capital N when I run it
and you'll see right here tells you a
little bit about the model use a summary
on there comes down here has our
residuals this is all the information on
there as far as like the minimum the
median the Max has all our coefficients
in there if you remember correctly from
our formula let's just go ahead and
switch back over to that we have M1 X1
M2 X2 M3 X3 plus C well that's right
here here's our intercept and then we
have our different values for each of
these we have our intercept our paid
organic and social and then it also
shows us error and information on the
error and uh one of the really nice
things about when you're working with r
you can come down here and you see um
where we have our Stars down here and it
says uh three stars really good two star
Stars maybe one star probably no
correlation and we can see with all of
these it has three stars on them so out
of three stars we get three stars in all
of these there's no four stars four
stars would mean that you have the exact
same data going in as coming out and
then if we're going to do this we want
to actually run a prediction on here and
that's that we saved our test uh data
for so let's come down here we'll do our
prediction we'll take this variable and
we'll sign it predict model and remember
the predict comes from when we imported
that package the ca tools that's all
part of the ca Tools in there so we're
going to predict we're going to use the
model and then we're going to use our
test data pretty straightforward quick
and easy we'll run this and then if we
go ahead and type in the predict it'll
print out what's in that variable and
you'll see down here the predicted
values it expects to come out it's going
to see our revenue is and it goes
through and it gives it both the line
number and the actual Revenue value so
that's quick and easy boy we got a
prediction really fast and this is also
how you would do it if you had new data
coming in after this you could predict
the revenue based on these other factors
so now that we have a training model
which or we train the data with our
training data or we trained the model
with our training data and we've done a
prediction on our model with our test
data we want to look this up and we took
a quick glance from our training thing
our training said it trained really well
but that's not the final word on it the
final word is going to be comparing it
so we want to go ahead and we going to
do comparing predicted versus actual
values what I'm going to do is I'm going
to do a plot and I'm going to do our
test and we're going to do test revenue.
type equals and we're going to put this
in as a type uh type one and or L and
then we have lty yal 1.8 and we're going
to set up for column blue that's a lot
of stuff in there let's go ahead and
just run that so we can see what that
looks like and what's that doing it's
going to generate this nice graph we see
on the right there's our graph you can
see the data comes up and down um we
plug plotting the prediction on there so
this is the values we predicted and then
let's go ahead and do uh lines and I
actually did this backwards so let me
try that again and we'll start with the
red one we're going to do the first one
in red and I want to start with the
actual test Revenue so here's our test
revenue and we'll go ahead and run this
and so we have our red plot over here in
red go and take a look at that pull that
over you can see how that looks goes up
and down hard to track there and then
we're going to do a couple things here
we'll plot our test Revenue
and it will make it a little pretty
although it's kind of hard to say see
the way I have it scrunched up here on
the right as far as size and sizing and
everything but we'll go ahead and have
our um columns in blue and run our
prediction on there too you can see they
overlap the two graphs so we have our
test revenue and our prediction and then
finally what I started with is we'll go
ahead and plot the prediction fully on
the bottom and run that and if you look
over here on the graph we put the blue
lines over the red lines and so you'll
see a couple spots where there's red
underneath there but for the most part
our predictions pretty right on so it
looks really tight looks like a good set
of predictions for what we're working on
and this is what we were looking at the
slide right before we started diving
into the r Studio we can see in the
slide here here's a red this a little
bit better spread out than uh what I
have on my screen for my R studio and
the graph shows a predicted Revenue we
see that the two lines are very close so
again they're tight it's right on this
is what we're looking for in our
prediction model but it's not good
enough just having a graph and you
always do both and the reason we do both
is you want to have a visual of it cuz
sometimes you look at these you get to
the graph you're like oh my gosh what is
that and then sometimes you look at the
graph you go that's right on how come my
um accuracy doesn't look right and you
realize that your accuracy might be off
or you're plotting it incorrectly so
let's go ahead and look up the accuracy
and we'll use rmsse that's going to be
the variable name we're going to give it
and sqrt the square root of the mean and
mean just means average and then we want
prediction minus sales revenue and then
we're going to take this whole thing and
we're going to square it so what we've
got here is we're going to go through
let's just explain this formula we're
using is I'm going to look at the what I
predict it for to be and what the actual
sales what's our prediction versus our
sales comparison to the revenue and when
we compare those two we're going to find
the average and then I'm going or we're
going to square each one of those values
and then we're going to average the
square and then find the square root of
that and that's quite a mouthful the
reason we do it this way the reason we
Square the value and then we find the
means is to generate an answer based on
doesn't matter whether it's plus or
minus there's a lot of other formulas
you can use there to check your accuracy
and all kinds of other things you can do
but a quick straightforward way of doing
it is just like this and then let's go
ahead and run
this and then we type in the rmsse and
that'll give us an actual printed value
out let's just see what that looks like
and so we have 86 66.
6338 for our accuracy and so when we
look at the
866.686.1808 everything from the
fundamentals to Advanced Techniques like
machine learning algorithm development
and unsupervised learning look no
further than our CCH postgraduate
program in Ai and machine learning in
partnership with IBM this a ml course
CES the latest tools and Technologies
from the AI ecosystem and features
master classes by celtech faculty and
IBM experts hecaton and ask me anything
sessions this program showcases keltech
ctm is excellence and IBM's industry
progress the artificial intelligence
course covers key Concepts like
statistics data science with python
machine learning deep learning NLP and
reinforcement learning through an
Interactive Learning model with live
sessions and roll now and unlock
exciting a ml opportunities the course
link is mentioned in the description box
below what is logistic regression let's
say we have to build a predictive model
or a machine learning model to predict
whether the passengers of the Titanic
ship have survived or not the Shipwreck
so how do we do that so we use logistic
regression to build a model for this how
do we use logistic regression so we have
the information about the passengers
their ID whether they have survived or
not their class and name and so on and
so forth and we use this information
where we already know whether the person
has survived or not that is the labeled
information and we help the system to
train based on this information based on
this labeled data this is known as
labeled data and during the process of
building the model we probably will
remove some of the non-essential
parameters or attributes here we only
take those attributes which are really
required to make these predictions and
once we train the model we run new data
through it whereby the model will
predict whether the passenger has
survived or not let's start with what is
supervised learning supervised learning
is one of the two main types of machine
learning methods here we use what is
known as labeled data to help the system
learn this is very similar to how we
human beings learn so let's say you want
to teach a child to recognize an apple
how do we do that we never tell the
child okay this is an apple has a
certain diameter on the top certain
diameter at the bottom and this has a
certain RGB color no we just show an
apple to the child and tell the child
this is Apple and then next time when we
show an apple child immediately
recognizes yes this is is an app
supervised learning works very similar
on the similar lines so where does
logistic regression fit into the overall
machine learning process machine
learning is divided into two types
mainly two types there is a third one
called re enforcement learning but we
will not talk about that right now so
one is supervised learning and the other
is unsupervised learning unsupervised
learning uses techniques like clustering
and Association and supervised learning
uses Tech techniques like classification
and regression now supervised learning
is used when you have labeled data you
have historical data then you use
supervised learning when you don't have
labeled data then you used unsupervised
learning it's in supervised learning
there are two types of techniques that
are used classification and regression
based on what is the kind of problem we
are solved let's say we want to take the
data and classify it it could be binary
classification like a zero or a one an
example of classification we have just
seen whether the passenger has survived
or not survived like a zero or one that
is known as binary classification
regression on the other hand is you need
to predict a value what is known as a
continuous value classification is for
discrete values regression is for
continuous values let's say you want to
predict the share price or you want to
predict the temperature that will be the
what will be the temperature tomorrow
that is where you use regression whereas
classification are discrete values is
will the customer buy the product or
will not buy the product will you get a
promotion or you will not get a
promotion I hope you're getting the idea
or it could be multiclass classification
as well let's say you want to build an
image classification model so the image
classification model would take an image
as an input and classify into multiple
classes whether this image is of a cat
or a dog or an elephant or a tiger so
there are multiple classes so not
necessarily binary classification so
that is known as multiclass
classification so we are going to focus
on classification because logistic
regression is one of the algorithms used
for classification now the name may be a
little confusing in fact whenever people
come across logistic regression it
always causes confusion because the name
has regression in it but we are actually
using this for performing classification
okay so yes it is logistic regression
but it is used for classification and in
case you're wondering is there something
similar for regression yes for
regression we have linear regression
keep that in mind so linear regression
is used for regression logistic
regression is used for classification so
in this video we are going to focus on
supervised learning and within
supervised learning we going to focus on
classification and then within
classification we are going to focus on
on logistic regression algorithm so
first of all classification so what are
the various algorithms available for
performing classification the first one
is decision tree there are of course
multiple algorithms but here we will
talk about a few decision trees are
quite popular and very easy to
understand and therefore they used for
classification then we have K nearest
neighbors this is another algorithm for
performing classification and then there
is logistic regression and this is what
we are going to focus on in this video
and we are going to go into a little bit
of details about logistic regression if
you want to become an AI expert and gain
handsome salary packages look at the
wide range of e IML courses by simply
learn in collaboration with top
universities across the globe so what
are you waiting for hurry up and enroll
now an year of experience is prefer to
enroll in these courses find the course
Link in the description box to D and
finally to phase detection system it is
a form of bio metric recognition a
method for identifying or confirming
someone's identity by glancing at their
face is called facial recognition people
can be identified by securing a match on
facial ID using this technique realtime
visuals videos and photos can be the
sources to run phe detection this
technology is mostly employed in
security and law enforcement open CV is
the best technology to create it a
python package called open CV is made
specifically to address computer vision
jobs computer vision is a process used
in the processing of images by computers
it is concerned with the in-depth
comprehension of digital photos or films
it is necessary to automate operations
that can be performed by human visual
systems therefore a computer should be
able to identify items like a statue or
a lamp poost or even the face of a human
being second one is chatbot it is the
best idea if you have chosen chatbot as
your project topic this will make your
resume more attractive in case you
looking for a job an oracle survey
suggests that 80% of the businesses uses
chatbot you can use Python Java Ruby C++
or PHP as a programming language to
develop a chatbot designing and building
NLP chatbots that accept speech and Text
data is made easier by dialogue flow you
can go through many projects to get the
rough idea there are many platforms
which will help you to build a chatbot
regardless of how excellent your chatbot
is there is always room for development
the finest chatboard developers
constantly enhance their bards over the
time using Ai and machine learning
social media recommendation system the
rise of web services like Netflix Amazon
and YouTube has increased the use of
recommended systems in our daily lives
they are algorithms that insist users in
finding information that is pertinent to
them recommenda systems are important in
some firms since they can generate a lot
of income or allow you to set yourself
apart from rivals in order to provide
recommendations it evaluates the
relationship between the user and the
object as well as the parallels between
users and positions coming to predicting
stock this application is widely
applicable everywhere as AI career
aspirant one will love to develop stock
prediction applications as it is full of
data this project would be ideal for
students who want to work in the finance
industry because it it can provide I
repeat because it can provide them a
better understanding of various aspects
of the field coming to medical diagnosis
this project is advantageous from a
medical standpoint AI projects can be
created to detect heart-based diseases
and also detect cancer it is intended to
offer patients with heart illness online
medical advice and guidance after
processing the data this system will
search the database for any illnesses
that might be connected to the given
details using data mining techniques
this intelligent system determines the
disease that the patient information
most closely
resembles based on the system diagnosis
users can then speak with qualified
medical professionals users of the
system can also view information about
various doctors coming to an important
project that is search engine search
engines are utilized by all we look for
information on the greatest product to
buy a nice area to hang out or solutions
to any questions we have MLP is quite
significant in modern search engines
because a lot lot of language processing
takes place there python is the widely
used language to develop any search
engine search engine is mainly confined
with lots and lots of data it is helpful
for any AI career aspirant next is
virtual assistants here the challenge is
to build a virtual assistant to assist
user why do you need virtual assistant
in your devices when you are building
your own it is also interesting for a ml
developer ER to build a virtual
assistant it involves NLP and data
mining voice-based virtual assistants
are popular today because they make life
easier for users NLP is utilized to
comprehend human language in order to
construct this system when a voice
command is received the system will
translate it into machine language and
store the commands in its database hate
speech detection in social media
automated hate speech detection is a
crucial weapon in the fight against hate
speech propagation especially on social
media for the job many techniques have
been developed including a recent
explosion of deep learning based system
for the objective of detecting hate
speech a number of techniques have been
investigated including conventional
classifiers classifiers based on deep
learning and combination of both of them
on the other hand a number of data set
benchmarks in including Twitter
sentiment analysis have been introduced
and made available for the evolution of
the performance of these
algorithms and the last one is
predicting house price you will need to
estimate the sale price of a brand new
home in any place for this assignment
the data set for this project includes
information on the cost of homes in
various City neighborhoods the UCI
machine learning repository is the place
where you may find the data set needed
for This research you will also receive
other data set with information on the
age of the population the city's crime
rate and the location of non- retail
Enterprises in addition to the pricing
of various residences it's an excellent
project to test your knowledge if you
are an up popular market research
projects that the market for artificial
intelligence will grow from 59.7 billion
in 2021 to4 22.4 billion in 2028
artificial intelligence Automation and
Robotics are disrupting almost every
business companies that don't invest in
AI Services risk becoming obsolete
whether in machine learning smart
applications Appliance digital
Assistance or autonomous vles numerous
companies stand to benefit from AI but a
handful of them have proven to be the
game changers for 2023 and Beyond so hey
everyone you are already watching simply
learn and here we are with the list of
top 10 AI companies in 2023 if you love
watching videos like these do hit the
Subscribe button to never miss an update
from Simply learn so let's explore and
learn about them one by one starting
with the 10th position which is uip path
robotic process automation or RPA is a
technology developed by uipath that
helps businesses to boost productivity
and reduce cost by automating timec
consuming and repetitive operations
software robots can be equipped with uip
paath AI Technologies to carry out
duties including reading document and
emails interpreting language and visual
cues and comprehending the content and
the intent of the communications the RPA
Market is estimated to be worth 38
billion and uip paath offers distinct
advantages over the competitors thanks
to its patent computer vision technology
and wide range of B Technologies in the
ninth position we have dinet the
surfaces offered by dinet include obser
availability and infrastructure
monitoring the devis AI engine developed
by the business can analyze 368 trillion
dependencies each second Davis can
quickly locate problems in a company's
digital ecosystem explain what went
wrong and evaluate and rate likely
commercial ramifications the company
estimated a market worth
285.0 million and is po to grow Revenue
by roughly 20% annually throughout at
least 2025 then at the eighth position
we have talented Technologies data
analytics software startup palent
Technologies uses AI to analyze data
according to revenue and market share
palente was ranked as the to AI software
platform globally by IDC in September
2022 the United States intelligence and
defense Industries account for a sizable
portion of palen's Revenue the business
just won a new 85.1 million contract
from the US Army Marshall command to
assist with the developing Ai and
machine learning Technologies to enhance
equipment reliability improve predictive
maintenance and improve Supply chains
palente May deliver revenues closer to
the higher end of analysis estimates
roughly 2.5 billion in 2023 then at the
seventh position is workday workday is a
provider of cloud-based applications
with an emphasis on human resources
management with the assistance of
workday's distinctive AI based
optimization engine businesses can
manage challenges with hiring and
stuffing fluctuation labor demand shift
scheduling and prioritizing and More in
April 2022 workday Incorporated Barista
and AI powered virtual assistant from
espressive into their platform workday
is Raising 2023 markets Worth to almost
5.53 billion representing year-over-year
growth of 22% now in the sixth position
we have intuitive surgical The DaVinci
surgical system offered by intuitive
surgical performs minimally invasive
procedures using Cutting Edge robots and
computerized visualization technology
with the help of a big data and AI
intuitive is developing tools that will
help surgeons by improving their
training and offering them realtime
Direction Des despite the fact that the
D in surgical system has been approved
by usfda for 22 years intuitive reported
a 133% year over-year increase is
installed in the system most recent
quarter the global intuitive surgical
Market is projected to reach $6.5
billion by 2023 moving towards the fifth
position we have IBM for years IBM has
been developing strategies to use its AI
supercomputer who altered the legal
banking academic and Healthcare
Industries IBM's clients harness the
power of AI to stay current on
everchanging regulations by providing
endtoend riskk and compliance management
IBM continues to dominate the market for
the other breakthroughs in Ai and its
Auto Ai and auto AML products can help
data scientist build and improve Ai and
machine learning models the market worth
of IBM in 2023 will range from 14.4
million to 16 billion following that in
the fourth position is NVIDIA high-end
chipmaker Nvidia provides provides the
significant computing power needed for
advanced AI applications in reality one
of the world's fastest supercomputers
Leonardo is powered by Nvidia Graphics
processing units the parent company of
Facebook meta platforms is building the
biggest artificial intelligence superc
computer in the world meta also owns
nvidia's Quantum infy band networking
system and
608a 100 nvda Graphics processing
processors Nvidia is a significant
supplier for fast growing Industries
like highend gaming business Graphics
cloud computing artificial intelligence
and advanced automative technology
nvidia's outlook for 2023 revenue is
expected to be around 6 billion here
comes the top three in the list and in
the third position we have Amazon AI is
integrated into every aspect of Amazon's
business including e-commerce search
engines targeted advertising and Amazon
web services Amazon Alexa one of the
most popular virtual assistant is
already present in many American Homes
Amazon said in August that it would pay
1.7 billion to acquire robots Corp RBT a
maker of Home robots this action can
allow Amazon to boost the use of AI in
household worldwide Amazon expects a 491
billion Market worth between 2023 and
2027 in the second position is alphabet
alphabet the company that owns Google
and YouTube uses Ai and Automation in
practically every part of its business
including ad pricing Gmail spam filters
and content promotion in addition it is
the parent company of the autonomous car
manufacturer Vio and the AI software
business Deep Mind which created history
in 2022 when it became the first
completely autonomous commercial taxi
service to operate on public routes the
market worth of alphabet in 2023 will be
around $70 billion and the first
position on the list is Microsoft in
2020 Microsoft announced the
construction of a new supercomputer
hosted on Azure Microsoft's cloud
computing Network the super Compu was
created in collaboration with open AI to
train AI models producing substantial AI
models and related infrastructure for
programmers and other businesses in
October Microsoft released Microsoft
designer a graphic design tool that
makes original social media post
invitations and other graphic using AI
technology Microsoft the tech giant is
expected to earn a market worth of 49.73
billion and with that we have come to
the end of this list of the top 10
artificial intelligence companies of
2023 opportunities in AI have increased
significantly in recent years the surge
in jobs related to AI is legitimate due
to its widespread involvement in crucial
Fields according to inded the pay range
for jobs involving artificial
intelligence is between
$160,000 to
$350,000 and can go even higher so we
have come up with the list of leading
career opportunities in artificial
intelligence along with companies hiring
for those roles but before we We Begin
here is a question for you who do you
think is currently leading the AI Market
let us know in the comment section below
so without any further delay let's get
started starting with our list we have
robotics engineer an engineer in
robotics create prototypes constructs
and test machines and updates the
software that manages them additionally
they investigate the most affordable and
secure way to make their robotic system
aspirant should have a bachelor degree
in computer science and UE a career in
robotics with additional training or
certification in automation is
advantageous robotics engineers are in
high demand with 9,000 plus job openings
in India and 8,000 plus openings in the
US top companies like Amazon Bosch and
fly base are hiring robotics engineers
with salaries as high as
$160,000 in the US and rupees 27 lakhs
in India next up on our list we have
product manager the the role of a
product manager is to recognize and
express user requirements market
research and creating competitive
evaluations and also creating a
product's vision and putting emphasis on
a product strengths and qualities
freshers cannot join as a product
manager it requires basic experience of
2 to three years in the field of product
and software development according to
india.com an average of 2,000 plus jobs
are currently available in India and
20,000 plus job vacancies are there in
the US with top companies hiring are
Tesla Amazon TCS sprinkler cognizant Etc
with a salary as high as 160,000 per
year in the US and rupees 20 lakhs perom
in India up next we have data scientist
they mainly deal with lots of data
extraction of Knowledge from all
collected data is the subject matter of
a data science it was also voted the
sexiest job of the 21st century good
news is there is no need for any special
degree as a data scientist you need to
have knowledge of machine learning
programming in Python R Java and
mathematical modeling being a fresher
you can join as a data scientist too
data scientists are in demand with
177,000 plus vacancies in the US and
3,000 plus vacancies in India and big
businesses like accenter TCS IBM Google
JP Morgan deoe Bank of America
recruiting data employees with salary
going as high as $200,000 per year in
the US and rupees 15 lakhs perom in
India moving forward in our list we have
ai data analyst data mining data
cleaning data interpretation are the
three main task of an AI data analyst
data cleaning allows for the Gathering
of the necessary information for data
analysis an AI data analyst make
inferences from the data using
statistical tools and techniques you
need a bachelor degree degree in
computer science or mathematics to work
as an AI data analyst to get this job
you must have a thorough understanding
of regression and be able to use msxl
according to indie.com there are
currently 700 plus AI data analyst jobs
available in the United States and
16,000 plus job vacancies in India with
top companies like Accenture IBM Vio TCS
and capam mini recruiting them and
salaries going as high as $100,000
per year in the US and rupees 8.5 lakhs
perom in India the next job opportunity
in AI is business intelligence engineer
a business intelligence developer's main
duty is to take both business s and AI
into account they evaluate complex data
sets to identify various business Trends
they assist in boosting a company's
earnings by planning creating and
sustaining business intelligence
solutions to become a business
intelligence engineer you are required
to have a bachelor Bor degree in
mathematics and computer science
according to ind.com there are 8,000
plus job available in India and 5,000
plus job vacancies in United States
companies hiring are Amazon Development
Center IBM AWS Etc with this salary
going as high as $100,000 per year in
the USA and rupees 15 lakhs per year in
India coming to the top three jobs
available in AI at number three we have
machine learning engineer artificial
intelligence is known to include machine
learning it runs simulations using the
varied data provided and produces
precise results they are constantly in
demand from the businesses and their
position seldom goes unfilled they
handle enormous amount of data and have
exceptional data management skills the
ability to code use computers and
understand mathematics a requirement for
success as a machine learning engineer a
master's degree in computer science or
mathematics is highly preferred the
necessary technological Stacks include
python R Scala and Java a deep
understanding of neural networks deep
learning and machine learning algorithm
is very helpful coming to job
opportunities glass door.com predicts
13,000 plus job vacancies in the US and
27,000 plus jobs available in India with
companies like mathwork Google Amazon
Microsoft azour IBM Etc hiring them with
Sal going as high as 150,000 perom in
the US and rupees 10 lakhs perom in
India coming at runnerup position we
have data architect data Architects are
it Specialists who utilize their
knowledge of computer science and
database architecture to assess and
analyze an organization's data
infrastructure fresher graduates cannot
become data Architects a bachelor degree
in computer science and computer
engineering or related subject is the
absolute minimum requirement to become a
data architect data Architects are in
high demand with 52,000 job vacancies in
the United States and 7,000 plus jobs in
India and top companies hiring data
Architects are Amazon IBM Tesla Intel
Vio with the average salary quite high
as
$200,000 per year in the United States
and rupees 21 lakhs perom in India and
Topping our list of job opportunities in
AI is AI engineer AI Engineers are
problem solvers who create test and use
various artificial intelligence models
they manage the AI infrastructure well
to create practical AI models they use
neural network knowledge and machine
learning methods these models enable one
to gain business insights which AIDS the
organization in making wise business
decisions ug or PG degree in the field
of AI or computer science is required in
addition addition to that you can hold
certifications from any reputed
organization which will add up to your
resume please check out an enroll to the
AI program by simply learn in
collaboration with puru University the
link is provided in the description
below job openings for an AI engineer
are quite high with 35,000 plus job
vacancies in the US and 4,000 plus jobs
in India and Tech giants like Accenture
TCS IBM Google JP Morgan Theo Bank of
America recruiting them with salary
going as high as
$200,000 per year in the US and rupees
10 lakhs perom in India term generative
AI has emerged seemingly out of nowhere
in recent months with a notable search
in interest according to Google Trends
even within the past year the spike in
curiosity can be attributed to the
introduction of generative model such as
d 2 B and
chgb however what does generative Ai and
tail as a part of our introductory
series on generative AI this video will
provide a comprehensive overview of a
subject starting from the basics the
explanation Will C to all levels of
familiarity ensuring that viewers gain a
better understanding of how this
technology operates and its growing
integration to our daily lives
generative AI is after all a tool that
is based on artificial intelligence a
professional who elit seats to switch
careers with AI by learning from the
experts then try giving a short simply
lar postgraduate program in Ai and
machine learning from perd University in
collaboration with IBM the link in the
description box should navigate to the
homepage where you can find a complete
overview of the program being offered
take action up skill and get ahead what
is generative AI generative AI is a form
of artificial intelligence processes the
capability of to generate a wide range
of Conta including text visual audio and
synthetic data the recent excitement
surrounding generative AI stems from the
userfriendly interfaces that allow users
to effortlessly create high quality text
graphics and video within a seconds now
moving forward let's see how does
generative AI Works generative AI begin
a prompt which can take form of text
image video design audio musical noes or
any input that AI system can process
various AI algorithm that generate new
content in response to the given prompt
this content can range from essay and
problem solution to realistic created
using images or audio of a person in the
early stages of generative AI utilizing
the technology involved submitting data
through an API or a complex process
developers need to acquain themselves
with a specialized tool and writing
application using programming language
like python some of the recent and fully
operational generative AI are Google
Bart Dal open AI chgb Microsoft Bing and
many more so now let's discuss chat GPT
D and B which are the most popular
generative AI interfaces so first is d 2
which was developed using open as GPT
implementation in 2021 exemplify a
multimodel AI application it has been
trained on a v data set of images and
their corresponding textual description
Dal is capable of estabilishing
connection between various media forms
such as Vision text audio it
specifically links the meaning of words
to visual elements open a introduced an
enhanced version called d 2 in 2022
which empowers user to generate imagery
in multiple Styles based on their
prompts and the next one is chat GPT in
November 2022 chat GP an AI power
chatbot built on open AI GPD 3.5
implementation gained immense popularity
worldwide open AI enable user to
interact with and F tune the chatbox
text response through a chat interface
with interactive feedback unlike earlier
version of GPT that was solely
accessible via an API CH GPT brought a
more interactive experience on March 14
2023 open a released GPT 4 chat GPT
integrate the conversational history
with a user making a genuine dialogue
Microsoft impressed by the success of
new chgb interface announced a
substantial investment in open Ai and
integrated a version of GPT into its
Bing search engine and the next one is
Bard Google B Google was also an earlier
Fortuner in advancing Transformer AI
techniques for language processing
protein analysis and other content types
it made some of these model open source
for researchers but were not made
available through a public interface in
response to Microsoft integration of GPT
into being Google hardly launched a
public facing chat Bo name Google Bart
bar debut was met by an error when the
language model incorrectly claimed that
the web telescope was the first to
discover a planet in a foreign solar
system as a consequences Google stock
price suffered a significant decline
meanwhile while Microsoft implementation
of chat GPT and GPT power system also
face criticism for producing inaccurate
result and displaying eratic behavior in
their early irritation so moving forward
let's see what are the use cases of
generative AI generative AI has broad
applicability and can be employed across
a wide range of use cases to generate
diverse form of content recent
advancement like GPT have made this
technology more accessible and
customizable for various application
some notable use cases for generative AI
are as follows chatbot implementation
generative AI can be utilized to develop
chatbots for customer service and
Technical Support enhancing interaction
with users and providing efficient
assistance the second one is language
dubbing enhancement in the dam in the
realm of movies and educational content
generative AI can contribute to
improving dubbing in different languages
ensuring accurate and high quality
translation and the third one is content
writing generative AI can assist in
writing email responses dating profiles
resums and term papers offering
valuable support and generating
customized content tailor to specific
requirement and the fourth one is Art
generation leveraging generative AI
artists can create photo realistic
artwork in various Styles enabling the
exploration of new artistic expression
and enhancing creativity the fifth one
is product demonstration videos
generative AI can hun to enhance product
demonstration video making them more
engaging visual appealing and effective
in showcasing product features and
benefits so generative AI versatility
allow it to employ it in many other
application making it a valuable tool
for Content creation and enhancing user
experience across diverse domains so
after seeing use cases of generative AI
let's see what are the benefits of
generative AI so generative AI offers
extensive application across various
business domains simplifying the
interpration and comprehension of
existing content while also enabling the
automated creation of a new content
devel Vel opers are actively exploring
ways to leverage generative AI in order
to enhance the optimize existing
workflows and even to reshape workflows
entirely to harness the potential of
Technology fully implementing generative
AI can bring numerous benefits including
automated content creation generative AI
can automate the manual process of
writing content saving time and effort
by generating text or other form of
content the next one is efficient email
response responding to emails can be
made more efficient with generative AI
reducing the effort required and
improving response time and the third
one is enhanced technical support
generative AI can improve responses to
specific technical queries providing
accurate and helpful information to
users or customers and the fourth one is
realistic person Generation by
leveraging generative AI it becomes
possible to create realistic
representation of people enabling
applications like virtual characters or
avatars and the fifth one one is
coherent information summarization
generative AI can summarize complex
information into a coherent narrative
distilling key points and making it
easier to understand and communicate
complex concept the implementation of
generative AI offers a range of
potential benefits steamingly process
and enhancing content Creation in
various areas of business operation so
after seeing advantages of generative AI
let's move forward and see what are the
limitations of generative AI early
implementation of generative AI serve as
Vivid examples highlighting the numerous
limitation associated with this
technology several challenges arise from
the specific approaches employed to
implement various use cases for instance
while a summary of a complex topic May
more reader friendly than explanation
incorporating multiple supporting
sources the ease of readability comes at
the expense of transparent identifying
the information sources so the first one
is when implementing or utilizing a
generative a application it is important
to consider the following limitation I
repeat the first one is lack of source
identification generative AI does not
always provide clear identification of
content Source making it difficult to
trace and verify origin of the
information the second one is assessment
of bias assessing the bias of original
sources used generative AI can be
challenging as it may be difficult to
determine the underlying perspective or
agenda of the data utilized in the
training process the third one is
difficult in identifying inaccurate
information generative AI can generate
realistic content making identifying
inaccuracy or falsehoods within the
generated output harder and the fourth
one is adaptability to a new
circumstances understanding how to
fine-tune generative AI for a new
circumstances or specific context can be
complex requiring careful consideration
and expertise to achieve desired result
and the fifth one is glossing over buas
pre and hatred generative AI results May
amplify or prep petate biases prejudices
or hateful content present in the
training data requiring Vigilant
scrutiny to prevent such issues so
awareness of these limitation is crucial
when the implementing of utilizing
generative AI as it helps users and
developers critically evaluate and
mitigate potential risk and challenges
associated with the technology so future
of generative AI furthermore advances in
AI development platforms will contribute
to the accelerated progress of research
and development in the realm of
generative AI the development will
Encompass various domains such as text
images videos 3D contact drugs Supply
chains logistic and business processes
while the current stand loan tools are
impressive the true transformative
impact generative AI will realize while
these capabilities are seemingly
integrated into into the existing tools
with regular use hello and welcome to
this video on edge AI for artificial
intelligence if you're interested in
learning more about how AI is
revolutionizing the world around us this
video is for you AI is a New Concept in
the world of AI that refers to devices
and sensors at the edge of the network
that can collect and process data in
real time this allows for faster more
accurate decision making and greater
efficiency in a variety of applications
such as self-driving Vehicles Smart
Homes and health monitoring so how is
Aji different from cloud computing and
Cloud Computing AI the main difference
is the location where the data is
processed AI enables devices at the edge
of the network to process data locally
without having to send it to the cloud
this can lead to faster more responsive
systems and can help you preserve
bandwidth and reduce latency in contrast
cloud computing AI processes data in the
cloud where it can assess vast amount of
data and compute resources but also
requires a reliable internet connection
and can be slower as as Aji becomes more
advanced it has the potential to
transform many Industries such as
Healthcare Transportation manufacturing
and a lot it can enable new applications
such as autonomous vehicles remote
surgery and real-time monitoring of
machines and equipments while cloud
computing AI has many benefits AI offers
the advantages of faster decision-
making reduced latency and improved
efficiency so if you're interested in
learning more about the future of AI Aji
is definitely a topic worth exploring on
that note hello everyone and welcome to
Simply learn in today's video we'll
understand what Edge Computing is and
how it is different from traditional
cloud computing but before we begin make
sure to subscribe to our channel to stay
updated with all the latest Technologies
and hit that Bell icon to never miss any
updates from us so without any further
delay let's get started but wait if you
want to become an expert in Edge
Computing then look no further than
Caltech postgraduate program in a
machine Lear learning offered by simply
learn which is an Onin program designed
to provide students with a comprehensive
understanding of AI and machine learning
Concepts tools and techniques this
program is created in collaboration with
CeX ctme and IBM and it is designed to
provide Learners with a strong
foundation in Ai and and machine
learning Concepts this program covers
various topics such as data science
statistics deep learning computer vision
and NLP and help you gain the right
skill set on various tools such as caras
matb tens oflow Jango and many more
furthermore the curriculum is structured
around interactive online classes live
sessions with industry experts and
Hands-On projects so why wait hurry up
enroll now in Caltech postgraduate
program in a machine learning and create
your own successful career link is add
in the description box below so make
sure you check that out so without any
further Ado let's jump directly into our
today's topic so firstly let us
understand what is Edge Computing the
term Edge Computing for AI refers to the
process of running a computations and
processing close to the data source or
Edge devices at the edge of a network as
opposed to relying on centralized
cloud-based servers in order to enable
real-time data analysis pick a response
times and a decreased Reliance on cloud
infrastructure it deploys AI models and
algorithms directly on edge devices or
local
servers AI Edge Computing typically
entails the deployment of lightweight AI
models designed to use on edge devices
with constrained computational power
then these models are frequently
deployed to the edge after being trained
using cloud-based resources The Edge
devices gather data from their sensors
or from outside sources process the data
locally using the AI models that have
been deployed and give prompt responses
or start processes based on the data but
what Le to the rise of Ed AI well with
rapid advancement in technology in every
sector and businesses are looking to
increase automation to increase workflow
productivity and security and for that
computer programs must be able to
identify patterns and Carry Out tasks
repeatedly and safely in order to assist
people now the range of task that human
perform however covers in different
situations that are impossible to fully
describe in codes and rules because the
world is very unstructured well with the
help of developments in haai machines
and gadgets can Now function with the
intelligence of human cognition whenever
they may be smart applications with AI
capabilities can learn to carry out the
same task under various conditions just
like in real life and here are the three
recent innovations that explain why
using AI models at the edge is on the
rise well first on the list we have
advancement of neural networks now over
the years as you know neural networks
and related AI infrastructure have
advanced and matured allowing for more
generalized machine learning
capabilities this means organizations
have to gain a deeper understanding on
How to Train AI models effectively and
deploy them in production at the
edge development in computational
infrastructure now running AI at the
Edge requires substantial computational
power the field has witnessed notable
advancements in computational
infrastructure to support AI at the edge
particularly recent strides in highly
parallel graphic processing units or
gpus and CPUs have enabled efficient
execution of neural networks gpos are
specifically designed to handle complex
parallel computation making them a
suitable choice for processing Ai
workloads and finally widespread usage
of iot devices now the widespread
adoption of Internet of think devices
has generated vast amounts of data these
devices such as industrial sensors smart
cameras and Robotics provide a rich
source of data that can be utilized for
AI analysis at the edge the availability
of diverse iot devices and their ability
to capture real-time data has
facilitated the deployment of AI models
at the edge but why deploy AI at the
edge what are the benefits of edge
Computing now Edge Computing for AI
addresses the limitations and challenges
associated with conventional Cloud
Centric AI methods so here are some of
the advantages which it offers firstly
decreased latency Now by performing AI
processing at the edge there is a
significant reduction in network latency
since data no longer needs to be
transmitted to the cloud and back this
is particularly beneficial for
applications that demand real-time
responses or decision making let's say
like autonomous vehicles or Industrial
Automation secondly real-time data
analysis Edge Computing for AI enables
IM data analysis and inference directly
on the edge devices themselves this is
especially valuable for time-sensitive
applications that require realtime
insights or predictions eliminating the
need for Reliance on cloud
connectivity enhanced privacy and
security well Edge Computing enhances
privacy and security by keeping data and
AI processing local sensitive
information can be processed on the edge
devices without being transmitted to the
cloud minimizing the risk of data
breaches and ensuring compliance with
data privacy regulations next we have
bandwidth optimization Edge Computing
reduces the volume of data uh that needs
to be transmitted to the cloud sending
only relevant or let's say summarized
data now this optimization minimizes
bandwidth requirements reduces Network
cost and even lightens the load on the
cloud infrastructure in total and
finally it offers offline functionality
Edge Computing enables AI models to
function even in situation which limited
or no internet connectivity this
capability is crucial for scenarios that
require continuous operation in remote
or disconnected environments such as
certain iot applications or field
deployments so these are some of the
reasons why we need to deploy AI at the
edge and the benefit it's offering now
comes the main question how exactly does
Edge Computing works well Edge Computing
is primarily focused on the proximity
and location of data processing and
storage the fundamental principle is
simple when it's not feasible to bring
data to a centralized data center the
alternative is to bring the data center
close to the data now in order to enable
machines to perform tasks like let's say
visual perception object detection
driving cars understanding speech
speaking walking and emulating other
human skills they need to replicate
human intelligence in a functional
manner so for that AI uses a deep neural
network as a data structure to emulate
human cognition these dnns undergo
training in data centers and are exposed
to numerous examples of specific task or
question type along with their correct
answer and once the training is complete
the model advances to become a inference
engine capable of addressing real world
questions now from there on here's how
it typically Works firstly deployment of
inference engine now in AJ deployments
the inference engine which is the
component responsible for making
predictions or performing AI tasks is
deployed on local computers or devices
located in diverse environments like
factories hospitals cars satellites
homes and much more problem
identification and data upload
now while the inference engine operates
at the edge it may encounter situation
where it struggles to provide accurate
predictions or encounter challenging
scenarios in such cases the problematic
data associated with these instances is
commonly identified and upload to a
centralized Cloud structure now from
there on there is a continuous
cloud-based model training now the cloud
infrastructure typically in a data
center receives the problematic data
from Edge devices data scientists and AI
experts can then utilize this data to
refine and train the original AI models
further this involves using the Cloud's
computation resources larger data sets
and collaboration tools for model
training and Improvement now and finally
with the updated model deployed at the
edge after being retrained the AI system
continues to operate gaining new data
and insights from its environment the AJ
model can collect further additional
data which can be periodically uploaded
to the cloud and further training and
continuous Improvement this feedback
loops ensures that the Aji models become
Progressive L smarter over time so this
is in a nutshell how Edge Computing for
AI Works hope you have understood well
let us talk some of the use cases of
edge Computing for AI in real world well
the combination of AI iot and Edge
Computing has opened up a multitude of
opportunities for Edge AI
revolutionizing the various aspects of
our daily lives so let's Del you into
some key areas where Edge AI is making a
significant impact first on the list we
have healthare haai is enhancing medical
Diagnostics and patient care for
instance by deploying AI algorithms on
medical imaging devices Radiologists can
accurately and efficiently identify and
analyze pathologies variable devices
with local Edge analytics enable
real-time monitoring of patient data
facilitating any early detection of
anomalies And Timely
interventions next autonomous vehicles
Edge a plays a pivotal role in enabling
realtime navigation and decision making
for autonomous vehicles AI algorithms
running on edge devices within Vehicles
process sensor data analyze the
environment and make split-second
decisions to ensure safe and efficient
driving agriculture hji is being used to
address agriculture challenges as well
such as pollination autonomous drones
equipped with h AI capabilities can
identify and pollinate plants aiding in
crop cultivation and maximizing yelds
this application demonstrates the
diverse potential of edge AI Beyond
traditional domains next we have smart
home and cities now Ed AI enhances the
functionality and efficiency of Smart
Homes and cities local processing and
Analysis of data from iot devices enable
real-time automation Energy Management
security surveillance and personalized
user experience AJ power system adapt to
individual preferences optimize
resources usage and respond to events in
real time next we have industrial
applications Ed AI is transforming
Industries thoroughly through real-time
monitoring predi maintenance and process
optimization Edge devices equipped with
AI algorithms analyze sens data detect
anomalies and predict equipment failures
reducing downtime and improving
efficiency next we have security and
surveillance while a plays a crucial
role in security and surveillance which
empire real-time video analytics for
security and surveillance applications
Edge devices with a capabilities can
analyze video feeds locally detect
suspicious activities like theft
identify objects or individuals of
interest and Trigger alerts or responses
enhancing security measures and reducing
response times and finally we have
personal devices H AI is increasingly
being integrated into personal devices
such as your smartphones laptops
variables and smart home assistants this
allows for on device AI processing
enabling faster and more personalized
experience without ring heavily on cloud
connectivity which can support voice
recognition facial recognition among
other features as well well as more
businesses and industries recognize the
advantage of AI its adoption is rapidly
growing the ability to process data
locally enhance privacy reduce latency
and make real-time decision Empire
organizations in various sectors to
enhance operations deliver better
services and improve user experience so
these were some of the real time
applications of hi and with that we've
come to the end of today's session on
what H Computing for AI is I hope you
understood if you want to become an AI
expert and gain handsome salary packages
look at the wide range of e IML courses
by simply learn in collaboration with
top universities across the globe so
what are you waiting for hurry up and
enroll now and year of experience is
preferred to enroll in these courses
find the course Link in the description
box what are the different types of
machine learning algorithms machine
learning algorithms are broadly
classified into three types the
supervised learning unsupervised
learning and reinforcement learning
supervised learning in turn consists of
techniques like Reg regression and
classification and unsupervised learning
we use techniques like Association and
clustering and reinforcement learning is
a recently developed technique and it is
very popular in gaming some of you must
have heard about alphago so this was
developed using reinforcement learning
primary difference between supervised
learning and unsupervised learning
supervised learning is used when we have
historical data and we have labeled data
which means that we know how the data is
classified so we know the classes if we
are doing classification or we know the
values when we are doing regression so
if we have historical data with these
values which are known as labels then we
use supervised learning in case of
unsupervised learning we do not have
past labeled data historical labeled
data so we use techniques like
Association and clustering to maybe form
clusters or new classes maybe and then
we move from there in case of
reinforcement learning the system learns
pretty much from scratch there is an
agent and there is an environment the
agent is given a certain Target and it
is rewarded when it is moving towards
that Target and it is penalized if it is
moving in a direction which is not
achieving that Target so it's more like
a carrot and stick model so what is the
difference between these three types of
algorithms supervised algorithms or
supervised learning algorithms are used
when you have a specific Target value
that you would like to predict the
target could be categorical having two
or more possible outcomes or classes if
you will that is what is classification
or the target could be a a value which
can be measured and that's where we use
regression like for example whether
forecasting you want to find the
temperature whereas in classification
you want to find out whether this is a
fraud or not a fraud or if it is email
spam whether it is Spam or not spam so
that is a classification example so if
you know or this is known as labeled
information if you have the labeled
information then you use supervised
learning in case of unsupervised
learning we have input data but we don't
have the labels or what the output is
supposed to be so that is when we use
unsupervised learning techniques like
clustering and Association and we try to
analyze the data in case of
reinforcement learning it allows the
agent to automatically determine the
ideal Behavior within a specific context
and it has to do this to maximize the
performance like for example playing a
game so the agent is told that you need
to score the maximum score possible
without losing lives so that is a Target
that is given to the agent and it is
allowed to learn from scratch play the
game itself multiple times and slowly it
will learn the behavior
which will increase the score and keep
the lives to the maximum that's an
example of reinforcement learning when
we look at a different machine learning
algorithms we can divide them into three
areas supervised
unsupervised reinforcement we're only
going to look at supervised today
unsupervised means we don't have the
answers and we're just grouping things
reinforcement is where we give positive
and negative feedback to our algorithm
to program it and it doesn't have the
information till after the fact but
today we're just looking at supervised
because that's where linear regression
fits in in supervised data we have our
data already there and our answers for a
group and then we use that to program
our model and come up with an answer the
two most common uses for that is through
the regression and classification now
we're doing linear regression so we're
just going to focus on the regression
side and in the regression we have
SIMPLE linear regression we have
multiple linear regression and we have
polom linear regression now on these
three simple linear regression is the
examples we've looked at so far where we
have a lot of data and we draw a
straight line through it multiple linear
regression means we have multiple
variables remember where we had the
rainfall and the crops we might add
additional variables in there like how
much food do we give our crops when do
we Harvest them those would be
additional information add into our
model and that's why it' be multiple
linear regression and finally we have
polinomial linear regression that is
instead of drawing a line we can draw a
curve line through it now that you see
where regression model fits into the
machine learning algorithms and we're
specifically looking at linear
regression let's go ahead and take a
look at applications for linear
regression let's look at a few
applications of linear regression
economic growth used to determine the
economic growth of a country or a state
in the coming quarter can also be used
to predict the GDP of a country product
price can be used to predict what would
be the price of a product in the future
we can guess whether it's going to go up
or down or should I buy today housing
sales to estimate the number of houses a
builder would sell and what price in the
coming months score predictions Cricut
fever to predict the number of runs a
player would score in the coming matches
based on the previous performance I'm
sure you can figure out other
applications you could use linear
regression for so let's jump in and
let's understand linear regression and
dig into the theory understanding linear
regression linear regression is a
statistical model used to predict the
relationship between independent and
dependent variables by examining two
factors the first important one is which
variables in particular are significant
predictors of the outcome variable and
the second one that we need to look at
closely is how significant is the
regression line to make predictions with
the highest possible accuracy if it's
inaccurate we can't use it so it's very
important we find out the most accurate
line we can get since linear regression
is based on drawing a line through data
we're going to jump back and take a look
at some ukian geometry the simplest form
of a simple linear regression equation
with one dependent and one independent
variable is represented by y = m * X
plus C and if you look at our model here
we plotted two points on here uh X1 and
y1 X2 and Y2 y being the dependent
variable remember that from before and X
being the independent variable so y
depends on whatever X is m in this case
is the slope of the line where m equals
the difference in the Y 2 - y1 and X2 -
X1 and finally we have C which is the
coefficient of the line or where happens
to cross the zero axis let's go back and
look at an example we used earlier of
linear regression we're going to go back
to plotting the amount of crop yield
based on the amount of rainfall and and
here we have our rainfall remember we
cannot change rainfall and we have our
crop yield which is dependent on the
rainfall so we have our independent and
our dependent variables we're going to
take this and draw a line through it as
best we can through the middle of the
data and then we look at that we put the
red point on the y axis is the amount of
crop yield you can expect for the amount
of rainfall represented by the Green Dot
so if we have an idea what the rainfall
is for this year and what's going on
then we can guess how good our crop are
going to be and we've created a nice
line right through the middle to give us
a nice mathematical formula let's take a
look and see what the math looks like
behind this let's look at the intuition
behind the regression line now before we
dive into the math and the formulas that
go behind this and what's going on
behind the
scenes I want you to note that when we
get into the case study and we actually
apply some python script that this math
that you're going to see here is already
done automatically for you you don't
have to to have it memorized it is
however good to have an idea what's
going on so if people reference the
different terms you'll know what they're
talking about let's consider a sample
data set with five rows and find out how
to draw the regression line we're only
going to do five rows because if we did
like the rainfall with hundreds of
points of data that would be very hard
to see what's going on with the
mathematics so we'll go ahead and create
our own two sets of data and we have our
independent variable X and our dependent
variable y Y and when X was 1 we got Y =
2 when X was uh 2 y was 4 and so on and
so on if we go ahead and plot this data
on a graph we can see how it forms a
nice line through the middle you can see
where it's kind of grouped going upwards
to the right the next thing we want to
know is what the means is of each of the
data coming in the X and the Y the means
doesn't mean anything other than the
average so we add up all the numbers and
divide by the total so so 1 + 2 + 3 + 4
+ 5 over 5 = 3 and the same for y we get
four if we go ahead and plot the means
on the graph we'll see we get 3 comma 4
which draws a nice line down the middle
a good estimate here we're going to dig
deeper into the math behind the
regression line now remember before I
said you don't have to have all these
formulas memorized or fully understand
them even though we're going to go into
a little more detail of how it works and
if you're not a math whz and you don't
know if you've never seen the sigma
character before which looks a little
bit like an e that's opened up that just
means summation that's all that is so
when you see the sigma character it just
means we're adding everything in that
row and for computers this is great
because as a programmer you can easily
iterate through each of the XY points
and create all the information you need
so in the top half you can see where
we've broken that down into pieces and
as it goes through the first two points
it computes the squared value of x the
squared value of one y and x * Y and
then it takes all of X and adds them up
all of Y adds them up all of X2 adds
them up and so on and so on and you can
see we have the sum of equal to 15 the
sum is equal to 20 all the way up to x *
Y where the sum equals 66 this all comes
from our formula for calculating a
straight line where y equals the slope *
X plus a coefficient C so when we go
down below and we're going to compute
more like the averages of these and
we're going to explain exactly what that
is in just a minute and where that
information comes from it's called the
square means error but we'll go into
that in detail in a few minutes all you
need to do is look at the formula and
see how we've gone about Computing it
line by line instead of trying to have a
huge set of numbers pushed into it and
down here you'll see where the slope m
equals and on the top part if you read
through the brackets you have the number
of data points times the sum of x * Y
which we computed one line at a time
there and that's just the 66 and take
all that and you subtract it from the
sum of x times the sum of Y and those
have both been computed so you have 15 *
20 and on the bottom we have the number
of lines times the sum of x squared
easily computed as 86 for the sum minus
I'll take all that and subtract the sum
of x s and we end up as we come across
with our formula you can plug in all
those numbers which is very easy to do
on the computer you don't have to do the
math on a piece of paper or calculator
and you'll get a slope of 6 and you'll
get your C coefficient if you continue
to follow through that formula you'll
see it comes out as equal to 2.2
continuing deeper into what's going
behind the scenes let's find out the
predicted values of Y for corresponding
values of X using the linear equation
where M = 6 and C = 2.2 we're going to
take these values and we're going to go
ahead and plot them we're going to
predict them so y equal 6 time where x =
1 + 2.2 = 2.8 so on and so on and here
the Blue Points represent the actual y
values and the brown points represent
the predicted yv values based on the
model we created the distance between
the actual and predicted values is known
as residuals or errors the best fit line
should have the least sum of squares of
these errors also known as e s if we put
these into a nice chart where you can
see X and you can see why where we
actual values were and you can see y
predicted you can easily see where we
take Yus y predicted and we get an
answer what is the difference between
those two and if we square that Yus y
prediction squared we can then sum those
squared values that's where we get the
64 plus the 36 + 1 all the way down
until we have a summation equals 2.4 so
the sum of squared errors for this
regression line is 2.4 we check this
error for each line and conclude the
best fit line having the least e Square
value in a nice graphical representation
we can see here where we keep moving
this line through the data points to
make sure the best fit line has the
least Square distance between the data
points and the regression line now we
only looked at the most commonly used
formula for minimizing the distance
there are lots of ways to minimize a
distance between the line and the data
points like sum of squared errors sum of
absolute errors root mean square error
it's Etc what you want to take away from
this is whatever formula is being used
you can easily using a computer
programming and iterating through the
data calculate the different parts of it
that way these complicated formulas you
see with the different summations and
absolute values are easily computed one
piece at a time up until this point
we've only been looking at two values X
and Y well in the real world it's very
rare that you only have two values when
you're figuring out a solution so let's
move on to the next top topic multiple
linear regression let's take a brief
look at what happens when you have
multiple inputs so in multiple linear
regression we have uh well we'll start
with the simple linear regression where
we had y = m + x + C and we're trying to
find the value of y now with multiple
linear regression we have multiple
variables coming in so instead of having
just X we have X1 X2 X3 and instead of
having just one slope each variable has
its own slope attach to it as you can
see here we have M1 M2 M3 and we still
just have the single coefficient so when
you're dealing with multiple linear
regression you basically take your
single linear regression and you spread
it out so you have y = M1 * X1 + M2 * X2
so on all the way to m to the n x to the
nth and then you add your coefficient on
there implementation of linear
regression now we get into my favorite
part let's understand how multiple
linear regression works by implementing
it in Python if you remember before we
were looking at a company and just based
on its R&D trying to figure out its
profit we're going to start looking at
the expenditure of the company we're
going to go back to that we're going to
predict its profit but instead of
predicting it just on the R&D we're
going to look at other factors like
Administration costs marketing costs and
so on and from there we're going to see
if we can figure out what the profit of
that company's going to be to start our
coding we're going to begin by importing
some basic basic libraries and we're
going to be looking through the data
before we do any kind of linear
regression we're going to take a look at
the data see what we're playing with
then we'll go ahead and format the data
to the format we need to be able to run
it in the linear regression model and
then from there we'll go ahead and solve
it and just see how valid our solution
is so let's start with importing the
basic libraries now I'm going to be
doing this in Anaconda Jupiter notebook
a very popular IDE I enjoy it it's such
a visual to look at and so easy to use
um just any ID for python will work just
fine for this so break out your favorite
python IDE so here we are in our Jupiter
notebook let me go ahead and paste our
first piece of code in there and let's
walk through what libraries we're
importing first we're going to import
numpy as NP and then I want you to skip
one line and look at import pandas as PD
these are very common tools that you
need with most of your linear regression
the numpy which stands for number python
is usually denoted as NP and you have to
almost have that for your SK learn
toolbox you always import that right off
the beginning pandas although you don't
have to have it for your sklearn
libraries it does such a wonderful job
of importing data setting it up into a
data frame so we can manipulate it
rather easily and it has a lot of tools
also in addition to that so we usually
like to use the pandas when we can and
I'll show you what that looks like the
other three lines are for us to get a
visual of this data and take a look at
it so we're going to import matplot
library. pyplot as PLT and then caborn
as SNS caborn works with the matplot
library so you have to always import
matplot library and then Seaborn sits on
top of it and we'll take a look at what
that looks like you could use any of
your own plotting libraries you want
there's all kinds of ways to look at the
data these are just very common ones and
the cbor is so easy to use it just looks
beautiful it's a nice representation
that you can actually take and show
somebody and the final line is the Amber
signed map plot Library inline that is
only because I'm doing an inline IDE my
interface in the Anaconda Jupiter
notebook requires I put that in there or
you're not going to see the graph when
it comes up let's go ahead and run this
it's not going to be that interesting
because we're just setting up variables
in fact it's not going to do anything
that we can see but it is importing
these different libraries and setup the
next step is load the data set and
extract independent and dependent
variables now here in the slide you'll
see companies equals pd. read CSV and it
has a long line there with the file at
the end one 1,000 companies. CSV you're
going to have to change this to fit
whatever setup you have and the file
itself you can request just go down to
the commentary below this video and put
a note in there and simply learn we'll
try to get in contact with you and
Supply you with that file so you can try
this coding yourself so we're going to
add this code in here and we're going to
see that I have companies equals pd.
reader CSV and I've changed this path to
match my computer c/s simplylearn
sl1000 company . CSV and then below
there we're going to set the x equals to
companies under the iocation and because
this is companies is a PD data set I can
use this nice notation that says take
every row that's what the colon the
first colon is comma except for the last
column that's what the second part is
where we have a colon minus one and we
want the values set into there so X is
no longer a data set a pandas data set
but we can easily extract the data from
our pandas data set with this notation
and then why we're we're going to set
equal to the last row well the question
is going to be what are we actually
looking at so let's go ahead and take a
look at that and we're going to look at
the companies. head which lists the
first five rows of data and I'll open up
the file in just a second so you can see
where that's coming from but let's look
at the data in here as far as the way
the panda sees it when I hit run you'll
see it breaks it out into a nice setup
this is what pandas one of the things
pandas is really good about is it looks
just like an Excel spreadsheet you have
your rows and remember when we're
programming we we always start with zero
we don't start with one so it shows the
first five rows 0 1 2 3 4 and then it
shows your different columns R&D spend
Administration marketing spend State
profit it even notes that the top are
column names it was never told that but
pandas is able to recognize a lot of
things that they're not the same as the
data rows why don't we go ahead and open
this file up in a CSV so you can
actually see the raw data so here I've
opened it up as a text editor and you
can see at the top we have R&D spin
comma Administration comma marketing
spin comma State comma profit carriage
return I don't know about you but I'd go
crazy trying to read files like this
that's why we use the pandas you could
also open this up in an Excel and it
would separate it since it is a comma
separated variable file but we don't
want to look at this one we want to look
at something we can read rather easily
so let's flip back and take a look at
that top part the first five row now as
nice as this format is where I can see
the data to me it doesn't mean a whole
lot maybe you're an expert in business
and Investments and you understand what
$165,300
compared to the administration cost of
$136,800 so on so on helps to create the
profit of
192,25
183 that makes no sense to me whatsoever
no pun intended so let's flip back here
and take a look at our next set of code
where we're going to graph it so we can
get a better understanding of our data
and what it means so so at this point
we're going to use a single line of code
to get a lot of information so we can
see where we're going with this let's go
ahead and paste that into our uh
notebook and see what we got going and
so we have the visualization and again
we're using SNS which is pandas as you
can see we imported the map plot
library. pyplot as PLT which then the
caborn uses and we imported the caborn
is SNS and then that final line of code
helps us show this in our um in line
coding without this it wouldn't display
and you can display it to a file and
other means and that's the matap plot
library in line with the Amber sign at
the beginning so here we come down to
the single line of code caborn is great
because it actually recognizes the panda
data frame so I can just take the
companies. core for coordinates and I
can put that right into the caborn and
when we run this we get this beautiful
plot and let's just take a look at what
this plot means if you look at this plot
on mine the colors are probably a little
bit more purplish in blue than the
original one uh we have the columns and
the rows we have R and D spending we
have Administration we have marketing
spending and profit and if you cross
index any two of these since we're
interested in profit if you cross index
profit with profit it's going to show up
if you look at the scale on the right
way up in the dark why because those are
the same data they have an exact
correspondence so R&D spending is going
to be the same as R&D spending and the
same thing with Administration costs are
right down the middle you get this dark
row or dark um diagonal row that shows
that this is the highest corresponding
data that's exactly the same and as it
becomes lighter there's less connections
between the data so we can see with
profit obviously profit is the same as
profit and next it has a very high
correlation with R&D spending which we
looked at earlier and it has a slightly
less connection to marketing spending
and even less to how much money we put
into the administration so now that we
have a nice look at the data data let's
go ahead and dig in and create some
actual useful linear regression models
so that we can predict values and have a
better profit now that we've taken a
look at the visualization of this data
we're going to move on to the next step
instead of just having a pretty picture
we need to generate some hard data some
hard values so let's see what that looks
like we're going to set up our linear
regression model in two steps the first
one is we need to prepare some of our
data so it fits correctly and let's go
ahead and paste this code into our
jupyter notebook and what we're bringing
in is we're going to bring in the
sklearn pre-processing where we're going
to import the label encoder and the one
hot encoder to use the label encoder
we're going to create a variable called
label encoder and set it equal to
capital L label capital E encoder this
creates a class that we can reuse for
transferring the labels back and forth
now about now you should ask what labels
are we talking about let's go take a
look at the data we processed before and
see what I'm talking about here if you
remember when we did the comp companies.
head and we printed the top five rows of
data we have our columns going across we
have column zero which is R&D spending
column one which is Administration
column two which is marketing spending
and column three is State and you'll see
under State we have New York California
Florida now to do a linear regression
model it doesn't know how to process New
York it knows how to process a number so
the first thing we're going to do is
we're going to change that New York
California and Florida and we're going
to change those to numbers that's what
this line of code does here x equals and
then it has the colon comma 3 in
Brackets the first part the colon comma
means that we're going to look at all
the different rows so we're going to
keep them all together but the only row
we're going to edit is the third row and
in there we're going to take the label
coder and we're going to fit and
transform the X also the third row so
we're going to take that third row we're
going to set it equal to a
transformation and that transformation
basically tells it that instead of
having a uh New York it has a zero or
one or a two and then finally we need to
do a one hot encoder which equals one
hot encoder categorical features equals
three and then we take the X and we go
ahead and do that equal to one hot
encoder fit transform X to array this
final transformation preps our data for
us so it's completely set the way we
need it as just a row of numbers even
though it's not in here let's go ahead
and print X and just take a look what
this data is doing you'll see you have
an array of arrays and then each array
is a row of numbers and if I go ahead
and just do row zero you'll see I have a
nice organized row of numbers that the
computer now understands we'll go ahead
and take this out there because it
doesn't mean a whole lot to us it's just
a row of numbers next on setting up our
data we have avoiding dummy variable
trap this is very important why because
the computer is automatically
transformed our header into the setup
and it's automatically transformed all
these different variables so when when
we did the encoder the encoder created
two columns and what we need to do is
just have the one because it has both
the variable in the name that's what
this piece of code does here let's go
ahead and paste this in here and we have
xal X colon comma 1 colon all this is
doing is removing that one extra column
we put in there when we did our one hot
encoder and our label en coding let's go
ahead and run that and now we get to
create our linear regression model and
let's see what that looks like here and
we're going to do that in two steps the
first step is going to be in splitting
the data now whenever we create a uh
predictive model of data we always want
to split it up so we have a training set
and we have a testing set that's very
important otherwise we'd be very
unethical without testing it to see how
good our fit is and then we'll go ahead
and create our multiple linear
regression model and train it and set it
up let's go ahead and paste this next
piece of code in here and I'll go ahead
and shrink it down a size or two so it
all fits on one line so from the sklearn
module selection we're going to import
train test split and you'll see that
we've created four completely different
variables we have capital x train
capital X test smaller case y train
smaller case y test that is the standard
way that they usually referenes when
we're doing different uh models usually
see that a capital x and you see the
train and the test and the lowercase Y
what this is is X is our data going in
in that's our R&D spin our
Administration our marketing and then Y
which we're training is the answer
that's the profit because we want to
know the profit of an unknown entity so
that's what we're going to shoot for in
this tutorial the next part train test
split we take X and we take y we've
already created those X has the columns
with the data in it and Y has a column
with profit in it and then we're going
to set the test size equals 0.2 that
basically means 20% so 20 % of the rows
are going to be tested we're going to
put them off to the side so since we're
using a th000 lines of data that means
that 200 of those lines we're going to
hold off to the side to test for later
and then the random State equals zero
we're going to randomize which ones it
picks to hold off to the side we'll go
ahead and run this it's not overly
exciting it's setting up our variables
but the next step is the next step we
actually create our linear regression
model now that we got to the linear
regression model we get that next piece
of the puzzle let's go ahe and put that
code in there and walk through through
it so here we go we're going to paste it
in there and let's go ahead and since
this is a shorter line of code let's
zoom up there so we can get a good look
and we have from the SK learn. linear
model we're going to import linear
regression now I don't know if you
recall from earlier when we were doing
all the math let's go ahead and flip
back there and take a look at that do
you remember this where we had this long
formula on the bottom and we were doing
all this suiz and then we also looked at
setting it up with the different lines
and then we also looked all the way down
to multiple linear regression where
we're adding all those formulas together
all of that is wrapped up in this one
section so what's going on here is I'm
going to create a variable called
regressor and the regressor equals the
linear regression that's a linear
regression model that has all that math
built in so we don't have to have it all
memorized or have to compute it
individually and then we do the
regressor do fet in this case we do XT
train and Y train because we're using
the training data X being the data in
and Y being profit what we're looking at
and this does all that math for us so
within one click and one line we've
created the whole linear regression
model and we fit the data to the linear
regression model and you can see that
when I run the regressor it gives an
output linear regression it says copy x
equals True Fit intercept equals true in
jobs equal one normalize equals false
it's just giving you some general
information on what's going on with that
regressor model now that we've created
our linear regression model let's go
ahead and use it and if you remember we
kept a bunch of to data aside so we're
going to do a y predict variable and
we're going to put in the X test and
let's see what that looks like scroll up
a little bit paste that in here
predicting the test set results so here
we have y predict equals regressor do
predict X test going in and this gives
us y predict now because I'm in Jupiter
in line I can just put the variable up
there and when I hit the Run button
it'll print that array out I could have
just as easily done print y predict so
if you're in a different IDE that's not
an inline setup like the Jupiter
notebook you can do it this way print y
predict and you'll see that for the 200
different test variables we kept off to
the side it's going to produce 200
answers this is what it says the profit
are for those 200 predictions but let's
don't stop there let's keep going and
take a couple look we're going to take
just a short detail here and calculating
the coefficients and the intercepts this
gives us a quick flash at what's going
on behind the line we're going to take a
a short detour here and we're going to
be calculating the coefficient and
intercepts so you can see what those
look like what's really nice about our
regressor we created is it already has
the coefficients for us and we can
simply just print regressor do
coefficient uncore when I run this
you'll see our coefficients here and if
we can do the regressor coefficient we
can also do the regressor intercept and
let's run that and take a look at that
this all came from the multiple
regression model and we'll flip over so
you can remember remember where this is
going into and where it's coming from
you can see the formula down here where
y = M1 * X1 + M2 * X2 and so on and so
on plus C the coefficient so these
variables fit right into this formula y
= slope 1 * column 1 variable plus slope
2 * column 2 variable all the way to the
m into the n and x to the N plus C the
coefficient or in this case you have -
8.89 to the^ of 2 etc etc times the
First Column and the second column and
the third column and then our intercept
is the minus
10309 Point boy it gets kind of
complicated when you look at it this is
why we don't do this by hand anymore
this is why we have the computer to make
these calculations easy to understand
and calculate now I told you that was a
short detour and we're coming towards
the end of our script as you remember
from the beginning I said if we're going
to divide this information we have to
make sure it's a valid model that this
model works and understand how good it
works so calculating the R squar value
that's what we're going to use to
predict how good our prediction is and
let's take a look at what that looks
like in code and so we're going to use
this from SK learn. metrics we're going
to import R2 score that's the R squar
value we're looking at the error so in
the R2 score we take our y test versus
our y predict y test is the actual
values we're testing that was the one
that was given to us so we know our true
the Y predict of those 200 values is
what we think it was true and when we go
ahead and run this we see we get a
9352 that's the R2 score now it's not
exactly a straight percentage so it's
not saying it's 93% correct but you do
want that in the upper 90s o and higher
shows that this is a very valid
prediction based on the R2 score and if
r squ value of .91 or 92 as we got on
our model remember it does have a random
generation involved this proves the
model is a good model which means
success yay we successfully trained our
model with certain predictors and
estimated the profit of the companies
using linear regression so now that we
have a successful linear regression
model if you are an aspiring machine
learning engineer looking for online
training and certifications from
prestigious universities and in
collaboration with leading experts then
search no more simply lears professional
certification program in Ai and machine
learning from P University in
collaboration with IBM should be your
right choice for more details use the
link in the description box below with
that in mind all right what is logistic
regression as mentioned earlier logistic
regression is an algorithm for
performing binary classification so
let's take an example and see how this
works let's say your car has not been
serviced for quite a few years and now
you want to find out if it it's going to
break down in the near future so this is
like a classification problem find out
whether your car will break down or not
so how are we going to perform this
classification so here's how it looks if
we plot the information along the X and
Y axis X is the number of years since
the last service was performed and Y is
the probability of your car breaking
down and let's say this information was
this data rather was collected from
several car users it's not just your car
but several car users so that is our
labeled data so the data has been
collected and um for for the number of
years and when the car broke down and
what was the probability and that has
been plotted along X and Y AIS so this
provides an idea or from this graph we
can find out whether your car will break
down or not we'll see how so first of
all the probability can go from 0 to 1
as you all aware probability can be
between 0 and one and as we can imagine
it is intuitive as well as the number of
years are on the Lower Side maybe one
year two years or 3 years till after the
service the chances of your car breaking
down are very limited right so for
example chances of your car breaking
down or the probability of your car
breaking down within 2 years of your
last service are 0.1 probability
similarly 3 years is maybe3 and so on
but as the number of years increases
let's say if it was six or seven years
there is almost a certainty that your
car is going to break down that is what
this graph shows so this is an example
of a application of the classification
algorithm and we will see in little
details how exactly logistic regression
is applied here one more thing needs to
be added here is that the dependent
variables outcome is discrete so if we
are talking about whether the car is
going to break down or not so that is a
dis value the Y that we are talking
about the dependent variable that we are
talking about what we are looking at is
whether the car is going to break down
or not yes or no that is what we are
talking about so here the outcome is
discrete and not a continuous value so
this is how the logistic regression
curve looks let me explain a little bit
what exactly and how exactly we are
going to uh determine the class at the
outcome rather so for a logistic
regression curve a threshold has to be
set saying that because this is a
probability calculation remember this is
a probability calculation and the
probability itself will not be zero or
one but based on the probability we need
to decide what the outcome should be so
there has to be a threshold like for
example 0.5 can be the threshold let's
say in this case so any value of the
probability below 5 is considered to be
zero and any value above 5 is considered
to be one so an output of let's
say8 will mean that the car will break
down so that is considered as an output
of one and let's say an output of 0. 29
is considered as zero which means that
the car will not break down so that's
the way logistic regression works now
let's do a quick comparison between
logistic regression and linear
regression because they both have the
term regression in them so it can cause
confusion so let's try to remove that
confusion so what is linear regression
linear regression is a process is once
again an algorithm for supervised
learning however here you're going to
find a continuous value you're going to
determine a continuous value it could be
the price of a real estate property it
could be your hike how much hike you're
going to get or it could be a stock
price these are all continuous values
these are not discrete compared to a yes
or a no kind of a response that we are
looking for in logistic regression so
this is one example of a linear
regression let's say at the HR team of a
company tries to find out what should be
the salary hike of an employee so they
collect all the details of their
existing employees their ratings and
their salary hikes what has been given
and that is the labeled information that
is available and the system learns from
this it is train and it learns from this
labeled information so that when a new
employees information is fed BAS based
on the rating it will determine what
should be the height so this is a linear
regression problem and a linear
regression example now salary is a
continuous value you can get 5,000
5,500 5,600 it is not discrete like a
cat or a dog or an apple or a banana
these are discrete or a yes or a no
these are discrete values right so this
where you are trying to find continuous
values is where we use linear regression
so let's say just to extend on this
scenario we now want to find out whether
this employee is going to get a
promotion or not so we want to find out
that is a discrete problem right a yes
or no kind of a problem in this case we
actually cannot use linear regression
even though we may have labeled data so
this is the labeled data So based on the
employee rating these are the ratings
and then some people got the promotion
promotion and this is the ratings for
which people did not get promotion that
is a no and this is the rating for which
people got promotion we just plotted the
data about whether a person has got an
employee has got promotion or not yes no
right so there is nothing in between and
what is the employees rating okay and
ratings can be continuous that is not an
issue but the output is discrete in this
case whether employee got promotion yes
no okay so if we try to plot that and we
try to find a straight line this is how
it would look and as you can see it
doesn't look very right because looks
like there will be lot of Errors the
root mean square error if you remember
for linear regression would be very very
high and also the the values cannot go
beyond zero or Beyond one so the graph
should probably look somewhat like this
clipped at zero and one but still the
straight line doesn't look right
therefore instead of using a linear
equation we need to come up with
something different and therefore the
logistic regression model looks somewhat
like this so we calculate the
probability and if we plot that
probability not in the form of a
straight line but we need to use some
other equation we will see very soon
what that equation is then it is a
gradual process right so you see here
people with some of these ratings are
not getting any promotions and then
slowly uh at certain rating they get
promotion so that is a gradual process
and U this is how the math behind
logistic regression looks so we are
trying to find the odds for a particular
event happening and this is the formula
for finding the odds so the probability
of an event happening divided by the
probability of the event not happening
so P if it is the probability of the
event happening probability of the
person getting a promotion and divided
by the probability of the person not
getting a promotion that is 1 minus
P so this is how you measure the odds
now the values of the odds range from
0er to Infinity so when this probability
is zero then the odds will the value of
the odds is equal to Z and when the
probability becomes one then the value
of the odds is 1 by 0 that will be
Infinity but the probability itself
remains between 0 and 1 now this is how
an equation of a straight line look so Y
is equal to Beta 0 + beta 1 x where beta
0 is the Y intercept and beta 1 is the
slope of the line if we take the odds
equation and take a log of both sides
then this would look somewhat like this
and the term logistic is actually
derived from the fact that we are doing
this we take a log of PX by 1 minus PX
this is an extension of the calculation
of odds that we have seen right and that
is equal to Beta 0 + beta 1 x which is
the equation of the straight line and
now from here if you want to find out
the value of PX you will see we can take
the exponential on both sides and then
if we solve that equation we will get
the equation of PX like this PX is equal
to 1 by 1 + e ^ of minus beta 0 + beta 1
x and recall this is nothing but the
equation of the line which is equal to y
y is equal to Beta 0 plus beta 1X so
that this is the equation also known as
the sigmoid function and this is the
equation of the logistic regression all
right and if this is plotted this is how
the sigmoid curve is obtained so let's
compare linear and logistic regression
how they are different from each other
let's go back so linear regression is
solved or used to solve regression
problems and logistic regression is used
to solve classific ation promps so both
are called regression but linear
regression is used for solving
regression problems where we predict
continuous values whereas logistic
regression is used for solving
classification problems where we have
have to predict discrete values the
response variables in case of linear
regression are continuous in nature
whereas here they are categorical or
discrete in nature and U the linear
regression helps to estimate the
dependent variable when there is a
change in the independent variable
whereas here in case of logistic
regression it helps to calculate the
probability or the possibility of a
particular event happening and linear
regression as the name suggests is a
straight line that's why it's called
linear regression whereas logistic
regression is a sigmoid function and the
curve is the shape of the curve is s
it's an s shaped curve this is another
example of application of logistic
regression in weather prediction whether
it's going to to rain or not rain now
keep in mind both are used in weather
prediction if we want to find the
discrete values like whether it's going
to rain or not rain that is a
classification problem we use logistic
regression but if we want to determine
what is going to be the temperature
tomorrow then we use linear regression
so just keep in mind that in weather
prediction we actually use both but
these are some examples of logistic
regression so we want to find out
whether it's going to be rain or not
it's going to be sunny or not it's going
to snow or not these are all logistic
regression examples a few more examples
classification of objects this is a
again another example of logistic
regression now here of course one
distinction is that these are multiclass
classification so logistic regression is
not used in its original form but it is
used in a slightly different form so we
say whether it is a dog or not a dog I
hope you understand so instead of saying
is it a dog or a cat or elephant we
convert this into saying so because we
need to keep it to Binary classification
so we say is it a dog or not a dog is it
a cat or not a cat so that's the way
logistic regression can be used for
classifying objects otherwise there are
other techniques which can be used for
performing multiclass classification in
healthcare logistic regression is used
to find the survival rate of a patient
so they take multiple parameters like
comma score and age and so on and so
forth and they try to predict the rate
of survival all right now finally let's
take an example and see how we can apply
logistic regression to predict the
number that is shown in the image so
this is actually a live demo I will take
you into Jupiter notebook and um show
the code but before that let me take you
through a couple of slides to explain
what we're trying to do so let's say you
have an 8 by8 image and the the image
has a number 1 2 3 4 and you need to
train your model to predict what this
number is so how do we do this so the
first thing is obviously in any machine
learning process you train your model so
in this case we are using logistic
regression so and then we provide a
training set to train the model and then
we test how accurate our model is with
the test data which means that like any
machine learning process we split our
initial data into two parts training set
and test set with the training set we
train our model and then with the test
set we we test the model till we get
good accuracy and then we use it for for
inference right so that is typical
methodology of uh uh training testing
and then deploying of machine learning
models so let's uh take a look at the
code and uh see what we are doing so
I'll not go line by line but just take
you through some of the blocks so first
thing we do is import all the libraries
and then we basically take a look at the
images and see what is the total number
of images we can display using mat plot
lip some of the images or a sample of
these images and um then we split the
data into training and test as I
mentioned earlier and we can do some
exploratory analysis and uh then we
build our model we train our model with
the training set and then we test it
with our test set and find out how
accurate our model is using the
confusion Matrix the heat map and use
heat map for visualizing this and I will
show you in the code what exactly is the
confusion Matrix and how it can be used
for finding the accuracy in our example
we got we get an accuracy of about 0.94
which is pretty good or 94% which is
pretty good all right so what is the
confusion Matrix this is an example of a
confusion Matrix and uh this is used for
identifying the accuracy of a uh
classification model or like a logistic
regression model so the most important
part in a confusion Matrix is that first
of all this as you can see this is a
matrix and the size of the Matrix
depends on how many outputs uh we are
expecting right so the the most
important part here is that the model
will be most accurate when we have the
maximum numbers in its diagonal like in
this case that's why it has almost 93
94% because the diagonals should have
the maximum numbers and the others other
than diagonals the cells other than the
diagonals should have very few numbers
so here that's what is happening so
there is a two here there are there's a
one here but most of them are along the
diagonal this what does this mean this
means that the number that has been fed
is zero and the number that has been
detected is also zero so the predicted
value and actual value are the same so
along the diagonals that is true which
means that let's let's take this
diagonal right if if the maximum number
is here that means that like here in
this case it is 34 which means that 34
of the images that have been fed or
rather actually there are two
misclassifications in there so 36 images
have been fed which have number four and
out of which 34 have been predicted
correctly as number four and one has
been predicted predicted as number eight
and another one has been predicted as
number nine so these are two
misclassifications okay so that is the
meaning of saying that the maximum
number should be in the diagonal so if
you have all of them so for an ideal
model which has let's say 100% accuracy
everything will be only in the diagonal
there will be no numbers other than zero
in all other cells so that is like a
100% accurate model okay so that's uh
gist of how to use this Matrix how to
use this uh confusion Matrix I know the
name uh is a little funny sounding
confusion Matrix but actually it is not
very confusing it's very straightforward
so you just plotting what has been
predicted and what is the labeled
information or what is the actual data
that's also known as the ground truth
sometimes okay these are some fancy
terms that are used so predicted label
and the actual label that's all it is
okay yeah so we are showing a little bit
more information here so 38 have been
predicted and here you you will see that
all of them have been predicted
correctly there have been 38 zeros and
the the predicted value and the actual
value is is exactly the same whereas in
this case right it has there are I think
37 + 5 yeah 42 have been fed the images
42 images are of Digit three and uh the
accuracy is only 37 of them have been
accurately predicted three of them have
been predicted as number seven and two
of them have been predicted as number
eight and so on and so forth okay all
right so with that let's go into Jupiter
notebook and see how the code looks so
this is the code in in Jupiter notebook
for logistic regression in this
particular demo what we are going to do
is train our model to recognize digits
which are the images which have digits
from let's say 0 to 5 or 0 to 9 and um
and then we will see how well it is
trained and whether it is able to
predict these numbers correctly or not
so let's get started so the first part
is as usual we are importing some
libraries that are required and uh then
the last line in this block is to load
the digits so let's go ahead and run
this code then here we will visualize
the shape of these uh digits so we can
see here okay if we take a look this is
how the shape is
1797 by 64 these are like 8 by8 images
so that's that's what is reflected in
this uh shape now from here onwards we
are basically once again importing some
of the libraries that are required like
numi and M plot and we will take a look
at uh some of the sample images that we
have loaded so the this one for example
creates a figure uh and then we go ahead
and take a few sample images
to see how they look so let me run this
code and so that it becomes easy to
understand so these are about five
images sample images that we are looking
at 0 1 2 3 4 so this is how the images
this is how the data is okay and uh
based on this we will actually train our
logistic regression model and then we
will test it and see how well it is able
to recognize so the way it works is the
pixel information so as you you can see
here this is an 8 by 8 pixel kind of a
image and uh the each pixel whether it
is activated or not activated that is
the information available for each pixel
now based on the pattern of this
activation and non-activation of the
various pixels this will be identified
as a zero for example right similarly as
you can see so overall each of these
numbers actually has a different pattern
of the pixel activation and that's
pretty much that our model needs to
learn for which number what is the
pattern of the activation of the pixels
right so that is what we are going to
train our model okay so the first thing
we need to do is to split our data into
training and test data set right so
whenever we perform any training we
split the data into training and test so
that the training data set is used to
train the system so so we pass this
probably multiple times uh and then we
test it with the test data set and the
split is usually in the form of there
and there are various ways in which you
can split this data it is up to the
individual preferences in our case here
we are splitting in the form of 23 and
77 so when we say test size as 20. 23
that means 23% of the entire data is
used for testing and the remaining 77%
is used for training so there is a
readily available function which is uh
called train test split so we don't have
to write any special code for the
splitting it will automatically split
the data based on the proportion that we
give here which is test size so we just
give the test size automatically
training size will be determined and uh
we pass the data that we want to split
and the the results will be stored in
xcore train train and Yore train for the
training data set and what is xcore
train this are these are the features
right which is like the independent
variable and Yore train is the label
right so in this case what happens is we
have the input value which is or the
features value which is in xor train and
since this is labeled data for each of
them each of the observations we already
have the label information saying
whether this digit is a zero or a one or
a two so that this this is what will be
used for comparison to find out whether
the the system is able to recognize it
correctly or there is an error for each
observation it will compare with this
right so this is the label so the same
way xcore train Yore train is for the
training data set xcore test Yore test
is for the test data set okay so let me
go ahead and execute this code as well
and then we can go and check quickly
what is the how many entries are there
and in each of this so xcore train the
shape is 13 83 by 64 and ycore train has
1383 because there is uh nothing like
the second part is not required here and
then xcore test shape we see is 414 so
actually there are 414 observations in
test and 1383 observations in train so
that's basically what these four lines
of code are are saying okay then we
import the logistic regression library
and uh which is a part of psychic learn
so we we don't have to implement the
logistic regression process itself we
just call these the function and uh let
me go ahead and execute that so that uh
we have the logistic regression Library
imported now we create an instance of
logistic regression right so logistic RR
is a is an instance of logistic
regression and then we use that for
training our model so let me first
execute this code so these two lines so
the first line basically creates an
instance of logistic regression model
and then the second line is where we are
passing our data the training data set
right this is our the the predictors and
uh this is our Target we are passing
this data set to train our model all
right so once we do this in this case
the data is not large but by and large
uh the training is what takes usually a
lot of time so we spend in machine
learning activities in machine learning
projects we spend a lot of time for the
training part of it okay so here the
data set is relatively small so it was
pretty quick so all right so now our
model has been trained using the
training data set and uh we want to see
how accurate this is so what we'll do is
we will test it out in probably faces so
let me first try out how well this is
working for uh one image okay I will
just try it out with one image my the
first entry in my test data set and see
whether it is uh correctly predicting or
not so and in order to test it so for
training purpose we use the fit method
there is a method called fit which is
for training the model and once the
training is done if you want to test for
uh a particular value new input you use
the predict method okay so let's run the
predict method and we pass this
particular image and uh we see that the
shape is or the prediction is four so
let's try a few more let me see for the
next 10 uh seems to be fine so let me
just go ahead and test the entire data
set okay that's basically what we will
do so now we want to find out how
accurately this has um performed so we
use the score method to find what what
is the percentage of accuracy and we see
here that it has performed up to 94%
Accurate okay so that's uh on this part
now what we can also do is we can um
also see this accuracy using what is
known as uh confusion Matrix so let us
go ahead and try that as well uh so that
we can also visualize how well uh this
model has uh done so let me execute this
piece of code which will basically
import some of the libraries that are
required and um we we basically create a
confusion Matrix an instance of
confusion matrix by running confusion
Matrix and passing these uh values so we
have so this confusion uncore Matrix
method takes two parameters one is the
Yore test and the other is the
prediction so what is the Yore test
these are the labeled values which we
already know for the test data set and
and predictions are what the system has
predicted for the test data set okay so
this is known to us and this is what the
system has uh the model has generated so
we kind of create the confusion Matrix
and we will print it and this is how the
confusion Matrix looks as the name
suggests it is a matrix and um the key
point out here is that the accuracy of
the model is determined by how many
numbers are there in the diagonal the
more the numbers in the diagonal the
better the accuracy is okay and first of
all the total sum of all the numbers in
this whole Matrix is equal to the number
of observations in the test data set
that is the first thing right so if you
add up all these numbers that will be
equal to the number of observations in
the test data set and then out of that
the maximum number of them should be in
the diagonal that means the accuracy is
predic good if the the numbers in the
diagonal are less and in all other
places there are a lot of numbers which
means the accuracy is very low the
diagonal indicates a correct prediction
this means that the actual value is same
as the predicted value here again actual
value is same as the predicted value and
so on right so the moment you see a
number here that means the actual value
is something and the predicted value is
something else right similarly here the
actual value is something and the
predicted value is something else so
that that is basically how we read the
confusion Matrix now how do we find the
accuracy you can actually add up the
total values in the diagonal so it's
like 38 + 44 Plus 43 and so on and
divide that by the total number of test
observations that will give you the
percentage accuracy using a confusion
Matrix now let us visualize this
confusion Matrix in a slightly more
sophisticated way uh using a heat heat
map so we will create a heat map with
some we'll add some colors as well it's
uh it's like a more visually visually
more appealing so that's the whole idea
so if we let me run this piece of code
and this is how the heat map uh looks uh
and as you can see here the diagonals
again are all the values are here most
of the values so which means reasonably
this seems to be reasonably accurate and
yeah basically the accuracy score is 94%
this is SC calculated as I mentioned by
adding all these numbers divided by the
total test values or the total number of
observations in test data set okay so
this is the confusion Matrix for
logistic
regression all right so now that we have
seen the confusion Matrix let's take a
quick sample and see how well uh the
system has classified and we will take a
few examples of the data so if we see
see here we we picked up randomly a few
of them so this is uh number four which
is the actual value and also the
predicted value both are four this is an
image of zero so the predicted value is
also zero actual value is of course zero
then this is the image of nine so this
is also been predicted correctly nine
and actual value is nine and this is the
image of one and again this has been
predicted correctly as like the actual
value Okay so so this was a quick demo
of logistic regression how to use
logistic regression to identify images
if you are an aspiring machine learning
engineer looking for online training and
certifications from prestigious
universities and in collaboration with
leading experts then search no more
simply learns professional certification
program in Ai and machine learning from
Pur University in collaboration with IVM
should be your right choice for more
details use the link in the description
box below with that in mind there's all
kinds of regression models that come out
of this so we put them side to by side
we have our linear regression which is a
predictive number used to predict a
dependent output variable based on
Independent input variable accuracy is a
measured uh using least squares
estimation so that's where you take uh
you could also use absolute value uh the
least squares is more popular there's
reasons for that mathematically and also
for computer
time uh but it does give you an an
accuracy based on the the least Square
estimation the best fit line is a
straight line and clearly that's not
always used in all the regression models
there's a lot of variations on that the
output is a predicted integer value
again this is what we're talking about
we're talking about linear regression
and we're talking about regression it
means the number is coming out linear
usually means we're looking for that
line versus a different model and it's
used in business domain forecasting
stocks uh it's used as a basis of of
most um uh predictions with numbers so
if you're looking at a lot of numbers
you're probably looking at a a linear
regression
model uh for instance if you do just the
high lows of the stock exchange and
you're you're going to take a lot more
of that if you want to make money off
the stock you'll find that the linear
regression model fits uh probably better
than almost any of the other models even
you know highend neural networks and all
these other different machine learning
and AI models because they're numbers
they're just a straight set of numbers
you have a high value a low value volume
uh that kind of thing so when you're
looking at something that straight
numbers um and are connected in that way
us you're talking about a linear
regression model and that's where you
want to start a logistic regression
model used to classify dependent output
variable based on Independent input
variable so just like the linear
regression model and like all of our
machine learning tools you have your
features coming in uh and so in this
case you might have uh label you know an
image or something like that is is
probably the very popular thing right
now labeling broccoli and vegetables or
whatever accuracy is measured using
maximum likelihood estimation the best
fit is given by a curve and we saw that
um we're talking about linear regression
you definitely are talking about a
straight line although there is other
regression models that don't use
straight lines and usually when you're
looking at a logistic regression the
math as you saw was still kind of a
ukian line but it's now got that sigmoid
activation which turns it into um a
heavily weighted curve and the output is
a binary value between zero and one and
it's used for classification image
processing as I mentioned is is what
people usually think of um although they
use it for classification of um like a
window of things so you could take a
window of stock history and you could
CLA generate classifications based on
that and separate the data that way if
it's going to be that this particular
pattern occurs it's going to be upward
trending or downward
trending in fact a number of stock uh uh
Traders use that not to tell them how
much to bid or what to bid uh but they
use it as to whether it's worth looking
at the stock or not whether the Stock's
going to go down or go up and it's just
a zero one do I care do I even want to
look at it so let's do a demo so you can
get a picture of what this looks like in
Python code let's predict the price at
which insurance should be sold to a
particular customer based on their
medical history we will also classify on
a mushroom data set to find the
poisonous and non-poisonous
mushrooms and when you look at these two
datas the first one uh we're looking at
the price so the price is a number um so
let's predict the price which the
insurance should be sold to and the
second one is we're looking at either
it's poisonous or it's not poisonous so
first off before we begin the demo I'm
in the Anaconda Navigator in this one
I've loaded the python
3.6 and using the Jupiter notebook and
you can use jupyter notebook by itself
um you can use the Jupiter lab which
allows multiple tabs it's basically the
notebook with tabs on it uh but the
Jupiter notebook is just fine and it'll
go into uh Google Chrome which is what
using for my Internet Explorer and from
here we open up new and you'll see
Python 3 and again this is loaded with
python
3.6 and we're doing the linear versus
logic uh regression or logit you'll see
L
git um is one of the one of the names
that kind of pops up when you do a
search on here uh but it is a logic
we're looking at the logistic regression
models and we'll start with the linear
regression uh because it's easy to
understand you draw a line through stuff
um and so in programming uh we got a lot
of stuff to unfold here in our in our uh
startup as we preload all of our
different
parts and let's go ahead and break this
up we have at the beginning import uh
pandas so this is our data frame uh it's
just a way of storing the data think of
a uh when you talk about data frame
think of a spread sheet you rows and
columns it's a nice way of viewing the
data and then we have uh we're going to
be bringing in our pre-processing label
encoder I'll show you what that is um
when we get down to it it's easier to
see in the data uh but there's some data
in here like um sex it's male or female
so it's not like an actual number it's a
either your one or the other that kind
of stuff ends up being encoded that's
what this label encoder is right here we
have our test split model if you're
going to build a model uh you do not
want to use all the data you want to use
some of the data and then test it to see
how good it is and if it can't have seen
the data you're testing on until you're
ready to test it on there and see how
good it is and then we have our uh
logistic regression model our
categorical one and then we have our
linear regression model these are the
two these right here let me just um um
clear all that there we go uh these two
right here are what this is all about
logistic versus uh linear is it
categorical are we looking for a true
false or are we looking for um a
specific
number and then finally um usually at
the very end we have to take and just
ask how accurate is our model did it
work um if you're trying to predict
something in this case we're going to be
doing um uh Insurance costs uh how close
to the insurance cost does it make
measure that we expect it to be you know
if you're an insurance company you don't
want to promise to pay everybody's
medical bill and not be able
to and in the case of the mushrooms you
probably want to know just how much at
risk you are for following this model uh
as to far as whether you're going to get
eat a poisonous mushroom and die or
not um so we'll look at both of those
and we'll get talk a little bit more
about the shortcomings and the um uh
value of these different processes
so let's go ahead and run this this has
loaded the data set on here and then
because we're in Jupiter notebook I
don't have to put the print on there we
just do data set and by and it prints
out all the different data on here and
you can see here for our insurance
because that's what we're starting with
uh we're loading that with our pandas
and it prints it in a nice format where
you can see the AG sex uh body mass
index number of children smoker so this
might be something that the insurance
company gets from the doctor it says hey
we're going to this is what we need to
know to give you a quote for what we're
going to charge you for your
insurance and you can see that it has uh
1,338 rows and seven columns you can
count the columns 1 2 3 4 five six seven
so there's seven columns on
here and the column we're really
interested in is charges um I want to
know what the charges are going to be
what can I expect not a very good Arrow
drawn
um what to expect them to charge on
there uh so is this going to be you know
is this person going to cost me uh
$16,814.23
mentioned it before but I'm going to
mention it again because pre-processing
data is so much of the work in data
science um sex well how do you how do
you deal with female versus male um are
you a smoker yes or no what does that
mean region how do you look at Region
it's not a number how do you draw a line
between Southwest and
Northwest um you know they they're
objects it's either your Southwest or
your Northwest it's not like I'm
southwest I guess you could do long
longitude and latitude but the data
doesn't come in like that it comes in as
true false or whatever you know it's
either your Southwest or your
Northwest so we need to do a little bit
of pre-processing of the data on here to
make this
work oops there we go okay so let's take
a look and see what we're doing with
pre-processing and again this is really
where you spend a lot of time with data
Sciences trying to understand how and
why you need to do that and so we're
we're going to do uh you'll see right up
here label uh and then we're going to do
the do a label encoder one of the
modules we brought in so this is SK
learns uh label
encoder I like the fact that it's all
pretty much automated uh but if you're
doing a lot of work with the label
encoder you should start to understand
how that
fits um and then we have uh label. fit
right here where we're going to go ahead
and do the data set uh. sex. drop
duplicates and then for data set sex
we're going to do the label transform
the data sex and so we're looking right
here at um male or female and so it
usually just converts it to a zer1
because there's only two choices on
here same thing with the smoker it's
zero or one so we're going to transfer
the trans change the smoker uh 01 on
this and then finally we did region down
here region does it a little bit
different we'll take a look at that and
um it's I think in this case it's
probably going to do it because we did
it on this label
transform um with this particular setup
it gives each region a number like 0 1
23 so let's go a and take a look and see
what that looks like go and run
this and you can see that our new data
set um has age that's still a number uh
Sex Is Zero or one uh so zero is female
one is male number of children we left
that alone uh smoker one or zero it says
no or yes on there we actually just do
one for no zero or no yeah one for no
I'm not sure how it organized them but
it turns the smoker into zero or one yes
or no uh and then region it did this as
uh 0 1 2 three so it's three
regions now a lot of times in in when
you're working with data science and
you're dealing with uh regions or even
word
analysis um instead of doing one column
and labeling it 0 1 2 3 A lot of times
you increase your features and so you
would have region Northwest would be one
column yes or no region Southwest would
be one colum yes or no true
01 uh but for this this this particular
setup this will work just fine on here
now that we spent all that time getting
it set up uh here's the fun part uh
here's the part where we're actually
using our setup on this and you'll see
right here we have our um y linear
regression uh data set dro the charges
because that's what we want to
predict and so our X I'm sorry our X
linear data set drop the charges because
that's where we're going to predict
we're predicting charges right here so
we don't want that as our input for our
features and our y output is charges
that's what we want to guess we want to
guess what the charges
are and then what we talked about
earlier is we don't want to do all the
data at once so we're going to take
um3 means 30% we're going to take 30% of
our data and it's going to be as the
train as the testing site so here's our
y test and our X test down there um and
so that part our model will never see it
until we're ready to test to see how
good it is and then of course right here
you'll see our um training set and this
is what we're going to train it we're
going to trade it on 70% of the day data
and then finally the big ones uh this is
where all the magic happens this is
where we're going to create our magic
setup and that is right here our linear
model we're going to set it equal to the
linear regression model and then we're
going to fit the data on
here and then at this point I always
like to pull up um if you if you if
you're working with a new model it's
good to see where it comes from and this
comes from the site kit uh learn
and this is the sklearn linear model
linear regression that we imported
earlier and you can see they have
different parameters the basic parameter
works great if you're dealing with just
numbers uh mentioned that earlier with
stock high lows this model will do as
good as any other model out there for do
if you're doing just the very basic high
lows and looking for a linear fit a
regression model fit um and what you one
of the things when I am looking at this
is I for
methods and you'll see here's our fit
that we're using right now and here's
our predict and we'll actually do a
little bit in the middle here as far as
looking at some of the parameters hidden
behind it the math that we talked about
earlier and so we go in this and we go
ahead and run this you'll see it loads
the linear regression model and just has
a nice output that says hey I loaded the
linear regression model and then the
second part is we did the fit and so
this model is now trained our our linear
model is now trained on the training
data and so one of the things we can
look at is the um um for idx and colon
name and enumerate X linear train
columns come an interesting thing this
prints out the coefficients uh so when
you're looking at the back end of the
data you remember we had that formula uh
BX X1 plus bxx2 plus the plus the uh
intercept and so forth these are the
actual coefficients that are in here
this is what it's actually multiplying
these numbers
by and you can see like region gets a
minus value so when it adds it up I
guess region you can read a lot into
these numbers uh gets very complicated
there's ways to mess with them if you're
doing a basic linear regression model
you usually don't look at them too
closely uh but you might start looking
in these and saying hey you know what uh
smoker look how smoker impacts the cost
um it's just massive uh so this is a
flag that hey the value of the smoker
really affects this model and then you
can see here where the body mass index
uh so somebody who is overweight is
probably less healthy and more likely to
have cost money and then of course age
is a factor um and then you can see down
here we have uh sex is than a factor
also and it just it changes as you go in
there negative number it probably has
its own meaning on there again it gets
really complicated when you dig into the
um workings and how the linear model
works on that and so um we can also look
at the intercept this is just kind of
fun um so it starts at this negative
number and then adds all these numbers
to it that's all that means that's our
intercept on there and that fits the
data we have on that and so you can see
right here we can go back and oops give
me just a second there we go we can go
ahead and predict the unknown data and
we can print that out and if you're
going to create a model to predict
something uh we'll go ahead and predict
it here's our y prediction value linear
model
predict and then we'll go ahead and
create a new data frame in this case
from our X linear test group we'll go
ahead and put the cost back into this
data frame and then the predicted cost
we're going to make that equal to our y
prediction and so when we pull this up
uh you can see here that we have uh the
actual cost and what we predicted the
cost is going to
be there's a lot of ways to measure the
accuracy on there uh but we're going to
go ahead and jump into our mushroom
data and so in this you can see here we
we've run our basic model we built our
coefficients you can see the intercept
the back end you can see how we're
generating a number here uh now with
mushrooms we want to yes yes or no we
want to know whether we can eat them or
not and so here's our mushroom file
we're going to go and run this take a
look at the data and again you can ask
for a copy of this file uh send a note
over to Simply
learn.com and you can see here that we
have a class um the cap shape cap
surface and so forth so there's a lot of
feature in fact there's 23 different
columns in here going
across and when you look at this um I'm
not even sure what these particular like
P e e PE I don't even know what the
class is on
this I'm going to guess by the notes
that the class is uh poisonous or
edible so if you remember before we had
to do a little precoding on our data uh
same thing with here uh we have our cap
shape which is b or X or k um we have
cap color uh these really aren't numbers
so it's really hard to do anything with
just a a single number so we need to go
ahead and turn those into a label
encoder which again there's a lot of
different encoders uh with this
particular label encoder is just
switching it to 0 1 2 3 and giving it an
integer
value in fact if you look at all the
columns all of our columns are labels
and so we're just going to go ahead and
uh loop through all the columns in the
data and we're going to transform it
into a um label encoder and so when we
run this you can see how this gets
shifted from uh xbxx K to 01 2 3 45 or
whatever it is class is 01 one being
poisonous zero looks like it's
editable and so forth on here so we're
just encoding it if you were doing this
project depending on the results you
might encode it differently like I
mentioned earlier you might actually
increase the number of features as
opposed to laboring at Z 0 1 2 3 4 5 um
in this particular example it's not
going to make that big of a difference
how we encode
it and then of course we're looking for
uh the class whether it's poisonous or
edible so we're going to drop the class
in our x uh Logistics model and we're
going to create our y Logistics model is
based on that class so here's our
XY and just like we did before we're
going to go ahead and split it uh using
30 % for
test 70% to program the model on
here and that's right here whoops there
we go there's our um train and
test and then you'll see here on this
next setup um this is where we create
our model all the magic happens right
here uh we go ahead and create a
logistics model I've up the max
iterations if you don't change this for
this particular problem you'll get a
warning that says this has not
converged um because that that's what it
does is it goes through the math and it
goes hey can we minimize the error and
it keeps finding a lower and lower error
and it still is changing that number so
that means it hasn't conversed yet it
hasn't find the lowest amount of error
it can and the default is 100 uh there's
a lot of settings in here so when we go
in here to let me pull that up from the
sklearn uh so we pull that up from the
sklearn
model you can see here we have our
logistic it has our different settings
on here that you can mess with most of
these work pretty solid on this
particular setup so you don't usually
mess a lot usually I find myself
adjusting the um iteration and I'll get
that warning and then increase the
iteration on there and just like the
other model you can go just like you did
with the other model we can scroll down
here and look for our
methods and you can see there's a lot of
of methods uh available on here and
certainly there's a lot of different
things you can do with it uh but the
most basic thing we do is we fit our
model make sure it's set right uh and
then we actually predict something with
it so those are the two main things
we're going to be looking at on this
model is fitting and predicting there's
a lot of cool things you can do that are
more advanced uh but for the most part
these are the two which um I use when
I'm going into one of these models and
setting them
up so let's go ahead close out of our
sklearn setup on there and we'll go
ahead and run this and you can see here
it's now loaded this up there we now
have a uh uh logistic model and we've
gone ahead and done a predict here also
just like I was showing you earlier uh
so here is where we actually predicting
the data so we we've done our first two
lines of code as we create the model we
fit the model to our training data and
then we go ahead and predict for our
test data now in the pr previous model
we didn't dive into the test score um I
think I just showed you a graph and we
can go in there and there's a lot of
tools to do this we're going to look at
the uh model score on this one and let
me just go ahead and run the model
score and it says that it's pretty
accurate we're getting a roughly 95%
accuracy well that's good one 95%
accuracy 95% accuracy might be good for
a lot of
things but when you look at something as
far as whether you're going to pick a
mushroom on the side of the trail and
eat it we might want to look at the
confusion Matrix and for that we're
going to put in our y listic test the
actual values of edible and unedible and
we're going to put in our prediction
value and if you remember on here U
let's see I believe it's poisonous was
one uh zero is edible so let's go ahead
and run that 01 zero is good so here is
um
a confusion Matrix and this is if you're
not familiar with these we have true
true true
false true false false false so it says
out of the edible mushrooms we correctly
labeled 121 mushrooms edible that were
edible and we correctly measured
1,113 poisonous mushrooms as
poisonous but here's the
kicker I labeled 56 edible mushrooms as
being um poisonous well that's not too
big of a deal we just don't eat them but
I measured 68 mushrooms as being edible
that were poisonous so probably not the
best choice to use this model to predict
whether you're going to eat a mushroom
or not and you'd want to dig a Little
Deeper before you uh start eat picking
mushrooms off the side of the trail so a
little warning there when you're looking
at any of these data models looking at
the error and how that error fits in
with what domain you're in domain in
this case being edible mushrooms uh be a
little careful make sure that you're
looking at them
correctly so we've looked at uh edible
or not edible we've looked at uh
regression model as far as U the end
values what's going to be the cost and
what our predicted cost is so we can
start figuring out how much to charge
these people for their
insurance and so these really are the
fundamentals of data science when you
pull them together uh when I say data
science I'm talking about your machine
learning
code and hopefully you got a little bit
out of here again you can contact our
simply learn team and get a copy of
these files or get more information on
this if you are an aspiring machine
learning engineer looking for online
training and certifications from
prestigious universities and in
collaboration with leading experts then
search no more simply learn professional
certification program in Ai and machine
learning from P University in
collaboration with IBM should be your
right choice for more details use the
link in the description box below with
that in
mind a confusion Matrix represents a
table layout of the different outcomes
of prediction and results of a
classification problem and helps
visualize its
outcomes and so you see here we have our
uh simple chart predicted and actual the
confusion Matrix helps us identify the
correct predictions of a model for
different individual classes as well as
the errors so you'll see here that the
values predicted by our classifier are
along the rows this is what we're going
to guess it is or our our model is
guessing what this is based on its
training so we've already trained the
model to um guess whether it's spam or
not spam or whatever it is you're
working on and then the actual values of
our data set are along the
columns so this is the actual value it's
supposed to be people who can speak
English will be classified as positives
so because they have a remember 01 do
you speak English yes no and you could
extend this that they might have do you
speak uh French do you speak whatever
languages and so you might have a whole
lot of classifiers that you would look
at each one of these people who cannot
speak English will be classified as
negatives so there'll be a zero so you
know zero ones the number of times our
actual positive values are equal to
predicted positive values gives us true
positive TP the number of times our
actual negative values are equal to
predictive negative values gives us true
negative
TN the number of times our model wrongly
predicts negative values as
positives gives us a false
positive
FP and you'll see when you're working
with these a lot you know memorizing
that it's false positive you can easily
figure out what that is and pretty soon
you're just looking at the FP or the TP
depending on what you're working on and
the number times our model wrongly
predicts positive values as negative
gives us a false negative
FP now I'm going to do a quick step out
here let's say you're working in the
medical and we're talking about
cancer uh do you really want a bunch of
false negatives you want zero under
false negative uh so when we look at
this confusion Matrix if you have 5%
false positives and 5% false negatives
it'd be much better to even have 20%
false positives because they go in and
test it and zero false negatives the
same might be true if you're working on
uh uh say uh a car driving is this a
safe place for the car to go well you
really don't want any false positives
you know yes this is safe right over the
cliff so again when you're working on
the project or whatever it is you're
working on this chart suddenly has huge
value uh we were talking about spam
email how many important emails say from
your banking overdraft charge coming in
that you want to be uh a true a false
negative you don't want it to go in the
spam folder likewise you want to get as
much of the spam out of there but you
don't want to miss anything really
important confusion Matrix metrics are
performance measures which help us find
the accuracy of our classifier there are
four main metrics accuracy precision
recall and F1 score the F1 score is the
one I usually hear the most and accuracy
is usually what you put on your chart uh
when you're sing in front of the
shareholders how accurate is it people
understand
accuracy um F1 score is a little bit
more on the math side and so you got to
be a little careful when you're quoting
F1 scores in the when you're sitting
there with all the shareholders because
a lot of them will just glaze over so
confusion Matrix metrics are performance
measures which help us find the accuracy
of our classifier there are four main
metrics accuracy the accuracy is used to
find the portion of the correctly
classified values it tells us how often
our classifier is right it is the sum of
all True Values divided by the total
values and this makes sense uh again
it's one of those
things I don't want to F you know
depends on what you're looking for are
you looking for not to miss any spam
mails are you looking to drive down the
road and not run anybody over
Precision is used to calculate the
model's ability to classify positive
values correctly it answers the question
when the model predicts a positive value
how often is it right it is the true
positive divided by the total number of
predicted positive values again this one
uh depends on what project you're
working on whether this is what you're
going to be focusing on uh so recall it
is used to calculate the model's ability
to predict positive values how often
does the model actually predict the
correct positive values it is the true
positives divided by the total number of
actual positive values and then your F1
score it is the harmonic mean of recall
and precision it is useful when you need
to take both precision and recall into
account consider the following two
confusion Matrix derived from two
different classifier to figure out which
one performance better we can find the
confusion Matrix for both of them and
you can see we're back to uh does it
classify whether they can speak English
or or non-speaker they speak some they
don't know the English language and so
we put these two uh uh confusion
matrixes out here we can go ahead and do
the math behind that we can look up the
accuracy that's a tpn plus TN over the
TF plus TN plus FP plus FN and so we get
an accuracy of0
8125
and we have a Precision if you do the
Precision which is your TP truth
positive over TP plus
FP uh we get
891 and if we do the recall we'll end up
with the 0 825 that's your TP over TP
plus FN and then of course your F1 score
which is 2 * Precision Time recall over
Precision plus
recall and we get the 0
857 and if we do that um with another
model let's say we had two different
models and we're trying to see which one
we want to use uh for whatever reason uh
we might go ahead and compute the same
things we have our accuracy our
precision and our recall and our F1
score and uh as we're looking at this we
might uh look at the accuracy because
that's really what we're interested in
is uh how many people are we able to
classify as being able to speak English
I really don't want to know if I'm you
know I I I I really don't want to know
if I if they're non-speakers um I'd
rather Miss 10 people speaking English
instead of 15 and so you can see from
these charts we'd probably go with the
first model because it does a better job
guessing who speaks English and has
higher accuracy because in this case
that is what we're looking
for so uh with that we'll go ahead and
pull up a demo so you can see what this
looks like in the python uh setup in in
the actual coding for this we'll go into
Anaconda Navigator if you're not
familiar with Anaconda uh it's a really
good tool to use as far as doing display
and demos and for quick development um
as a data scientist I just love the
package now if you're going to do
something heavier lifting uh there's
some limitations with anaconda and with
the setup but in general you can do just
about anything in here with your Python
and for this we'll go with Jupiter
notebook uh Jupiter lab is the same as
Jupiter notebook you'll see they now
have integration with pie charm if you
work in py charm uh certainly there's a
lot of other Integrations that Anaconda
has and we've opened up um my simply
learn files I work on and create a new
file called confusion Matrix demo and
the first thing we want to note is the
data we're working with uh here I've
opened it up in a word pad or notepad or
whatever uh you can see it's got a row
of uh headers uh comma separated and
then all the data going down below
and then I saved this in the same file
so I don't have to remember what path
I'm working on uh of course if you have
your data separated and you're working
with a lot of data you probably want to
put in a different folder or file
depending on what you're doing and the
first thing we want to do is go ahead
and import our tools uh we're going to
use the pandas that's our data frame if
you haven't had a chance to work with
the data frame please review pandas data
frame going to Simply learn you can pull
up the pandas data frame um tutorial on
there and then we're going to going to
use uh the scikit framework which is all
denoted as
sklearn and I can just pull this in you
can see here's the um
scikit-learn dorg with a stable version
that you can import into your
Python and from here we're going to use
the train test split for splitting our
data we're going to do some
pre-processing we're going to do use the
logistic regression model that's our
actual uh machine learning model we're
using and then what this this particular
setup is about is we're going to do the
accuracy score the confusion Matrix and
the classifier report so let me go ahead
and run that and bring all that
information
in and just like we opened the file we
need to go ahead and load our data in
here uh so we're going to go ahead and
do our pandas read CSV and then just
because we're in jupyter Notebook we can
just put data to read the data in here a
lot of times we well actually let me
just do this I prefer to do the just the
head of the datea or the top
part and you can see we have age sex um
I'm not sure what CP stands for test BPS
cholesterol uh so a lot of different
measurements uh if you were in this
domain you'd want to know what all these
different measurements mean I don't want
to focus on that too much because when
we're talking about data science a lot
of times you have no idea what the data
means if you've ever looked up the
breast cancer measurement is just a
bunch of measurements and numbers uh
unless you're a doctor you're going to
have no idea what those measurements
mean but if it's your specialty and your
domain you better know them so we're
going to go ahead and create Y and it's
going to we're going to set it equal to
the Target uh so here's our Target value
here and it's either one or
zero so we have a classifier if you're
dealing with one zero true false what do
you have you have a classifier
and then our X is going to be uh
everything except for the Target uh so
we're going to go ahead and drop the
target axis equals one remember that's
columns versus uh the index or rows axis
equals zero would would give you an
error but you would drop like row two
and then we'll go ahead and just print
that out so you can see what we're
looking at and uh here we have um Y data
X data uh and you can see from the X
data we have the X head
and we can go ahead and just do
print the Yad
data and run
that so this is all loading the data
that we've done so far uh if there's a
confusion in there go back and rewind
the tape and review it and then we need
to go ahead and split our data into our
X train X test y train y test and then
keep in mind you always want to split
the data before we do the scaler and the
reason is is that you want the scaler on
the training data uh to be set on the
training data data or fit to it but not
on the test data think of this as being
out in the field uh you're not it could
actually alter your results uh so it's
always important to do make sure
whatever you do to the training data or
whatever um fit you're doing is always
done on the training not on the test and
then we want to go ahead and scale the
data now we are working with um linear
regression model I I'll mention this
here in a minute when we get to the
actual model uh so some sometimes you
don't need to scale it when you're
working with linear regression models
it's not going to change your result as
much as say a neural network where it
has a huge
impact uh but we're going to go ahead
and take here's our XT train X test YT
train y test we create our scaler we go
ahead and scale uh the scale is going to
fit the X
train and then we're going to go ahead
and take our X train and transform it
and then we also need to take our X test
and transform it based on the scale on
here so that our X is now between that
nice minus one to one and so this is all
uh our pre- dat setup and hopefully uh
all of that looks fairly familiar to you
if you've done a number of our other
classes and you're up to the setup on
here and then we want to go ahead and do
is create our model and we're going to
use the logistic regression model and
from the logistic regression model uh
we're going to go ahead and fit our X
train and Y train and then we'll run our
predicted value on here and so let's go
ahead and run that and so now we are uh
we actually have like our X test and our
prediction so if you remember
from our Matrix we're looking for the
actual versus the prediction and and how
those
compare and if I take this back up here
you're going to notice that we imported
the accuracy score the confusion Matrix
and the classification report uh and
there's of course our logistic
regression the model we're using for
this and I did mention I was going to
talk a little bit about scaler and the
regression
model the scaler on a lot of your
regression models uh your basic Mass
standard regression models and I'd have
to look it up for the logistic
regression model when you're using a
standard regression model you don't need
to scale the data uh it's already just
built in by the way the model
Works in most cases uh but if you're in
a neural network and there's a lot of
other different setups then you really
want to take this and uh fit that on
there and so we can go in and do the
accuracy uh and this is if you remember
correctly we were looking at the
accuracy with the english- speaking uh
so this is saying our accuracy as to
whether this person is I believe this is
the heart data
set um it's going to be accurate about
85% of the time as far as whether it's
going to predict the person's going to
have um a heart condition or the one as
it comes up with the 01 on there which
would mean at this point that you have
an 85% uh being correct on telling
someone they're extremely high risk for
a heart attack kind of
thing and so we want to go ahead and uh
create our confusion Matrix and let me
just do
that of course the software does
everything for us so we'll go ahead and
run this and you can see right here um
here's our
25 uh prediction uh correct predictions
right
here and if you remember from our slide
I'll just bring this over so it's a nice
visual we have our true positive false
positive uh so we had 25 which were true
true that it said hey this person's
going to be high risk at um heart and we
had four that were still high risk that
it said we false um so out of these 25
people or out of these 29 people and
that that makes sense because you have
085 uh out of 29 people it was correct
on 25 of them and so uh here's our
accuracy score we were just looking at
that our accuracy is your true positive
and your true negative over all of them
so how true is it and there was our
accuracy um coming up here
085 and then we have our nice Matrix
generated from that uh and you can see
right here is a similar Matrix we had
going for from the slide and this starts
to this should start asking questions at
this point um so if you're in a board
meeting or you're working with this you
really want to start looking at this
data here and saying well is this good
enough is uh this number of people and
hopefully you'd have a much larger data
set it my is my confusion Matrix showing
for the true positive and uh false
positive is that acceptable for what
we're doing uh and of course if you're
going to put together whatever data
you're putting out you might want to
separate the uh true negative false
positive false negative true positive
and you can simply do that uh by doing
the confusion Matrix uh and then of
course the Ravel part lets you um set
that up so you can just split that right
up into a nice tupal and the final thing
we want to show you here in the coding
on this part is the confusion Matrix
metric and so we can come in here and
just use the Matrix equals
classification report the Y test and the
predict and then we're going to take
that classification report and go ahead
and print that out and you can see here
it does a nice job uh giving you your
accuracy uh your micro average your
weighted average um you have your
Precision your recall your F1 score and
your support all in one window window
you can start looking at this data and
saying oh okay our precision's at
83 uh 87 for getting a a positive and 83
for the negative side for a zero and we
start talking about whether this is a
valid information or not to use and when
we're looking at a heart attack
prediction we're only looking at one
aspect what's the chances of this person
having a heart attack or not um you
might have something where we went back
to language
maybe you also want to know whether they
speak English or Hindi uh or French and
you can see right here that we can now
take our confusion Matrix and just
expand it as big as we need to depending
on how many different classifiers we're
working on if you are an aspiring
machine learning engineer looking for
online training and certifications from
prestigious universities and in
collaboration with leading experts then
search no more simply learns
professional certification program in Ai
and machine learning from P University
in collaboration with IBM should be your
right choice for more details use the
link in the description box below with
that in mind what is a decision tree
let's go through a very simple example
before we dig in deep decision tree is a
tree shaped diagram used to determine a
course of action each branch of the tree
represents a possible decision
occurrence or reaction let's start with
a simple question how do I identify a
random vegetable from a shopping bag so
we have this group of vegetables in here
and we can start off by asking a simple
question is it red and if it's not then
it's going to be the purple fruit to the
left probably an eggplant if it's true
it's going to be one of the red fruits
is a diameter greater than two if false
it's going to be a what looks to be a
red chili and if it's true it's going to
be a bell pepper from the capsicum
family so it's a
capsicum problems that decision tree can
solve so let's look at the two different
categories the decision tree can be used
on it can be used on the classification
the true false yes no and it can be used
used on regression where we figure out
what the next value is in a series of
numbers or a group of data in
classification the classification tree
will determine a set of logical if then
conditions to classify problems for
example discriminating between three
types of flowers based on certain
features in regression a regression tree
is used when the target variable is
numerical or continuous in nature we fit
the regression model to the Target
variable using each of the independent
variables each split is made based on
the sum of squared error before we dig
deeper into the mechanics of the
decision tree let's take a look at the
advantages of using a decision tree and
we'll also take a glimpse at the
disadvantages the first thing you'll
notice is that it's simple to understand
interpret and visualize it really shines
here because you can see exactly what's
going on in a decision tree little
effort is required for data preparation
so you don't have to do special scaling
there's a lot of things you don't have
to worry about when using a decision
tree it can handle both numerical and
categorical data as we discovered
earlier and nonlinear parameters don't
affect its performance so even if the
data doesn't fit an easy curved graph
you can still use it to create an
effective decision or prediction if
we're going to look at the advantages of
a decision tree we also need to
understand the disadvantages of a
decision tree the first disadvantage is
overfitting overfitting occurs when the
algorithm captures noise in the data
that means you're solving for one
specific instance instead of a general
solution for all the data High variance
the model can get unstable due to small
variation in data low bias tree a highly
complicated decision tree tends to have
a low bias which makes it difficult for
the model to work with new data decision
tree important terms before we dive in
further we need to look at some basic
terms we need to have some definitions
to go with our decision tree in the
different parts we're going to be using
we'll start with entropy entropy is a
measure of Randomness or
unpredictability in the data set for
example we have a group of animals in
this picture there's four different
kinds of animals and this data set is
considered to have a high entropy you
really can't pick out what kind of
animal it is based on looking at just
the four animals as a big clump of of uh
entities so as we start splitting it
into subgroups we come up with our
second definition which is Information
Gain in Information Gain it is a measure
of decrease in entropy after the data
set is split so in this case based on
the color yellow we've split one group
of animals on one side as true and those
who aren't yellow as false as we
continue down the yellow side we split
based on the height true or false equals
10 and on the other side height is less
than 10 true or false and as you see as
we split it the entropy continues to be
less and less and less and so our
Information Gain is simply the entropy
E1 from the top and and how it's changed
to E2 in the bottom and we'll look at
the uh deeper math although you really
don't need to know a huge amount of math
when you actually do the programming in
Python because they'll do it for you but
we'll look on the actual math of how
they compute entropy finally we went to
the different parts of our tree and they
call the leaf node Leaf node carries the
classification or the decision so it's a
final end at the bottom the decision
node has two or more branches this is
where we're breaking the group up into
different parts and finally you have the
root node the topmost decision node is
known as the root
node how does a decision tree work
wonder what kind of animals I'll get the
jungle today maybe you're the hunter
with a gun or if you're more into
photography you're a photographer with a
camera so let's look at this group of
animals and let's try to classify
different types of animals based on
their features using a decision tree so
the problem statement is to classify the
different types of animals based on
their features using a decision tree the
data set is looking quite messy and the
entropy is high in this this case so
let's look at a training set or a
training data set and we're looking at
color we're looking at height and then
we have our different animals we have
our elephants our giraffes our monkeys
and our tigers and they're of different
colors and shapes let's see what that
looks like and how do we split the data
we have to frame the conditions that
split the data in such a way that the
Information Gain is the highest note
gain is the measure of decrease in
entropy after splitting so the formula
for entropy is the SU that's what this
symbol looks like that looks like kind
of like a e funky e of K where I equals
1 to k k would represent the number of
animal the different animals in there
where value or P value of I would be the
percentage of that animal times the log
base 2 of the same the percentage of
that animal let's try to calculate the
entropy for the current data set and
take a look at what that looks like and
don't be afraid of the math you don't
really have to memorize this math just
be aware that it's there and this is
what's going on in the background and so
we have three giraffes two tigers one
monkey two elephants a total of eight
animals gather and if we plug that into
the formula we get an entropy that
equals 3 over 8 so we have three drafts
a total of eight times the log usually
they use base two on the log so log base
2 of 3 over8 plus in this case let's say
it's the elephants 2 over 8 two
elephants over total of 8 * log base 2 2
over 8 plus 1 monkey over total of 8 log
basee 2 1 over 8 and plus 2 over 8 of
the Tigers log base 2 over 8 and if we
plug that into our computer our
calculator I obviously can't do logs in
my head we get an entropy equal to
.571 the program will actually calculate
the entropy of the data set similarly
after every split to calculate the gain
now we're not going to go through each
set one at a time to see what those
numbers are we just want you to be aware
that this is a Formula or the
mathematics behind behind it gain can be
calculated by finding the difference of
the subsequent entropy values after a
split now we'll try to choose a
condition that gives us the highest gain
we will do that by splitting the data
using each condition and checking that
the gain we get out of them the
condition that gives us the highest gain
will be used to make the first split can
you guess what that first split will be
just by looking at this image as a human
it's probably pretty easy to split it
let's see if you're right if you guessed
the color yellow you're correct let's
say the condition that gives us the
maximum gain is yellow so we will split
the data based on the color yellow if
it's true that group of animals goes to
the left if it's false it goes to the
right the entropy after the splitting
has decreased considerably however we
still need some splitting at both the
branches to attain an entpe value equal
to zero so we decid to split both the
nodes using height as a condition since
every Branch now contains single label
type we can say that entropy in this
case has reached the least value and
here you see we have the giraffes the
Tigers the monkey and the elephants all
separated into their own groups this
tree can now predict all the classes of
animals present in the data set with
100% accuracy that was easy use case
loan repayment prediction let's get into
my favorite part and open up some Python
and see what the programming code and
the scripting looks like in here we're
going to want to do a prediction and we
start with this individual here who's
requesting to find out how good his
customers are going to be whether
they're going to repay their loan or not
for his bank and from that we want to
generate a problem statement to predict
if a customer will repay loan amount or
not and then we're going to be using the
decision tree algorithm in Python let's
see what that looks like and let's dive
into the code in our first few steps of
implementation we're going to start by
importing the necessary packages that we
need from Python and we're going to load
up our data and take a look at what the
data looks like so the first thing I
need is I need something to edit my
Python and run it in so let's flip on
over and here I'm using the Anaconda
Jupiter notebook now you can use any
python IDE you like to run it in but I
find the jupyter notebook's really nice
for doing things on the Fly and let's go
ahead and just paste that code in the
beginning and before we start let's talk
a little bit about what we're bringing
in and then we're going to do a couple
things in here we have to make a couple
changes as we go through this first part
of the import the first thing we bring
in is numpy as NP that's very standard
when we're dealing with uh mathematics
especially with uh very complicated
machine learning tools you almost always
see the numpy come in for your num your
numers it's called number python it has
your mathematics in there in this case
we actually could take it out but
generally you'll need it for most of
your different things you work with and
then we're going to use pandas as PD
that's also a standard the pandas is a
data frame setup and you can liken this
to uh taking your basic data and storing
it in a way that looks like an Excel
spreadsheet so as we come back to this
when you see NP or PD those are very
standard uses you'll know that that's
the pandas and I'll show you a little
bit more when we explore the data in
just a minute then we're going to need
to split the data so I'm going to bring
in our train test and split and this is
coming from the sklearn package cross
validation in just a minute we're going
to change that and we'll go over that
too and then there's also the sk. tree
import decision tree classifier that's
the actual tool we're using remember I
told you don't be afraid of the
mathematics it's going to be done for
you well the decision tree classifier
has all that mathematics in there for
you so you don't have to figure it back
out again and then we have sklearn
metrics for accuracy score we need to
score our our setup that's the whole
reason we're splitting it between the
training and testing data and finally we
still need the sklearn import tree and
that's just the basic tree function is
needed for the decision tree classifier
and finally we're going to load our data
down here and I'm going to run this and
we're going to get two things on here
one we're going to get an error and two
we're going to get a warning let's see
what that looks like so the first thing
we had is we have an error why is this
error here well it's looking at this it
says I need to read a file and when this
was written the person who wrote it this
is their path where they stored the file
so let's go ahead and fix
that and I'm going to put in here my
file path I'm just going to call it full
file name and you'll see it's on my C
drive and this this very lengthy setup
on here where I stored the data 2. CSV
file don't worry too much about the full
path because on your computer it'll be
different the data. 2 CSV file was
generated by simply learn if you want a
copy of that you can comment down below
and request it here in the
you and then if I'm going to give it a
name full file name I'm going to go
ahead and change it here to
full file name so let's go ahead and run
it now and see what
happens and we get a
warning when when you're coding
understanding these different warnings
and these different errors that come up
is probably the hardest lesson to learn
so let's just go ahead and take a look
at this and use this as a uh opportunity
to understand what's going on here if
you read the warning it says the cross
validation is depreciated so it's a
warning on it's being removed and it's
going to be moved in favor of the model
selection so if we go up here we have
sklearn Doc crossvalidation and if you
research and go to sklearn site you'll
find out that you can actually just swap
it right in there with model
selection and so when I come in here and
I run it again that removes a warning
what they've done is they've had two
different developers develop it in two
different branches and then they decided
to keep one of those and eventually get
rid of the other one that's all that is
and very easy and quick to
fix before we go any further I went
ahead and opened up the data from this
file remember the the data file we just
loaded on here the dataor 2. CSV let's
talk a little bit more about that and
see what that looks like both as a text
file because it's a comma separated
variable file and in a spreadsheet this
is what it looks like as a basic text
file you can see at the top they've
created a header and it's got 1 2 3 4
five columns and each column has data in
it and let me flip this over because
we're also going to look at this uh in
an actual spreadsheet so you can see
what that looks like and here I've
opened it up in the open Office calc
which is pretty much the same as um
Excel and zoomed in and you can see
we've got our columns and our rows of
data little easier to read in here we
have a result yes yes no we have initial
payment last payment credit score house
number if we scroll way
down we'll see that this occupies a,1
lines of code or lines of data with uh
the first one being a column and then
1,000 lines of
data now as a
programmer if you're looking at a small
amount of data I usually start by
pulling it up in different sources so I
can see what I'm working
with but in larger data you won't have
that option it'll just be um too too
large so you need to either bring in a
small amount that you can look at it
like we're doing right now or we can
start looking at it through the python
code so let's go ahead and move on and
take the next couple steps to explore
the data using python let's go ahead and
see what it looks like in Python to
print the length and the shape of the
data so let's start by printing the
length of the database we can use a
simple Lin function from Python and when
I run this you'll see that it's a th
long and that's what we expected there's
a thousand lines of data in there if you
subtract the column head and this is one
of the nice things when we did the uh
balance data from the panda read CSV
you'll see that the header is row zer
row so it automatically removes a
row and then shows the data separate it
does a good job sorting that data out
for us and then we can use a different
function and let's take a look at that
and again we're going to utilize the
tools in
Panda and since the balance uncore data
was loaded as a panda data
frame we can do a shape on it and let's
go ahead and run the shape and see what
that looks
like what's nice about this shape is not
only does it give me the length to the
data we have a th lines it also tells me
there's five columns so we were looking
at the data we had five columns of data
and then let's take one more step to
explore the data using Python and now
that we've taken a look at the length
and the shape let's go ahead and use the
uh Panda's module for head another
beautiful thing in the data set that we
can utilize so let's put that on our
sheet here and we have print data set
and balance data. head and this is a
panda print statement of its own so it
has its own print feature in there and
then we went ahead and gave a label for
our print job here of data set just a
simple print statement and when we run
that and let's just take a closer look
at that let me zoom in
here there we
go pandas does such a wonderful job of
making this a very clean readable data
set so you can look at the data you can
look at the column headers you can have
it uh when you put it as a head it
prints the first five Lin of the data
and we always start with zero so we have
five lines we have 0 1 2 3 4 instead of
1 2 3 4 5 that's a standard scripting
and programming set as you want to start
with the zero position and that is what
the data head does it pulls the first
five rows of data puts in a nice format
that you can look at and view very
powerful tool to view the data so
instead of having to flip and open up an
Excel spreadsheet or open Office Cal or
trying to look at a word dock where it's
all scrunched together and hard to read
you can now get a nice open view of what
you're working with we're working with a
shape of a thousand long five wide so we
have five columns and we do the full
data head you can actually see what this
data looks like the initial payment last
payment credit scores house number so
let's take this now that we've explored
the data and let's start digging into
the decision tree so in our next step
we're going to train and build our data
tree and to do that we need to First
separate the date out we're going to
separate into two groups so that we have
something to actually train the data
with and then we have some data on the
side to test it to see how good our
model is remember with any of the
machine learning you always want to have
some kind of test set to to weigh it
against so you know how good your model
is when you distribute it let's go ahead
and break this code down and look at it
in pieces so first we have our X and
Y where do X and Y come from well X is
going to be our data and Y is going to
be the answer or the target Target you
can look at it source and Target in this
case we're using X and Y to denote the
data in and the data that we're actually
trying to guess what the answer is going
to be and so to separate it we can
simply put in x equals the balance of
the data. values the first brackets
means that we're going to select all the
lines in the database so it's all the
data and the second one says we're only
going to look at columns 1 through five
remember always start with zero zero is
a yes or no and that's whether the loan
went default or not so we want to start
with one if we go back up here that's
the initial payment and it goes all the
way through the house
number well if we want to look at uh 1
through five we can do the same thing
for Y which is the answers and we're
going to set that just equal to the zero
row so it's just the zero row and then
it's all rows going in there so now
we've divided this into two different
data sets one of them with the data
going in and one with the
answers next we need to split the
data and here you'll see that we have it
split into four different parts the
first one is your X training your X test
your y train your y
test simply put we have X going in where
we're going to train it and we have to
know the answer to train it with and
then we have X test where we're going to
test that data and we have to know in
the end what the Y was supposed to be
and that's where this train test split
comes in that we loaded earlier in the
modules this does it all for us and you
can see they set the test size equal to3
so that's roughly 30% will be used in
the test and then we use a random state
so it's completely random which rows it
takes out of there and then finally we
get to actually build our decision tree
and they've called it here Clore entropy
that's the actual decision tree or
decision tree classifier and in here the
they've added a couple variables which
we'll explore in just a minute and then
finally we need to fit the data to that
so we take our clf entropy that we
created and we fit the X train and since
we know the answers for X train are the
Y train we go ah and put those in and
let's go ahead and run this and what
most of these sklearn modules do is when
you set up the variable in this case
when we set the clf ental decision tree
classifier it automatically prints out
what's in that decision tree there's a
lot of variables you can play within
here and it's quite beyond the scope of
this tutorial to go through all of these
and how they work but we're working on
entropy that's one of the options we've
added that it's completely a random
state of 100 so 100% And we have a max
depth of three now the max depth if you
remember above when we were doing the
different graphs of animals means it's
only going to go down three layers
before it stops and then we have minimal
samples of leaves is five so it's going
to have at least five leavs at the end
so I'll have at least three splits will
have no more than three layers and at
least five end leaves with the final
result at the bottom now that we've
created our decision tree classifier not
only created it but trained it let's go
ahead and apply it and see what that
looks like so let's go ahead and make a
prediction and see what that looks like
we're going to paste our predict code in
here and before we run it let's just
take a quick look at what's this doing
here we have a variable y predict that
we're going to do and we're going to use
our variable clf entropy that we
created and then you'll see do predict
and that's very common in the sklearn
modules that their different tools have
the predict when you're actually running
a prediction in this case we're going to
put our X test data in here now if you
delivered this for use an actual
commercial use and distributed it this
would be the new loans you're putting in
here to guess whether the person's going
to be we uh pay them back or not in this
case though we need to test out the data
and just see how good our sample is how
good of our tree does at predicting the
loan payments and finally since Anaconda
jupyter notebook is works as a command
line for python we can simply put the Y
predict e in to print it I could just as
easily have put the
print and put brackets around y predict
to print it out we'll go ahead and do
that it doesn't matter which way you do
it and you'll see right here that runs a
prediction this is roughly 300 in here
remember it's 30% of a th000 so you
should have about 300 answers in here
and this tells you which each one of
those lines of our test went in there
and this is what our y predict came out
so let's move on to the next step where
we're going to take this data and try to
figure out just how good a model we have
so here we go since SK learn does all
the heavy lifting for you and all the
math we have a simple line of code to
let us know what the accuracy is and
let's go ahead and go through that and
see what that means and what that looks
like let's go ahead and paste this in
and let me zoom in a little bit there we
go so you have a nice full picture and
we'll see here we're just going to do a
print accuracy is and then we do the
accuracy score and this was something we
imported um earlier if you remember at
the very beginning let me just scroll up
there real quick so you can see where
that's coming from that's coming from
here down here from sklearn Docs import
accuracy score and you could probably
run a script make your own script to do
this very easily how accurate is it how
many out of 300 do we get right and so
we put in our y test that's the one we
ran the predict on and then we put in
our y predict that's the answers we got
and we're just going to multiply that by
100 because this is just going to give
us an answer as a decimal and we want to
see it as a percentage and let's run
that and see what it looks like and if
you see here we got an accuracy of 93 .
66667 so when we look at the number of
loans and we look at how good our model
fit we can tell people it has about a
93.6 fitting to it so just a quick recap
on that we now have accuracy setup on
here and so we have created a model that
uses the decision tree algorithm to
predict whether a customer will repay
the loan or not the accuracy of the
model is about
94.6% the bank can now use this model to
decide whether it should approve the
loan request from a particular customer
or not and so this information is really
powerful we might not be able to as
individuals understand all these numbers
CU they have thousands and numbers that
come in but you can see that this is a
smart decision for the bank to use a
tool like this to help them to predict
how good their uh profits going to be
off of the loan balances and how many
are going to default or not if you are
an aspiring machine learning engineer
looking for online training and
certifications from prestigious
universities and in collaboration with
leading experts then SE such no more
simply learn professional certification
program in Ai and machine learning from
P University in collaboration with IBM
should be your right choice for more
details use the link in the description
box below with that in mind let us dig
deep into the theory of exactly how it
works and let's look at what is random
Forest random forest or random decision
Forest is a method that operates by
constructing multiple decision trees the
decision of the majority of the trees is
chosen by the random Forest as the final
decision and this's uh we have some nice
Graphics here we have a decision tree
and they actually use a real tree to
denote the decision tree which I love
and given a random some kind of picture
of a fruit this decision tree decides
that the output is it's an apple and we
have a decision tree 2 where we have
that picture of the fruit goes in and
this one decides that it's a lemon and
the decision 3 tree gets another image
and it decides it's an apple and then
this all comes together in what they
call the random forest and this random
Forest then looks at it and says okay I
got two votes for apple one vote for
lemon the majority is Apples so the
final decision is apples to understand
how the random Forest works we first
need to dig a little deeper and take a
look at the random forest and the actual
decision tree and how it builds that
decision tree and looking closer at how
the individual decision trees work we'll
go ahead and continue to use the fruit
example since we're talking about trees
and forests a decision tree is a tree
shaped diagrammed used to determine a
course of action each branch of the tree
represents a possible decision
occurrence or reaction so in here we
have a bowl of fruit and if you look at
that it looks like um they switch from
lemons to oranges so we have oranges
cherries and apples and the first
decision of the decision tree might be
is a diameter greater than or equal to
three and if it says false it knows that
they're cherries because everything else
is bigger than that so all the cherries
fall into that decision so we have all
that data we're training we can look at
that we know that that's what's going to
come up is the color orange well goes hm
orange or red well if it's true then it
comes out as the orange and if it's
false that leaves apples so in this
example it sorts out the fruit in the
bowl or the images of the fruit a
decision tree these are very important
terms to know because these are very
Central to understanding the decision
tree and when working with them the
first is entropy everything on the
decision tree and how it makes those
decision is based on entropy is a
measure of Randomness or
unpredictability in the data set uh then
they also have Information Gain the leaf
node the decision node and the root node
we'll cover these other four terms as we
go down the tree but let's start with
entropy so starting with entropy we have
here a high amount of Randomness what
that means is that whatever is coming
out of this decision if it was going to
guess based on this data it would
wouldn't be able to tell you whether
it's a lemon or an apple it would just
say it's a fruit uh so the first thing
we want to do is we want to split this
apart and we take the initial data set
we're going to create a data set one and
a data set two we just split it in two
and if you look at these new data sets
after splitting them the entropy of each
of those sets is much less so for the
first one whatever comes in there it's
going to sort that data and it's going
to say okay if this data goes this
direction it's probably an apple and if
if it goes into the other direction it's
probably a lmon so that brings us up to
Information Gain it is the measure of
decrease in the entropy after the data
set is split what that means in here is
that we've gone from one set which has a
very high entropy to two lower sets of
entropy and we've added in the values of
E1 for the first one and E2 for the
second two which are much lower and so
that information gain is increased
greatly in this example and so you can
find that the information grain simply
equals uh decision E1 minus E2 as we're
going down our list of uh definitions
we'll look at the leaf node and the leaf
node carries the classification or the
decision so we look down here to the
leaf node we finally get to our set one
or our set two when it comes down there
and it says okay this object's gone into
set one if it's gone into set one it's
going to be split by some means and
we'll either end up with apples on the
leaf node or a lemon on the leaf node
and on the right it'll either be an
apple or lemons those Leaf nodes are
those final decisions or
classifications uh that's the definition
of leaf node in here if we're going to
have a final Leaf where we make the
decision we should have a name for the
nodes above it and they call those
decision nodes a decision node decision
node has two or more branches and you
can see here where we have the uh five
apples and one lemon and in the other
case the five lemons and one apple they
have to make a choice of which tree It
Goes Down based on some kind of
measurement or information given to the
tree and that brings us to our last
definition the root node the topmost
decision node is known as the root node
and this is where you have all of your
data and you have your first decision it
has to make or the first split in
information so far we've looked at a
very general image um with the group
being split let's look and see exactly
what that means to split the data and
how do we make those decisions on there
uh let's go in there and find out how
does a decision tree work so let's try
to understand this and let's use a
simple example and we'll stay with the
fruit we have a bowl of fruit and so
let's create a problem statement and the
problem is we want to classify the
different types of fruits in the bowl
based on different features the data set
in the the bowl is looking quite messy
and the entropy is high in this case so
if this bow was our decision maker it
wouldn't know what choice to make it has
so many choices which one do you pick
Apple grapes or lemons and so we look in
here we're going to start with a dra a
training set so this is our data that
we're training our data with and we have
a number of options here we have the
color and under the color we have red
yellow purple uh we have a diameter uh
331 331 and we have a label Apple lemon
Grapes apple lemon grapes and how do we
split the data we have to frame the
conditions to split the data in such a
way that the Information Gain is the
highest it's very key to note that we're
looking for the best gain we don't want
to just start sorting out the smallest
piece in there we want to split it the
biggest way we can and so we measure
this decrease in entropy that's what
they call it entropy there's our entropy
after splitting and now we'll try to
choose a condition that gives us the
highest gain we will do that by
splitting the data using each condition
and checking the gain that we get out of
them the conditions that give us the
highest gain will be used to make the
first split so let's take a look at
these different conditions we have color
we have diameter and if we look
underneath that we have a couple
different values we have diameter equals
3 color equals yellow red diameter
equals 1 and when we look at that you'll
see over here we have 1 2 3 four threes
that's a pretty hard selection so let's
say the condition gives us the maximum
gain of three so we have the most pieces
fall into that range so our first split
from our decision node is we split the
data based on the diameter is it greater
than or equal to three if it's not
that's false it goes into the grae bowl
and if it's true it goes into a bowl
fold of lemon and apples the entropy
after splitting has decreased
considerably so now we can make two
decisions if you look at they very uh
much less chaos going on there this node
has already attained an entropy value of
zero as you can see there's only one
kind of label left for this Branch so no
further splitting is required for this
node however this node on the right is
still requires a split to decrease the
entropy further so we split the right
node further based on color if you look
at this if I split it on color that
pretty much cuts it right down the
middle it's the only thing we have left
on our choices of color and diameter too
and if the color is yellow it's going to
go to the right bowl and and if it's
false it's going to go to the left Bowl
so the entropy in this case is now zero
so now we have three bowls with zero
entropy there's only one type of data in
each one of those bowls so we can
predict a lemon with 100% accuracy and
we can predict the Apple also with 100%
accuracy along with our grapes up there
so we've looked at kind of a basic tree
in our forest but what we really want to
know is how does a random Forest work as
a whole so to begin our uh um random
Forest classifier let's say we already
have built three trees and we're going
to start with the first tree that looks
like this just like we did in the
example this tree looks at the diameter
if it's greater than or equal to three
it's true otherwise it's false so one
side goes to the smaller diameter one
side goes to larger diameter and if the
color is orange it's going to go to the
right true we're using oranges now
instead of lemons and if it's red it's
going to go to the left false and we
build a second tree very similar but
split differently instead of the first
one being split by a diameter uh this
one when they created it if you look at
that first Bowl it has a lot of red
objects so it says is the color red
because that's going to bring our
entropy down the fastest and so of
course if it's true it goes to the left
if it's false it goes to the right and
then it looks at the shape false or true
and so on and so on and tree three is
the diameter equal to one and it came up
with this because there's a lot of
cherry in this bowl so that would be the
biggest split on there is is the
diameter equal to one that's going to
drop the entropy the quickest and as you
can see it splits it into true if it
goes false and they've added another
category does it grow in the summer and
if it's false it goes off to the left if
it's true it goes off to the right let's
go ahead and bring these three trees you
can see them all in one image so this
would be three completely different
trees categorizing a fruit and let's
take a fruit now let's try this and this
fruit if you look at it we've blackened
it out you can't see the color on it so
it's missing data remember one of the
things we talked about earlier is that a
random Forest works really good if
you're missing data if you're missing
pieces so this fruit has an image but
maybe a person had a black and white
camera when they took the picture and
we're going to take a look at this and
it's going to have um they put the color
in there so ignore the color down there
but the diameter equals three we find
out it grows in the summer equals yes
and the shape is a circle and if you go
to the right you can look look at what
one of the decision trees did this is
the third one is a diameter greater than
equal to three is the color orange well
it doesn't really know on this one but
it if you look at the value it say true
and it go to the right tree 2 classifies
it as cherries is a color equal red is
the shape a circle true it is a circle
so this would look at it and say oh
that's a cherry and then we go to the
other classifier and it says is the
diameter equal one well that's false
does it grow in the summer true so it
goes down and looks at as oranges so how
does this random Forest work the first
one says it's an orange the second one
said it was a cherry and the third one
says H it's an orange and you can guess
that if you have two oranges and one
says it's a cherry uh when you add that
all together the majority of the vote
says orange so the answer is it's
classified as an orange even though we
didn't know the color and we're missing
data on it I don't know about you but
I'm getting tired of fruit so let's
switch and I did promise you we'd start
looking at a case example and get into
some python coding today we're going to
use the case the iris flower analysis oo
this is the exciting part as we roll up
our sleeves and actually look at some
python coding before we start the python
coding we need to go ahead and create a
problem statement wonder what species of
Iris do these flowers belong to let's
try to predict the species of the
flowers using machine learning in Python
let's see how it can be done so here we
begin to go ahead and implement our
python code and you'll find that the
first half of our implementation is all
about organizing and exploring the data
coming in let's go ahead and take this
first step which is loading the
different modules into Python and let's
go ahead and put that in our favorite
editor whatever your favorite editor is
in this case I'm going to be using the
Anaconda Jupiter notebook which is one
of my favorites certainly there's
notepad++ and eclipse and dozens of
others or just even using the python on
Terminal window any of those will work
just fine to go ahead and explore this
python coding so here we go let's go
ahead and flip over to our jupyter
notebook and I've already opened up a
new page for Python 3 code and I'm just
going to paste this right in there and
let's take a look and see what we're
bringing into our python the first thing
we're going to do is from the sklearn
dod sets import load Iris now this isn't
the actual data this is just the module
that allows us to bring in the data the
load Iris and the iris is so popular
it's been around since 1936 when Ronald
Fischer published a paper on it and
they're measuring the different parts of
the flower and based on those
measurements predicting what kind of
flower it is and then if we're going to
do a random Forest classifier we need to
go ahead and import a random forest
classifier from the sklearn module so SK
learn. Ensemble import random forest
classifier and then we want to bring in
two more modules um and these are
probably the most commonly used modules
in Python on and data science with any
of the um other modules that we bring in
and one is going to be pandas we're
going to import pandas as PD PD is the
common term used for pandas and pandas
is basically creates a data format for
us where when you create a panda data
frame it looks like an Xcel spreadsheet
and you'll see that in a minute when we
start digging deeper into the code panda
is just wonderful cuz it plays nice with
all the other modules in there and then
we have numpy which is our numbers
Python and the numbers python allows us
to do different mathematical sets on
here we'll see right off the bat we're
going to take our NP and we're going to
go ahead and Seed the randomness with it
with zero so np. random. seed is seeding
that as zero this code doesn't actually
show anything we're going to go ahead
and run it cuz I need to make sure I
have all those loaded and then let's
take a look at the next module on here
the next six slides including this one
are all about exploring the data
remember I told you half of this is
about looking at the data and getting it
all set so let's go ahead and take this
code right here the script and let's get
that over into our Jupiter notebook and
here we go we've gone ahead and uh run
the import now I'm going to paste the
code down
here and let's take a look and see
what's going on the first thing we're
doing is we're actually loading the iris
data and if you remember up here we
loaded the module that tells it how to
get the iris data now we're actually
assigning that data to the variable Iris
and then we're going to go ahead and use
the DF to Define data frame and that's
going to equal PD and if you remember
that's pandas as PD so that's our pandas
and Panda data frame and then we're
looking at Iris data and columns equals
Iris feature names and we're going to do
the DF head and let's run this so you
can understand what's going on
here the first thing you want to notice
is that our DF has created uh what looks
like an Excel spreadsheet and in this
Excel spreadsheet we have set the
columns so up on the top you can see the
four different columns and then we have
the data iris. data down below it's a
little confusing without knowing where
this data is coming from so let's look
at the bigger picture and I'm going to
go print I'm just going to change this
for a moment and we're going to print
oliv Iris and see what that looks like
so when I print oliv Iris I get this
long list of information and you can
scroll through here and see all the
different titles on there what's
important to notice is that first off
there's a brackets at the beginning so
this is a python
dictionary and in a python dictionary
you'll have a key or a label and this
label pulls up whatever information
comes after it so feature names which we
actually used over here under columns is
equal to an array of SE length SE width
petal length petal width these are the
different names they have for the four
different columns and if you scroll down
far enough you'll also see data down
here oh goodness it came up right
towards the top and uh data is equal to
the different data we're looking
at now there's a lot of other things in
here like Target we're going to be
pulling that up in a minute and there's
also the names uh the target names which
is further down and we'll show you that
also in a minute let's go ahead and set
that back to the head and this is one of
the neat features of pandas and Panda
data frames is when you do DF dohe head
or the panda datf frame. head it'll
print the first five lines of the data
set in there along with the headers if
you have them in this case we have the
column headers set to Iris features and
in here you'll see that we have 0 1 2 3
4 in Python most arrays always start at
zero so when you look at the first five
it's going to be 0 1 2 3 4 not 1 2 3 45
so now we've got our Iris data imported
into a data frame let's take a look at
the next piece of code in here and so in
this section here of the code we're
going to take a look at the Target and
let's go ahead and get this into our
notebook this piece of code so we can
discuss it a little bit more in detail
so here we are in our Jupiter notebook
I'm going to put the code in here and
before I run it I want to look at a
couple things going on so we have a DF
species and this is interesting cuz
right here you'll see where I have DF
species in Brackets which is uh the key
code for creating another column and
here we have iris. Target now these are
both in the pandas setup on here so in
pandas we can do either one I could have
just as easily done Iris and then in
Brackets Target depending on what I'm
working on both are um acceptable let's
go ahead and run this code and see how
this changes and what we've done is
we've added the target from the iris
data set as another column on the
end now what species is this is what
we're trying to predict so we have our
data which tells us the answer for all
these different pieces and then we've
added a column with the answer that way
when we do our final setup we'll have
the ability to program our our neural
network to look for these this different
data and know what aosa is or a Vera
color which we'll see in just a minute
or virginica those are the three that
are in there and now we're going to add
one more column I know we're organizing
all this data over and over again it's
kind of fun there's a lot of ways to
organize it what's nice about putting
everything onto one data frame is I can
then do a print out and it shows me
exactly what I'm looking at and I'll
show you where you where that's
different where you can alter that and
do it slightly differently but let's go
ahead and put this into our script up to
that now and here we go we're going to
put that down here and we're going to
run that and let's talk a little bit
about what we're doing now we're
exploring data and one of the challenges
is knowing how good your model is did
your model work and to do this we need
to split the data and we split it into
two different parts they usually call it
the training and the testing and so in
here we're going to go ahead and put
that in our database so you can see it
clearly and we've set it DF remember you
can put brackets this is creating
another column is train so we're going
to use part of it for training and this
equals NP remember that stands for numpy
random. uniform so we're generating a
random number between 0 and one and
we're going to do it for each of the
rows that's where the length DF comes
from so each row gets a generated number
and if it's less than 75 it's true and
if it's greater than 75 it's false this
means we're going to take 75% of the
data roughly because there's a
Randomness involved and we're going to
use that to train it and then the other
25% we're going to hold off to the side
and use that to test it later on so
let's flip back on over and see what the
next step is is so now that we've
labeled our database for which is
training and which is testing let's go
ahead and sort that into two different
variables train and test and let's take
this code and let's bring it into our
project and here we go let's paste it on
down here and before I run this let's
just take a quick look at what's going
on here is we have up above we created
remember there's our def. head which
prints the first five rows and we've
added a column is train at the end and
so we're going to take that we're going
to create two variables we're going to
create two new data frames one's called
train one's called test 75% in train 25%
in test and then to sort that out we're
going to do that by doing DF or main
original data frame with the iris data
in it and if DF is train equals true
that's going to go in the train and if
DF is train equals false it goes in the
test and so when I run this we're going
to print out the number in each one
let's see what that looks like and
you'll see that it puts 118 in the
training module and it puts 32 in the
testing module which lets us know that
there was 150 lines of data in here so
if you went and looked at the original
data you could see that there's 150
lines and that's roughly 75% in one and
25% for us to test our model on
afterward so let's jump back to our code
and see where this goes in the next two
steps we want to do one more thing with
our data and that's make it readable to
humans um I don't know about you but I
hate looking at zeros and ones so let's
start with the features and let's go
ahead and take those and make those
readable to humans and let's put that in
our
code let's see here we go paste it in
and you'll see here we've done a couple
very basic things we know that the
columns in our data frame again this is
a panda thing the DF
columns and we know the first four of
them 0 1 2 3 that'd be the first four
are going to be the features or the
titles of those columns and so when I
run this you'll see down here that it
creates an index sea length sea width
pedal length and pedal width and this
should be familiar because if you look
up here here's our column titles going
across and here's the first
four one thing I want you to notice here
is that when you're in a command line
whether it's Jupiter notebook or you're
running command line in the uh terminal
window if you just put the name of it
it'll print it out this is the same as
doing
print
features and the Shand is you just put
features in here if you're actually
writing a code and saving the script and
running it by remote you really need to
put the print in there but for this when
I run it you'll see it gives me the same
thing but for this we want to go ahead
and we'll just leave it as features
because it doesn't really matter and
this is one of the fun thing about
Jupiter notebooks is I'm just building
the code as we go and then we need to go
ahead and create the labels for the
other part so let's take a look and see
what that for our final step in prepping
our data before we actually start
running the training and the testing is
we're going to go ahead and convert the
species on here into something the
computer understands so let's put this
code into our script and see where that
takes
us all right here we go we set y equal
to pd.
factorize train species of zero so let's
break this down just a little bit we
have our pandas right here PD factorize
what is factorize doing I'm going to
come back to that in just a second let's
look at what trained species is and why
we're looking at the group zero on there
and let's go up here and here is our
species remember this on that we created
this whole column here for species and
then it has Sosa Sosa Sosa Sosa and if
you scroll down enough you You' also see
virginica and Vera color we need to
convert that into something the computer
understands zeros and ones so the train
species of zero because this is in the
format of a of an array of arrays so you
have to have the zero on the end and
then species is just that column
factorize goes in there and looks at the
fact that there's only three of them so
when I run this you'll see that y
generates an array that's equal to in
this case it's a training set and it's
zeros ones and twos representing the
three different kinds of flowers we have
so now we have something that computer
understands and we have a nice table
that we can read and understand and now
finally we get to actually start doing
the predicting so here we go uh we have
two lines of code oh my goodness that
was a lot of work to get to two lines of
code but there is a lot in these two
lines of code so let's take a look and
see what's going on here and put this
into our our full script that we're
running and let's paste this in here and
let's take a look and see what this is
we have we're creating a variable CF and
we're going to set this equal to the
random forest classifier and we're
passing two variables in here and
there's a lot of variables you can play
with as far as these two are concerned
they're very standard in jobs all that
does is to prioritize it not something
to really worry about usually when
you're doing this on your own computer
you do in jobs equals 2 if you're
working in a large larger or big data
and you need to prioritize it
differently this is what that number
does is it changes your priorities and
how it's going to run across the system
and things like that and then the random
state is just how it starts zero is fine
for
here but uh let's go ahead and run
this we also have cf. fit train features
comma Y and before we run it let's talk
about this a little bit more
cf. fit so we're fitting we're training
it it we are actually creating our
random Forest classifier right here this
is the code that does everything and
we're going to take our training set
remember we kept our test off to the
side and we're going to take our
training set with the features and then
we're going to go ahead and put that in
and here's our Target the Y so the Y is
01 and two that we just created and the
features is the actual data going in
that we put into the training set and
let's go ahead and run
that and this is kind of an interesting
thing because it printed out the random
force
classifier and everything around it and
so when you're running this in your
terminal window or in a script like this
this automatically treats this like just
like when we were up here and I typed in
y and I printed out y instead of print y
this does the same thing it treats this
as a variable and prints it out but if
you were actually running your code that
wouldn't be the case and what is printed
out is it shows shows us all the
different variables we can change and if
we go down here you can actually see in
jobs equals 2 you can see the random
State equals z those are the two that we
sent in there you would really have to
dig deep to find out all these different
meanings of all these different settings
on here some of them are
self-explanatory if you kind of think
about it a little bit like Max features
is auto so all the features that we're
putting in there is just going to
automatically take all four of them
whatever we send it it'll take some of
them might have so many features because
you're processing words there might be
like 1.4 million features in there
because you're doing legal documents and
that's how many different words are in
there at that point you probably want to
limit the maximum features that you're
going to process and leaf nodes that's
the end nodes remember we had the fruit
and we're talking about the leaf nodes
like I said there's a lot in this we're
looking at a lot of stuff here so you
might have uh in this case there's
probably only think three leaf nodes
maybe four you might have thousands of
leaf notes at which point you do need to
put a cap on that and say okay it can
only go so far and then we're going to
use all of our resources on processing
this and that really is what most of
these are about is limiting the process
and making sure we don't uh overwhelm a
system and there's some other settings
in here again we're not going to go over
all of them warm start equals false warm
start is if you're programming it one
piece at a time externally since we're
not we're not going to have like we're
not going to continually to train this
particular Learning Tree and again like
I said there's a lot of things in here
that you'll want to look up more detail
from the
sklearn and if you're digging in deep
and running a major project on here for
today though all we need to do is fit
our train our features and our Target y
so now we have our training model what's
next if we're going to create a
model we now need to test it remember we
set aside the test feature test group
25% of the data so let's go ahead and
take this code and let's put it into our
uh script and see what that looks like
okay here we go and we're going to run
this
and it's going to come out with a bunch
of zeros ones and twos which represents
the three type of flowers the satsa the
virginica and the Versa color and what
we're putting into our predict is the
test features and I always kind of like
to know what it is I am looking at so
real quick we're going to do
test features and remember features is
an
array of SEO length SEO width petal
length petal width so when we put it in
this way it actually loads all these
different columns that we loaded into
features so if we did just features let
me just do features in here so you can
see what features looks like this is
just playing with the with Panda's data
frames you'll see that it's an index so
when you put an index in like
this into test features into test it
then takes those columns and creates a
panda data frames from those columns and
in this this case we're going to go
ahead and put those into our predict so
we're going to put each one of these
lines of data the 5.0 3.4
1.5.2 and we're going to put those in
and we're going to predict what our new
um Forest classifier is going to come up
with and this is what it predicts it
predicts uh 0000 01211 222 and and again
this is the flower type satos of
virginica and Versa color so now that
we've taken our test features let's
explore that let's see exactly what that
data means to us so the first thing we
can do with our predicts is we can
actually generate a different prediction
model when I say different we're going
to view it differently it's not that the
data itself is different so let's take
this next piece of code and put it into
our
script so we're pasting it in here and
you'll see that we're doing uh predict
and we've added underscore proba for
probability so there's our cf. predict
probability so we're we're running it
just like we ran it up here but this
time with this we're going to get a
slightly different result and we're only
going to look at the first 10 so you'll
see down here instead of looking at all
of them uh which was uh what 27 you'll
see right down here that this generates
a much larger field on the probability
and let's take a look and see what that
looks like and what that means so when
we do the predict underscore pra
for probability it generates three
numbers so we had three leaf nodes at
the end and if you remember from all the
theory we did this is the predictors the
first one is predicting a one for satsa
it predicts a0o for virginica and it
predicts a zero for versacolor and so on
and so on and so on and let's um you
know what I'm going to change this just
a little bit let's look at
10 to 20 just because we can
and we start to get in a little
different of data and you'll see right
down here it gets to this one this line
right here and this line has 0 0.5
0.5 and so if we're going to vote and we
have two equal votes it's going to go
with the first one so it says uh satsa
gets zero votes virginica gets 0.5 votes
versacolor gets .5 votes but let's just
go with the virginica since these two
are equal and so on and so on down the
list you can see how they vary on here
so now we've looked at both how to do a
basic predict of the features and we've
looked at the predict probability let's
see what's next on here so now we want
to go ahead and start mapping names for
the plants we want to attach names so
that it makes a little more sense for us
and this what we're going to do in these
next two steps we're going to start by
setting up our predictions and mapping
them to the name so let's see what that
looks like and let's go ahead and paste
that code in here and run it and this
goes along with the next piece of code
so we'll skip through this quickly and
then come back to it a little bit so
here's
iris. Target
names and uh if you remember correctly
this was the the names that we've been
talking about this whole time the satsa
virginica versacolor and then we're
going to go ahead and do the prediction
again we run we could have just set a
variable equal to this instead of
rerunning it each time but we're going
ahead and run it again cf. predict test
features remember that returns the zeros
the ones and the twos and then we're
going to set that equal to predictions
so this time we're actually putting it
in a variable and when I run
this it distributes and it comes out as
an array and the array is satsa sosa
Sosa stosa Sosa we're only looking at
the first five we could actually do
let's do the first 25 just so we can see
a little bit more on there and you'll
see that it starts mapping it to all the
different flower types the Versa color
and the virginica in there and let's see
how this goes with the next one so let's
take a look at the top part of our
species in here and we'll take this code
and put it in our
script and let's put that down here and
paste it there we go and we'll go ahead
and run it and let's talk about both
these sections of code here and how they
go together the first one is our
predictions and I went ahead and did uh
predictions through 25 let's just do
five and so we have sitosis ptosis
ptosis sitosis that's what we're
predicting from our test model and then
we come down here and we look at test
species I remember I could have just
done test. species. head and you'll see
it says Sosa Sosa Sosa Sosa and they
match so the first one is what are
Forest is doing and the second one is
what the actual data is now is we need
to combine these so that we can
understand what that means we need to
know how good our forest is how good it
is at predicting the features so that's
where we come up to the next step which
is lots of fun we're going to use a
single line of code to combine our
predictions and our actuals so we have a
nice chart to look at and let's go ahead
and put that in our script in our
Jupiter notebook here let's see let's go
ahead and paste that in and then I'm
going to because I'm on the Jupiter
notebook I can do a control minus so you
can see the whole line
there there we go resize it and let's
take a look and see what's going on here
we're going to create in pandas remember
PD stands for pandas and we're doing a
cross tab this function takes two sets
of data and creates a chart out of them
so when I run it you'll get a nice chart
down here and we have the predicted
species so across the top you'll see the
setosa vers color vinica and the actual
species setosa versacolor virginica and
so the way to read this chart and let's
go ahead and take a look on how to read
this chart here when you read this chart
you have Sosa where they meet you have
verol where they meet and you have
virginica where they meet and they're
meeting where the actual and the
predicted agree so this is the number of
accurate predictions so in this case it
equals 30 if you had 13 + 5 + 12 you get
30 and then we notice here where it says
virginica but it was supposed to be
versacolor this is inaccurate so now we
have two two inaccurate predictions and
30 accurate predictions so we'll say
that the model accuracy is 93 that's
just 30 ided by 32 and if we multiply by
100 we can say that it is 93% accurate
so we have a 93% accuracy with our model
I did want to add one more quick thing
in here on our scripting before we wrap
it up so let's flip back on over to my
script in here we're going to take this
uh line of code from up above I don't
know if you remember it but predicts
equals the iris. target names so we're
going to map it to the names
and we're going to run the prediction
and we read it on test features but you
know we're not just testing it we want
to actually deploy it so at this point I
would go ahead and change this and this
is an array of arrays this is really
important when you're running these to
know that so you need the double
brackets and I could actually create
data maybe let's let's just do two
flowers so maybe I'm processing more
data coming in and we'll put two flowers
in here and then uh I actually want to
see what the answer is so let go ahead
and type in PRS and print that out and
when I run this you'll see that I've now
predicted two flowers that maybe I
measured in my front yard as Versa color
and Versa
color not surprising since I put the
same data in for each one this would be
the actual uh end product going out to
be used on data that you don't know the
answer
for so that's going to conclude our
scripting part of this if you are an
aspiring machine learning engineer
looking for online training and
certifications from prestigious
universities and in collaboration with
leading experts then search no more
simply learns professional certification
program in Ai and machine learning from
Pur University in collaboration with IVM
should be your right choice for more
details use the link in the description
box below with that in mind by now we
all know machine learning models make
predictions by learning from the past
data available so we have our input
values our machine learning model Builds
on those inputs of what we already know
and then we use that to create a
predicted output is that a dog little
kid looking over there watching the
black cat cross their path no dear you
can differentiate between a cat and a
dog based on their
characteristics cats cats have sharp
claws uses to climb smaller length of
ears meows and purs doesn't love to play
around dogs have D claws bigger length
of ears barks loves to run around you
usually don't see a cat running around
people although I do have a cat that
does that where dogs do and we can look
at these we can say we can evaluate the
sharpness of the claws how sharp are
their claws and we can evaluate the
length of the ears and we can usually
sort out cats from dogs based on even
those two
characteristics now tell me if it is a
cat or a dog not question usually little
kids know cats and dogs by now unless
they live a place where there's not many
cats or dogs so if we look at the
sharpness of the claws the length of the
ears and we can see that the cat has a
small smaller ears and sharper claws
than the other animals its features are
more like cats it must be a cat sharp
claws length of ears and it goes in the
cat group because KNN is based on
feature similarity we can do
classification using KNN classifier so
we have our input value the picture of
the black cat it goes into our trained
model and it predicts that this is a cat
coming out so what is KNN what is the
KNN algorithm K nearest neighbors is
what that stands for it's one of the
simplest supervised machine learning
algorithms mostly used for
classification so we want to know is
this a dog or it's not a dog is it a cat
or not a cat it classifies a data point
based on how its neighbors are
classified KNN stores all available
cases and classifies new cases based on
a similarity measure and here we gone
from cats and dogs right into wine
another favorite of mine KNN stores all
available cases and classifies new cases
based on a similarity measure and here
you see we have a measurement of sulfur
dioxide versus the chloride level and
then the different win they've tested
and where they fall on that graph based
on how much sulfur dioxide and how much
chloride K and KNN is a perimeter that
refers to the number of nearest
neighbors to include in the majority of
the voting process and so if we add a
new glass of wine there red or white we
want to know what the neighbors are in
this case we're going to put k equal 5
we'll talk about K in just a minute a
data point is classified by the majority
of votes from its five nearest neighbors
here the unknown point would be
classified as red since four out of five
neighbors are red so how do we choose K
how do we know k equals five I mean
that's was the value we put in there so
we're going to talk about it how do we
choose the factor K KN andn algorithm is
based on feature similarity choosing the
right value of K is a process called
parameter tuning and is important for
better accuracy so at k equal 3 we can
classify we have a question mark in the
middle as either a as a square or not is
it a square or is it in this case a
triangle and so if we set k equals to
three we're going to look at the three
nearest neighbors we're going to say
this is a square and if we put k equals
to 7 we classify as a triangle depending
on what the other data is around and you
can see as the K changes depending on
where that point is that drastically
changes your answer and uh we jump here
we go how do we choose the factor of K
you'll find this in all machine learning
choosing these factors that's the the
face you get it's like oh my gosh did I
choose the right K did I set it right my
values in whatever machine learning tool
you're looking at so that you don't have
a huge bias in One Direction or the
other and in terms of knnn the number of
K if you choose it too low the bias is
based on it's just too noisy it's it's
right next to a couple things and it's
going to pick those things and you might
get a skewed answer and if your K is too
big then it's going to take forever to
process so you're going to run into
processing issues and ource issues so
what we do the most common use and
there's other options for choosing K is
to use the square root of n so in is a
total number of values you have you take
the square root of it in most cases you
also if it's an even number so if you're
using like in this case squares and
triangles if it's even you want to make
your K value odd that helps it select
better so in other words you're not
going to have a balance between two
different factors that are equal so
usually take the square root of N and if
it's even you add one to it or subtract
one from it and that's where you get the
K value from that is the most common use
and it's pretty solid it works very well
when do we use KNN we can use KNN when
data is labeled so you need a label on
it we know we have a group of pictures
with dogs dogs cats cats data is Noise
free and so you can see here when we
have a class and we have like
underweight 140 23 Hello Kitty normal
that's pretty confusing we have a high
variety of data coming in so it's very
nois easy and that would cause an issue
data set is small so we're usually
working with smaller data sets where I
you might get into gig of data if it's
really clean doesn't have a lot of noise
because KNN is a lazy learner I.E it
doesn't learn a discriminative function
from the training set so it's very lazy
so if you have very complicated data and
you have a large amount of it you're not
going to use the knnn but it's really
great to get a place to start even with
large data you can sort out a small
sample and get an idea of what that
looks like using the KNN and also just
using for smaller data sets KNN works
really good how does a KNN algorithm
work consider a data set having two
variables height in centimeters and
weight in kilograms and each point is
classified as normal or underweight so
we can see right here we have two
variables you know true false they're
either normal or they're not they're
underweight on the basis of the given
data we have to classify the below set
as normal or underweight using KNN so if
we have new data coming in that says 57
kilg and 177 cm is that going to be
normal or underweight to find the
nearest neighbors we'll calculate the
ukian distance according to the ukian
distance formula the distance between
two points in the plane with the
coordinates XY and ab is given by
distance D equals the Square t of x - a^
2 + y - B squared and you can remember
that from the two edges of a triangle
we're Computing the third Edge since we
know the X side and the yide let's
calculate it to understand clearly so we
have our unknown point and we placed it
there in red and we have our other
points where the data is scattered
around the distance D1 is a square root
of 170 minus 167 squ + 57 - 51 2ar which
is about 6.7 and distance 2 is about 13
and distance three is about 13.4
similarly we will calculate the ukian
distance of UN in data point from all
the points in the data set and because
we're dealing with small amount of data
that's not that hard to do and it's
actually pretty quick for a computer and
it's not a really complicated Mass you
can just see how close is the data based
on the ukian distance hence we have
calculated the ukian distance of unknown
data point from all the points as
showing where X1 and y1 equal 57 and 170
whose class we have to classify so now
we're looking at that we're saying well
here's the ukan distance who's going to
be their closest neighbors now now let's
calculate the nearest neighbor at k
equal 3 and we can see the three closest
neighbors puts them at normal and that's
pretty self-evident when you look at
this graph it's pretty easy to say okay
what you know we're just voting normal
normal normal three votes for normal
this is going to be a normal weight so
majority of neighbors are pointing
towards normal hence as per KNN
algorithm the class of 57170 should be
normal so recap of knnn positive integer
K is specified along with a new sample
we select the K entries in our database
which are closest to the new sample we
find the most common classification of
these entries this is the classification
we give to the new sample so as you can
see it's pretty straightforward we're
just looking for the closest things that
match what we got so let's take a look
and see what that looks like in a use
case in Python so let's dive into the
predict diabetes use case so use case
predict diabetes the objective predict
whether a person will be diagnosed with
diabetes or not we have a data set of 7
68 people who were or were not diagnosed
with diabetes and let's go ahead and
open that file and just take a look at
that data and this is in a simple
spreadsheet format the data itself is
comma separated very common set of data
and it's also a very common way to get
the data and you can see here we have
columns a through I that's what 1 2 3 4
5 6 7 8 um eight columns with a
particular tribute and then the ninth
column which is the outcome is whether
they have diabetes as a data scientist
the first thing you should be looking at
is insulin well you know if someone has
insulin they have diabetes cuz that's
why they're taking it and that could
cause issue on some of the machine
learning packages but for a very basic
setup this works fine for uh doing the
KNN and the next thing you notice is it
it didn't take very much to open it up
um I can scroll down to the bottom of
the data there's
768 it's pretty much a small data set
you know at 769 I can easily fit this
into my ram on my computer I can look at
it I can manipulate it and it's not
going to really tax just a regular
desktop computer you don't even need an
Enterprise version to run a lot of this
so let's start with importing all the
tools we need and before that of course
we need to discuss what IDE I'm using
certainly you can use any uh particular
editor for python but I like to use for
doing uh very basic visual stuff the
Anaconda which is great for doing demos
with the Jupiter notebook and just a
quick view of the Anaconda Navigator
which is the new release P out there
which is really nice you can see under
home I can choose my application we're
going to be using python 36 I have a
couple different uh versions on this
particular machine if I go under
environments I can create a unique
environment for each one which is nice
and there's even a little button there
where I can install different packages
so if I click on that button and open
the terminal I can then use a simple pip
install to install different packages
I'm working with let's go ahead and go
back under home and we're going to
launch our notebook and I've already you
know kind of like uh the old cooking
shows I've already prepared a lot of my
stuff so we don't have to wait for it to
launch because it takes a few minutes
for it to open up a browser window in
this case I'm going it's going to open
up Chrome because that's my default that
I use and since the script is pre-done
you'll see you have a number of windows
open up at the top the one we're working
in and uh since we're working on the KNN
predict whether a person will have
diabetes or not let's go and put that
title in there and I'm also going to go
up here and click on Cell actually we
want to go ahead and first insert a cell
below and then I'm going to go back up
to the top cell and I'm going to change
the cell type to markdown that means
this is not going to run as python it's
a markdown language so if I run this
first one it comes up in nice big
letters which is kind of nice remind us
what we're working on and by now you
should be familiar with doing all of our
Imports we're going to import the pandas
as PD import numpy as NP pandas is the
pandas data frame and numpy is a number
array very powerful tools to use in here
so we have our Imports so we've brought
in our pandas are numpy our two general
python tools and then you can see over
here we have our train test split by now
youed should be familiar with splitting
the data we want to split part of it for
training our thing and then training our
particular model and then we want to go
ahead and test the remaining data just
see how good it is pre-processing a
standard scaler pre-processor so we
don't have a bias of really large
numbers remember in the data we had like
number of pregnancies isn't going to get
very large where the amount of insulin
they take can get up to 2 56 so 256
versus 6 that will skew results so we
want to go ahead and change that so
they're all uniform between minus one
and one and then the actual tool this is
the K neighbors classifier we're going
to use and finally the last three are
three tools to test all about testing
our model how good is it me just put
down test on there and we have our
confusion Matrix our F1 score and our
accuracy so we have our two general
python modules we're importing and then
we have our six module specific from the
sklearn setup and then we do need to go
ahead and run this so these are actually
imported there we go and then move on to
the next step and so in this set we're
going to go ahead and load the database
we're going to use pandas remember
pandas is PD and we'll take a look at
the data in Python we looked at it in a
simple spreadsheet but usually I like to
also pull it up so that we can see what
we're doing so here's our data set
equals pd. read CSV that's a pandas
command and the diabetes folder I just
put in the same folder where my IPython
script is if you put in a different
folder you'd need the full link on there
we can also do a quick length of uh the
data set that is a simple python command
Len for length we might even let's go
ahead and print that we'll go print and
if you do it on its own line link. data
set in the jupyter notebook it'll
automatically print it but when you're
in most of your different setups you
want to do the print in front of there
and then we want to take a look at the
actual data set and since we're in
pandas we can simply do data set head
and again let's go ahead and add the
print in there if you put a bunch of
these in a row you know the data set one
head data set two head it only prints
out the last one so I us always like to
keep the print statement in there but
because most projects only use one data
frame Panda data frame doing it this way
doesn't really matter the other way
works just fine and you can see when we
hit the Run button we have the 768 lines
which we knew and we have our
pregnancies it's automatically given a
label on the left remember the head only
shows the first five lines so we have 0o
through four and just a quick look at
the data you can see it matches what we
looked at before we have pregnancy
glucose blood pressure all the way to
age and then the outcome on the end and
we're going to do a couple things in
this next step we're going to create a
list of columns where we can't have zero
there's no such thing as zero skin
thickness or zero blood pressure zero
glucose to uh any of those you'd be dead
so not a really good Factor if they
don't if they have a zero in there
because they didn't have the data and
we'll take a look at that cuz we're
going to start replacing that
information with a couple of different
things and let's see what that looks
like so first we create a nice list as
you can see we have the values we talked
about glucose blood pressure skin
thickness uh and this is a nice way when
you're working with columns is to list
the columns you need to do some kind of
transformation on very common thing to
do and then for this particular setup we
certainly could use the there's some
Panda tools that will do a lot of this
where we can replace the na but we're
going to go ahead and do it as a data
set column equals data set column.
replace this is this is still pandas you
can do a direct there's also one that
that you look for your n a lot of
different options in here but the Nan
nump Nan is what that stands for is is
non doesn't exist so the first thing
we're doing here is we're replacing the
zero with a numpy none there's no d data
there that's what that says that's what
this is saying right here so put the
zero in and we're going to replace zeros
with no data so if it's a zero that
means the person's well hopefully not
dead hopefully they just didn't get the
data the next thing we want to do is
we're going to create the mean which is
the in integer from the data set from
the column. mean where we skip Naas we
can do that that is a panda's command
there the skip na so we're going to
figure out the mean of that data set and
then we're going to take that data set
column and we're going to replace all
the
npn with the means why did we do that
and we could have actually just uh taken
this step and gone right down here and
just replace zero and Skip anything
where except you could actually there's
a way to skip zeros and then just
replace all the zeros but in this case
we want to go ahead and do it this way
so you could see that we're switching
this to a non-existent value then we're
going to create the mean well this is
the average person so if we don't know
what it is if they did not get the data
and the data is missing one of the
tricks is you replace it with the
average what is the most common data for
that this way you can still use the rest
of those values to do your computation
and it kind of just brings that
particular value of those missing values
out of the equation let's go ahead and
take this and we'll go ahead and run it
doesn't actually do anything so we're
still preparing our data if you want to
see what that looks like we don't have
anything in the first few lines so it's
not going to show up but we certainly
could look at a row let's do that let's
go into our data set with's print a data
set
and let's pick in this case let's just
do glucose and if I run this this is
going to print all the different glucose
levels going down and we thankfully
don't see anything in here that looks
like missing data at least on the ones
it shows you can see it skipped a bunch
in the middle that's what it does if you
have too many lines in Jupiter notebook
it'll skip a few and and go on to the
next in a data set let me go and remove
this and we'll just zero out that and of
course before we do any processing
before proceeding any further we need to
split the data is set into our train and
testing data that way we have something
to train it with and something to test
it on and you're going to notice we did
a little something here with the panda
database code there we go my drawing
tool we've added in this right here off
the data set and what this says is that
the first one in pandas this is from the
PD pandas it's going to say within the
data set we want to look at the iocation
and it is all rows that's what that says
so we're going to keep all the rows but
we're only looking at zero column 0 to 8
remember column 9 here it is right up
here we printed in in here is outcome
well that's not part of the training
data that's part of the answer yes
column 9 but it's listed as eight number
eight so 0er to eight is nine columns so
uh eight is the value and when you see
it in here zero this is actually 0 to 7
it doesn't include the last one and then
we go down here to Y which is our answer
and we want just the last one just
column 8 and you can do it this way with
this particular notation and then if you
remember we imported the train test
split that's part of the sklearn right
there and we simply put in our X and our
y we're going to do random State equals
zero you don't have to necessarily seed
it that's a seed number I think the
default is one when you seed it I'd have
to look that up and then the test size
test size is 0.2 that simply means we're
going to take 20% of the data and put it
aside so that we can test it later
that's all that is and again we're going
to run it not very exciting so far we
haven't had any print out other than to
look at the data but that is a lot of
this is prepping this data once you prep
it the actual lines of code are quick
and easy and we're almost there with the
actual writing of our KNN we need to go
ahead and do a scale the data if you
remember correctly we're fitting the
data in a standard scaler which means
instead of the data being from you know
5 to 303 in one column and the next
column is 1 to six we're going to set
that all so that all the data is between
minus one and one that's what that
standard scaler does keeps it
standardized and we we only want to fit
the scaler with the training set but we
want to make sure the testing set is the
X test going in is also transformed so
it's processing it the same so here we
go with our standard scaler we're going
to call it scor X for the scaler and
we're going to import the standard
scalar into this variable and then our X
train equals score x. fit transform so
we're creating the scaler on the xtrain
variable and then our X test we're all
going to transform it so we've trained
and transformed the X train and then the
X test isn't part of that training it
isn't part of that of training the
Transformer it just gets transformed
that's all it does and again we're going
to go and run this and if you look at
this we've now gone through these steps
all three of them we've taken care of
replacing our zeros for key columns that
shouldn't be zero and we replace that
with the means of those columns that way
that they fit right in with our d data
models we've come down here and we split
the data so now we have our test data
and our training data and then we've
taken and we scaled the data so all of
our data going in now no we don't tra we
don't train the Y part the Y train and Y
test that never has to be trained it's
only the data going in that's what we
want to train in there then Define the
model using K neighbors classifier and
fit the train data in the model so we do
all that data prep and you can see down
here we're only going to have a couple
lines of code where we're actually
building our model and training it
that's one of the cool things about
Python and how far we've come it's such
an exciting time to be in machine
learning because there's so many
automated tools let's see before we do
this let's do a quick length of and
let's do y we want let's just do length
of Y and we get 768 and if we import
math we do math. square root let's do y
train there we go it's actually supposed
to be XT train before we do this let's
go ahead and do import math and do math
square root length of Y test and when I
run that we get
12.49 I want to see show you where this
number comes from we're about to use 12
is an even number so if you know if
you're ever voting on things remember
the neighbors all vote don't want to
have an even number of neighbors voting
so we want to do something odd and let's
just take one away we'll make it 11 let
me delete this out of here that's one of
the reasons I love Jupiter notebook cuz
you can flip around and do all kinds of
things on the fly so we'll go ahead and
put in our classifier we're creating our
classifier now and it's going to be the
K neighbors classifier n neighbors equal
11 remember we did 12 minus 1 for 11 so
we have an odd number of neighbors P
equals 2 because we're looking for is it
are they diabetic or not and we're using
the ukian metric there are other means
of measuring the distance you could do
like square square means value there's
all kinds of nature of this but the
ukian is the most common one and it
works quite well it's important to
evaluate the model let's use the
confusion Matrix to do that and we're
going to use the confusion Matrix
wonderful tool and then we'll jump into
the F1 score and finally accuracy score
which is probably the most commonly used
quoted number when you go into a meeting
or something like that so let's go ahead
and paste that in there and we'll set
the cm equal to confusion Matrix y test
y predict so those are the two values
we're going to put in there and let me
go ahead and run that and print it out
and the way you interpret this is you
have the Y predicted which would be your
title up here we could do uh let's just
do
p predicted across the top and actual
going down actual it's always hard to to
write in here actual that means that
this column here down the middle that's
the important column and it means that
our prediction said 94 and prediction
and the actual agreed on 94 and 32 this
number here the 13 and the 15 those are
what was wrong so you could have like
three different if you're looking at
this across three different variables
instead of just two you'd end up with
the third row down here and the column
going down the middle so in the first
case we have the the and I believe the
zero is a 94 people who don't have
diabetes the prediction said that 13 of
those people did have diabetes and were
at high risk and the 32 that had
diabetes it had correct but our
prediction another 15 out of that 15 it
classified as incorrect so you can see
where that classification comes in and
how that works on the confusion Matrix
then we're going to go ahead and print
the F1 score let me just run that and
you see we get a 69 in our F1 score the
F1 takes into account both sides of the
balance of false positives where if we
go ahead and just do the accuracy
account and that's what most people
think of is it looks at just how many we
got right out of how many we got wrong
so a lot of people when you're a data
scientist and you're talking to other
data scientists they're going to ask you
what the F1 score the F score is if
you're talking to the general public or
the uh decision makers in the business
they're going to ask what the accuracy
is and the accuracy is always better
than the F1 score but the F1 score is
more telling it lets us know that
there's more false positives than we
would like on here but 82% not too bad
for a quick flash look at people's
different statistics in running an
sklearn and running the knnn the K
nearest neighbor on it so we have
created a model using KNN which can
predict whether a person will have
diabetes or not or at the very least
whether they should go get a checkup and
have their glucose checked regularly or
not the print accurate score we got the
0818 was pretty close to what we got and
we can pretty much round that off and
just say we have an accuracy of 80%
tells us that is a pretty Fair Fit in
the model if you are an aspiring machine
learning engineer looking for online
training and certifications from
prestigious universities and in
collaboration with leading experts then
search no more simply learns
professional certification program in Ai
and machine learning from Pur University
in collaboration with IBM should be your
right choice for more details use the
link in the description box below with
that in mind and find out why support
Vector machine so in this example last
week my son and I visited a fruit shop
dad is that an apple or a strawberry so
the question comes up what fruit did
they just pick up from the Fruit Stand
after a couple of seconds you can figure
out that it was a strawberry so let's
take this model a step further and let's
uh why not build a model which can
predict an unknown data and in this
we're going to be looking at some sweet
strawberries or crispy apples we wanted
to be able to label those two and decide
what the fruit is and we do that by
having data already put in so we already
have a bunch of strawberries we know our
strawberries and they're already labeled
as such we already have a bunch of
apples we know our apples and are
labeled as such then once we train our
model that model then can be given the
new data and the new data is this image
in this case you can see a question mark
on it and it comes through and goes it's
a strawberry in this case we're using
the support Vector Machine model svm is
a supervised learning method that looks
at data and sorts it into one of two
categories and in this case we're
sorting the strawberry into the
strawberry site at this point you should
be asking the question how does the
prediction work before before we dig
into an example with numbers let's apply
this to our fruit scenario we have our
support Vector machine we've taken it
and we've taken labeled sample of data
strawberries and apples and we draw a
line down the middle between the two
groups this split now allows us to take
new data in this case an apple and a
strawberry and place them in the
appropriate group based on which side of
the line they fall in and that way we
can predict the unknown as colorful and
tasty as the fruit example is let's take
a look at another example with some
numbers involved and we can take a
closer look at how the math Works in
this example we're going to be
classifying men and women and we're
going to start with a set of people with
a different height and a different
weight and to make this work we'll have
to have a sample data set of female
where we have their height and weight
174 65 17488 and so on and we'll need a
sample data set of the male they have a
height 17990 180 to 80 and so on let's
go ahead and put this on a graph so you
have a nice visual so you can see here
we have two groups based on the height
versus the weight and on the left side
we're going to have the women on the
right side we're going to have the men
now if we're going to create a
classifier let's add a new data point
and figure out if it's male or female so
before we can do that we need to split
our data first we can split our data by
choosing any of these lines in this case
we draw two lines through the data in
the middle that separates some men from
the women but to predict the gender of a
new data point we should split the data
in the best possible way and we say the
best possible way because this line has
a maximum space that separates the two
classes here you can see there's a clear
split between the two different classes
and in this one there's not so much a
clear split this doesn't have the
maximum space that separates the two
that is why this line best splits the
data we don't want to just do this by
eyeballing it and before we go further
we need to add some technical terms to
this we can also say that the distance
between the points and the line should
be as far far as possible in technical
terms we can say the distance between
the support vector and the hyper plane
should be as far as possible and this is
where the support vectors are the
extreme points in the data set and if
you look at this data set they have
circled two points which seem to be
right on the outskirts of the women and
one on the outskirts of the men and
hyperplane has a maximum distance to the
support vectors of any class now you'll
see the line down the middle and we call
this the hyperplane because when you're
dealing with multiple dimensions it's
really not just a line but a plane of
intersections and you can see here where
the support vectors have been drawn in
dash lines the math behind this is very
simple we take D+ the shortest distance
to the closest positive point which
would be on the Min side and D minus is
the shortest distance to the closest
negative point which is on the women's
side the sum of D+ and D minus is called
the distance margin or the distance
between the two support vectors that are
shown in the dash lines and then by
finding the largest distance margin we
can get the optimal hyper plane once
we've created an optimal hyperplane we
can easily see which side the new data
fits in and based on the hyperplane we
can say the new data point belongs to
the male gender hopefully that's clear
how that works on a visual level as a
data scientist you should also be asking
what happens if the hyperplane is not
optimal if we select a hyperplane having
low margin then there is a high chance
of misclassification this particular svm
model the one we discussed so far is
also called or referred to as the
lsvm so far so clear but a question
should be coming up we have our sample
data set but instead of looking like
this what if it looked like this where
we have two sets of data but one of them
occurs in the middle of another set you
can see here where we have the blue and
the yellow and then blue again on the
other side of our data line in this data
set we can't use a hyper plane so when
you see data like this it's necessary to
move away from a 1D view of the data to
a two-dimensional view of the data and
for the transformation we use what's
called a kernel function the kernel
function will take the 1D input and
transfer it to a two-dimensional output
as you can see in this picture here the
1D when transferred to a two-dimensional
makes it very easy to draw a line
between the two data sets what if we
make it even more complicated how do we
perform an svm for this type of data set
here you can see we have a
two-dimensional data set where the data
is in the middle surrounded by the green
data on the outside in this case we're
going to segregate the two classes we
have our sample data set and if you draw
a line through it's obviously not an
optimal hyperplane in there so to do
that we need to transfer the 2D to a 3D
array and when you translate it into a
three-dimensional array using the kernel
you can see where you can place a
hyperplane right through it and easily
split the data before we start looking
at a programming example and dive into
the script let's look at the advantage
of the support Vector machine we'll
start with high dimensional input space
or sometimes referred to as the curse of
dimensionality we looked at earlier one
dimension two Dimension three dimension
when you get to a thousand dimensions a
lot of problems start occurring with
most algorithms that have to be adjusted
for the svm automatically does that in
high dimensional space one of the high
dimensional space one high-dimensional
space that we work on is sparse document
vectors this is where we tokenize the
words in document so we can run our
machine learning algorithms over over
them I've seen ones get as high as 2.4
million different tokens that's a lot of
vectors to look at and finally we have
regularization parameter the
regularization parameter or Lambda is a
parameter that helps figure out whether
we're going to have a bias or
overfitting of the data whether it's
going to be overfitted to very specific
instance or it's going to be biased to a
high or low value with the svm it
naturally avoids the overfitting and
bias problems that we see in many other
algorithms these three advantages of the
support Vector machine make it a very
powerful tool to add to your repertoire
of machine learning tools now we did
promise you a used case study we're
actually going to dive into some Python
Programming and so we're going to go
into a problem statement and start off
with the zoo so in the zoo example we
have um family members going to the zoo
and we have the young child going dad is
that a group of crocodiles or alligators
well that's hard to differentiate and
zoos are a great place to start looking
at science and understanding how things
work especially as a young child and so
we can see see the parents sitting here
thinking well what is the difference
between a crocodile and an alligator
well one crocodiles are larger in size
alligators are smaller in size snout
width the crocodiles have a narrow snout
and alligators have a wider snout and of
course in the modern day and age the
father sitting here is thinking how can
I turn this into a lesson for my son and
he goes let a support Vector machine
segregate the two groups I don't know if
my dad ever told me that but that would
be funny now in this example we're not
going to use actual measurements and
data we're just using that for imagery
and that's very common in a lot of
machine learning algorithms and setting
them up but let's roll up our sleeves
and we'll talk about that more in just a
moment as we break into our python
script so here we arrive in our actual
coding and I'm going to move this into a
python editor and just a moment but
let's talk a little bit about what we're
going to cover first we're going to
cover in the code the setup how to
actually create our svm and you're going
to find that there's only two lines of
code that actually create it and the
rest of it is done so quick and fast
that it's all here in the first page and
we'll show you what that looks like as
far as our data cuz we're going to
create some data I talked about creating
data just a minute ago and so we'll get
into the creating data here and you'll
see this nice correction of our two
blobs and we'll go through that in just
a second and then the second part is
we're going to take this and we're going
to bump it up a notch we're going to
show you what it looks like behind the
scenes but let's start with actually
creating our setup I like to use the
Anaconda Jupiter notebook because it's
very easy to use but you can use any of
your favorite python editors or setups
and go in there but let's go ahead and
switch over there and see what that
looks like so here we are in the
Anaconda python notebook or anaconda
jupyter notebook with python we're using
Python 3 I believe this is 3.5 but it
should be work in any of your 3x
versions and uh you'd have to look at
the sklearn and make sure if you're
using a 2X version or an earlier version
let's go and put our code in there and
one of the things I like about the
Jupiter notebook is I go up to view and
I'm going to go ahead and toggle the
line numbers on to make it a little bit
easier to talk about and we can even
increase the size because this is edited
in in this case I'm using Google Chrome
Explorer and that's how it opens up for
the editor although anyone any like I
said any editor will work now the first
step is going to be our Imports and
we're going to import four different
parts the first two I want you to look
at are line one and line two are numpy
as NP and matplot library. pyplot as PLT
now these are very standardized Imports
when you're doing work the first one is
the numbers python we need that because
part of the platform we're using uses
that for the numpy array and I'll talk
about that in a minute so you can
understand why we want to use a numpy
array versus a standard python array and
normally it's pretty standard setup to
use NP for numpy the map plot library is
how we're going to view our data so this
has uh you do need the NP for the SK
learn module but the map plot library is
purely for our use for visualization and
so you really don't need that for the
svm but we're going to put it there so
you have a nice visual aid and we can
show you what it looks like that's
really important at the end when you
finish everything so you have a nice
display for everybody to look at and
then finally we're going to I'm going to
jump one ahead to line number four
that's the SK learn. dat sets. samples
generator import make blobs and I told
you that we were going to make up data
and this is a tool that's in the SK
learn to make up data I personally don't
want to go to the zoo getting trouble
for jumping over the fence and probably
get eaten by the crocodiles or
alligators as I work on measuring their
snouts width and length instead we're
just going to make up some data and
that's what that make blobs is It's a
Wonderful tool if you're ready to test
your your uh setup and you're not sure
about what data you're going to put in
there you can create this blob and it
makes it really easy to use and finally
we have our actual svm the sklearn
import svm on line three so that covers
all our Imports we're going to create
remember I used the make blobs to create
data and we're going to create a capital
x and a lowercase y equals make blobs in
samples equals 40 so we're going to make
40 lines of data it's going to have two
centers with a random State equals 20 so
each each each group is going to have 20
different pieces of data in it and the
way that looks is that we'll have under
X um an XY plane so I have two numbers
under X and Y will be Z or one that's
the two different centers so we have yes
or no in this case alligator or
crocodile that's what that represents
and then I told you that the actual SK
learner the svm is in two lines of cod
code and we see it right here with clf
equal svm do SVC kernel equals linear
and I set Cal to one although in this
example since we are not uh regularizing
the data since we want it to be very
clear and easy to see I went ahead you
can set it to a thousand a lot of times
when you're not doing that but for this
thing linear because it's a very simple
linear example we only have the two
dimensions and it'll be a nice linear
hyper plane it'll be a nice linear line
instead of a full plane so we're not
dealing with a huge amount of data and
then all we have have to do is do cf.
fit X comma Y and that's it clf has been
created and then we're going to go ahead
and display it and I'm going to talk
about this display here in just a second
but let me go ahead and run this code
and this is what we've done is we've
created two blobs you'll see the blue on
the side and then kind of an orangish uh
on the other side that's our two sets of
data they represent one represents
crocodiles and one represents alligators
and then we have our measurements in
this case we have like the uh width and
length of the snout and I did I was
going to come up here and talk just a
little bit about our plot and you'll see
PLT that's what we imported we're going
to do a scatter plot that means we're
just putting dots on there and then look
at this notation I have the capital x
and then in brackets I have a colon
comma zero that's from numpy if you did
that in a regular array you'll get an
error in a python array you have to have
that in a numpy array it turns out that
our make blobs returns a numpy array and
this notation is great because what it
means is the first part is the colon
means we're going to do all the rows
that's all the data in our blob we
created under capital x and then the
second part has a comma zero we're only
going to take the first value and then
if you notice we do the same thing but
we're going to take the second value
remember we always start with zero and
then one so we have column zero and
column one and you can look at this as
our XY plots the first one is the X plot
and the second one is the Y plot so the
first one is on the bottom 0 2 4 6 8 and
10 and then The Second One X of the one
is the four 5 6 7 8 9 10 going up the
left hand side s equal 30 is just the
size of the dot so we can see them
instead real tiny dots and then cmap
equals plt.com paired and you'll also
see the C equals y That's the color
we're using two colors 01 and that's why
we get the nice blue and the two
different colors for the alligator and
the crocodile now you can see here that
we did this the actual fit was done in
two lines of code code a lot of times
there'll be a third line where we
regularize the data we set it between
like minus one and one and we reshape it
but for this it's not necessary and it's
also kind of nice because you can
actually see what's going on and then if
we wanted to we wanted to actually run a
prediction let's take a look and see
what that looks like and to predict some
new data and we'll show this again as we
get towards the end of digging in deep
you can simply assign your new data in
this case I'm giving it a uh width and
length 34 and a width and length
56 and note that I put the data as a set
of brackets and then I have the brackets
inside and the reason I do that is
because when we're looking at data it's
designed to process a large amount of
data coming in we don't want to just
process one line at a time and so in
this case I'm processing two lines and
then I'm just going to print and you'll
see cf. predict new data so the clf and
the predict part is going to give us an
answer and let's see what that looks
like and you'll see 01 so predicted the
first one the 3 four is going to be on
the one side and the 56 is going to be
on the other side so one came out as a
alligator and one came out as a
crocodile now that's pretty short
explanation for the setup but really we
want to dig in and see what going on
behind the scenes and let's see what
that looks like so the next step is to
dig in deep and find out what's going on
behind the scenes and also put that in a
nice pretty graph we're going to spend
more work on this than we did actually
generating the original model and you'll
see here that we go through a few steps
and I'll move this over to our editor in
just a second we come in we create our
original data it's exactly identical to
the first part and I'll explain why we
redid that and show you how not to redo
that and then we're going to go in there
and add in those lines we're going to
see what those lines look like and how
to set those up and finally we're going
to plot all that on here and show it and
you'll get a nice graph with the what we
saw earlier when we were going through
the theory behind this where it shows
the support vectors and the Hy hyper
plane and those are done where you can
see the support vectors as the dash
lines and the solid line which is the
hyper plane let's get that into our
Jupiter notebook before I scroll down to
a new line I want you to notice line 13
it has Plot show and we're going to talk
about that here in just a second but
let's scroll down to a new line down
here and I'm going to paste that code in
and you'll see that the plot show has
moved down below let's scroll up a
little bit and if you look at the top
here of our new Section 1 2 3 and and
four is the same code we had before and
let's go back up here and take a look at
that we're going to fit the values on
our svm and then we're going to plot
scatter it and then we're going to do a
plot show if you are an aspiring machine
learning engineer looking for online
training and certifications from
prestigious universities and in
collaboration with leading experts then
search no more simply learn professional
certification program in Ai and machine
learning from Pur University in
collaboration with IBM should be your
right choice for morea dets use the link
in the description box below with that
in mind what is naive Bay let's start
with a basic introduction to the Bay
theorem named after Thomas baze from the
1700s who first coined this in the
western literature naive Bay classifier
works on the principle of conditional
probability as given by the base theorem
before we move ahead let us go through
some of the simple Concepts in the
probability that we will be using let us
consider the following example of
tossing two coins here we have two
quarters and if we look at all the
different possibilities of what they can
come up as we get that they can come up
as head heads they come up as head tail
tell head and tell tail when doing the
math on probability we usually denote
probability as a p a capital P so the
probability of getting two heads equals
1/4 you can see in our data set we have
two heads and this occurs once out of
the four possibilities and then the
probability of at least one tail occurs
3/4 of the time you'll see on three of
the coin tosses we have tails in them
and out of four that's 3/4s and then the
prob ability of the second coin being
head given the first coin is tail is 1/2
and the probability of getting two heads
given the first coin is ahead is 1/2
we'll demonstrate that in just a minute
and show you how that math works now
when we're doing it with two coins is
easy to see but when you have something
more complex you can see where these Pro
these formulas really come in and work
so the base theorem gives us the
conditional probability of an event a
given another event B has occurred in
this case the first coin toss will be B
and the second coin toss a this could be
confusing because we've actually
reversed the order of them and go from B
to a instead of a to B you'll see this a
lot when you work in probabilities the
reason is we're looking for event a we
want to know what that is so we're going
to label that a since that's our focus
and then given another event B has
occurred in the Baye theorem as you can
see on the left the probability of a
occurring given B has occurred equals
the probability of B occurring given a
has occurred time the probability of a
over the probability of B this simple
formula can be moved around just like
any algebra formula and we could do the
probability of a after a given B times
probability of b equals a probability of
B given a Time probability of a you can
easily move that around and multiply it
and divide it out let us apply base
theorem to our example here we have our
two quarters and we'll notice that the
first two probabilities of getting two
heads and at least one tail we compute
directly off the data so you can easily
see that we have one example HH out of
four 1/4 and we have three with tells in
them giving us three quarters or 34
75% the second condition the second uh
set three and four we're going to
explore a little bit more in detail now
we stick to a simple example with two
coins because you can easily understand
the math the probability of throwing a
tail doesn't matter what comes before it
and the same with the head so still
going to be 50% or 1/2 but when that
come when that probability gets more
complicated let's see you have a D6 dice
or some other instance then this formula
really comes in handy but let's stick to
the simple example for now in this
sample space let a be the event that the
second coin is head and B be the event
that the first coin is Tails again we
reversed it because we want to know what
the second event is going to be so we're
going to be focusing on a and we write
that out as a probability of a given B
and we know this from our formula that
that equals the probability of B given a
times the probability of a over the
probability of B and when we plug that
in we plug in the probability of the
first coin being Tails given the second
coin is heads and the probability of the
second coin being heads given the first
coin being over the probability of the
first coin being Tails when we plug that
data in and we have the probability of
the first coin being Tails given the
second coin is heads times the
probability of the second coin being
heads over the probability of the first
coin being tails you can see it's a
simple formula to calculate we have 1 12
* 1/2 over 1/2 or 1 12 = .5 5 or 1/4 so
the base theorem basically calculates
the conditional probability of the
occurrence of an event based on prior
knowledge of conditions that might be
related to the event we will explore
this in detail when we take up an
example of online shopping further in
this tutorial understanding naive Bay
and machine learning like with any of
our other machine learning tools it's
important to understand where the naive
Bay fits in the hierarchy so under the
machine learning we have supervised
learning and there is other things like
unsupervised learning there's also
reward system This falls under the
supervised learning and then under the
supervised learning there's
classification there's also regression
but we're going to be in the
classification side and then under
classification is your naive Bay let's
go ahead and glance into where is naive
Bay used let's look at some of the use
scenarios for it as a classifier we use
it in face recognition is this Cindy or
is it not Cindy or whoever or it might
be used to identify parts of the face
that they then feed into another part of
the face recognition program this is the
eye this is the nose this is the mouth
weather prediction is it going to be
rainy or sunny medical recognition news
prediction it's also used in medical
diagnosis we might diagnose somebody as
either as high risk or not as high risk
for cancer or heart disease or other
ailments and news classification when
you look at the Google news and it says
well is this political or is this world
news or a lot of that's all done with
the naive Bay understanding naive Bay
classifier now we already went through a
basic understanding with the coins and
the two heads and two tails and head
tail tail heads Etc we're going to do
just a quick review on that and remind
you that the naive Baye classifier is
based on the Baye theorem which gives a
conditional probability of in event a
given event B and that's where the
probability of a given b equals the
probability of B given a times
probability of a over probability of B
remember this is an algebraic function
so we can move these different entities
around we could multiply by the
probability of B so it goes to the left
hand side and then we could divide by
the probability of a given B and just as
easily come up with a new formula for
the probability of B to me staring at
these algebraic functions kind of gives
me a slight headache it's a lot better
to see if we can actually understand how
this data fits together in a table and
let's go ahead and start applying it to
some actual data so you can see what
that looks like so we're going to start
with the shopping demo problem statement
and remember we're going to solve this
first in table form so you can see what
the math looks like and then we're going
to solve it in Python and in here we
want to predict whether the person will
purchase a product are they going to buy
or don't buy very important if you're
running a business you want to know how
to maximize your profits or at least
maximize the purchase of the people
coming into your store and we're going
to look at a specific combination of
different variables in this case we're
going to look at the day the discount
and the free delivery and you can see
here under the day we want to know
whether it's uh on the weekday you know
somebody's working they come in after
work or or maybe they don't work weekend
you can see the bright colors coming
down there celebrating not being in work
or holiday and did we offer a discount
that day yes or no did we offer free
delivery that day yes or no and from
this we want to know whether the
person's going to buy based on these
traits so we can maximize them and find
out the best system for getting somebody
to come in and purchase our goods and
products from our store now having a
nice visual is great but we do need to
dig into the data so let's go ahead and
take a look at the data set we have a
small sample data set of 30 rows we're
showing you the first 15 of those roads
for this demo now the actual data file
you can request just type in below under
the comments on the YouTube video and
we'll send you some more information and
send you that file as you can see here
the file is very simple columns and rows
we have the day the discount the free
delivery and did the person purchase or
not and then we have under the day
whether it was a weekday a holiday was
it the weekend this is a pretty simple
set of data and long before computers
people used to look at this data and
calculate this all by hand so let's go
ahead and walk through this and see what
that looks like when we put that into
tables also note in today's world we're
not usually looking at three different
variables and 30 rows nowadays because
we're able to collect data so much we're
usually looking at 27 30 variables
across hundreds of rows the first thing
we want to do is we're going to take
this data and uh based on the data set
containing our three inputs Day discount
and free delivery we're going to go
ahead and populate that to frequency
tables for each of tribute so we want to
know if they had a discount how many
people buy and did not buy uh did they
have a discount yes or no do we have a
free delivery yes or no on those days
how many people made a purchase how many
people didn't and the same with the
three days of the week was it a weekday
a weekend a holiday and did they buy yes
or no as we dig in deeper to this table
for our base theorem let the event buy
ba a now remember when we looked at the
coins I said we really want to know what
the outcome is did the person buy or not
and that's usually event a is what
you're looking for and the independent
variables discount free delivery and day
BB so we'll call that probability of B
now let us calculate the likelihood
table for one of the variables let's
start with day which includes weekday
weekend and holiday and let us start by
summing all of our rows so we have the
uh weekday row and out of the weekdays
there's 9 plus 2 so is 11 weekdays
there's eight weekend days and 11
holidays wow it's a lot of holidays and
then we want to sum up the total number
of days so we're looking at a total of
30 days let's start pulling some
information from our chart and see where
that takes us and when we fill in the
chart on the right you can see that nine
out of 24 purchases are made on the
weekday 7even out of 24 purchases on the
weekend and eight out of 24 purchases on
a holiday and out of all the people who
come in 24 out of 30 purchase you can
also see how many people do not purchase
on the week dat it's two out of six
didn't purchase and so on and so on we
can also look at the totals and you'll
see on the right we put together some of
the formulas the probability of making a
purchase on the weekend comes out 11 out
of 30 so out of the 30 people who came
into the store throughout the weekend
weekday and holiday 11 of those
purchases were made on the weekday and
then you can also see the probability of
them not making a purchase and this is
done for doesn't matter which day of the
week so we call that probability of no
buy would be 6 over 30 or02 so there's a
20% chance that they're not going to
make a purchase no matter what day of
the week it is and finally we look at
the probability of BF a in this case
we're going to look at the probability
of the weekday and not buying two of the
no buys were done out of the weekend out
of the six people who did not make
purchases so when we look at that
probability of the weekday without a
purchase is going to be 33 or
33% let's take a look at this at
different probabilities and uh based on
this likelihood table let's go ahead and
calculate conditional probabilities as
below the first three we just did the
probab ability of making a purchase on
the weekday is 11 out of 30 or roughly
36 or 37% 367 the probability of not
making a purchase at all doesn't matter
what day of the week is roughly 0.2 or
20% and the probability of a week day no
purchase is roughly two out of six so
two out of six of our no purchases were
made on the weekday and then finally we
take our P of a if you looked we've kept
the symbols up there we got P of
probability of B probability of a
probability of B if a we should remember
that the probability of a if B is equal
to the first one times the probability
of no per buys over the probability of
the weekday so we could calculate it
both off the uh table we created we can
also calculate this by the formula and
we get the 367 which equals or uh 33 * 2
over 367 which equals. 179 or roughly uh
17 to 18%
and that'd be the probability of no
purchase done on the weekday and this is
important because we can look at this
and say as the probability of buying on
the weekday is more than the probability
of not buying on the weekday we can
conclude that customers will most likely
buy the product on a weekday now we've
kept our chart simple and we're only
looking at one aspect so you should be
able to look at the table and come up
with the same information or the same
conclusion that should be kind of
intuitive at this point next we can take
the same setup we have the frequency
tables of all three independent
variables now we can construct the
likelihood tables for all three of the
variables we're working with we can take
our day like we did before we have
weekday weekend and holiday we filled in
this table and then we can come in and
also do that for the discount yes or no
did they buy yes or no and we fill in
that full table so now we have our
probabilities for a discount and whether
the discount leads to a purchase or not
and the probability for free delivery
does that lead to a purchase or not and
this is where it starts getting really
exciting let us use these three
likelihood tables to calculate whether a
customer will purchase a product on a
specific combination of Day discount and
free delivery or not purchase here let
us take a combination of these factors
day equals holiday discount equals yes
free delivery equals yes let's dig
deeper into the math and actually see
what this looks like and we're going to
start with looking for the probability
of them not purchasing on the following
combinations of days we are actually
looking for the probability of a equal
no buy no purchase and our probability
of B we're going to set equal to is it a
holiday did they get a discount yes and
was it a free delivery yes before we go
further let's look at the original
equation the probability of a if B
equals the probability of B given the
condition a and the probability times
probability of a over the probability of
B occurring now this is basic algebra so
we can multiply this information
together so when you see the probability
of a given B in this case the condition
is b c and d or the three different
variables we're looking at and when you
see the probability of B that would be
the conditions we're actually going to
multiply those three separate conditions
out probability of you'll see that in
just a second in the formula times the
full probability of a over the full
probability of B so here we are back to
this and we're going to have let a equal
no purchase and we're looking for the
probability of B on the condition a
where a sets for three different things
remember that equals the probability of
a given the condition B and in this case
we just multiply those three different
variables together so we have the
probability of the discount times the
probability of free delivery times the
probability is the day equal a holiday
those are our three variables of the
probability of a if B and then that is
going to be multiplied by the
probability of them not making a
purchase and then we want to divide that
by the total probabilities and they're
multiplied together so we have the
probability of a discount the
probability of a free delivery and the
probability of it being on a holiday
when we plug those numbers in we see
that one out of six were no purchase on
a discounted day two out of six were a
no purchase on a free delivery day and
three out of six were a no purchase on a
holiday those are our three
probabilities of a of B multiplied out
and then that has to be multiplied by
the probability of a no purchase and
remember the pro probability of a no buy
is across all the data so that's where
we get the 6 out of 30 we divide that
out by the probability of each category
over the total number so we get the 20
out of 30 had a discount 23 out of 30
had a yes for free delivery and 11 out
of 30 were on a holiday we plug all
those numbers in we get
178 so in our prob ability math we have
a178 if it's a no buy for a holiday a
discount and a free delivery let's turn
that around and see what that looks like
if we have a purchase I promise this is
the last page of math before we dig into
the python script so here we're
calculating the probability of the
purchase using the same math we did to
find out if they didn't buy now we want
to know if they did buy and again we're
going to go by the day equals a holiday
discount equals yes free delivery equals
yes and let a equal buy now right about
now you might be asking why are we doing
both calculations why why would we want
to know the no buys and buys for the
same data going in well we're going to
show you that in just a moment but we
have to have both of those pieces of
information so that we can figure it out
as a percentage as opposed to a
probability equation and we'll get to
that normalization here in just a moment
let's go ahead and walk through this
calculation and as you can see here the
probability of a on the condition of b b
being all three categories did we have a
discount with a purchase do we have a
free delivery with a purchase and did we
is a day equal to Holiday and when we
plug this all into that formula and
multiply it all out we get our
probability of a discount probability of
a free delivery probability of the day
being a holiday times the overall
probability of it being a purchase
divided by again multiplying the three
variables out the full probability of
there being a discount the full
probability of being a free delivery and
the full probability of there being a
day equal holiday and that's where we
get this 19 24 * 21 over 24 * 8 over 24
* the P of a 24 over 30 divided by the
probability of the discount the free
delivery times the day or 20 over 30 23
over 30 * 11 over 30 and that gives us
our
986 so what are we going to do with
these two pieces of data we just
generated well let's go ahead and go
over them we have a probability of
purchase equals 986 we have a
probability of no purchase equals 178 so
finally we have a conditional
probabilities of purchase on this day
let us take that we're going to
normalize it and we're going to take
these probabilities and turn them into
percentages this is simply done by
taking the sum of probabilities which
equal
98686 plus. 178 and that equals the
1.64 if we divide each probability by
the sum we get the percentage and so the
likelihood of a purchase is 84
4.71% and the likelihood of no purchase
is
15.29% given these three different
variables so it's if it's on a holiday
if it's uh with a discount and has free
delivery then there's an 84.7 1% chance
that the customer is going to come in
and make a purchase hooray they
purchased our stuff we're making money
if you're owning a shop that's like is
the bottom line is you want to make some
money so you can keep your shop open and
have a living now I promised you that we
were going to be finishing up the math
here with a few pages so we're going to
move on and we're going to do two steps
the first step is I want you to
understand why you want to why you want
to use the naive Bay what are the
advantages of naive bays and then once
we understand those advantages we're
just look at that briefly then we're
going to dive in and do some python
coding advantages of naive Baye
classifier so let's take a look at the
six advantages of the naive Baye
classifier and we're going to walk
around this lovely wheel looks like an
origami folded paper the first one is
very simple and easy to Implement
certainly you could walk through the
tables and do this by hand you got to be
a little careful because the notations
can get confusing you have all these
different probabilities and I certainly
mess those up as I put them on you know
is it on the top or the bottom got to
really pay close attention to that when
you put it into python it's really nice
because you don't have to worry about
any of that you let the python handle
that the python module but understanding
it you can put it on a table and you can
easily see how it works and it's a
simple algebraic function it needs less
training data so if you have smaller
amounts of data this is great powerful
tool for that handles both continuous
and discrete data it's highly scalable
with number of predictors and data
points so as you can see you just keep
multiplying different probabilities in
there and you can cover not just three
different variables or sets you can now
expand this to even more categories
number five it's fast it can be used in
real time predictions this is so
important this is why it's used in a lot
of our predictions on online shopping
carts uh referrals spam filters is
because there's no time delay as it has
to go through and figure out a neural
network or one of the other mini setups
where you're doing classification and
certainly there's a lot of other tools
out there in the machine learning that
can handle these but most of them are
not as fast as the naive bays and then
finally it's not sensitive to irrelevant
features so it picks up on your
different probabilities and if you're
short on date on one probability you can
kind of it automatically adjust for that
those formulas are very automatic and so
you can still get a very solid
predictability even if you're missing
data or you have overlapping data for
two completely different areas we see
that a lot in doing census and studying
of people and habits where they might
have one study that covers one aspect
and another one that overlaps and
because the two overlap they can then
predict the unknown ons for the group
that they haven't done the second study
on or vice versa so it's very powerful
in that it is not sensitive to the
irrelevant features and in fact you can
use it to help predict features that
aren't even in there so now we're down
to my favorite part we're going to roll
up our sleeves and do some actual
programming we're going to do the use
case text classification now I would
challenge you to go back and send us a
note on the notes below underneath the
video and request the data for the
shopping cart so you can plug that into
python code and do that on your own time
so you can walk through it since we walk
through all the information on it but
we're going to do a python code doing
text classification very popular for
doing the naive Bay so we're going to
use our new tool to perform a text
classification of news headlines and
classify news into different topics for
a News website as you can see here we
have a nice image of the Google news and
then related on the right subgroups I'm
not sure where they actually pulled the
actual data we're going to use from it's
one of the standard sets but certainly
this can be used on any of our news
headlines and classification so let's
see how it can be done using the naive
Bay classifier now we're at my favorite
part we're actually going to write some
python script roll up our sleeves and
we're going to start by doing our
Imports these are very basic Imports
including our news group and we'll take
a quick glance at the Target names then
we're going to go ahead and start
training our data set and putting it
together we'll put together a nice graph
because it's always good to have a graph
to show what's going on and once we've
traded it and we've shown you a graph of
what's going on then we're going to
explore how to use it and see what that
looks like now I'm going to open up my
favorite editor or inline editor for
python you don't have to use this you
can use whatever your editor that you
like whatever uh interface IDE you want
this just happens to be the Anaconda
Jupiter notebook and I'm going to paste
that first piece of code in here so we
can walk through it let's make it a
little bigger on the screen so you have
a nice view of what's going on uh and
we're using Python 3 in this case 3.5 so
this would work in any of your 3x if you
have it set up correctly should also
work in a lot of the 2x you just have to
make sure all the the versions of the
modules match your python version and in
here you'll notice the first line is
your percentage met plot library in line
now three of these lines of code are all
about plotting the graph this one let's
The Notebook notes and this is the
inline setup that we want the graphs to
show up on this page without it in a
notebook like this which is an Explorer
interface it won't show up now a lot of
idees don't require that a lot of them
like on if I'm working on one of my
other setups it just has a pop up and
the graph pops up on there so you have a
that setup also but for this we want the
matap plot library in line and then
we're going to import numpy as NP that's
number python which has a lot of
different formulas in it that we use for
both of our sklearn module and we also
use it for any of the upper math
functions in Python and it's very common
to see that as NP nump as NP the next
two lines are all about our graphing
remember I said three of these were
about graphing well we need our map plot
library. pyplot as p PLT and you'll see
that PLT is a very common setup as is
the SNS and just like the NP and we're
going to import caborn as SNS and we're
going to do the SNS doet now caborn sits
on top of pip plot and it just makes a
really nice heat map it's really good
for heat maps and if you're not familiar
with heat maps that just means we give
it a color scale the term comes from the
brighter red it is the hotter it is in
some form of data and you can set it to
whatever you want and we'll see that
later on so those you'll see see that
those three lines of code here are just
importing the graph function so we can
graph it and as a data scientist you
always want to graph your data and have
some kind of visual it's really hard
just to shove numbers in front of people
and they look at it and it doesn't mean
anything and then from the SK learn. dat
sets we're going to import the fetch 20
news groups very common one for
analyzing tokenizing words and setting
them up and exploring how the words work
and how do you categorize different
things when you're dealing with
documents and then we set our data equal
to fetch news groups so our data
variable will have the data in it and
we're going to go ahead and just print
the target names data. Target names and
let's see what that looks like and
you'll see here we have alt atheism comp
Graphics comp osms windows.
miscellaneous and it goes all the way
down to talk politics. miscellaneous
talk religion. miscellaneous these are
the categories they've already assigned
to this news group and it's called Fetch
20 because you'll see there's I believe
there's 20 different Topics in here or
20 different categories as we scroll
down now we've gone through the 20
different categories and we're going to
go ahead and start defining all the
categories and set up our data so we're
actually here going to go ahead and get
it get the data all set up and take a
look at our data and let's move this
over to our Jupiter notebook and let's
see what this code does first we're
going to set our categories now if you
noticed up here I could have just as
easily set this equal to data. Target
cor names because it's the same thing
but we want to kind of spell it out for
you so you can see the different
categories it kind of makes it more
visual so you can see what your data is
looking like in the background once
we've created the categories we're going
to open up a train set so this training
set of data is going to go into fetch 20
news groups and it's a subset in there
called train and categories equals
categories so we're pulling out those
categories that match and then if you
have a train set you should also have
the testing set we have test equals
fetch 20 news groups subset equals test
and categories equals categor ories
let's go down one SI so it all fits on
my screen there we go and just so we can
really see what's going on let's see
what happens when we print out one part
of that data so it creates train and
under train it creates train. dat and
we're just going to look at data piece
number five and let's go ahead and run
that and see what that looks like and
you can see when I print train. dat
number 5 under train it prints out one
of the Articles this is article number
five you can go through and read it on
there and we can also go in here and
change this to test which should look
identical because it's splitting the
data up into different groups train and
test and we'll see test number five is a
a different article but another article
in here and maybe you're curious and you
want to see just how many articles are
in here we could do length of train.
data and if we run that you'll see that
the training data has
11,314 articles so we're not going to go
through all those those articles that's
a lot of articles but um we can look at
one of them just so you can see what
kind of information is coming out of it
and what we're looking at and we'll just
look at number five for today and here
we have it rewarding the Second
Amendment IDs VT line 58 lines 58 in
article uh Etc and you can scroll all
the way down and see all the different
parts to there now we've looked at it
and that's pretty complicated when you
look at one of these articles to try to
figure out how do you wait this if you
look down here we have different words
and maybe the word from well from is
probably in all the AR so it's not going
to have a lot of meaning as far as
trying to figure out whether this
article fits one of the categories or
not so trying to figure out which
category fits in based on these words is
where the challenge comes in now that
we've viewed our data we're going to
dive in and do the actual predictions
this is the actual naive Bas and we're
going to throw another model at you or
another module at you here in just a
second we can't go into too much detail
but it deals specifically working with
words and text and what they call
tokenizing those words so let's take
this code and let's uh skip on over to
our Jupiter notebook and walk through it
and here we are in our jupyter notebook
let's paste that in there and I can run
this code right off the bat it's not
actually going to display anything yet
but it has a lot going on in here so the
top we had the print module from the
earlier one I didn't know why that was
in there so we're going to start by
importing our necessary packages and
from the sklearn features extraction.
text we're going to import tfidf
vectorizer I told you we're throw a
module at you we can't go too much into
the math behind this or how it works you
can look it up the notation for the math
is usually tf.idf and that's just a way
of weighing the words and it weighs the
words based on how many times they're
used in a document how many times or how
many documents they're used in and it's
a well-used formula it's been around for
a while it's a little confusing to put
this in here uh but let's let it know
that it just goes in there and waits the
different words in the document for us
that way we don't have to wait and if
you put a weight on it if you remember I
was talking about that up here earlier
if these are all emails they probably
all have the word from in them from
probably has a very low weight it has
very little value in telling you what
this document's about same with words
like in an article in articles in cost
of un maybe cost might or were words
like criminal weapons destruction these
might have a heavier weight because they
describe a little bit more what the
article's doing well how do you figure
out all those weights the different
articles that's what this module does
that's what the tfidf vectorizer is
going to do for us and then we're going
to import our SK learn. naive Bas and
that's our multinomial NB multinomial
naive Bay pretty easy to understand that
where that comes from and then finally
we have the sky learn pipeline import
make pipeline now the make pipeline is
just a cool piece of code because we're
going to take the information we get
from the tfidf vectorizer and we're
going to pump that into the multinomial
INB so a pipeline is just a way of
organizing how things flow it's used
commonly you probably already guess what
it is if you've done any businesses they
talk about the sales pipeline if you're
on a work crew or project manager you
have your pipeline of information that's
going through where your projects and
what has to be done in what order that's
all this pipeline is we're going to take
the tfid vectorizer and then we're going
to push that into the multinomial in B
now we've designated that as the
variable model we have our pipeline
model and we're going to take that model
and this is just so elegant this is done
in just a couple lines of code model.fit
and we're going to fit the data and
first the train data and then the train
Target now the train data has the
different articles in it you can see the
one we were just looking at and the
train. target is what category they
already categorized that that particular
article collapse and what's Happening
Here is the train data is going into the
tfid vectorizer so when you have one of
these articles it goes in there it
weights all the words in there so
there's thousands of words with
different weights on them I remember
once running a model on this and I
literally had 2.4 million tokens go into
this so when you're dealing like large
document bases you can have a huge
number of different words it then takes
those words gives them a weight and then
based on that weight based on the words
and the weights and then puts that into
the multinomial NB and once we go into
our naive Bay we want to put the train
Target in there so the train data that's
been mapped to the tfid vectorizer is
now going through the multinomial INB
and then we're telling it well these are
the answers these are the answers to the
different documents so this document
that has all these words with these
different weights from the first part is
going to be whatever category it comes
out of maybe it's the um talk show or
the article on religion miscellaneous
once we fit that model we can then take
labels and we're going to set that equal
to model. predict most of the sklearn
use the term. predict to let us know
that we've now trained the model and now
we want to get some answers and we're
going to put our test data in there
because our test data is the stuff we
held off to the side we didn't train it
on there and we don't know what's going
to come up out of it and we just want to
find out how good our labels are do they
match what they should be now now I've
already run this through there's no
actual output to it to show this is just
setting it all up this is just training
our model creating the label so we can
see how good it is and then we move on
to the next step to find out what
happened to do this we're going to go
ahead and create a confusion Matrix and
a heat map so the confusion Matrix which
is confusing just by its very name is
basically going to ask how confused is
our answer did it get it correct or did
it Miss some things in there or have
some Miss labels and then we're going to
put that on a heat map so we'll have
some nice colors to look at to see how
that plots out let's go ahead and take
this code and see how that uh take a
walk through it and see what that looks
like so back to our Jupiter notebook
going to put the code in there and let's
go ahead and run that code take it just
a moment and remember we had the in line
that way my graph shows up on the in
line here and let's walk through the
code and then we'll look at this and see
what that means so make it a little bit
bigger there we go no reason not to use
a whole screen too big so we have here
from sklearn metrics import confusion
Matrix and that's just going to generate
a set of data that says I the prediction
was such the actual truth was either
agreed with it or is something different
and it's going to add up those numbers
so we can take a look and just see how
well it worked and we're going to set a
variable mat equal to confusion Matrix
we have our test Target our test data
that was not part of the training very
important in data science we always keep
our test data separate otherwise it's
not a valid model if we can't properly
test it with new data and this is the
labels we created from that test data
these are the ones that we predict it's
going to be so we go in and we create
our SN heat map the SNS is our caborn
which sits on top of the PIP plot so
when we create a SNS do heat map we take
our confusion Matrix and it's going to
be uh matt. T and do we have other
variables that go into the SNS do heat
map we're not going to go into detail
what all the variables mean The
annotation equals true that's what tells
it to put the numbers here so you have
the 166 the 1 the 00001 format d and c
bar equals false have to do with the uh
format if you take those out you'll see
that some things disappear and then the
X tick labels and the Y tick labels
those are our Target names and you can
see right here that's the alt atheism
comp Graphics comp osms windows.
miscellaneous and then finally we have
our PLT dox label remember the SNS or
the caborn sits on top of our map plot
Library our PLT and so we want to just
tell that X Lael equals a true is is
true the labels are true and then the Y
label is prediction label so when we say
a true this is what it actually is and
the prediction is what we predicted and
let's look at this graph because that's
probably a little confusing the way I
rattled through it and what I'm going to
do is I'm going to go ahead and flip
back to the slides cuz they have a black
background they put in there that helps
it shine a little bit better so you can
see the graph a little bit easier so in
reading this graph what we want to look
at is how the color scheme has come out
and you'll see a line right down the
middle diagonally from upper left to
bottom right what that is is if you look
at the labels we have our predicted
label on the left and our true label on
the right those are the numbers where
the prediction and the true come
together and this is what we want to see
is we want to see those lit up that's
what that heat map does as you can see
that it did a good job of finding those
data and you'll notice that there's a
couple of red spots on there where it
missed you know it's a little confused
when we talk about talk religion
miscellaneous versus talk politics
miscellaneous social religion Christian
versus Alt atheism it mislabeled some of
those and those are very similar topics
so you could understand why it might
mislabel them but overall it did a
pretty good job if we're going to create
these models we want to go ahead and be
able to use them so let's see what that
looks like to do this let's go ahead and
create a definition a function to run
and we're going to call this function
let me just expand that just a notch
here there we go I like mining big
letters predict categories we want to
predict the category we're going to send
it s a string and then we're sending it
train equals train we have our training
model and then we had our pipeline model
equals model this way we don't have to
resend these variables each time the
definition knows that because I said
train equals train and I put the equal
for model and then we're going to set
the prediction equal to the model.
predicts so it's going to send whatever
string we send to it it's going to push
that string through the pipeline the
model pipeline it's going to go through
and uh tokenize it and put it through
the TF IDF convert that into numbers and
weights for all the different documents
and words and then it'll put that
through our naive bays and from it we'll
go ahead and get our prediction we're
going to predict what value it is and so
we're going to return train. Target
names predict of zero and remember that
the train. target names that's just
categories I could have just as easily
put uh categories in there. predict of
zero so we're taking the prediction
which is a number and we're converting
it to an actual category we're
converting it from um I don't know what
the actual numbers are let's say zero
equals alt atheism so we're going to
convert that zero to the word or one
maybe it equals comp Graphics so we're
going to convert number one into comp
Graphics that's all that is and then we
got to go ahead and and then we need to
go ahead and run this so I load that up
and then once I run that we can start
doing some predictions I'm going go
ahead and type in predict category and
let's just do predict category Jesus
Christ and it comes back and says it's
social religion Christian that's pretty
good now note I didn't put print on this
one of the nice things about the Jupiter
notebook editor and a lot of inline
editors is if you just put the name of
the variable out as returning the
variable train. Target names it'll
automatically print that for you in your
own ID you might have to put in print
let's see where else we can take this
and maybe you're a space science buff so
how about sending load to
International Space
Station and if we run that we get
science space or maybe you're a uh
automobile buff and let's do um oh they
were going to tell me Audi is better
than BMW but I'm going to do BMW is
better than an Audi so maybe you're a
car buff and we run that and you'll see
it says recreational I'm assuming that's
what recc stands for Autos so I did a
pretty good job labeling that one how
about uh if we have something like a
caption running through there president
of India and if we run that it comes up
and says talk politics
miscellaneous so when we take our
definition or our function and we run
all these things through Kudos we made
it we were able to correctly classify
text into different groups based on
which category they belong to using the
naive Bas classifier now we did throw in
the pipeline the TF IDF vectorizer we
threw in the graphs those are all things
that you don't necessarily have to know
to understand the naive Bay setup or
classifier but they're important to know
one of the main uses for the naive Bay
is with the TF IDF tokenizer vectorizer
where it tokenizes the word and as
labels and we use the pipeline because
you need to push all that data through
and it makes it really easy and fast you
don't have to know those to understand
naive BS but they certainly help for
understanding the industry in data
science and we can see our categorizer
our naive Bas classifier we were able to
predict the category religion space
motorcycles Autos politics and properly
classify all these different things we
pushed into our prediction and our train
model if you are an aspiring machine
learning engineer looking for online
training and certifications from
prestigious universities and in
collaboration with leading experts then
search no more simply learns
professional certification program in Ai
and machine learning from Pur University
in collaboration with IBM should be your
right choice for more details use the
link in the description box below with
that in
mind so what is kain's clustering kain's
clustering is an unsupervised learning
algorithm in this case you don't have
labeled data unlike in supervised
learning so you have a set of data and
you want to group them and as the name
suggests you want to put them into
clusters which means objects that are
similar in nature similar in
characteristics need to be put together
so that's what K means clustering is all
about the term k is basically is a
number so so we need to tell the system
how many clusters we need to perform so
if K is equal to 2 there will be two
clusters if K is equal to three three
clusters and so on and so forth that's
what the k stands for and of course
there is a way of finding out what is
the best or Optimum value of K for a
given data we will look at that so that
is K means clustering so let's take an
example K means clustering is used in
many many scenarios but let's take an
example of Cricket the game of cricket
let's say you received data of a lot of
players from maybe all over the country
or all over the world and this data has
information about the runs scored by the
people or by the player and the wickets
taken by the player and based on this
information we need to Cluster this data
into two clusters batsman and Bowlers so
this is an interesting example let's see
how we can perform this so so we have
the data which consists of primarily two
characteristics which is the runs and
the wickets so the bowlers basically
take wickets and the batsman score runs
there will be of course a few Bowlers
who can score some runs and similarly
there will be some batsmen who will Who
would have taken a few wickets but with
this information we want to Cluster
those players into batsmen and Bowlers
so how does this work let's say this is
how the data is so there are information
there is information on the y- AIS about
the Run scored and on the x-axis about
the wickets taken by the players so if
we do a quick plot this is how it would
look and um when we do the clustering we
need to have the Clusters like shown in
the third diagram out here so we need to
have a cluster which consists of people
who have scored High runs which is
basically the batsman and then we need a
cler with people who have taken a lot of
wickets which is typically the bowlers
there may be a certain amount of overlap
but we will not talk about it right now
so with K min's clustering we will have
here that means K is equal to two and we
will have two clusters which is batsmen
and Bowlers so how does this work the
way it works is the first step in
cayman's clustering is the allocation of
two centroids randomly so two points are
assigned as so-called centroids so in
this case we want two clusters which
means K is equal to 2 so two points have
been randomly assigned as centroids keep
in mind these points can be anywhere
there are random points they are not
initially they are not really the
centroids centroid means it's a central
point of a given data set but in this
case when it starts off it's not really
the central
okay so these points though in our
presentation here we have shown them one
point closer to these data points and
another closer to these data points they
can be assigned randomly anywhere okay
so that's the first step the next step
is to determine the distance of each of
the data points from each of the
randomly assigned centroids so for
example we take this point and find the
distance from this centroid and the
distance from from this centroid this
point is taken and the distance is form
from this centroid and this Cent and so
on and so forth so for every point the
distance is measured from both the
centroids and then whichever distance is
less that point is assigned to that
centroid so for example in this case
visually it is very obvious that all
these data points are assigned to this
centroid and all these data points are
assigned to this centroid and that's
what is represented here in blue color
and in this yellow color the next step
is to actually determine the central
point or the actual centroid for these
two clusters so we have this one initial
cluster this one initial cluster but as
you can see these points are not really
the centroid centroid means it should be
the central position of this data set
Central position of this data set so
that is what needs to be determined as
the next step so the central point point
or the actual centroid is determined and
the original randomly allocated centroid
is repositioned to the actual centroid
of this new clusters and this process is
actually repeated now what might happen
is some of these points may get
reallocated in our example that is not
happening probably but it may so happen
that the distance is found between each
of these data points once again with
these centroids and if there is if it is
required some points may be reallocated
we will see that in a later example but
for now we will keep it simple so this
process is continued till the centroid
repositioning stops and that is our
final cluster so this is our so after
iteration we come to this position this
situation where the centroid doesn't
need any more repositioning and that
means our algorithm has converged
convergence has occurred and we have
have the cluster two clusters we have
the Clusters with a centroid so this
process is repeated the process of
calculating the distance and
repositioning the centroid is repeated
till the repositioning stops which means
that the algorithm has converged and we
have the final cluster with the data
points and the centroids so this is what
you're going to learn from this session
we will talk about the types of
clustering what is K me's clustering
application of K me's clustering K me's
clustering is done using distance
measure so we will talk about the common
distance measures and then we will talk
about how kain's clustering works and go
into the details of K's clustering
algorithm and then we will end with a
demo and a use case for K's clustering
so let's begin first of all what are the
types of clustering there are primarily
two categories of clustering I
hierarchical clustering and then
partitional clustering and each of these
categories are further subdivided into
agglomerative and divisive clustering
and K means and fuzzy c means clustering
let's take a quick look at what each of
these types of clustering are in
hierarchical clustering the Clusters
have a tree like structure and
hierarchical clustering is further
divided into agglomerative and divisive
agglomerative clustering is a bottom up
approach we begin with each element as a
separate cluster and merge them into
successively larger clusters so for
example we have a b CDE e f we start by
combining BNC form one cluster d and e
form one more then we combine d and f
one more bigger cluster and then add BC
to that and then finally a to it
compared to that divisive clustering or
divisive clustering is a top- down
approach we begin with the whole set and
proceed to divide it into to
successively smaller clusters so we have
a bcde e f we first take that as a
single cluster and then break it down
into a b c d e and f then we have
partitional clustering split into two
subtypes k means clustering and fuzzy c
means in kin's clustering the objects
are divided into the number of clusters
mentioned by the number K that's where
the K comes from so if we say k is equal
to 2
the objects are divided into two
clusters C1 and C2 and the way it is
done is the features or characteristics
are compared and all objects having
similar characteristics are clubbed
together so that's how K me's clustering
is done we will see it in more detail as
we move forward and fuzzy c means is
very similar to K means in the sense
that it clubs objects that have similar
characteristics together but while in
kin's clustering two object objects
cannot belong to or any object a single
object cannot belong to two different
clusters in c means objects can belong
to more than one cluster so that is the
primary difference between K means and
fuzzy c means so what are some of the
applications of Cain's clustering kin's
clustering is used in a variety of
examples or variety of business cases in
real life starting from academic
performance diagnostic system search
engines and while sensor networks and
many more so let us take a little deeper
look at each of these examples academic
performance So based on the scores of
the students students are categorized
into a b c and so on clustering forms a
backbone of search engines when a search
is performed the search results need to
be grouped together the search engines
very often use clustering to do this and
similarly in case of wireless sensor
networks the clustering algorithm plays
the role of finding the cluster heads
which collects all the data in its
respective cluster so clustering
especially K means clustering uses
distance measure so let's take a look at
what is distance measure so while these
are the different types of clustering in
this video we will focus on K means
clustering so distance measure tells how
similar some objects are so the
similarity is measured using what is is
known as distance measure and what are
the various types of distance measures
there is ukian distance there is
Manhattan distance then we have squared
ukian distance measure and cosine
distance measure these are some of the
distance measures supported by K means
clustering let's take a look at each of
these what is ukian distance measure
this is nothing but the distance between
two points so we have learned in high
school how to find the distance between
two points this is a little
sophisticated formula for that but we
know a simpler one is square root of Y2
minus y1 s + X2 - X1 s so this is an
extension of that formula so that is the
ukian distance between two points what
is the squared ukian distance measure
it's nothing but the square of the ukian
distance as the name suggests so instead
of taking the square root we leave
the square as it is and then we have
Manhattan distance measure in case of
Manhattan distance it is the sum of the
distances across the x-axis and the Y
AIS and note that we are taking the
absolute value so that the negative
values don't come into play so that is
the Manhattan distance measure then we
have cosine distance measure in this
case we take the angle between the two
vectors formed by joining the points
from the origin so that is the cosine
distance measure okay so that was a
quick overview about the various
distance measures that are supported by
K means now let's go and check how
exactly K means clustering works okay so
this is how kain's clustering works this
is like a flowchart of the whole process
there is a starting point and then we
specify the number of clusters that we
want now there are couple of ways of
doing this we can can do by trial and
error so we specify a certain number
maybe K is equal to 3 or four or five to
start with and then as we progress we
keep changing until we get the best
clusters or there is a technique called
elbow technique whereby we can determine
the value of K what should be the best
value of K how many clusters should be
formed so once we have the value of K we
specify that and then the system will
assign that many s centroid so it picks
randomly that to start with randomly
that many points that are considered to
be the centroids of these clusters and
then it measures the distance of each of
the data points from these centroids and
assigns those points to the
corresponding centroid from which the
distance is minimum so each data point
will be assigned to the centroid Which
is closest to it and thereby we have K
number of initial clusters however this
is not the final clusters The Next Step
it does is for the new groups for the
Clusters that I have been formed it
calculates the mean position thereby
calculates the new centroid position the
position of the centroid moves compared
to the randomly allocated one so it's an
iterative process once again the
distance of each point is measured from
this new centroid point point and if
required the data points are reallocated
to the new centroids and the mean
position or the new centroid is
calculated once again if the centroid
moves then the iteration continues which
means the convergence has not happened
the clustering has not converged so as
long as there is a movement of the
centroid this iteration keeps happening
but once the centroid stops moving which
means that the cluster has converged or
the clustering process has converged
that will be the end result so now we
have the final position of the centroid
and the data points are allocated
accordingly to the closest centroid I
know it's a little difficult to
understand from this simple flowchart so
let's do a little bit of visualization
and see if we can explain it better
let's take an example if we have a data
set for a grocery shop so let's say we
have a data set for a grocery shop and
now we want to find out how many
clusters this has to be spread across so
how do we find the optimum number of
clusters there is a technique called the
elbow method so when these clusters are
formed there is a parameter called
within sum of squares and the lower this
value is the better the cluster is that
means all these points are very close to
each other so we use this within sum of
squares as a measure to find the optimum
number of clusters that can be formed
for a given data set so we create
clusters or we let the system create
clusters of a variety of numbers maybe
of 10 10 clusters and for each value of
K the within SS is measured and the
value of K which has the least amount of
within s SS or WSS that is taken as the
optimum value of K so this is the
diagrammatic representation so we have
on the y- AIS the within sum of squares
or WSS and on the x-axis we have the
number of clusters so as you can imagine
if you have K is equal to 1 which means
all the data points are in a single
cluster the wies value will be very high
because they are probably scattered all
over the moment you split it into two
there will be a drastic fall in the
within SS value and that's what is
represented here but then as the value
of K increases the decrease the rate of
decrease will not be so high it will
continue to decrease but probably the
rate of decrease will not be high so
that gives us an idea so from here we
get an idea for example the optimum
value of K should be either two or three
or at the most four but beyond that
increasing the number of clusters is not
dramatically changing the value in WSS
because that pretty much gets stabilized
okay now that we have got the value of K
and let's assume that these are our
delivery points the next step is
basically to assign two centroids
randomly so let's say C1 and C2 are the
centroids assigned randomly now the
distance of each location
from the centroid is measured and each
point is assigned to the centroid Which
is closest to it so for example these
points are very obvious that these are
closest to C1 whereas this point is far
away from C2 so these points will be
assigned which are close to C1 will be
assigned to C1 and these points or
locations which are close to C2 will be
assigned to C2 and then so this is the
how the initial group gring is done this
is part of C1 and this is part of C2
then the next step is to calculate the
actual centroid of this data because
remember C1 and C2 are not the centroids
they've been randomly assigned points
and only thing that has been done was
the data points which are closest to
them have been assigned to them but now
in this step the actual centroid will be
calculated which may be for each of
these data sets somewhere in the middle
so that's like the mean point that will
be calculated and the centroid will
actually be positioned or repositioned
there same with C2 so the new centroid
for this group is C2 in this new
position and C1 is in this new position
once again the distance of each of the
data points is calculated from these
centroids now remember it's not
necessary that the distance Still
Remains the or each of these data points
still remain in the same group by
recalculating the distance it may be
possible that some points get
reallocated like so you see this so this
point earlier was closer to C2 because
C2 was here but after recalculating
repositioning it is observed that this
is closer to C1 than C2 so this is the
new grouping so some points will be
reassigned and again the centroid will
be calculated and if the centroid
doesn't change so that is a ative
process iterative process and if the
centroid doesn't change once the
centroid stops changing that means the
algorithm has converged and this is our
final cluster with this as the centroid
C1 and C2 as the centroids these data
points as a part of each cluster so I
hope this helps in understanding the
whole process iterative process of K
means clustering so let's take a look at
the K means clustering algorithm let's
say we have X1 X2 X3 n number of points
as our inputs and we want to split this
into K clusters or we want to create K
clusters so the first step is to
randomly pick K points and call them
centroids they are not real centroids
because centroid is supposed to be a
center point but they are just called
centroids and we calculate the distance
of each and every input point from each
of the centroids so the distance of X1
from C1 from C2 C3 each of the distances
we calculate and then find out which
distance is the lowest and assign X1 to
that particular random centroid repeat
that process for X2 calculate its
distance from each of the centroid C1 C2
C3 up to CK and find which is the lowest
distance and assign X2 to that part
particular centroid same with X3 and so
on so that is the first round of
assignment that is done now we have K
groups because there are we have
assigned the value of K so there are K
centroids and uh so there are K groups
all these inputs have been split into K
groups however remember we picked the
centroids randomly so they are not real
centroids so now what we have to do we
have to calculate the actual centroids
for each of these groups which is like
the main position which means that the
position of the randomly selected
centroids will now change and they will
be the main positions of this newly
formed K groups and once that is done we
once again repeat this process of
calculating the distance right so this
is what we are doing as a part of step
four we repeat step two and three so so
we again calculate the distance of X1
from the centroid C1 C2 C3 and then see
which is the lowest value and assign X1
to that calculate the distance of X2
from C1 C2 C3 or whatever up to CK and
find whichever is the lowest distance
and assign X2 to that centroid and so on
in this process there may be some
reassignment X1 was probably assigned to
Cluster C2 and after doing this
calculation maybe now X1 is assigned to
C1 so that kind of reallocation may
happen so we repeat the steps two and
three till the position of the centroids
don't change or stop changing and that's
when we have convergence so let's take a
detail look at at each of these steps so
we randomly pick K cluster centers we
call them centroids because they are not
initially they are not really the
centroids so we let let us name them C1
C2 up to CK and then step two we assign
each data point to the closest Center so
what we do we calculate the distance of
each x value from each C value so the
distance between X1 C1 distance between
X1 C2 X1 C3 and then we find which is
the lowest value right that's the
minimum value we find and assign X1 to
that particular centroid then we go next
to X2 find the distance of X2 from C1 X2
from C2 X2 from C3 and so on up to CK
and then assign it to the point or to
the centroid which has the lowest value
and so on so that is Step number two in
Step number three We Now find the actual
centroid for each group so what has
happen as a part of Step number two we
now have all the points all the dat data
points grouped into K groups because we
we wanted to create K clusters right so
we have K groups each one may be having
a certain number of input values they
need not be equally distributed by the
way based on the distance we will have K
groups but remember the initial values
of the C1 C2 were not really the
centroids of this groups right we
assigned them randomly so now in step
three we actually calculate the centroid
of each group which means the original
point which we thought was the centroid
will shift to the new position which is
the actual centroid for each of these
groups okay and we again calculate the
distance so we go back to step two which
is what we calculate again the distance
of each of these points from the newly
positioned centroids and if required we
reassign
these points to the new centroids so as
I said earlier there may be a
reallocation so we now have a new set or
a new group we still have K groups but
the number of items and the actual
assignment may be different from what
was in step two here okay so that might
change then we perform step three once
again to find the new centroid of this
new group so we have again a new set of
clusters new centroids and new
assignments we repeat this step two
again once again we find and then it is
possible that after iterating through
three or four or five times the centroid
will stop moving in the sense that when
you calculate the new value of the
centroid that will be same as the
original value or there will be very
marginal change so that is when we say
conversion has occurred and that is our
final cluster that's the formation of
the final cluster all right so let's see
a couple of demos of uh K mean's
clustering we will actually see some
live Demos in uh python notebook using
python notebook but before that let's
find out what's the problem that we are
trying to solve the problem statement is
let's say Walmart wants to open a chain
of stores across the State of Florida
and and uh it wants to find the optimal
store locations now the issue here is if
they open too many stores close to each
other obviously the they will not make
profit but if they if the stores are too
far apart then they will not have enough
sales so how do they optimize this now
for an organization like Walmart which
is an e-commerce giant they already have
the addresses of their customers in
their database so they can actually use
this information or this data and use K
means clustering to find the optimal
location now before we go into the
python notebook and show you the Live
code I wanted to take you through very
quickly a summary of the code in the
slides and then we will go into the
python notebook so in this block we are
basically importing all the required
Library
like numpy matplot Leb and so on and we
are loading the data that is available
in the form of let's say the addresses
for Simplicity sake we will just take
them as some data points then the next
thing we do is quickly do a scatter plot
to see how they are related to each
other with respect to each other so in
the scatter plot we see that there are a
few distinct groups all already being
formed so you can actually get an idea
about how the cluster would look and how
many clusters what is the optimal number
of clusters and then starts the actual K
means clustering process so we will
assign each of these points to the
centroids and then check whether they
are the optimal distance which is the
shortest distance and assign each of the
points data points to the centroids and
then go through this ative process till
the whole process converges and finally
we get an output like this so we have
four distinct
clusters and um which is we can say that
this is how the population is probably
distributed across Florida State and uh
the centroids are like the location
where the store should be the optimum
location where the store should be so
that's the way we determine the best
locations for the store and that's how
we can help Walmart find the best
locations for the stores in Florida so
now let's take this into python notebook
let's see how this looks when we are
learning running the code live all right
so this is the code for K means
clustering in Jupiter notebook we have a
few examples here which we will
demonstrate how K means clustering is
used and even there is a small
implementation of C's clustering as well
okay so let's get started okay so this
block is basically importing the various
libraries that are required like M plot
lip and numpy and so on and so forth
which would be used as a part of the
code then we are going and creating
blobs which are similar to clusters now
this is a very neat feature which is
available in psychic learn make blobs is
a nice feature which creates clusters of
data sets so that's a wonderful
functionality that is readily available
for us to create some test data kind of
thing okay so that's exactly what we are
doing here we are using make blobs and
we can specify how many clusters we want
so centers we are mentioning here so it
will go ahead and so we just mentioned
four so it will go ahead and create some
test data for us and this is how it
looks as you can see visually also we
can figure out that there are four
distinct classes or clusters in this
data set and that is what make blobs
actually provides now from here onwards
we will basically run the standard K
means functionality that is readily
available so we really don't have to
implement Kines itself the kin
functionality or the the function is
readily available you just need to feed
the data and we'll create the Clusters
so this is the code code for that we
import K means and then we create an
instance of K means and we specify the
value of K this ncore clusters is the
value of K remember K means in K means K
is basically the number of clusters that
you want to create and it is a integer
value so this is where we are specifying
that so we have K is equal to 4 and so
that instance is created we take that
instance and as with any other machine
learning functionality fit is what we
use the function or the method rather
fit is what we use to train the model
here there is no real training uh kind
of thing but that's the call okay so we
are calling fit and what we are doing
here we are just passing the data so X
has these values the data that has been
created right so that is what we are
passing here and uh this will go ahead
and create the Clusters and uh then we
are
using after doing uh fit We Run The
predict which basically assigns for each
of these observations which cluster it
belongs to all right so it will name the
Clusters maybe this is cluster one this
is two three and so on or will actually
start from zero cluster 0o 1 2 and three
maybe and then for each of the
observations it will assign based on
which cluster it belongs to it will
assign a value so that is stored in Yore
K means when we call predict that is
what it does and we can take a quick
look at these uh Yore K means or the
cluster numbers that have been assigned
for each observation so this is the
cluster number assigned for observation
one maybe this is for observation two
observation three and so on so we have
how many about I think 300 samples right
so all the 300 samples there are 300
values Val here each of them the cluster
number is given and the cluster number
goes from 0 to three so there are four
clusters so the numbers go from 0 1 2 3
so that's what is seen here okay now so
this was a quick example of generating
some dummy data and then clustering that
okay and this can be applied if you have
proper data you can just load it up into
X for example here and then run the C so
this is the central part of the C
clustering program example so you
basically create an instance and you
mention how many clusters you want by
specifying this parameter andore
clusters and that is also the value of K
and then pass the data to get the values
now the next section of this code is the
implementation of a k means now this is
kind of a rough implementation of the K
means algorithm so we will just walk you
through I will walk you through the code
uh at each step what it is doing and
then we will see a couple of more
examples of how K means clustering can
be used in maybe some real life examples
real life use cases all right so in this
case here what we are doing is basically
implementing K means clustering and
there is a function for a library
calculates for a given two pairs of
points it will calculate the the
distance between them and see which one
is the closest and so on so this is like
this is pretty much like what K means
does right so it calculates the distance
of each point or each data set from
predefined CID and then based on
whichever is the lowest this particular
data point is assigned to that CID so
that is basically available as a
standard function and we will be using
that here so as explained in the slides
the first step that is done in case of K
means clustering is to randomly assign
some centroids so as a first step we
randomly allocate a couple of centroids
which we call here we are calling a
centers and then we put this in a loop
and we take it through an iterative
process for each of the data points we
first find out using this function
airwise distance argument for each of
the points we find out which one which
Center or which randomly selected
centroid is the closest and accordingly
we assign that data or the data point to
that particular centroid or cluster and
once that is done for all the data
points we calculate the new centroid by
finding out the mean position with the
the center position right so we
calculate the new centroid and then we
check if the new centroid is the
coordinates or the position is the same
as the previous centroid the the
positions we will compare and if it is
the same that means the process has
converged so remember we do this process
till the centroids or the centroid
doesn't move anymore right so the
centroid gets relocated each time this
reallocation is done so the moment it
doesn't change anymore the position of
the centroid doesn't change anymore we
know that convergence has occurred so
till then so you see here this is like
an infinite Loop while true is an
infinite Loop Loop it only breaks when
the centers are the same the new center
and the old Center positions are the
same and once that is uh done we return
the centers and the labels now of course
as explained this is not a very
sophisticated and advanced
implementation very basic implementation
because one of the flaws in this is that
sometimes what happens is the centroid
the position will keep moving but in the
change will be very minor so in that
case also with that is actually
convergence right so for example the
change is
01 we can consider that as convergence
otherwise what will happen as this will
either take forever or it will be never
ending so that's a small flaw here so
that is something additional checks may
have to be added here but again as
mentioned this is not the most
sophisticated implementation this is
like a kind of a rough implementation of
the K means clustering okay so if we
execute this code this is what we get as
the output so this is the definition of
this particular function and then we
call that find underscore clusters and
we pass our data X and the number of
clusters which is four and if we run
that and plot it this is the output that
we get so this is of course each cluster
is represented by a different color so
we have a cluster in green color yellow
color and so on and so forth and these
big points here these are the centroids
this is the final position of the
centroids and as you can see visually
also this appears like a kind of a
center of all these points here right
similarly this is like the center of all
these points here and so on so this is
the example or this is an example of a
implementation of K means clustering and
uh next we will move on to see a couple
of examples of how K means clustering is
used in maybe some real life scenarios
or use cases in the next example or demo
we're going to see how we can use K's
clustering to perform color compression
we will take a couple of images so there
will be two examples and uh we will try
to use Cam's clustering to compress the
colors this is a common situation in
image processing when you have an image
with millions of uh colors but then you
cannot render it on some devices which
may not have enough memory uh so that is
the scenario where where something like
this can be used so before again we go
into the python notebook let's take a
look at quickly the the code as usual we
import the libraries and then we import
the image and uh then we will flatten it
so the reshaping is basically we have
the image information is stored in the
form of pixels and if the images like
for example phot 27 by 640 and it has
three colors so that's the overall
dimension of the of the initial image we
just reshape it and um then feed this to
our algorithm and this will then create
clusters of only 16 clusters so this
this colors there are millions of colors
and now we need to bring it down to 16
colors so we use K is equal to 16 and
and um this is how when we visualize
this is how it looks there are these are
all about 16 million possible colors the
input color space as 16 million possible
colors and we just some compress it to
16 colors so this is how it would look
when we compress it to 16 colors and
this is how the original image looks and
after compression to 16 colors this is
how the new image looks as you can see
there is not a lot of
information that has been lost though
the image quality is definitely reduced
a little bit so this is an example which
we are going to now see in Python
notebook let's go into the python
notebook and once again as always we
will import some libraries and load this
image called flower.jpg okay so let we
load that and this is how it looks this
is the original image image which has I
think 16 million colors and uh this is
the shape of this image which is
basically what is the shape is nothing
but the overall size right so this is
427 pixel by 640 pixel and then there
are three layers which is this three
basically is for RGB which is red green
blue so color image will have that right
so that is the shape of this now what we
need to do is data let's take a look at
how data is looking so let me just
create create a new cell and show you
what is in data basically we have
captured this
information so data is what let me just
show you
here all right so let's take a look at
China what are the values in China and
uh if we see here this is how the data
is stored this is nothing but the pixel
values okay so this is like a matrix and
each one has about for for this 427 by
640 pixels all right so this is how it
looks now the issue here is these values
are large the numbers are a large so we
need to normalize them to between zero
and one right so that's why we will
basically create one more variable which
is data which will contain the values
between zero and one and the way to do
that is divide by 255 so we divide China
by 255 and we get the new values in data
so let's just run this piece of code and
this is the shape so we now have also
yeah what we have done is we changed
using reshape we converted into the
three-dimensional into a two-dimensional
data set and let us also take a look at
how let me just
insert probably a cell here and take a
look at how data is looking all right so
this is how data is looking and now you
see this is the values are between zero
and one right so if you earlier noticed
in case of china the values were large
numbers now everything is between zero
and one this is one of the things we
need to do all right so after that the
next thing that we need to do is to
visualize this and uh we can take random
set of maybe 10,000 points and plot it
and check and see how this looks so let
us just plot this and so this is how the
original the color the pixel
distribution is these are two plots one
is red against Green and another is red
against Blue and this is the original
distribution of the color so then what
we will do is we will use C's clustering
to create just 16 clusters for the
various colors and then apply that to
the image now what will happen is since
the data is large because there are
millions of colors using regular K means
maybe a little time consuming so there
is another version of Kes which is
called mini batch kin so we will use
that which is which processes in the
overall concept Remains the Same but
this basically processes it in smaller
batches that's the only thing okay so
the results will pretty much be the same
so let's go ahead and execute this piece
of code and also visualize this so that
we can see that there are this this is
how the 16 colors uh would look so this
is red against Green and this is red
against Blue there is uh quite a bit of
similarity between this orig
color schema and the new one right so it
doesn't look very very completely
different or anything like that now we
apply this the newly created colors to
the image and uh we can take a look how
this is uh looking now we can compare
both the images so this is our original
image and this is our new image so as
you can see there is not a lot of
information that has been lost uh it
pretty much looks like the original
image yes we can see that for example
here there is a little bit uh it appears
a little dullish compared to this one
right because uh we kind of took off
some of the finer details of the color
but overall the high level information
has been maintained at the same time the
main advantage is that now this can be
this is an image which can be rendered
on a device which may not be that very
sophisticated now let's take one more
example with a different image in the
second example we will take an image of
the suop palace in China and we repeat
the same process this is a high
definition color image with millions of
colors and also uh
three-dimensional uh now we will reduce
that to 16 colors using K means
clustering and um we do the same process
like before we reshape it and then we
cluster the colors to 16 and then we
render the image once again and we will
see that the color the quality of the
image slightly deteriorates as you can
see here this has much finer details in
this which are probably missing here but
then that's the compromise because there
are some devices which may not be able
to handle this kind of high density
images so let's run this code in Python
notebook all right so let's apply the
same technique for another picture which
is uh even more intricate and has
probably much complicated color schema
so this is the image now once again uh
we can take a look at the shape which is
427 by 640 by 3 and this is the new data
would look somewhat like this compared
to the flower image so we have some new
values here and we will also bring this
as you can see the numbers are much big
so we will much bigger so we will now
have to uh scale them down to values
between 0er and one and that is done by
dividing by 255 so let's go ahead and uh
do that and reshape it okay so we get a
two dimensional Matrix and uh we will
then as a next step we will go ahead and
visualize this how it looks the the 16
colors and this is basically how it
would look 16 million colors and and now
we can create the Clusters out of this
the 16 cin clusters we will create so
this is how the distribution of the
pixels would look with 16 colors and
then we go ahead and uh apply this and
visualize how it is looking for with the
with the new just the 16 color so once
again as you can see this looks much
richer in color but at the same time and
this probably doesn't have as we can see
it doesn't look as rich as this one but
nevertheless the information is not lost
the shape and all that stuff and this
can be also rendered on a slightly a
device which is probably not that
sophisticated okay so that's pretty much
it so we have seen two examples of how
color compression can be done uh using
kin's clustering and we have also seen
in the previous examples of how to
implement K means the code to roughly
how to implement K means clustering and
we use some sample data using blob to
just execute the C's clustering if you
are an aspiring machine learning
engineer looking for online training and
certifications from prestigious
universities and in collaboration with
leading experts then search no more
simply learns professional certification
program in Ai and machine learning from
Pur University in collaboration with IVM
should be your right choice for more
details use the link in the description
box below with that in mind so let's go
a little deeper into higher archal
clustering let's consider we have a set
of cars and we have a group similar we
want to group similar ones together so
below we have you'll see four different
cars down there and uh we get two
clusters of car types sedan and SUV so
if you're just looking at it you can
probably think oh yeah we'll put the
sedans together and the SUVs together
and then at last we can group everything
into one cluster so we just have just
cars so you can see as we have this we
make a nice little tree this is very
common when you see anybody talks about
hierarchial clustering this is usually
what you see and what comes out of it we
terminate when we are left with only one
cluster so we have as you can see we
bring them all together we have one
cluster we can't bring it together
anymore because they're all together
hierarchial clustering is separating
data into different groups based on some
measure of similarity so we have to find
a way to measure what makes them alike
what makes some different AG glomera
clustering is known as a bottom up
approach remember I said think of that
as bringing things together so you see I
think the Latin term aglo is together
because you have your glomerate rocks
where all the different pieces of rocks
are in there so we want to bring
everything together that's a bottom up
and then divisive is we're going to go
from the top down so we take one huge
cluster and we start dividing it up into
two clusters into three four five and so
on digging even deeper into how hierarch
clustering Works let's consider we have
a few points on a plane so this plane is
2D so we have an XY coordinates kind of
makes it easy we're going to start with
measure the distance so we want to
figure a way to compute the distance
between each point each data point is a
cluster of its own remember if we're
going from the bottom up a glomera then
we have each point being its own cluster
we try to find the least distance
between two data points to form a
cluster and then once we find those with
the least distance between them we start
grouping them together so we start
forming clusters of multiple points this
is represented in a tree like structure
called dendogram so there's another key
word dendogram and you can see it is
it's just the branch we've looked at
before and we do the second group the
same so it gets its own uh dendogram and
the third gets its own dendogram and
then we might group two groups together
so now those two groups are all under
one dendogram because they're closer
together than the P1 and P2 and and then
we terminate when we are left with one
cluster so we finally bring it all
together you can see on the right how
we've come up all the way up to the top
whoops and we have the gray hierarchial
box coming in there and connecting them
so we have just one cluster and that's a
good place to terminate because there is
no way we can bring them together any
further so how do we measure the
distance between the data points I mean
this is really where it starts getting
interesting up until now you can kind of
eyeball and say hey these look together
but when you have thousands of data
points how are we going to measure those
distances and there is a lot of ways to
get the distance measure so let's go ah
a and take a look at that distance
measures will determine the similarity
between two elements and it will
influence the shape of the Clusters and
we have ukian distance measure we have
squared ukian distance measure which is
almost the same thing but with less
computations and we have the Manhattan
distance measure which will give you
slightly different results and we have
the cosine distance measure which again
is very similar to the ukian playing
with triangles and sometimes it compute
faster depending on what kind of data
you're looking at so let's start with
eidan distance measure the most common
is we want to know the distance between
the two points so if we have Point p and
point Q the idian distance is the
ordinary straight line it is the
distance between the two points in ukian
space and you should recognize D equals
in this case we're going to sum all the
points so if there was more than one
point we could figure out the distance
to the not more than one points this is
the sum of more than two Dimensions so
we can can have the distance between
each of the different dimensions squared
and that will give us and then take the
square root of that and that gives us
the actual distance between them and
this should look familiar from ukian
geometry maybe you haven't played too
much with multiple Dimensions so the
summation sybol might not look familiar
to you but it's pretty straightforward
as you add the distance between each of
the two different points squared so if
your y difference was 2 - 1^ squared
would be two and then you take the
difference between the X again squared
and if there was a z coordinates it
would be uh you know Z1 minus Z2 squared
and then take the square root of that
and sum it all or sum it all together
and then take the square root of it so
to make it compute faster since the
difference in distances whether one is
farther apart or closer together than
the other we can do What's called the
squared ukian distance measurement this
is identical to the ukian measurement
but we don't take the square root at the
end there's no reason to it certainly
gives us the exact distance but as far
as doing calculation as to which one's
bigger or smaller than the other one it
won't make a difference so we'll just go
with the so we just get rid of that div
that final square root computes faster
and it gives us the pretty much ukian
squared distance on there now the
Manhattan distance measurement is a
simple sum of horizontal and vertical
components or the distance between two
points measured along axes at right
angles now this is different because
you're not looking at the direct line
between them and in certain cases the
the individual distances measured will
give you a better result now generally
that's not true most times you go with
ukian squared method because that's very
fast and easy to see but the Manhattan
distance is you measure just the Y value
and you take the absolute value of it
and you measure just the X difference
you take the absolute value of it and
just the Z and if you had more you know
different dimensions in there a b c d EF
however many dimensions you would just
take the absolute value of the
difference of those dimensions and then
then we have the cosine distance
similarity measures the angle between
the two vectors and as you can see as
the two vectors get further and further
apart the cosine distance gets larger so
it's another way to measure the distance
very similar to the ukan so you're still
looking at the same kind of measurement
so it should have a similar result as
the first two but keep in mind the
Manhattan will have a very different
result and you can end up with a bias
with the Manhattan if your data is very
skewed if one set of values is very
large and another set of values is very
small but that's a little bit beyond the
scope of this it's just important to
know that about the Manhattan distance
so let's dig into the agglomerative
clustering and agglomerative clustering
begins with each element as a separate
cluster and then we merge them into a
larger cluster how do we represent a
cluster of more than one point so we're
going to kind of mix the distance
together with the actual gomera and see
what that looks like and we're actually
going to have three key questions s that
are going to be answered here so how do
we represent a cluster of more than one
point so we want to look at the math
what it looks like mathematically and
geometrically how do we determine The
Nearness of clusters when to stop
combining clusters always important to
have your computer script or your
whatever you're working on have a
termination point so it's not going on
eternally we've all done that if you do
any kind of computer programming or or
writing of script let's assume that we
have six data points in a ukian space so
again we're dualing with X Y and Z in
this case just X and Y so how do we
represent a cluster of more than one
point let's take a look at that and
first we're going to make use of
centroids very common terminology in a
lot of machine learning languages when
we're grouping things together so we're
going to make use of centroid which is
the average of its points and you can
see here we're going to take the one two
and the 21 and we're going to group them
together because they're close and if we
were looking at all the points we'd look
for those that are closest and start
with those and we're going to take those
two we're going to compute a point in
the middle and we'll give that point
five 1.5 1.5 and that's going to be the
centroid of those two points and next we
start measuring like another group of
points we got 41 5 0 when they're pretty
close together so we'll go ahead and set
up a centroid of those two points in
this case it would be the
4.5 and 05 would be the measurements on
those two points and once we have the
centroid of the two groups we find out
that the next closest point to a
centroid is over on the left and so
we're going to take this and say oh 0 0
is closest to the 1.5 1.5 centroid so
let's go ahead and group that together
and we compute a new centroid based on
those three points so now we have a
centroid of 1.1 or one comma 1 and then
we also do this again with the last
point the 53 and it computes into the
first group and you can see our dagram
on the right is growing so we have each
of these points are become connected and
we start grouping them together and
finally we get a centroid of that group
too and then finally the last thing we
want to do is combine the two groups by
their centroids and you can see here we
end up with one large group and it'll
have its own centroid although usually
they don't compute the last centroid we
just put them all together so when do we
stop combining clusters well hopefully
it's pretty obvious to you in this case
when they all got to be one but there
are actually many approaches to it so
first pick a number of clusters K up
front and this is done in the fact that
we don't want to look at 200 in clusters
we only want to look at the top five
clusters or something like that so we
decide the number of clusters required
in the beginning and we terminate when
we reach the value K so if you looked
back on our clustering let me just go
back a couple screens you'll see how we
clustered these all together and we
might want just the two clusters and so
we look at just the top two or maybe we
only want three clusters and so we would
compute which which one of these has a
wider spread to it or something like
that there's other computations to know
how to connect them and we'll look at
that in just a minute but to note that
when we pick the K value we want to
limit the information that's coming in
so that can be very important especially
if you're feeding it into another
algorithm that requires three values or
you set it to four values and you need
to know that value coming in so we might
take the clustering and say okay only uh
three clusters that's all we want for K
so the possible challenges this only
makes sense when we know the data well
so when you're clustering with K
clusters you might already know that
domain and know that that makes sense
but if you're exploring brand new data
you might have no idea how many clusters
you really need to explore that data
with let's consider the value of K to be
two so in this case in our previous
example we stop and we are left with two
clusters and you can see here that this
is where they came together the best
while still keeping separate the data
the second approach is stop when the
next merge would create a cluster with
low cohesion so we keep clustering till
the next merge of clusters creates a bad
cluster low cohesion setup on there that
means the point is so close to being
between two clusters it doesn't make
sense to bring them together but how is
cohesion defined oh let's dig a little
deeper into cohesion the diameter of a
cluster so we're looking at the actual
diameter of our cluster and the diameter
is the maximum distance between any pair
of points in the cluster we terminate
when the diameter of a new cluster
exceeds the threshold so as that d dier
gets bigger and bigger we don't want the
two circles or clusters to overlap and
we have radius of a cluster radius is
the maximum distance of a point from
centroid we terminate when the radius of
a new cluster exceeds the threshold
again we're not we don't want things to
overlap so when it crosses that
threshold and is overlapping with other
data we stop so let's look at divisive
clustering remember we went from the
bottom up now we want to go from the top
down divisive clustering approach begins
with a whole set and proceeds to divide
it into smaller clusters so we start
with a single cluster composed of all
the data points we split it into
different clusters this can be done
using monothetic divisive methods what
is monothetic divisive method and we'll
go backwards and let's consider the
example we took in the agglomerative
clustering to understand this so we
consider a space with six points in it
just like we did before same points we
had before and we name each point in the
cluster so we have in this case we just
gave it a letter value a b CDE EF since
we follow top down approach in divisive
clustering obtain all possible splits
into two columns so we want to know
where you could split it here and we
could do like a Ab split and a cdef
split we could do BCE ADF and you can
see this starts generating a huge amount
of data
ABCDEF and so for each split we can
compute cluster sum of squares and we
can see here the actually have the
formul out for us B J12 = N1 of absolute
value of xus absolute value of x^ SAR so
again we're Computing all the different
distances and they're squared back to
your kind of ukian distances on that and
so we can actually compute B AJ between
clusters 1 and two and we have the mean
of the cluster and the grand mean
depending on number of members in the
cluster and we select the cluster with
the largest sum of squares let's assume
that the sum of squared distance is
largest for the third split we have up
above and that's where we split the ABC
out and if we split the ABC out we're
left with the DF on the other side we
again find the sum of squared distances
and split it into clusters so we go from
ABC we might find that the a splits into
b c and d into EF and again you start to
see that hierarchial dendogram coming
down as we start splitting everything
apart and finally we might have a splits
in b and c and then each one gets their
own DF and it continues to divide until
we get get little nodes at the end and
every data has its own point or until we
get to K if we have set a k value so
we've kind of learned a little bit about
the background and some of the math in
hierarchial clustering let's go ahead
and dive into a demo and our demo today
is going to be for the problem statement
we're going to look at us oil so a US
oil organization needs to know it cells
in various States in us and cluster of
the states based on the sales so what
are the steps involved in setting this
problem up so the steps we're going to
look at and this is really useful for
just about any processing of data
although I believe we're going to be
doing this in R today we're going to
import the data set so we'll explore our
data a little bit there create a scatter
plot it's always good to have a visual
if you can once you have a visual you
can know if you're really far off in the
model you choose to Cluster the data in
or how many splits you need and then
we're going to normalize the data so
we're going to fix the data up so it
processes correctly we'll talk more
detail about normalization when we get
there and then calculate the ukian
distance and finally we'll create our
dendogram so it looks nice and pretty
and we have something we can show to our
shareholders so that they have something
to go on and know why they gave us all
that money and salary for the year so we
go ahead and open up R and we're
actually using R Studio which is the
really has some nice features in it it
automatically sets up the three Windows
where we have our script file on the
upper left and then we can execute that
script and it'll come down and put it
into the console bottom left and execute
it and then we have our plots off to the
right and I've got it zoomed in
hopefully not too large a font but large
enough that you can see it and let's
just go ahead and take a look at some of
the script going in
here it's clustering analysis and we're
going to work we'll call it my data and
we're going to assign it in R this is a
symbol for assigning and we're going to
go read CSV
read CSV
file and we'll put that in Brackets and
let's before we go any further let's
just look at the data outside of R it's
always nice to do if you
can and the file is going to be called
utilities. CSV this would also be the
time to get the full path so you have
the right path to your file and remember
that you can always Post in the comments
down
below and when you post down there just
let us know you want to connect up with
simply learn so that they can get you
this file so you can get the same file
we're working on and you can repeat it
and see how this works this is
utilities. CSV it's a comma separated
variable file so it's pretty
straightforward and you can see here
they have the city fixed charge and a
number of different features to the data
and so we have our Ro cost load demand
cells nuclear fuel cost on here and then
going down the other side we have US
cities Arizona Boston they have Central
us so I guess they're grouping a number
of areas together the Commonwealth area
you can see down here Nevada New England
Northern us Oklahoma the Pacific region
and so on so we have a nice little chart
of different data they brought
in and so I'm going to take that
complete path that ends in the
utilities.
CSV and we're going to import that file
let me just enlarge this all the way
oops I had an extra set of brackets here
somehow maybe I missed a set of brackets
this can happen if you're not careful
you can get brackets on one side and not
the other or in this case I got double
brackets on each side there we go and
then the magic hot keys in this case are
your controll enter which will let me go
ahead and run the script and so I've now
loaded the data and as you can see I
went ahead and shrunk the plot since
we're going to be looking at the window
down below and we can simply convert the
data to a string now all of us do this
automatically the first time we say hey
just print it all out as a string and
then we get this huge mess of stuff that
doesn't make a whole lot of sense so
when you can see here they have uh you
can probably kind of pull it together as
looking at it but let's go ahead and
just do the head I we'll do the head of
my
data there we go and control enter on
that and the head shows the first five
rows you'll see this in a lot of
different scripts in R it's you type in
head and then in Brackets you put your
data and it comes through and list the
first five rows as you can see below and
it shows Arizona Boston Central and it
has the same columns we just looked at
so we have the fixed charge the RO the
cost the load the D demand I'm not an
expert in oil so I'm not even sure what
D demand is cells I'm guessing nuclear
how much of it supplied by nuclear and
the actual fuel
cost and then the different states at
It's s or different areas is and one of
the wonders of R is all these cool easy
to use tools that are so quick so we'll
do
Pairs and pairs creates a nice graph so
let me go ahead and run
this uh whoops the reason it gave me an
error is because I forgot to resize it
so let me bring my plot way out so we
can see it and let's run that again and
you'll see here that we have a nice
graph of the different data and how it
plots together how the different points
kind of come
together this is neat because if you
look at this I would look at this and
say hey this is a great candidate for
some kind of clustering and the reason
is is when I look at any two pairs let's
go down to say cells and fuel cost
towards the bottom right and when you
look at them where they cross over you
sort of see things how they group
together but it's a little confusing you
can't really pinpoint how how they group
together you could probably look at
these two and say yeah there's pretty
good commonalities there and if you look
at any of the other pairs you'll start
seeing some patterns there also and so
we really want to know what are the
patterns on all of them put together not
just any two of them but the whole setup
let me go ahead and Shrink my um this
down for just a second just a notch here
and let's create a uh scatter plot oops
and this is simply just use the term
plot and brackets and then which values
do we want to plot and if we remember
when we looked at the data earlier let
me just go back this
way in this case let's go ahead and
compare two just two values to see how
they
look and we'll do fuel cost and cells
and it's in my data so we got to let it
know which two columns we're looking at
next to each other and it will open up
our plot thing and then go ahead and
execute that and we can see on those
closeup of what we were just looking at
in the
pairs and if I was eyeballing this I
would say oh look there's a kind of a
cluster up here of five items and this
one it's hard to tell which cluster it'd
be with but maybe it's six you go in the
top one and you have a middle cluster
and a bottom cluster maybe two different
clusters so you can sort of group them
together fuel cost and the cells and see
how they connect with each other and
again that's only two different values
we're looking at so in the long run we
want to look at all of them and then the
people in the back they sent me the
script so we can go ahead and add labels
so with my data text fuel cost cells the
labels equals City position four these
are numbers you can kind of play with
till they look nice and oops again I
forgot to resize my plot it doesn't like
having it too small we'll run
that I mistype something in
here oh I did a lowercase D and they did
a capital D there we go so now we can go
in here and do this with my data and you
can see little hard to see on my screen
with all the the different things in
there it plots the actual cities so we
can now see where all the cities are in
connection with in this case fuel cost
and cells so you have a nice label to go
with the
graph and then we can also go ahead and
plot in this case let's do oh the r o r
oops and we'll do that
also with the cells and do my
data remember to leave it lowercase this
time so when we plot those two it'll
come over here and it's going to I'm
surprised it didn't give me an
error and then we'll also add in the U
with statement so we put some nice
labels in
there and it's going to be the same as
before except instead of doing the fuel
cost cells we want the r cells so we'll
execute
that oops and of course it gives me an
error because I shrunk it down so let's
redo those two
again there we go and we can see now we
have the RO with cells and we'd probably
get a slightly different clustering here
if I was looking at these cities they're
probably different than what we had
before but you could probably look at
this and say h these kind of go together
and those kind of go together but again
we're going to be looking at all of them
instead of just one or
two
and so at this point we want to dive
into the next step we're going to start
looking at a little bit of coding or
scripting
here this is very important because
we're going to be looking at
normalization we put that in there
normalization and if you've done any of
the other machine learning skills and
setup this should start to look very
normal in your pre-processing of data
whether you're in r or python or any of
these scripts we really want want to
make sure you normalize your data so
it's not biased remember we're dealing
with distances and if I have let's say
the r is even look at this graph here on
the right you can see where my RO varies
between 8 and 14 that's a very small
variable and our cells varies between
4,000 and
16,000 so you can imagine the distance
between 4,000 and 8,000 which is the
distance of 4,000 versus 8 to 10 versus
two the cells is going to dominate so if
we do any kind of special work on this
it's going to look at cells and it's
going to Cluster them just by the cells
alone and then Ro might have a little
tiny effect of two versus 4,000 we want
to level the playing
field turns out there's actually a
number of ways in script to
normalize so I'm just going to put in
the code that they put together in the
back for me and let's talk about it a
little bit so we have Z we're going to
assign it to my
data and let's go ahead
and we're going to do a little reshaping
across all
rows or I mean across all columns so
each of the rows is going to have a
little reshaping there and then we're
going to get M which stands for means
and we're going to apply it to my data
so again we want to go ahead and create
a um the most common variable in
there
and then s is going to be SD stands for
standard
deviation so instead of just doing a lot
of times what they do with normalization
of data is we just reshape the data
everything between zero and one so that
if the lower end is eight that now
becomes zero and the upper end is 14
that now becomes
one that doesn't help if it is not a
linear set of data so with this we're
going to look for the means and the
standard deviation for reshaping the
data and that way the most common values
now become the kind of like the center
point and then the standard deviation is
how big the spread so we want the
standard deviation to be equal amongst
all of them and then finally we go ahead
and take Z and with the Z we're going to
reassign it and we're going to scale the
original my data which we
re-shaped based on M and based on the
standard
deviation and the two in here that just
means we're looking at everything in
kind of a XY kind of
plot and we can quickly run these contrl
enter contrl enter contrl enter contrl
enter so now we have Z which is a scaled
version of my
data and now we can go ahead and
calculate the ukan
distance oops C
you there we
go
and in um R this is so easy once you've
gotten to here we've done all that pre
dat
processing we'll call it
distance and we'll assign this
to dist so
di is the computation for getting the
ukian distance and we can just put Z in
there because we've already reformatted
and scaled Z to fit what we want let me
go ahead and just hit enter on that and
I'm going to widen my left- hand side
again I'm always curious what does this
data look like so let's just type in
distance which will print the variable
down below oops you have to hit control
enter and this prints out a huge amount
of
information as you can see it just kind
of streams down
there and let's go ahead and enlarge
this and I don't know about you but when
I look look at something like this it
doesn't mean a whole lot to me other
than I see 2 3 4 5 six and then you kind
of have the top part 6 17 18 so I
imagine this is like a huge chart is
what we're looking
at and we can go ahead and use print oh
distance
[Applause]
digits equal three and let's run
that oops I keep forgetting that it has
to go through the graph on the right and
we see a different slightly different
output in here let me just open this up
so we can see what we're looking at and
by cutting down the distance you can
start to see the patterns here of it's
looking at the different
distances so if I go to the top we have
the distance between one and two one and
three one and four one and five one and
six and then two and three so on
obviously distance between itself is
zero and it doesn't repeat the data so
we don't care to see two versus one
again we already know the distance
between one and
two and so we have a nice view of all
the distances in the chart and that's
what we're looking at right here and
it's a little easier to read that's why
we did the print statement up here to do
digits equals three make it a little bit
smaller we could even just do digits oh
let's just do two see what that looks
like we might lose some data on this one
if it's uh if something's way off but we
have a nice setup and we can see the
different distances and that's what we
were computed here between each of the
points
and then the whole reason we're doing
this is to get ourselves a nice dagram
going a nice clustering dagram we'll do
a couple of these looking at different
things we'll take a variable HC DOL and
we're going to assign
it h
cluster and then
distance that easy we've already
computed the distances so the H
clustering does all the work for us and
let me hit enter on there so now we have
our HCL which is assign the H clustering
computation based on distances and a
part and then I'm going to expand my
graph cuz we would like to go ahead and
see what this looks like and we can
simply plot
that and hit the control enter so it
runs and look at that we have a really
nice clustering dendogram except when I
look at it the first thing I notice is
it really shows like numbers down below
now if you were a shareholder and some
need scientist came up to you and said
look at this this is what it means you'd
be looking at that going what the heck
does 394 19118 mean so let's give it
some words
there so let's do the same plot with our
HCL HC there we
go and let's add in labels and this is
just one of the commands and plots so we
have labels equals my
data and then and under my data we want
to know the city and we'll have it hang
minus one that's just the instructions
to make it pretty so we'll run
that oops I accidentally ran just hang
my one let me try that again there we go
okay so now you can see what it's done
and the and the hang my one turns it
sideways that way we can see uh Central
which is Central us and Kentucky and we
start to actually get some information
off our clustering setup and the
information you start looking at is that
we put all the information together you
probably want to look at Central America
and Kentucky together Oklahoma and Texas
has a lot of commonality as does Arizona
and Southern us and you could even group
all five of those Florida Oklahoma Texas
Arizona and Southern us these regions
for some reason share a lot of
similarities and so we want to start
asking what those similarities are but
this gives us a place to look it says
hey these things really go together you
should be grouping these together as far
as your cells and what's what you're
doing and then one of the things you
might want to do is there's also we can
do the dagram average and this changes
how we do the
clustering so it looks very similar like
we had before there's our HCL we're
going to assign it we're going to do an
H cluster we're still doing on distance
oops distance and this time we're going
to set the method to average so we can
change the methodology in which it
computes the values and before if you
remember correctly we did median median
a little bit different than means we did
the most common one and then we want the
average of the median and let's go ahead
and run
that and then we can
plot and here's our
HCL
oops there we go here's our HCL and I
can run that plot and you can see this
changed a little bit so our way it
computes and groups things looks a
little different than it did before and
let's go and put the cities back in
there and do the Hang control copy let
me just real quickly copy that down
here because we want the same
labels and again you can see Nevada
Idaho puette I remember we were looking
at um Southern Us in Arizona Texas and
Oklahoma Florida so the grouping really
hasn't changed too much so we still have
a very similar grouping it's almost kind
of flipped it as far as the uh distance
based on average has but this is
something you could actually take to the
shareholders and say hey look these
things are connected and at which point
you want to explore a little deeper as
to why they're connected because they're
going to ask you okay how are they
connected and why do we
care but that's a little bit beyond the
actual scope of what we're working
working on
today but we are going to cover
membership What's called the clustering
membership on
there and let's create a member we'll
just call it Member One oops there
member. one and we're going to assign to
this we're going to do cut
tree and cut
tree it limits it so what that means is
I take my uh HC
dot l in
here oops there we
gol and so I'm taking the cluster I just
built and we want to take that cluster
and we want to limit it to just a depth
of three so we go ahead and do that and
run that one oops let me go run there we
go so now I've created my member
one and then whoops let me just move
this out of the way we're going to do an
Aggregate and we're going to we going to
use Z remember Z from above and we're
going to turn Member One into a list and
then we're going to aggregate that
together based on the mean let me go
ahead and hit enter run
that I did member L it's actually Member
One there we
go and if we take a look at this we now
have our group one fixed charge and then
all your
different columns listed there and most
of them should come up kind of looking
between zero and one but you'll see a
lot of variation because we're varying
it based on the means so it's a means
and standard deviation not just forcing
it in between zero and one which is a
much better way usually to normalize
your data than just doing the 01
setup and finally we can actually uh
look at the actual
values and the same chart we just did
oops I made a mistake on there with my
data there we go
okay and again we now have our actual
data instead of looking at just the if
you looked up here it's all between zero
and one and when we look down here we
know have some actual connections and
how far distance this different data is
uh again because more of a domain issue
in understanding the oil company and
what these different values means and
you can look at these as being the
distances between different items so a
little bit different View and you have
to really dig deep into this data what
we really want you to take away from
this is the dagram and the charts that
we did
earlier and that is the cluster output
and our nice dagram so this would be
stage one in data analysis of the cells
again you'd have to have a lot more
domain experience to find out what all
the individual numbers we looked at mean
and what the distance is and what's
difference between Central America and
Kentucky and why they're similar and why
it groups all of Central Kentucky
Florida Oklahoma Texas Arizona and
Southern us together into one larger
group so I'd be Way Beyond the scope of
this but you can see how we start to
explore data and we start to see things
in here where things are grouped
together in ways we might not have seen
before and this is a good start for
understanding and giving advice for sale
and marketing maybe Logistics City
development there's all kinds of things
that kind of come together in the
hierarchial clustering as you begin to
explore data and we just want to point
out that we get three clusters of
regions with the highest cells region
with average cells region with the
lowest cells again those are some of the
things that they clustered it around and
you could actually see where things are
going on or lacking you know as this
case if you're the lowest cells no one
wants to be in the region of the lowest
celles if you are an aspiring machine
learning engineer looking for online
training and certifications from
prestigious universities and in
collaboration with leading experts then
search no more simply learn professional
certification program in Ai and machine
learning from Pur University in
collaboration with IBM should be your
right choice for more details use the
link in the description box below with
that in mind regression algorithms is
used to find out the connection between
dependent and independent variables
dependent variables are nothing but a
variable that we are trying to predict
or forecast and independent variables
are the factor that influence the
analysis it is usually used to make
projections medical researchers
frequently utilize linear regression to
study the connection between patient
blood pressure and dosage researcher may
give patient different quantities of a
specific medication and track the how
their blood pressure changes a
regression model might use doses as a
predictor variable and blood pressure as
a response
variable there are some popular
regression algorithms that come under
supervised machine learning first one is
linear regression regression trees
nonlinear regression basal linear
regression after discussing what is
supervised learning and its types let's
move forward to see what actually
regression analysis
is a St technique known as regression
analysis is used to simulate the
relationship between dependent and
independent variables using one or more
independent variables regression
analysis more precisely enables us to
comprehend and when other independent
variables are held constant changes in
the dependent variables value that
correspond to an independent variables
are changing prediction include values
that are continuous or real such as
temperature age pay and cost there are
some following concept or terminologies
that must be understood in order to
completely grasp regression analysis the
first one is dependent variable in a
regression analysis the primary variable
we wish to predict or comprehend is the
dependent variable here dependent
variables are also known as Target
variable the next one is independent
variable the term independent variables
also known as predictor refers to the
elements that impact the dependent
variables or are used to forecast their
values and the third one is outliers an
observation that is extremely high or
extremely low in relation to the other
observed values is referred as an
outlier an outlier should be avoided as
it might affect the outcomes the fourth
one is multicolinearity the situation is
set to as occuring when independent
variables have a higher correlation with
one another than other variables it
shouldn't be included in the data set
because it causes issues when
determining which variable has the
greatest impact and the fifth one is
overfitting and underfitting overfitting
is the problem that occurs whenever
system problems well well with the
training data set but poorly with the
test data set under fiting is a term
used when an algorithm does not perform
well even with the training data set
after understanding what regression
analysis is and its terminologies let's
move forward and see why we use
regression analysis including those
involving the weather sales marketing
Trends and other factors in this
situation we need regression analysis
that can make forecast more precisely so
let's understand the concept of
regression analysis using an example
business frequently use linear
regression to understand the
relationship between advertising
spending and income for instance they
might run a simple linear model with
advertising as a response variable and
revenue as a predictor variable the
regression model would look like this
the V equal to Beta 0 plus B1 add
spending where there are no advertising
the coefficient zero would indicate the
whole expected Revenue when advertising
expenditures are increased by one unit
coefficient will represent the typical
IAL change in the total revenue that is
$1 if beta 1 is negative then higher
advertising expenditure will result in
lower Revenue if if beta 1 is nearly
zero advertising expenditures have
little impact on revenue and if beta one
is positive it would imply that more
advertising expenditures are linked to
higher
income the amount of betan will
determine whether a corporation decide
to or decrease its advertising
budget
regression means creating a graph that
connects variable the best fit the given
data point the regression analysis model
can then make prediction about data
using the plot regression analysis is
defined as showing a line or a curve
that goes through all the data points on
the target predictive graph in such a
way that vertical distance between the
data points and the regression is the
smallest whether a model has captured a
strong link between determined by the
distance between data points and the
line there are some more used case of
regression analysis like we can use
temperature and other variable to
predict or forecast rain regression
analysis can be used for identification
of market trends for predicting traffic
accidents caused by reckless driving we
can use regression analysis for the same
and many more so by understanding why we
need regression analysis moving forward
let's see some popular regression
algorithms in
detail the first one is linear
regression linear regression is one of
the most famous and straightforward
machine learning algorithm utilized for
predictive analysis linear regression
show the linear connection between
dependent and independent factors the
line of the equation is y = mx + b here
y stand for the response variable or a
dependent variable whether X is for the
predictor variable or an independent
variable here m is the estimated slope
and B is for the estimated intercept and
the next one is regression trees a
regression trees is work through a cycle
known as binary recursive which is
iterative interaction that devoids the
information into segments or branches
and afterward keep splitting the data
into a smaller group as the technique
climbs each branch this tree is used for
the dependent variable which continuous
values for example a regression tree is
name food which is divided into segments
V and nonv further it keep splitting
into smaller groups and the third one is
nonlinear regression nonlinear regession
is a type of regession examination
wherein information is to fit a model
and afterward communicated with the
numerical function simple linear
regression relates two Factor X and Y
with a straight line Y = MX plus b while
nonlinear regression relates the two
factor in a nonlinear relationship
nonlinear regression can be predict
population growth over time or the
relationship between GTP and a country's
time and the fourth one is BAS and
linear regression the algorithm is a to
deal with a linear regression in which
statical examination is attempted inside
the setting of basian inference linear
regression and basian regression can
generate the same prediction and with
the help of basan processing we can
retrieve the complete variety of
inferential solution instead of a point
estimate after seeing some regression
analysis algorithm let's move forward
and see advantages and disadvantages of
different regression analysis
models and the first model is linear
regression model the advantages are work
well irrespective of data set size and
the second one is gives information
about the relevance of features and the
disadvantages are the assumptions of
linear regression and the next one is
polinomial regression the advantages are
works on any size of the DAT set and the
second one is works very well on
nonlinear problems and disadvantages are
we need to choose the right polinomial
degree for the variance stateof the next
one is support Vector regression
algorithms and the advantages are easily
adaptable works very well on nonlinear
problems and disadvantages are difficult
to understand and not well known and the
next one is decision tree regression the
advantages of decision trees are no need
to apply scaling and the second one is
works very well on both linear and
nonlinear problems and disadvantages are
poor results on a small dat set and
overfitting can easily occur and the
last one is random Forest regression the
advantages are powerful and second one
is accurate and disadvantages are no
interpretably and the second one is
overfitting can easily occur after
seeing advantages and disadvantages of
different types of regression analysis
model let's move forward and see some
applications of regression
analysis the first one is forecasting
regression analysis is frequently used
in business to predict potential
opportunity and dangers for examples
demand analysis predicts how many items
a buyer is likely to purchase demand
however is not only the dependent
variables when it comes to business much
more than just direct income can be
predicted using regression analysis and
the second one is capm capital asset
pricing model the linear regression
model is a key component of the capital
asset pricing model capm which
determines the relationship between an
asset expected return and the associated
Market risk premium financial analyst
typically use it for forecast corporate
return and operational performance when
doing financial analysis and the third
one is comparing with competition it can
be used to evaluate how well a business
is doing financially in comparison to a
certain rival it can also be used to
figure out how the stock prices of two
companies are related to one another
this can be extended to find the
correlation between two competing
companies it might help the company
identifying the effectors affecting the
sales in contrast to the comparable
company these methods can help a small
business succeed quickly within a short
period of time and the last one is
identifying problem regression is a
beneficial for identifying incorrect
judgment as well as for providing
factual support for the management for
instance a retail store manager might
believe that extending the hours of
operation will greatly increase sales if
you are an aspiring machine learning
engineer looking for online training and
certifications from prestigious
universities and in collaboration with
leading experts then search no more
simply learns professional certification
program in Ai and machine learning from
per University in collaboration with IVM
should be your right choice for more
details use the link in the description
box below with that in
mind so let's go ahead and talk about
overfitting uh when we talk about
overfitting it's a scenario where the
machine learning model tried tries to
learn from the details along with the
noise and the data tries to fit each
data point on the curve uh you can see
that
um if you plug in your coordinates
you're just going to get the whatever is
fited every point on the data stream
there's no average there's no two points
that might have the you me know y might
have two different answers because uh if
the wind blows a certain way um and your
efficiency of your car maybe you have a
headwind so your car might might alter
how efficient it is as it goes and so
there's going to be this variance on
here and this says no you can't have any
variance what's you know the this is
it's going to be exactly this it can't
be any you can't be the same speed or
the same car and have a slightly
different
efficiency so as the model has very less
flexibility it fails to predict new data
points and thus the model rejects every
new data point during the
prediction uh so you'll get like a
really high error on here
and so uh reasons for overfitting uh
data used for training is not cleaned
and contains noise garbage values in it
you can spend so much time cleaning your
data and it's so important it's so
important that if you have if you have
some kind of something wrong with the
data coming in it needs to be addressed
whether it's a source of the data maybe
they use in medical different measuring
tools uh so you now have to adjust for
data that came in from hospital a versus
Hospital b or even off of machine a and
machine B that's testing something and
those those numbers are coming in
wrong the model has a high variance uh
again wind is a good example I was
talking about that with the car you may
have 100 tests but because the wind's
blowing it's all over the
place uh size of training data used is
not enough so a small amount of data is
going to also cause this problem you
only have a few points and you try to
plot everything the model is too complex
uh this comes up a lot we put too many
pieces together and how they interact
can't even be tracked um and so you have
to go back break it up and find out
actually what correlates and what
doesn't so what is underfitting a
scenario where machine learning models
can neither
learn the relationship between the data
points nor predict or classify a new
data point and you can see here we have
our efficiency of our car and our line
drawn and it's just going to be way off
for both the training and the predicting
data as the model doesn't fully learn
the patterns it accepts every new data
point during the prediction so instead
of looking for a general pattern uh we
just kind of accept
everything data used for training is not
cleaned and contains noise garbage and
values again underfitting and
overfitting same issue you got to clean
your
data the model has a high
bias uh we've seen this in all kinds of
things
from uh the mo the most common is the
driving cars to facial identification or
whatever it is the model itself when
they build it might have a bias towards
one thing and this would be an
underfitted model would have that bias
because it's averaged it out so if you
have um five people from India and 10
people
from um Africa and 20 people from the US
you've created a bias uh because it's
looking at the 20 people and you only
have a small amount of data to work
with size of training data used is not
enough uh that goes with the size I was
just talking about so we have a model
with a high bias we have size of
training data used it's not enough the
model is too
simple again this a one straight line
through all the data when it need has a
slight shift to it for other
reasons so what is a good fit uh a
linear curve that best fits the data is
neither overfitting or underfitting
models but is just right and of course
we have the nice examples here where we
have overfitting lines going up and down
every Point starting to be include glued
underfitting um the line really is off
from where the data is and then a good
fit is got to get rid of that minimized
that um error coming through so this is
all exciting but what does this look
like so we really need to jump in and
put a code together and see what this
looks like when we're programming for
this uh demo we'll bring up our trusty
anaconda and go into Jupiter notebook
for
python move myself out of the way here
uh and so we're going to start off this
is going to be a demo on overfitting and
underfitting using
python
and let's start with our uh
Imports now if you've been through
enough of these tutorials we don't want
to spend a huge amount of time on what
we're bringing in and what we're doing
uh so you should be up on doing this
with python and how to bring in your
different modules uh we're going to
bring in the sklearn or the uh s kit
processing SK learn. neural network
import
MLP regressor so there's our regressor
model right there uh that's going to be
our linear regression model and we have
our metrics mean absolute error if you
remember we had our U that's how we
figure out how well it fits is how far
off that error is based on the um mean
square error value msse and then of
course uh numpy because we just like to
work with numpy it's a great data array
uh we always import it as MP that's the
most common way of doing it and then we
have SK learn model selection import
validation curve so we're going to look
at a validation curve to see how good
our models are and then we have the uh
data set we'll use the um very famous
Iris data set and that's embedded in the
S kit so the S key learn data sets have
a load Iris in there and then we have
the matplot library because if you're
doing any kind of demo or showing this
off to your shareholders we want to have
something nice to display it
on and then we have sklearn model
selection we're going to import import K
fold and we'll talk about about that
when we get to it uh and then we're
going to go ahead and do um just for our
numpy we're going to do like a random
seed for random numbers and then for our
plot style where you use the GG plot
that's just some backend setup um you
could even probably leave the plot style
out uh depending on what version of um
you're using of um depending on what
version you're using a map plot
library and then we'll go ahead and run
this uh it's not going to do anything
that we can visibly see cuz it's just
loading those modules and then we also
want to load our Iris uh data in here
and the iris data has an iris data and
Iris Target uh we're going to load that
as X and
Y and just so you can have an idea what
we're talking about we're going to go
ahead and
print um X and just the first bit of X
we'll just do the top of X and we'll
also print y so you can see what the top
of Y looks like uh print
y
uh and since we're in numpy we're going
to go ahead and do our own thing if this
was uh of course some pandas we could
just do the head of it and see what it
looks like and you can see here we've
loaded this up and in X we have these
different measurements that they take of
the flower the iris flowers from this
particular data set and what kind of
flower it is it's going to be a zero a
one or a two is actually what the target
comes out of even though that doesn't
show in
here and so we're going to come in here
and we're going to use the k folds cross
validation with 20
folds um and a good catch that this was
a model selection we're we're we're
going through and we're selecting
different parts of the data in here here
we use K folds cross validation with 20
folds k equal 20 to evaluate the
generalization efficiency of the model
within each fold we will then estimate
the training and test error using the
training and test sets
respectfully
so here we have our KF equals KF uh
kfold here's our splits on the top then
we need to go ahead and have our list
training error we're going to create an
array for that we're going to list our
testing error and for train index and
test index in KF dosit X XT train y uh
XT train and X test we're going to go
ahead and split up our our data our X
values and the same thing with the Y
Valu so now we have an X train and an X
test a y train and a y
test and then here's our model our m m
uh MLP
regressor that's your linear regressor
model in there and we have used a
multi-layer patron MLP so this is a
neural network uh multi-layer Patron
that's what the MLP is for regressor
means that it is dealing with numbers
we're not categorizing
things
um and then let's go ahead uh kind of
went off the screen here we'll just go
ahead and bring that down it's a class
of feed forward artificial neural
networks uh they kind of loosely call
ITN don't get caught up in the Ann nnn
mnn there's NN is neural network and
then everybody puts their own flavor on
it depending on what they're doing uh so
if you see the NN you know you're
dealing with the neural
network so we go ahead and fit our data
here's our model.fit we have XT train
and Y train and the Y train data we're
going to predict equals uh model.
predict XT Trin so here's our prediction
of what it's going to be so we've
trained it and we' predicted it we've
trained our our the train data and then
we have our y train and then we have our
y test and the Y test equals a model
predict X
test now notice what we did here is
we're going to use our model to predict
what we think y should be but this is
the training Set uh so we've trained it
with this data now we want to see how
good our model fits our training data
and then we want to see how well it fits
our testing data so we take our fold
training error mean absolute error y
train y train data predict and we're
going to do our full testing error the
mean absolute error of Y test and Y test
data
predict and we do this and here's our
mean absolute error there's our um uh a
little bit different connotation but
that's that's taking the Square value um
and finding the in this case just using
the absolute value so instead of the
Square value we get rid of the minus and
pluses by using an absolute value and we
find the average of that and that works
the same way as doing a squared value
and then we take our list training error
and we're going to and just depend it
for each uh each one of these runs we go
through so every time we fold the
data think of it like this uh we want to
go ahead and take a piece data that's
going to be one piece of the data and
we're going to look
at each section and we want to go
through each section to see how well it
does and splits it up this way we have a
nice picture uh when we're looking at it
from a distance I do this a lot when I
do X and when I split my X train and my
y train I'll take 2/3 of the data and
then oneir of the data and then I'll
switch it and I'll do three different
models so I can really see how well it
tests out and how that averages out this
is the same thing but with the uh with
the K fold we're doing and we're doing
it across 20
sections we'll go ahead and run
this and when we run this it's not too
exciting because we're just loading up
the data and appending it into our
list and so we want to take with this uh
is we're going to go ahead and plot it
and this is where we can really see
what's going on this is where where it
gets exciting uh so we take it and we're
going to create a couple subplots uh
that way we have a nice nice setup down
here we're splitting it up into a couple
different uh
graphs and let's go ahead and run this
and then we'll walk through it a little
bit uh so our subplot comes in
um there's our subplot and then our PLT
plot we're going to do in there range
one uh we're going to ahead and do the
splits plus one in p list training error
uh Ravel um this is of course just a
code to how we properly set it up on
there so that it sees it it correctly
and then we have our xlabel which is our
number fold uh plot the Y Lael training
error plot the title training error
cross folds plot the tight layout plot
the subplot so we're going to move on to
this is one two one one two2 there's our
one just one and
two um it has to do with how it how it
layers it on there for doing multiple
plots because you can do all kinds of
cool things with our plot our pip plot
Library uh and again we're going to go
ahead and do the the same thing for the
error and we end up with our training
error across folds and our testing error
across
folds and so you can see these different
folds how they kind of Spike and how
they look and so we're talking about
overfitting or underfitting we're
comparing these two graphs and if one of
them is more off than the other one uh
if you're looking at these two graphs
you got to say hey is this one uh um
overfit or underfit and this is always a
good question to ask I mean what do we
got going here is that overfit or is
that underfit and I would say based on
these two graphs and the um training
data uh the training data is more
sporadic than the testing
data so I would look at this and say hey
uh this might need to be fit a little
bit better um maybe we don't have enough
data with the iris we probably don't
something else is going on here so it's
a little underfit maybe a different
model would fit better um I would not
use a neural network model for this I
would actually use just a basic linear
reg uh linear model on
this a lot of different choices uh but
this gives you an idea what we're
looking at is how chaotic are these two
is it getting better or is it getting
worse if at some point the training data
gets so much better than the testing
data you know you've overfitted and
that's where you start running into the
overfitting this to me looks like it's
underfit so that concludes uh
underfitting and overfitting if you are
an aspiring machine learning engineer
looking for online training and
certifications from PR presious
universities and in collaboration with
leading experts then search no more
simply learns professional certification
program in Ai and machine learning from
Pur University in collaboration with IVM
should be your right choice for more
details use the link in the description
box below with that in mind machine
learning has improved our lives in a
number of wonderful ways today let's
talk about some of these I'm Rahul from
Simply learn and these are the top 10
applications of machine learning first
let's talk about virtual personal
assistance Google Assistant Alexa
Cortana and Siri now we've all used one
of these at least at some point in our
lives now these help improve our lives
in a great number of ways for example
you could tell them to call someone you
could tell them to play some music you
could tell them to even schedule an
appointment so how do these things
actually work first they record whatever
you're saying send it over to a server
which is usually in a cloud decode it
with the help of machine learning and
neural networks and then provide you
with an output so if you ever noticed
that these systems don't work very well
without the internet that's because the
server couldn't be contact conted next
let's talk about traffic predictions now
say I wanted to travel from Buckingham
Palace to L's cricket ground the first
thing I would probably do is to get on
Google Maps so search
it and let's put it
here
so here we have the path you should take
to get to large cricket ground now here
the map is a combination of red yellow
and blue now the blue regions signify a
clear road that is you won't encounter
traffic there yellow indicate that
they're slightly congested and red means
they're heavily congested so let's look
at the map a different version of the
same map and here as I told you before
red means heavily congested yellow means
slow moving and blue means clear so how
exactly is Google able to tell you that
the traffic is clear slow moving or
heavily congested so this is with the
help of machine learning and with the
help of two important measures first is
the average time that's taken on
specific days at specific times on that
route the second one is the real-time
location data of vehicles from Google
Maps and with the help of sensors some
of the other popular map services are
Bing Maps maps.me and here we go next up
we have social media personalization so
say I want to buy a drone and I'm on
Amazon and I want to buy a DJI mavic Pro
the thing is it's close to one lap so I
don't want to buy it right now but the
next time I'm on Facebook I'll see an
advertisement for the product next time
I'm on YouTube I'll see an advertisement
even on Instagram I'll see an
advertisement so here with the help of
machine learning Google has understood
that I'm interested in this particular
product hence it's targeting me with
these advertisements this is also with
the help of machine learning let's talk
about email spam filtering now this is a
spam that's in my inbox now how does
Gmail know what's spam and what's not
spam so Gmail has an entire collection
of emails which have have already been
labeled as spam or not spam so after
analyzing this data Gmail is able to
find some characteristics like the word
lottery or winner from then on any new
email that comes to your inbox goes
through a few spam filters to decide
whether it's spam or not now some of the
popular spam filters that Gmail uses is
content filters header filters General
Blacklist filters and so on next we have
online fraud detection now there are
several ways that online fraud can take
place for example there's identity theft
where they steal your identity accounts
where these accounts only last for how
long the transaction takes place and
stop existing after that and man in the
middle attacks where they steal your
money while the transaction is taking
place the feed forward neural network
helps determine whether a transaction is
genuine or fraudulent so what happens
with feed forward neural networks that
the outputs are converted into hash
values and these values become the
inputs for the next round so for every
real transaction that takes place
there's a specific pattern a fraudulent
transaction would stand out because of
the significant changes that it would
cause with the hash values Stock Market
trading machine learning is used
extensively when it comes to Stock
Market trading now you have stock market
indices like nikai they use long
short-term memory neural networks now
these are used to classify process and
predict data when there are time lags of
unknown size and duration now this is
used to predict stock market trends
assisted medical technology now medical
technology has been innovated with the
help of machine learning diagnosing
diseases has been easier from which we
can create 3D models that can predict
where exactly there are lesions in the
brain it works just as well for brain
tumors and Ice kemic stroke lesions they
can also be used in fetal Imaging and
cardiac analysis now some of the medical
fields that machine learning will help
assistant is disease identification
personalized treatment drug Discovery
clinical research and radiology and
finally we have automatic translation
now say you're in a foreign country and
you see Billboards and signs that you
don't understand that's where automatic
translation comes of help now how does
automatic translation actually work the
technology behind it is the same as the
sequence to sequence learning which is
the same thing that's used with chat
Bots here the image recognition happens
using convolutional neural networks and
the text is identified using optical
character recognition furthermore the
sequence to sequence algorithm is also
used to translate the text from one
language to the other so first we will
import some major libraries of python so
here I will write
import pandas
aspd
import numpy as NP
then
import
cbor as
SNS okay then
import
skar dot model
selection
put train
underscore test underscore
split before that I will
import mat plot
Li P
plot as
PLT okay
then I will write here from
eSalon dot
matrix
import
accuracy
4 then
from
Escalon do
Matrix
import
classification to
report and import R then import
string
okay then press enter so it is
saying here I have to write
from everything seems
good loading let's
see okay till then napai is a python
Library used for working with arrays it
also has function for working with
domain of linear algebra and
matrices it is an open source project
and you can use it
freely number stand for numerical
python pandas so panda is a software
Library written for Python programming
language for data manipulation and
Analysis in particular it offers data
structure and operation for manipulating
numerical tables and time ser
series then cbon an open- Source python
Library based on M plot lib is called
cbon it is utilized for data exploration
and data visualization with data frames
and the pandas Library cbone functions
with ease then matplot lip for Python
and its numerical extension numpy
matplot lip is a crossplatform for the
data visualization and graphical
charting package as a result it presents
a strong open Source suitable for matlb
the apis for met plot lib allow
programmers to incorporate graphs into
gii applications then this train test
split we may build our training data and
the test data with the aid of escalan
train test split function this is so
because the original data set often
serves as both the training data and the
test data starting with the single data
set we divide it into two data sets to
obtain the information needed to create
a model like H and test
accuracy score the accuracy score is
used to gge the model's Effectiveness by
calculating the ratio of total true
positive to Total to negative across all
the model prediction this R regular
expression the function in the model
allow you to determine whether a given
text fits a given regular expression or
not which is known as
re okay then string a collection of
letters words or other character is
called a string it is one of the basic
data structure that serves as the
foundation of manipulating data the Str
Str class is a built-in string class in
Python because python strings are
immutable they cannot be modified after
they have been
formed okay so now let's import the data
set we will be going to import two data
set one for the fake news and one for
the True News or you can say not fake
news okay so I will write here
efcore P =
to PD
do read uncore
CSV or what can I say
DF
okay score
fake
okay
then fake do CSV you can download this
data set from the description box below
then data dot true equals to pd.
read
CSV sorry
CSC then fake news sorry true true.
CSV okay then press
enter
so these are the two data set you can
download these data set from the
description box below so let's see the
board data set okay then I will write
here
dataor
fake do
head so this is the fake data okay
then data underscore true
Dot and this is the true
data okay this is not fake so if you
want to see your top five rows of the
particular data set you can use head and
if you want to see the last five rows of
the data set you can use tail instead of
head
okay so let me give some space for the
better
visual so now we will insert column
class as a Target feature okay then I
will write here
dataor
fake
plus equals to
0 then data underscore
true and
plus = to
1 okay
okay
then I will write here data underscore
fake dot shape and data underscore
true do
shape okay then press
enter so the shape method return the
shape of an array the shape is a tuple
of integers these number represent the
length of the corresponding oning array
dimension in other words a tle
containing the quantities of entries on
each axis is an array shape Dimension so
what's the meaning of
shape in the fake word in this data set
we have 2 3 4 8 1 rows and five columns
and in this data set true we have 2 1 41
7 rows and five column okay so these are
the rows column rows column for the
particular data
set so now let's move and let's remove
the last 10 rows for the manual testing
okay then I will write here data
underscore
fake let's go
manual
testing to dataor
fake.
tail for the last 10 rows I have to
write here
10 okay so for
I in
range 2 3
4 8 1 sorry
zero comma 2
3
470 comma
minus1
okay
then DF underscore not DF
data underscore
fake dot
drop
one instead of one I can write here
I
comma is equals to0
place to
true then
data not
here data
underscore same I will write for I will
copy from
here and I will paste it here and I will
make the particular changes so here I
can write
true here I can write
true
okay then I have to change a
[Music]
number 2
1
416
write 2 1 40
6 Min
-1
same so press
enter x =
0 since X maybe you mean d0 or so of
this okay we will put here double
course I'm putting
this. drop IX is z in place
okay also write equals to equals
to
yeah
so okay AIS is not
defined now it's working
so let me
see now
data
_ pick.
shape
okay and data do
true data underscore
true do
shape as you can
see 10 rows are deleted from each data
set yeah so I will write here data
underscore fake underscore
manual
testing
class equals to
zero and data
uncore true
underscore manual underscore
testing CL equals
to
one
okay just ignore this
warning then let's
see data
underscore F unders go
manual
testing. head
as you can see we have this and then
data dot sorry underscore
truecore
manual
testing dot
head
10
okay this is this is the uh true data
set so here I will merge
data goore
merge
to PD
dot
concat concat is used for the
conation dataor
fake data
underscore
comma
XIs = to
Z then data uncore
merge do
head the top 10
rows
yeah as you can see the data is merged
here
okay first it will come for the fake
news and then with the for the True
News and let's merge true and fake data
frames
okay we did this
and let's merge the column then data do
merge dot columns or let's see the
columns
it has not defined what data underscore
merge these are the column name title
Tex subject date class
okay
now let's remove those columns which are
not required for the further process so
here I will write data
underscore or equals to data underscore
March
prop title we don't
need
then subject we don't need
then so
one so let's check some null
values it's giving error
here because of
this that's good then
data dot is
null
some
Center so no null values okay then let's
do the random shuffling of the data
frames okay for that we have to write
here data equals to data do
sample
one
than data okay
data do
head okay now you can see here the
random shuffling is
done and one for the true data set and
zero for the fake news one
okay then let me write here data
dot
reset underscore
index
Place equals to
True data dot
drop comma X's equals to
1 then comma in
place
equals to
True
okay then let me see columns now data
dot
columns so here we have two columns only
rest we have deleted
okay let me see data dot
add yeah
everything seems
good let's proceed further and let's
create a function to process the text
okay for that I will write
[Music]
here
but okay you can use any
name
text then text equals
to text.
lower okay and
text to r dot for the
substring remove these
things uh from
the datas okay
so for that I'm writing
herea okay then text equals to R do
substring
comma comma
text okay then I have to write text
equals
to R do
substring
W
ww
dot
s+
comma comma
text okay then text equals
to I do
substring
then
comma
okay then text equals to R do
substring
then
percentage
as again percentage for ar. SK function
right here
string do
punctuation
comma then comma then
text
right then text equals to R do
substring
and
N
comma x equals to r
dot
subing right
here and again d
then
again okay then
comma then again
text okay then at the end I have to
write here return return text so
everything like uh this this type of
special character will be removed from
the data set okay let's run this let's
see yeah so here I will addite DF sorry
not DF
data
data
then
texts to
data dot
apply to the function name wordp word
opt
okay press enter yeah so so now let's uh
Define the dependent and independent
variables okay x equals to
data
text and Y equals
to
data
class okay then splitting training and
testing
data okay sorry so here I will write
xcore
train comma X underscore
test uh then Yore
train comma Yore test = to train
underscore testore
split then X comma
y comma
test let's go size equals to 0.25
okay press
enter so now let's convert XT to vectors
for that I have to write
here Z is
X so here I will write from
Escalon dot
feature
extraction Dot
text
import T
vectorizer
okay then
vectorization equals to T
fid
vectorizer
okay
then V
underscore
train equals
to
vectorization r a T1
factorization do
fit then
transform X
train okay then X Vore test equals
to
factorization dot
transform xor
test okay then press
enter
uh so now let's see how of first model
logistic
regression so here I will write
from eSalon
dot linear uncore
model OKAY
import
logistic
regression then allot equals
to
logistic
regression have to write here L
do
fit then XV
dot not DOT x dot train
comma X Vore test okay let's
enter XV
TR okay here I have to write y
train okay and press
enter will work so here I will write
prediction and score linear
regression
l r.
predict Vore
test okay let's see the accuracy
score for that I have to write LR DOT
score then XV underscore
test comma
Yore
test okay let's see the accurac accy so
here as you can see accuracy is quite
good
98% okay now let's
print the
classification
put Yore test
comma prediction of linear regression
okay
so this is you can see Precision score
then F1 score then support value
accuracy okay so now we will uh do this
same for the decision free gradient
boosting classifier random Forest
classifier okay then we will do model
testing then we will predict this
scol okay so now for the decision tree
classification so for that I have to
import from
skar
dot
tree
import
decision
three
classifier okay then at the short form I
will write here I will copy it from
here
then okay and I have to write the same
as this so I will copy it from
here
and
yeah let's change linear
regression
to season Tre classification
yeah
okay then I will write here
same
dtal to dt.
predict X
Vore
test still loading it's it will take
time
okay till then let me write here for the
accuracy it do
score
Vore test comma
y
okay let's wait
okay
R the accuracy
so as you can see accuracy is good than
this linear regression okay logistic
regression okay so let
me show you the let me
predict
print so this is the accuracy score this
is the all the report
yeah now let's move for the uh gradient
boosting
classifier okay for that I write from
Escalon dot
emble
Port
radiant
boting
classifier
classifier I will write here
GB equals to let me copy it from
here
okay I will give here random
let goore
State equals to
zero wait wait wait wait so I will write
here GB dot
fit X Vore train comma Yore train okay
then press
enter here I will write predict _ GB
wasu GB dot hit sorry
redit
three DOT
test doore
test till then it's loading so I will
write here uh it's for the score then I
will add GB do
score then
Vore test
comma Yore test okay so let's wait it is
running this
part till then let me write for the
printing
this okay it's taking
time taking time still taking
time what if I will run
this it's not coming because of
this yeah it's done now so you can see
the
accuracies not good
than decision Tre but yeah it is also
good 99
point4 something okay
so now let's check for the last one
random
Forest first I will
do for the random Forest we have to
write from eson
dot
symbol
import
random
Forest
classifier
okay and here I will write
RF
to right I will copy it from
here
then random
state to
then RF dot
fit Vore
train comma ycore
train okay then press
enter and
predict
underscore
RC
RF equals
to RF do
predict Vore test
okay till then I will write here it's
still loading it will take time so till
then I will write for the score score
accuracy
score XV uncore test comma Yore test
okay then I will write here till then
print
classification
port and Yore
test
comma will take time little bit
so uh it run the accuracy score is 99 it
is also
good so now I will write the code for
the model testing so I will get back to
you but after writing the code
so so I have made two functions one for
the output label and one for the manual
testing okay so it will predict the all
the from all models from
the repeat so it will
predict the the news is fake or not from
all the models okay so for that let me
write here
news to
string
what okay then
I will write here manual
underscore
testing
so here I will you can add any news from
the you can copy it from the Internet or
whatever from wherever you want so I'm
just copying from the
internet okay from the Google the news
which is not fake okay I'm adding which
is not fake because I already know I
searched on Google so I'm entering this
so just run it let's see what is
showing okay string input object is not
callable okay let me check this
first okay I have to give here s Str
only yeah let's
check okay I have to add here again the
script yeah manual testing is not
defined let me see manual
testing okay I have to edit
something it is just GB and it is just
RF GBC is not defined okay okay so what
I have to do I have to remove
this
this okay everything seems
sorted
now
as I said to you I just copied this news
from the internet I already know the
news is not fake so it is showing not a
fake news okay so now what I will do I
will
copy one fake news from the
internet and let's see it is detecting
it or not
okay so let me run this
and let me add the news for
this
so all the models are predicting right
it is a fake news or you can add your
own script like this is the fake news
okay I hope you guys understand till
here so I hope you guys must have
understand how to detect a fake news
using machine learning you can you can
copy copy any news from the internet and
you can check it is fake or not okay or
if your model is predicting right or not
if you are an aspiring machine learning
engineer looking for online training and
certifications from prestigious
universities and in collaboration with
leading experts then search no more
simply learns professional certification
program in Ai and machine learning from
Pur University in collaboration with IVM
should be your right choice for more
details use the link in the description
box below with that in mind open CV open
source computer vision library is an
open source computer vision and machine
learning software Library it is written
in C++ but has a binding for various
programming languages such as python
Java MLB open CV was designed with a
goal of providing a common
infrastructure for computer vision
applications and to accelerate the use
of machine learning perception in
commercial product open CV is widely
used in a variety of indes including
robotics automative and health
Healthcare it's supported by a large
community of developers researchers and
users who contribute to its development
and provide support to its users it is
supported by a large community of
developers researchers and users who
contribute to its development and
provide support to its users now let's
see what is object detection object
detection is a computer vision
technology that involves identifying and
localizing the object of interest within
an image or a video it is a challenging
task as it involves not only recognizing
the presence of an object but also
detecting its precise location and size
within the image or video object
detection algorithm typically used deep
learning techniques such as CNN to
analyze the image or video for
identifying the objects these algorithm
can also determine the boundaries of the
object by drawing a bounding box around
them so after understanding what is
object detection now let's move on to
the programming part so this is a kernel
python kernel here we will change this
name so here I will write object
detection
demo okay
so so first we will import some major
Library like open CV so for that we will
write import CV2 and the next one is
import matplot
lib P
plot as PLT so why we are writing PLT
because we can't write again and again M
plot li. P plot okay it's a long so we
can write a short form
PL so yeah so let's run this so what is
opencv opencv is an open source software
library for computer vision and machine
learning the opencv full form is open
source computer vision library was
created to provide a shared
infrastructure for application for
computer vision and to speed up the use
of machine learning perception in
consumer products open CV as a BSD
licensed software make it simple for
companies to use and change the code
okay so there are some predefined
packages and libraries that make our
life simple and open CV is one of them
second one is M PL lip mat BL lip is a
easy to use and an amazing visualize
library in Python it is built on numpy
array and designed to work with broader
CPI stack and consist of several plots
like line bath scatter histogram and
many others okay so moving forward we
will import our file okay so here I will
write
config file equals to this is our file
name SSD
uncore mobile
net
V3
large
Coco
202 14.
DB okay okay so you can find this file
on the description box
below Frozen model equals
to I explain you every single thing
inference
graph. PB okay so let me run it first
mobile net as a name applied the mobile
net model is designed to used in mobile
application and its tens oflow first
mobile computer vision model mobile net
used depthwise separable convolutions it
significantly reduces the numbers of
parameter when compared to the network
with regular convolutions with the same
depth in the NS this result in the light
weight of the deep neural network so
mobile net is a class of CNN that was
open source by Google and therefore this
gives us an excellent starting point for
training our classifiers that are
insanely small and insanely fast okay so
what is this large Coco this is a data
set Coco data set like with applications
such as object detection segmentation
and captioning the Coco data set is
widely understood by the
state-of-the-art of neural network its
versatility and the multi-purpose scene
variation serve best to train a computer
vision model and Benchmark its
performance okay so what is coko the
common object in context is one of the
most popular large scale label images
data set available for public use it
represent a handful of object we
encounter on a daily basis and contains
image in notations in 80 categories I
will show you the categories I have with
over 1.5 million object instances okay
so modern day AI driven solution are
still not capable of producing absolute
accuracy and result which comes down to
the fact that Coco data set is a major
Benchmark for CV to train test and
polish refine models for faster scaling
of The annotation Pipeline on the top of
that the Coco data set is a supplement
me to transfer learning where the data
used for one model serves as a starting
point for the another so what is frozen
inference graph like freezing is the
process to identify and save all the
required graphs like weights and many
others in a single file that you can
usually use a typical transflow model
contains four files and this contains a
complete graph okay so forward let's
create one model here I will write model
to CV2 do
DNN
model
model and then config
file so here I'm giving the parameters
two parameters like frozen model and
config
file score here yeah run it first okay
there is
error okay so error is CV2 do DNN
detection model return is
result an exception set the question
comes what is the meaning of deduction
model or DNN detection model so this
class represent the high level API for
object detection networks detection
models allows to set parameters for
pre-processing input image detection
model creates net from file and with
trained weights and config sets it
processing input runs forward pass and
return the result
deductions okay moving forward let's set
the class
labels okay
class
labels file
name to labels.
dxt I will put this file on the
description box below you can download
from there
open file
name
as
labels
District
so here I created one array of name
class labels so this is the file name
what I'm doing I'm putting this label
file into these class labels okay so
here if I will
print class class
labels so these are the 80 categories in
the Coco data
set okay this person bicycle car
motorbike Aeroplane bus train these all
are the a
categories I will put this file label.
txt in the description box below you can
download from there okay
fine so let's print the length of the
Coco data set or you can see class
labels
is 80 as you can see I have already told
you the length will be
80 so here let's set this some model
input size scaling mean and all so I
will write here model dot set
input size
320 comma
320 do
set
input
scale
1.0 SL
127.5 okay I will explain you don't
worry model do
set
set
input
[Music]
mean
127.5 comma
127.5 comma
127.5 okay and then model do
set
input
B will
be what is set input
size
okay so set input size is a size of new
frame a shape of the new blob L than
zero okay so this is the size of the new
frame the second one is set input scale
the set input scale is a scale factor of
the value for the frame or you can say
the parameter will be the multiplier of
the frame values or you can say
multiplier for the frame values okay so
input mean so it set the mean value for
the frame the frame in which the photo
will come the video will come or my
webcam will come so it set the mean
value for the frame or the for
parameters mean scalar with the mean
values which are subtracted from the
channels you can say and the last one is
set input swap RB so it set the flag
swap RB for the every frame we don't
have to put every time a single frame
for a particular image it will be set
the true for the all the images okay so
parameters will be SB flag which
indicates the swap first and the last
channels so moving forward we will Port
one
image I am
read do
jpg do
I am
sure so this is the size of 320 by 320
okay so first thing is you can download
this the random picture from the Google
I took from Google itself so now what we
will do we will set the class
index the confidence
value value
the B box B box is the boundary box
which I will create for the particular
person cycle motorbike and the car okay
equals to
model the confidence
threshold threshold is used
for if my model will confirm its the
particular image which is the texting is
correct it will print the name
okay so let me
print
print
class class index is coming 1 2 3
4 okay so one means
person two means Bicycle three means car
and four means motorbike this is the
class index index for particular label I
will do I will print the
boxes
font
scale = to three and the font equals to
CV2
dot font
her
L for
class index and the confidence and the
boxes
and
Dot
PN conf
this
that box the boundary
box
okay then I will write here CV2 do
rectangle make the
rectangle set the
image and box
is 5 comma 0 comma 0 this is the color
of the box and this will be the
thickness okay then I will write CV2 dop
put
text image
then class
labels I will write class index minus
one because always index X start with
zero that's fine and the box
is
z
and comma
boxes
one
body okay
F
comma
scale
font
scale
color
to this will be the text color 0 comma
255 comma
0 and the
thickness
three let me run
it uh no error okay now PLT do I am
show then CV2 do
CVT
color
then CV
T2 dot
color
color than
BG to
brg that is why we wrote swap RB equals
to true because every time we will
convert BGR to
brg
sorry GB RGB so we don't have to write
again and again it will convert all the
files into RGB okay run
it
okay as you can see the motorbike is
coming bicycle is coming the person is
coming the car will car is coming okay
so it's detecting the
right for the image now we will do this
for the video and for the
webcam we are done with this image one
and then now I will write
here okay so this is will do for the
video for the video I will write here
cap equals to capture you can write any
name so CV2
dot
video
capture so you can take any random video
I took this
pixels
George can comment
down share the
link app dot is
open so here I will write
cap equals to
CV2 sorry
CV2 dot
video
capture
zero and if
not cap do is
open
then
and
raise input
output
error
and open the
video can't open the
video here everything will be the same
font
scale equals to
three okay font equals to CV2
dot
font
her
10 okay so here I will write while
true comma
frame equals to cap do
sheet this is for the reading of the
file the same I will write class
index comma
confidence boundary
boxum model do
detect CL and the
confidence
threshold
to
0.55 okay everything is the same we did
before so here I will
print
class
index okay so here I will write
if
and of the class
index
does not equals to
zero then what to perform is here I have
to write
four class
index comma
confidence comma
boxes
inip
CL index Dot
flatten flatten is a layers
okay
confides
flatten e
box and
if
class index is greater than equals to
80 then
what to do then I will copy from
here okay the same thing I have to write
here
so here I will write CV2 dot I'm
show this will be the return in the
frame object
detection by simply
learn and
frame so if CV2 dot vit
key
2
and
zero
FFX =
to o
d q okay
then here I will write
break will be break
when get into two the weight key will be
two okay I will tell you what is the wa
ke here I will write cap do
release and CV2
dot
destroy
all
windows
okay so now let me
run let's see error there will be error
okay
Python Programming
modules let me run it
again
Keys
okay video is
here the video is here as you can see
see bicycle the person the person the
bus car traffic light the person person
so our object detection for the video is
coming right okay person okay person
traffic light
[Music]
bus this is how you can do for the video
okay so now let's we will do for the uh
webcam
live so this is for the video so
if we want to do for the webcam we okay
so we need to just
change one one thing only we have to
change instead of giving the file we
have to write one here okay the rest
will be the
same got it so I have to just shut down
my webcam so let me shut down the webcam
and get back to
you as you can
see this is a 320 by 320 box
so so this is coming right okay so I if
I will show this the mobile phone is
coming right now okay so this is how you
can do the correct
object detection if you are an aspiring
machine learning engineer looking for
online training and certifications from
prestigious universities and in
collaboration with leading experts then
search no more simply learn professional
certification program in Ai and machine
learning from Pur University in
collaboration with IBM should be your
right choice for more details use the
link in the description box below with
that in mind so first we will open
command promt to write a command to open
Jupiter notebook so here we will write
Jupiter
notebook then press enter so it will
take some time
yeah so this is the landing page of
jupyter notebook and here you have to
new then Python
3 so this is how the jupyter notebook UI
look likees so at first we will import
some major libraries of python which
help in creating a mass detection system
so here I will write
import CV2 comma o OS and press enter
then I will give the path then data path
equals to
C then
slash
users then
SL
SLP
9375 then
slash
desktop then SL then
face
mask
detection then slash data
set
okay so here it will be slash my bad
sorry and
slash okay it's look
fine so here I will write categories
equals to OS
do
list directory L
di then I will assign data underscore
path then I will create some
labels equals
to uh here I will
write
I will write here
I
for I in
range then here I will give
length of
categories
yeah then press enter Then here I will
write label
then D directory equals
to directory
then zip then
categories comma
labels so here I will write
print then label
directory
okay then I will
print
categories then I will print
labels okay then press enter
yeah so here CV means capturing video
the CV2 function in open CV can read
video video capture by using pass zero
as a function parameter we can access
our webcam we may pass rtsp URL in the
function parameter to capture CCTV
footage which is quite helpful for video
analysis then OS this this OS module the
OS module in Python has function for
adding and deleting folders retrieving
their contents changing the directory
locating the current directory and more
before you can communicate with the
underlying operation system you must
import the OS
module and this OS list directory to
retrieve a list of all files and folders
in the given directory use Python os.
list directory method the list of files
or directory in the current working
directory will be returned if no
directory is
specified then label the the TK inter
visit class called a label is used to
show text or an image the user views the
label they cannot be interact with it so
moving forward I will write code so just
stay with me after that I will explain
you line wise
okay so here I will
write that image _ size equals to
100 then I will create two classes
data then and
Target okay
aray then I will write here for
category in
categories
folder
underscore
path equals to OS
do
path then dot
join then
data underscore
path comma
category okay then
IMG image names equals to os.
list
directory then folder
path just stay with me I will explain
you line
wise okay so here I will write
for IMG
name in
IMG
names
IMG uncore
paath equals to os.
paath
dot
join then I will write here
folder I
repeat I repeat so here we write folder
underscore
path then
IMG underscore
name so IMG equals to
CV2 do IM am
read then
IMG
path so here I will write
try that gray equals to
CV2 Dot
CVT then Capital C
color then IMG comma
CV2
dot then
color then underscore
BGR to
RGB
okay
so I will go for the gray
one C color
BGR to RGB
yeah so I will write here
resize okay not in capital letter
resize equals to
CV2 do
resize then
Gray comma
IMG
size comma IMG
size why this two IMG size because like
length and uh like width and breadth
would be
sh like 100 so press enter so I will
write data
Dot
upend then
resized then
Target do
append then
label then
directory of
category
okay
so I will write here accept
exception as
e so here I will
print
then
exception comma
e okay then press enter let let see
there should be no
error so it is loading let's
wait okay no
error so image size should be like 100
by 100 so that is why I wrote here 100
then image size 100 by 100 and I made
two arrays like for one for data then
one for
targets
so this for this gray was to see do CVT
color image so converting the image into
gray scale okay then this line resize
equals to CV2 do resize gray IMG so
resizing the gray scale into like 100 by
100 since we need a fixed common size
for all the images in the data set
okay then this target do append label
direct so appending the image like and
the label categories into the list list
what is list data set so like for the
last print exception e so if any
exception raised the exception will be
printed here and the pass to the next
image
okay so moving forward let's import numi
and save this data and the target okay
so I will write here
import
numpy as
NP
right then I will write here
data equals to NP do
array then again
data by
255
0 okay then again data equals to NP
dot
reshape then data
comma
data dot
shape then
zero comma IMG
size comma IMG
size comma
1 okay then press enter Then for Target
let me put do it this like this then
Target equals to NP do
array then
Target
okay
okay so here I will write
from gas
dot
jtil import
NPS okay so here I will write new
underscore
Target equals to npor
utils dot
2 underscore
categories categorical I guess
categorical then Target
okay then press
enter so here I will save np.
save this
data comma
data okay and np.
save this
target comma new Target
okay so press
enter
yeah so numai numai is a python Library
used for working with arrays it also has
a function for working with in the
domain of linear algebra and matrices it
is an open source project and you can
use it freely numai stand for numerical
Python and this scas a python Pyon
interface for artificial neutral network
is provided by the open source software
package known as scas the tensorflow
library interface is provided by the
kasas a number of backends were
supported by kasas up until version 2.3
including T oflow Microsoft
cognitive so this is the part of the
data processing this all this part of
the data processing so let me write here
data pre-processing PR
processing so the data pre-processing
part is done now we will create the
another file of this python for the
training CNN CNN means convocation
neural network okay test
data
comma
Trainor
Target comma
testore Target equals to
train underscore
split then we will split on data and
Target comma test size should
be
0.1 okay so now I will give press enter
okay train split train split is not
defined
okay it is
train test dis
spit
yeah so now I will write
checkpoint equals to
model
checkpoint then
then model
I repeat so here I will write model
comma
EPO then Z
3D Dot
model okay
so here I write comma
monitor equals to Value
loss
underscore
loss
comma
bubos =
to0 comma
save
best
only equals
to through comma
mode equals to mode should be
Auto
okay then I will press
enter Then here I will write
history equals to model
dot
fit to
train and let go
data
comma okay train underscore
Target
comma APS = to
20 comma call
backs callback equals
to check
Point comma
validation validation underscore
split equals to 0.2 the best ratio okay
so let me press enter okay there is one
error model. fit c
x okay there should be a spelling
mistake
Eep
PS to
20 do model it is is
fine then
monitor go to Value
loss then BOS equal to
zero save best only equals to true then
mode okay
so here I have to
write maybe it will so it will take some
time to go till 20 okay so it will
download by by one so we will wait for a
while so the model checkpoints are
completed so here I will write
print then
model Dot
evaluate okay here I will write test
underscore
data comma
test underscore
Target then press
enter okay so I hope you guys understand
till here if you have any question or
any query regarding any code just put as
in commands our team will shortly
provide you the correct solution okay so
moving forward we will do we will create
another file for detecting mask okay so
I will go here and then new file
python so I will write here
detecting
okay yeah so I will import some
libraries here kasas do
models import
load
models we will
import
CV2 and we will
import
numai as
NP so press enter okay num by as
NP okay so presenter so what I will do I
will write here model equals to load
uncore
model
then OKAY kasas model from kasas cannot
import
name okay
so it is model only yeah so here I will
add model equals to load
model so I will write here
model
0177 do
model
okay okay
my so here I will write face underscore
classifier equals to
CB2 Dot
cascate cascate
classifier
so this is one file for fontal phas
default so you can find this file on the
description box below so I will write
here her
casket
underscore
frontal
phore
default do
XML okay
so
Source equals
to
CV2
dot
video
capture zero it will open our camera so
labels
underscore
directory equals
to
zero Mass
and one
for no
mask
okay so
color directory equals
to like no mask for red and mask for
green okay so
zero then 0 comma 2 uh 55 comma 0
then again comma for one there should be
0 comma 0 comma
25
okay so I hope you guys understand till
here so if you have any question or any
query regarding any code so just put as
in comments our team will shortly
provide you the correct
solution
so the code is
written so let me run this for this
first I have to like so first I have to
close my this person on screen
camera so the code is running
fine now it is showing no
mask now it is showing
mask so I hope you guys understand till
here if you have any queries any
question regarding any code just put as
this command our team will shortly
provide you the correct solution if you
are an aspiring machine learning
engineer looking for online training and
certifications from prestigious
universities and in collaboration with
leading experts then search no more
simply lar professional certification
program in Ai and machine learning from
Pur University in collaboration with IBM
should be your right choice for more
details use the link in the description
box below with that in mind we'll talk
about interview questions for machine
learning now this video will probably
help you when you're attending
interviews for machine learning
positions and the attempt here is to
probably consolidate 30 most commonly
asked uh questions and to help you in
answering these questions we tried our
best to give you the best possible
answers but of course what is more
important here is rather than the
theoretical knowledge you need to kind
of add to the answer answers or
supplement your answers with your own
experience so the responses that we put
here are a bit more generic in nature so
that if there are some Concepts that you
are not clear this video will help you
in kind of getting those Concepts
cleared up as well but what is more
important is that you need to supplement
these responses with your own practical
experience okay so with that let's get
started so one of the first questions
that you may face is what are the
different types of machine learning now
what is the best way to respond to this
there are three types of machine
learning if you read any material you
will always be told there are three
types of machine learning but what is
important is you would probably be
better off emphasizing that there are
actually two main types of machine
learning which is supervised and
unsupervised and then there is a third
type which is reinforcement learn so
supervised learning is where you have
some historical data and then you feed
that data to your model to learn now you
need to be aware of a keyword that they
will be looking for which is labeled
data right so if you just say pass data
or historical data the impact may not be
so much you need to emphasize on labeled
data so what is labeled data basically
let's say if you're trying to do train
your model for classification you need
to be aware of for your existing data
which class each of the observations
belong to right so that is what is
labeling so it is nothing but a fancy
name you must be already aware but just
make it a point to throw in that keyword
labeled so that will have the right
impact okay so that is what is
supervised learning when you have
existing labeled data which you then use
to train your model that is known as
supervised learning and unsupervised
learning is when you don't have this
labeled data so you have data it is not
labeled so the system has to figure out
a way to do some analysis on this okay
so that is unsupervised learning and you
can then add a few things like what what
are the ways of Performing uh supervised
learning and unsupervised learning or
what are some of the techniques so
supervised learning we we perform or we
do uh regression and classification and
unsupervised learning uh we do
clustering okay and cl string can be of
different types similarly regression can
be of different types but you don't have
to probably elaborate so much if they
are asking for uh just the different
types you can just mention these and
just at a very high level you can but if
they want you to elaborate give examples
then of course I think there is a
different question for that we will see
that later then the third so we have
supervised then we have unsupervised and
then reinforcement you need to provide a
little bit of information around that as
well because it is sometimes a little
difficult to come up with a good
definition for reinforcement learning so
you may have to little bit elaborate on
how reinforcement learning works right
so reinforcement learning works in in
such a way that it basically has two
parts to it one is the agent and the
environment and the agent basically is
working inside of this environment and
it is given a Target that it has to
achieve and uh every time it is moving
in the direction of the target so the
agent basically has to take some action
and and every time it takes an action
which is moving the agent towards the
Target right towards a goal a Target is
nothing but a goal okay then it is
rewarded and every time it is going in a
direction where it is away from the goal
then it is punished so that is the way
you can little bit explain and uh this
is used primarily or very very impactful
for teaching the system to learn games
and so on examples of this are basically
used in alphao you can throw that as an
example where alphao used reinforcement
learning to actually learn to play the
game of Go and finally it defeated the
go world champion all right this much of
information that would be good enough
okay then there could be a question on
overfitting uh so the question could be
what is overfitting and how can you
avoid it so what is overfitting so let's
first try to understand understand the
concept because sometimes overfitting
may be a little difficult to understand
overfitting is a situation where the
model has kind of memorized the data so
this is an equivalent of memorizing the
data so we can draw an analogy so that
it becomes easy to explain this now
let's say you're teaching a child about
some recognizing some fruits or
something like that okay and you're
teaching this child about recognizing
let's say three fruits apples oranges
and pineapples Okay so so this is a a
small child and for the first time
you're teaching the child to recognize
fruits then so what will happen so this
is very much like that is your training
data set so what you will do is you'll
take a basket of fruits which consists
of apples oranges and pineapples okay
and you take this basket to this child
and uh there may be let's say hundreds
of these fruits so you take this basket
to this child and keep showing each of
this fruit and then first time obviously
the child will not know what it is so
you show an apple and you say hey this
is Apple then you show maybe an orange
and say this is orange and so on and so
for and then again you keep repeating
that right so till that basket is over
this is basically how training work in
machine learning also that's how
training works so till the basket is
completed maybe 100 fruits you keep
showing this child and then in the
process what has happened the child has
pretty much memorized these so even
before you finish that basket right by
the time you are halfway through the
child has leared about recognizing the
Apple orange and pineapple now what will
happen after halfway through initially
you remember it made mistakes in
recognizing but halfway through now it
has learned so every time you show a
fruit it will exactly 100% accurately it
will identify it will say the child will
say this is an apple this is an orange
and if you show a pineapple it will say
this is a pineapple right so that means
it has kind of memorized this data now
let's say you bring another BOS of
fruits and it will have a mix of maybe
apples which were already there in the
previous set but it will also have in
addition to Apple it will probably have
a banana or maybe another fruit like a
jack fruit right so this is an
equivalent of your test data set which
the child has not seen before some parts
of it it probably has seen like the
apples it has seen but this banana and
jack fruit it has not seen so then what
will happen in the first round which is
an equivalent of your training data set
towards the end it has 100% it was
telling you what the fruits are right
Apple was accurately recognized orange
were was accurately recognized and
pineapples were accurately recognized
right so that is like a 100% accuracy
but now when you get another a fresh set
which were not a part of the original
one what will happen all the apples
maybe it will be able to recognize
correctly but all the others like the
jack fruit or the banana will not be
recognized by the child right so this
this is an analogy this is an equivalent
of overfitting so what has happened
during the training process it is able
to recognize or reach 100% accuracy
maybe very high accuracy okay and we
call that as very low loss right so that
is the technical term so the loss is
pretty much zero and accuracy is pretty
much 100% whereas when you use testing
there will be a huge error which means
the loss will be pretty high and
therefore the accuracy will be also low
okay this is known as overfitting this
is basically a process where training is
done training process is it goes very
well almost reaching 100% accuracy but
while testing it really drops down now
how can you avoid it so that is a
extension of this question there are
multiple ways of avoiding overfitting
there are techniques like what do you
call regularization that is the most
common technique that is used uh for uh
avoiding overfitting and within
regularization there can be a few other
subtypes like Dropout in case of neural
networks and a few other examples but I
think if you give example or if you give
regularization as the technique probably
that should be sufficient so so there
will be some questions where the
interviewer will try to test your
fundamentals and your knowledge and
depth of knowledge and so on and so
forth and then there will be some
questions which are more like trick
questions that will be more to stump you
okay then the next question is around
the methodology so when we are
performing machine learning training we
split the data into training and test
right so this question is around that so
the question is what is training set and
test set in machine learning model and
how is the split done so the question
can be like that so in machine learning
when we are trying to train the model so
we have a three-step process we train
the model and then we test the model and
then once we are satisfied with the test
only then we deploy the model so what
happens in the train and test is that
you remember the labeled data so let's
say you have thousand records with
labeling information now one way of
doing it is you use all the Thousand
records for training and then maybe
right which means that you have exposed
all these thousand records during the
training process and then you take a
small set of the same data and then you
say okay I will test it with this okay
and then you probably what will happen
you may get some good results all right
but there is a flaw there what is the
flaw this is very similar to human
beings it is like you are showing this
model the entire data as a part of
training okay so obviously it has become
familiar with the entire data so when
you're taking a part of that again and
you're saying that I want to test it
obviously you will get good results so
that is not a very accurate way of
testing so that is the reason what we do
is we have the label data of this
thousand records or whatever we set
aside before starting the training
process we set aside a portion of that
data and we call that test set and the
remaining we call as training set and we
use only this for training our model now
the training process remember is not
just about aing one round of this data
set so let's say now your training set
has 800 records it is not just one time
you pass this 800 records what you
normally do is you actually as a part of
the training you may pass this data
through the model multiple times so this
thousand records may go through the
model maybe 10 15 20 times till the
training is perfect till the accuracy is
high till the errors are minimized okay
now so which is fine which means that
here that is what is known as the model
has seen your data and gets familiar
with your data and now when you bring
your test data what will happen is this
is like some new data because that is
where the real test is now you have
trained the model and now you are
testing the model with some data which
is kind of new that is like a situation
like like a realistic situation because
when the model is deployed that is what
will happen it will receive some new
data not the data that it has already
seen right so this is a realistic test
so you put some new data so this data
which you have set aside is for the
model it is new and if it is able to
accurately predict the values that means
your TR training has worked okay the
model got drained properly but let's say
while you're testing this with this test
data you're getting lot of errors that
means you need to probably either change
your model or retrain with more data and
things like that now coming back to the
question of how do you split this what
should be the ratio there is no fixed uh
number again this is like individual
preferences some people split it into
50/50 50% test and 50% training Some
people prefer to have a larger amount
for training and a smaller amount for
test so they can go by either 60/40 or
7030 or some people even go with some
odd numbers like
6535 or uh
63.33% ment or Preparation so these are
all some questions around that area
there is again no one answer one single
good answer to this it really varies
from situation to situation and
depending on what exactly is the problem
what kind of data it is how critical it
is what kind of data is missing and what
is the type of corruption so there a
whole lot of things this is a very
generic question and therefore you need
to be little careful about responding to
this as well so probably have to
illustrate this again if you have
experience in doing this kind of work in
handling data you can illustrate with
example saying that I was on one project
where I received this kind of data these
were the columns where data was not
filled or these were the this many rows
where the data was missing that would be
in fact a perfect way to respond to this
question but if you don't have that
obviously you have to provide some good
answer I think it really depends on what
exactly the situation is and there are
multiple ways of handling the missing
data or corrupt data now let's take a
few examples now let's say you have data
where some values in some of the columns
are missing and you have pretty much
half of your data having these missing
values in terms of number of rows okay
that could be one situation another
situation could be that you have records
or data missing but when you do some
initial calculation how many records are
corrupt or how many row or observations
as we call it has this missing data
let's assume it is very minimal like 10%
okay now between these two cases how do
we so let's assume that this is not a
mission critical situation and in order
to fix this 10% of the data the effort
that is required is much higher and
obviously effort means also time and
money right so it is not so Mission
critical and it is okay to let's say get
rid of these records so obviously one of
the easiest ways of handling the data
part or missing data is remove those
records or remove those observations
from your analysis so that is the
easiest way to do but then the downside
is as I said in as in the first case if
let's say 50% of your data is like that
because some column of the other is
missing so it is not like every in every
place in every Row the same column is
missing but you have in maybe 10% of the
records column one is missing and
another 10% column two is missing
another 10% column 3 is missing and so
on and so so it adds up to maybe half of
your data set so you cannot completely
remove half of your data set then the
whole purpose is lost okay so then how
do you handle then you need to come up
with ways of filling up this data with
some meaningful value right that is one
way of handling so when we say
meaningful value what is that meaningful
value let's say for a particular column
you might want to take a mean value for
that column and fill wherever the data
is missing fill up with that mean value
so that when you're doing the
calculations your analysis is not
completely way off so you have values
which are not missing first of all so
your system will work number two these
values are not so completely out of
whack that your whole analysis goes for
a toss right there may be situations
where if the missing values instead of
putting mean maybe a good idea to uh
fill it up with the minimum value or
with a zero so or with a maximum value
again as I said there are so many
possibilities so there is no like one
correct answer for this you need to
basically talk around this and
illustrate with your experience as I
said that would be the best otherwise
this is how you need to handle this
question okay so then the next question
can be how can you choose a classifier
based on a training set data size so
again this is one of those questions uh
where you probably do not have like a
one size fits-all answer first of all
you may not let's say decide your
classifier based on the the training set
size maybe not the best way to decide
the type of the classifier and uh even
if you have to there are probably some
thumb rules which we can use but then
again every time so in my opinion the
best way to respond to this question is
you need to try out few classifiers
irrespective of the size of the data and
you need to then decide on your
particular situation which of these
classifiers are the right ones this is a
very generic issue so you will never be
able to just by if somebody defines a a
problem to you and somebody even if if
they show the data to you or tell you
what is the data or even the size of the
data I don't think there is a way to
really say that yes this is the
classifier that will work here no that's
not the right way so you need to still
uh you know test it out get the data try
out a couple of classifiers and then
only you will be in a position to decide
which classifier to use you try out
multiple classifier see which one gives
the best accuracy and only then you can
decide then you can have a question
around confusion Matrix so the question
can be explain confusion Matrix right so
confusion Matrix I think the best way to
explain it is by taking an example and
drawing like a small diagram otherwise
it can really become tricky so my
suggestion is to take a piece of pen and
paper and uh explain it by drawing a
small Matrix and confusion Matrix is
about to find out this is used
especially in classification uh learning
process and when you get the results
when the our model predicts the results
you compare it with the actual value and
try to find out what is the accuracy
okay so in this case let's say this is
an example of a confusion Matrix and uh
it is a binary Matrix so you have the
actual values which is the labeled data
right and which is so you have how many
many yes and how many no so you have
that information and you have the
predicted values how many yes and how
many no right so the total actual values
the total yes is 12 + 1 13 and they are
shown here and the actual value NOS are
9 + 3 12 okay so that is what this
information here is so this is about the
actual and this is about the predicted
similarly the predicted values there are
yes are 12 + 3 15 yeses and no are 1 + 9
10 NOS okay so this is the way to look
at this confusion Matrix okay and out of
this what is the meaning convey so there
are two or three things that needs to be
explained outright the first thing is
for a model to be accurate the values
across the diagonal should be high like
in this case right that is one number
two the total sum of these values is
equal to the total observations in the
test data set so in this case for
example you have 12 plus + 3 15 + 10 25
so that means we have 25 observations in
our test data set okay so these are the
two things you need to First explain
that the total sum in this Matrix the
numbers is equal to the size of the test
data set and the diagonal values
indicate the accuracy so by just by
looking at it you can probably have a
idea about is this uh an accurate model
is the model being accurate if they're
all spread out equally in all these four
boxes that means probably the accuracy
is not very good okay now how do you
calculate the accuracy itself right how
do you calculate the accuracy itself so
it is a very simple mathematical
calculation you take some of the
diagonals right so in this case it is 9
+ 12 21 and divide it by the total so in
this case what will it be let's me uh
take a pen so your your diagonal values
is equal to if I say d is equal to 12 +
9 so that is 21 right and the total data
set is equal to right we just calculated
it is 25 so what is your accuracy it is
21 by your accuracy is equal to 21 by 25
and this turns out to be about
85% right so this is 85% so that is our
accuracy okay so this is the way you
need to explain draw diagram Give an
example and maybe it may be a good idea
to be prepared with an example so that
it becomes easy for you don't have to
calculate those numbers on the fly right
so couple of uh hints are that you take
some numbers which are with which add up
to 100 that is always a good idea so you
don't have to really do this complex
calculations so the total value will be
100 and then diagonal values you divide
once you find the diagonal values that
is equal to your percentage okay all
right so the next question can be a
related question about false positive
and false negative so what is false
positive and what is false negative now
once again the the best way to explain
this is using a piece of paper and Pen
otherwise it will be pretty difficult to
to explain this so we use the same
example of the confusion Matrix and uh
we can explain that so A confusion
Matrix looks somewhat like this and um
when we just take yeah it looks somewhat
like this and we continue with the
previous example where this is the
actual value this is the predicted value
and uh in the actual value
we have 12 + 1 13 yeses and 3 + 9 12 Nos
and the predicted values there are 12 +
3 15 yeses and uh 1 + 9 10 NOS okay now
this particular case which is the false
positive what is a false positive first
of all the second word which is positive
okay is referring to the predicted value
so that means the system has predicted
it as a positive but the real value so
this is what the false comes from but
the real value is not positive okay that
is the way you should understand this
term false positive or even false
negative so false positive so positive
is what your system has predicted so
where is that system predicted this is
the one positive is what yes so you
basically consider this row okay now if
you consider this row so this is this is
all positive values this entire row is
positive values okay now the false
positive is the one which where the
value actual value is negative predicted
value is positive but the actual value
is negative so this is a false positive
right and here is a true positive so the
predicted value is positive and the
actual value is also positive okay I
hope this is making sense now let's take
a look at what is false negative false
negative so negative is the second term
that means that is the predicted value
that we need to look for so which are
the predicted negative values this row
corresponds to predicted negative values
all right so this row corresponds to
predicted negative values and what they
are asking for false so this is the row
for predicted negative values and the
actual value is this one right this is
predicted negative and the actual value
is also negative therefore this is a
true negative so the false negative is
this one predicted is negative but
actual is positive right so this is the
false negative so this is the way to
explain and this is a way to look at
false positive and false negative same
way there can be true positive and true
negative as well so again positive the
second term you will need to use to
identify the predicted row right so if
we say true positive positive we need to
take for the predicted part so predicted
positive is here okay and then the first
term is for the actual so true positive
so true in case of actual is yes right
so so true positive is this one okay and
then in case of actual the negative now
we are talking about let's say true
negative true negative negative is this
one and the true comes from here so this
is true negative right nine is true
negative the actual value is also
negative and the predicted value is also
negative okay so that is the way you
need to explain this the terms false
positive false negative and true
positive true negative then uh you might
have a question like what are the step
steps involved in the machine learning
process or what are the three steps in
the process of developing uh machine
learning model right so it is around the
methodology that is applied so basically
the way you can probably answer in your
own words but the way the model
development of the machine learning
model happens is like this so first of
all you try to understand the problem
and try to figure out whether it is a
classification problem or a regression
problem based on that you select a few
algorithms and then you start the
process of training these models okay so
you can either do that or you can after
due diligence you can probably decide
that there is one particular algorithm
that which is most suitable usually it
happens through trial and error process
but at some point you will decide that
okay this is the model we are going to
use okay so in that case we have the
model algorithm and the model decided
and then you need to do the process of
training the model and testing the model
and this is where if it is supervised
learning you split your data the label
data into training data set and test
data set and you use the training data
set to train your model and then you use
the test data set to check the accuracy
whether it is working fine or not so you
test the model before you actually put
it into production right so once you
test the model you're satisfied it's
working fine then you go to the next
level which is putting it for production
and then in production obviously new
data will come and uh the inference
happens so the model is readily
available and only thing that happens is
new data comes and the model predicts
the values whether it is regression or
classification now so this can be an
iterative process so it is not a
straightforward process where you do the
training do the testing and then you
move it to production now so during the
training and test process there may be a
situation where because of either
overfitting or or things like that the
test doesn't go through which means that
you you need to put that back into the
training process so that can be an
iterative process not only that even if
the training and test goes through
properly and you deploy the model in
production there can be a situation that
the data that actually comes the real
data that comes with that this model is
failing so in which case you may have to
once again go back to the drawing board
or initially it will be working fine but
over a period of time maybe due to the
change in the nature of the data once
again the accuracy will deteriorate so
that is is again a recursive process so
once in a while you need to keep
checking whether the model is working
fine or not and if required you need to
tweak it and modify it and so on and so
forth so net net this is a continuous
process of um tweaking the model and
testing it and making sure it is up to
date then you might have question around
deep learning so because deep learning
is now associated with AI artificial
intelligence and so on so can be as
simple as what is a deep learning so I
think the best way to respond to this
could be deep learning is a part of
machine learning and then then obviously
the the question would be then what is
the difference right so deep learning
you need to mention there are two key
parts that interviewer will be looking
for when you're are defining deep
learning so first is of course deep
learning is a subset of machine learning
so machine learning is still the bigger
let's say uh scope and deep learning is
one one part of it so then what exactly
is the difference deep learning is
primarily when we are implementing these
our algorithms or when we are using
neural networks for doing our training
and classification and regression and
all that right so when we use neural
network then it is considered as deep
learning and the term deep comes from
the fact that you can have several
layers of neural networks and these are
called Deep neural networks and
therefore the term deep you know deep
learning uh the other difference between
machine learning and deep learning which
the interviewer may be wanting to hear
is that in case of machine learning the
feature engineering is done manually
what do we mean by feature engineering
basically when we are trying to train
our model we have our training data
right so we have our training label data
and uh this data has several let's say
if it is a regular table it has several
columns now each of these columns
actually has information about a feature
right so if we are trying to predict
predict the height weight and so on and
so forth so these are all features of
human beings let's say we have sensus
data and we have all this so those are
the features now there may be probably
50 or 100 in some cases there may be 100
such features now all of them do not
contribute to our model right so we as a
data scientist we have to decide whether
we should take all of them all the
features or we should throw away some of
them because again if we take all of
them number one of course your accuracy
will probably get affected but also
there is a computational part so if you
have so many features and then you have
so much data it becomes very tricky so
in case of machine learning we manually
take care of identifying the features
that do not contribute to the learning
process and thereby we eliminate those
features and so on right so this is
known as feature engineering and in
machine learning we do that manually
whereas in deep learning where we use
neural networks the model will
automatically determine which features
to use and which to not use and
therefore feature engineering is also
done automatically so this is a
explanation these are two key things
probably will add value to your response
all right so the next question is what
is the difference between or what are
the differences between machine learning
and deep learning so here this is a
quick comparison table between machine
learning and deep learning and in
machine learning learning enables
machines to take decisions on the own
based on past data so here we are
talking primarily of supervised learning
and um it needs only a small amount of
data for training and then works well on
lowend system so you don't need large
machines and most features need to be
identified in advance and manually coded
so basically the feature engineering
part is done manually and uh the problem
is divided into parts and solved
individually and then combine so that is
about the machine learning part in deep
learning deep learning basically Ena
Ables machines to take decisions with
the help of artificial neural network so
here in deep learning we use neural n so
that is the key differentiator between
machine learning and deep learning and
usually deep learning involves a large
amount of data and therefore the
training also requires usually the
training process requires high-end
machines uh because it needs a lot of
computing power and the Machine learning
features are the or the feature
engineering is done automatically so the
neural networks takes care of doing the
feature engineering as well
and in case of deep learning therefore
it is said that the problem is handled
end to end so this is a quick comparison
between machine learning and deep
learning in case you have that kind of a
question then you might get a question
around the uses of machine learning or
some real life applications of machine
learning in modern business the question
may be worded in different ways but the
the meaning is how exactly is machine
learning used or actually supervised
machine learning it could be a very
specific question around supervised
machine learning so this is like give
examples of supervised machine learning
use of supervised machine learning in
modern business so that could be the
next question so there are quite a few
examples or quite a few use cases if you
will for supervised machine learning the
very common one is email spam detection
so you want to train your application or
your system to detect between spam and
non-spam so this this is a very common
business application of supervised
machine learning so how does this work
the way it works is that you obviously
have historical data of of your emails
and they are categorized as spam and not
spam so that is what is the labeled
information and then you feed this
information or the all these emails as
an input to your model right and the
model will then get trained to detect
which of the emails are to detect which
is Spam and which is not spam so that is
the training process and this is
supervised machine learning because you
have labeled data you already have
emails which are tagged as spam or not
spam and then you use that to train your
model right so this is one example now
there are a few industry specific
applications for supervised machine
learning one of the very common ones is
in healthcare Diagnostics in healthcare
Diagnostics you have these images and
you want to train models to detect
whether from a particular image whether
it can find out if the person is sick or
not whether a person has cancer or not
right so this is a very good example of
supervised machine learning here the way
it works is that existing images it
could be x-ray images it be MRI or any
of these images are available and they
are tacked saying that okay this x-ray
image is defective of the person as an
ill or it could be cancer whichever
illness right so it is stacked as
defective or clear or good image and
defective something like that so we come
up with the binary or it could be
multiclass as well saying that this is
defective to 10% this is 25% and so on
but let's keep it simple you can give an
example of just a binary classification
that would be good enough so you can say
that in healthcare Diagnostics using
image we need to detect whether a person
is ill or whether a person is having
cancer or not so here the way it works
is you feed labeled images and you allow
the model to learn from that so that
when New Image is fed it will be able to
predict whether this person is having
that illness or not having cancer or not
right so I think this would be a very
good example for supervised machine
learning in modern business all right
then we can have a question like so
we've been talking about supervised and
um unsupervised and so there can be a
question around semi-supervised machine
learning so what is semi-supervised
machine learning now semi-supervised
learning as the name suggests it falls
between supervised learning and
unsupervised learning but for all
practical purposes it is considered as a
part of supervised learning and the
reason this has come into existence is
that in supervised learning you need
labeled data so all your data for
training your model has to be labeled
now this is a big problem in many
Industries or in many under many
situations getting the label data is not
that easy because there's a lot of
effort in labeling this data let's take
an example of the diagnostic images we
can just let's say take X-ray images now
there are actually millions of x-ray
images available all over the world but
the problem is they are not labeled so
the images are there but whether it is
effective or whether it is good that
information is not available along with
it right in a form that it can be used
by a machine which means that somebody
has to take a look at these images and
usually it should be like a doctor and
uh then say that okay yes this image is
clean and this image is cancerous and so
on and so forth now that is a huge
effort by itself so this is where
semi-supervised learning comes into play
so what happens is there is a large
amount of data maybe a part of it is
labeled then we try some techniques to
label the remaining part of the data so
that we get completely labeled data and
then we train our model so I know this a
little long winding explanation but
unfortunately there is no a quick and
easy definition for semi-supervised
machine learning this is the only way
probably to explain this concept we may
have another question as um what are
unsupervised Miss Mach learning
techniques or what are some of the
techniques used for performing
unsupervised machine learning so it can
be worded in different ways so how do we
answer this question so unsupervised
learning you can say that there are two
types clustering and Association and
clustering is a technique where similar
objects are put together and there are
different ways of finding similar
objects so their characteristics can be
measured and if they have in most of the
characteristics if they are similar then
they can be put together this is
clustering then Association you can I
think the best way to explain
Association is with an example in case
of Association you try to find out how
the items are linked to each other so
for example if somebody bought a maybe a
laptop the person has also purchased a
mouse so this is more in an e-commerce
scenario for example so you can give
this as an example so people people who
are buying laptops are also buying the
mouse so that means there is an
association between laptops and mouse or
maybe people who are buying bread are
also buying butter so that is a
Association that can be created so this
is unsupervised learning one of the
techniques okay all right then we have
very fundamental question what is the
difference between supervised and
unsupervised machine learning so machine
learning these are the two main types of
machine learning supervised and unised
and in case of supervised and again here
probably the keyword that the person may
be wanting to hear is labeled data now
very often people say yeah we have
historical data and if we run it it is
supervised and if we don't have
historical data yes but you may have
historical data but if it is not labeled
then you cannot use it for supervised
learning so it is it's very key to
understand that we put in that keyword
labeled okay so when we have labeled
data for training our model then we can
use supervised learning and if we do not
have labeled data then we use
unsupervised learning and there are
different algorithms available to
perform both of these types of uh
trainings so there can be another
question a little bit more theoretical
and conceptual in nature this is about
inductive machine learning and deductive
machine learning so the question can be
what is the difference between inductive
machine learning and deductive machine
learning or somewhat in that manner so
that the exact phrase or exact question
can vary they can ask for examples and
things like that but that could be the
question so let's first understand what
is inductive and deductive training
inductive training is induced by
somebody and you can illustrate that
with a small example I think that always
helps so whenever you're doing some
explanation try as much as possible as I
said to give examples from your work
experience or give some analogies and
that will will also help a lot in
explaining as well and for the
interviewer also to understand so here
we'll take an example or rather we will
use an analogy so inductive training is
when we induce some knowledge or the
learning process into a person without
the person actually experiencing it okay
what can be an example so we can
probably tell the person or show a
person a video that fire can burn the F
burn his finger or fire can cause damage
so what is happening here this person
has never probably seen a fire or never
seen anything getting damaged by fire
but just because he has seen this video
he knows that okay fire is dangerous and
if fire can cause damage right so this
is inductive learning compared to that
what is deductive learning so here you
draw conclusion or the person draws
conclusion out of experience so we will
stick to the analogy so compared to the
showing a video Let's assume a person is
allowed to play with fire right and then
he figures out that if he puts his
finger it's burning or if throws
something into the fire it burns so he
is learning through experience so this
is known as deductive learning okay so
you can have applications or models that
can be trained using inductive learning
or deductive learning all right I think
uh probably that explanation will be
sufficient the next question is are KNN
and K means clustering similar to one
another or are they same right because
that the letter K is kind of common
between them okay so let us take a
little while to understand what these
two are one is KNN and another is kin K
stands for K nearest neighbors and K
means of course is the clustering
mechanism now these two are completely
different except for the letter K being
common between them K andn is completely
different K means clustering is
completely different KNN is a
classification process and therefore it
comes under supervised learning whereas
K means clustering is actually a
unsupervised okay when you have K and N
when you want to implement KN andn which
is basically K nearest neighbors the
value of K is a number so you can say k
is equal to 3 you want to implement K&N
with K is equal to 3 so which means that
it performs the classification in such a
way that how does it perform the
classification so it will take three
nearest objects and that's why it's
called nearest neighbor so basically
based on the distance it will try to
find out its nearest objects that are
let's say three of the nearest objects
and then it will check whether the class
they belong to which class right so if
all three belong to one particular class
obviously this new object is also
classified as that particular class but
it is possible that they may be from two
or three different classes okay so let's
say they are from two classes and then
if they are from two classes now usually
you take a odd number you assign odd
number to so if there are three of them
and two of them belong to one class and
then one belongs to another class so
this new object is assigned to the class
to which the two of them belong now the
value of K is sometimes tricky whether
should you use three should you use five
should you use seven that can be tricky
because the ultimate classification can
also vary so it's possible that if
you're taking K as three the object is
probably in one particular class but if
you take K is equal to five maybe the
object will belong to a different class
because when you're taking three of them
probably two of them belong to class one
and one belong to class two whereas when
you take five of them it is possible
that only two of them belong to class
one and three of them belong to class
two so which means that this object will
belong to class two right so you see
that so it is the class allocation can
vary depending on the value of K now K
means on the other hand is a clustering
process and it is unsupervised where
what it does is the system will
basically identify how the objects are
how close the objects are with respect
to some of their features okay and but
the similarity of course is the the
letter K and in case of K means also we
specify its value and it could be three
or five or seven there is no technical
limit as such but it can be any number
of clusters that uh you can create okay
so based on the value that you provide
the system will create that many
clusters of similar objects so there is
a similarity to that extent that K is a
number in both the cases but actually
these two are completely different
processes we have what is known as KN
based classifier and people often get
confused thinking that kniv base is the
name of the person who found this uh
classifier or who developed this
classifier which is not 100% true base
is the name of the person b y s is the
name of the person but naive is not the
name of the person right so naive is
basically an English word and that has
been added here because of the nature of
this particular classifier naive based
classifier is a probability based
classifier and uh it makes some
assumptions that presence of one feature
of a class is not related to the
presence of any other feature of maybe
other classes right so which is not a
very strong or not a very what do you
say accurate assumption because these
features can be related and so on but
even if we go with this assumption this
whole algorithm works very well even
with this assumption and uh that is the
good side of it but the term comes from
there so that is the explanation that
you can give then there can be question
around reinforcement learning it can be
paraphrased in multiple ways one could
be can you explain how a system can play
a game of chess using reinforcement
learning or it can be any game so the
best way to explain this is again to
talk a little bit about what
reinforcement learning is about and then
elaborate on that to explain the process
so first of all reinforcement learning
has an environment and an agent and the
agent is basically performing some
actions in order to achieve a certain
goal and this goals can be anything
either if it is related to game then the
goal could be that you have to score
very high score a high value High number
or it could be that your number of lives
should be as high as possible don't lose
lives so these could be some of them
more advanced examples could be for
driving the automotive industry
self-driving cars they actually also
make use of reinforcement learning to
teach the car how to navigate through
the roads and so on and so forth that is
also another example now how does it
work so if the system is basically there
is an agent and environment and every
time the agent takes a step or performs
a task which is taking it towards the
goal the final goal let's say to
maximize the score or to minimize the
number of lives and so on or minimize
the deaths for example it is rewarded
and every time it takes a step which
goes against that goal right contrary or
in the reverse Direction it is penalized
okay so it is like a carrot and stick
system now how do you use this to create
a game of chess or to create a system to
play a game of chess now the way this
works is and this could probably go back
to this alphago example where alphago
defeated a human Champion so the way it
works is in reinforcement learning the
system is allowed for example if in this
case we're talking about Chess so we
allow the system to first of all watch
playing a game of chess so it could be
with a human being or it could be the
system itself there are computer games
of Chess right so either this new
learning system has to watch that game
or watch a human being play the game
because this is reinforcement uh
learning is pretty much all visual so
when you're teaching the system to play
a game the system will not actually go
behind the scenes to understand the
logic of your software of this game or
anything like that it is just visually
watching the screen and then it learns
okay so reinforcement learning to a
large extent it works on that so you
need to create a mechanism whereby your
model will be able to watch somebody
playing the game and then you allow the
system also to start playing the game so
it pretty much starts from scratch okay
and as it moves forward it it it's at
right at the beginning the system really
knows nothing about the game of chess
okay so initially it is a clean slate it
just starts by observing how you playing
so it will make some random moves and
keep losing badly but then what happens
is over a period of time so you need to
now allow the system or you need to play
with the system not just 1 2 3 4 or five
times but hundreds of times thousands of
times maybe even hundreds of thousands
of times and that's exactly how alphao
has done it played millions of games
between itself and the system right so
for the game of chess also you need to
do something like that you need to allow
the system to pess and uh then learn on
its own over a period of repetition so I
think you can probably explain it to
this much to this extent and it should
be uh sufficient now this is another
question which is again somewhat similar
but here the size is not coming into
picture so the question is how will you
know which machine learning algorithm to
choose for your classification problem
now this is not only classification
problem it could could be a regression
problem I would like to generalize this
question so if somebody asks you how
will you choose how will you know which
algorithm to use the simple answer is
there is no way you can decide exactly
saying that this is the algorithm I'm
going to use in a variety of situations
there are some guidelines like for
example you will obviously depending on
the problem you can say whether it is a
classification problem or a regression
problem and then in that sense you are
kind of restricting yourself to if it is
a classification problem there are you
can only apply a classification
algorithm right to that extent you can
probably let's say limit the number of
algorithms but now within the
classification algorithms you have
decision trees you have svm you have
logistic regression is it possible to
outright say yes so for this particular
problem since you have explained this
now this is the exact algorithm that you
can use that is not possible okay so we
have to try out a bunch of algorithms
see which one one gives us the best
performance and best accuracy and then
decide to go with that particular
algorithm so in machine learning a lot
of it happens through trial and error
there is uh no real possibility that
anybody can just by looking at the
problem or understanding the problem
tell you that okay in this particular
situation this is exactly the algorithm
that you should use then the questions
may be around application of machine
learning and this question is
specifically around how Amazon is able
to recommend other things to buy so this
is around recommendation engine how does
it work how does the recommendation
engine work so this is basically the
question is all about so the
recommendation engine again Works based
on various inputs that are provided
obviously something like uh you know
Amazon a website or e-commerce site like
Amazon collects a lot of data around the
customer Behavior who is purchasing what
and if somebody is buying a particular
thing they're also buying something else
so this kind of Association right so
this is the unsupervised learning we
talked about they use this to associate
and Link or relate items and that is one
part of it so they kind of build
association between items saying that
somebody buying this is also buying this
that is one part of it then they also
profile the users right based on their
age their gender their geographic
location they will do some profiling and
then when somebody is logging in and
when somebody is shopping kind of the
mapping of these two things are done
they try to identify obviously if you
have logged in then they know who you
are and your information is available
like for example your age maybe your
gender and uh where you're located what
you purchased earlier right so all this
is taken and the recommendation engine
basically uses all this information and
comes up with recommendations for a
particular user so that is how the
recommendation engine work and with that
we have come to the end of AI full
course I hope you found it valuable and
interesting please ask any questions
about the topics covered in this video
in the comment section below our experts
will be happy to assist you thanks for
watching staying ahead in your career
requires continuous learning and
upskilling whether you're a student
aiming to learn today's top skills or a
working professional looking to advance
your career we've got you covered
explore our impressive catalog of
certification programs in cuttingedge
domains including data science cloud
computing cyber security AI machine
learning or digital marketing designed
in collaboration with leading
universities and top corporations and
delivered by industry experts choose any
of our programs and set yourself on the
path to Career Success click the link in
the description to know
more
hi there if you like this video
subscribe to the simply learn YouTube
channel and click here to watch similar
videos to nerd up and get certified
click
here