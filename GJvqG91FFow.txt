1
00:00:00,000 --> 00:00:01,740
so hello there and welcome to another

2
00:00:01,740 --> 00:00:03,780
tutorial my name is Tammy bakshi and

3
00:00:03,780 --> 00:00:04,920
today we're going to be talking about

4
00:00:04,920 --> 00:00:07,319
what makes track GPT just so good and

5
00:00:07,319 --> 00:00:09,660
how you can build your own micro version

6
00:00:09,660 --> 00:00:11,639
of chat GPT and deploy it to your own

7
00:00:11,639 --> 00:00:14,580
Discord server as a bot uh now in my

8
00:00:14,580 --> 00:00:16,980
last video I talked about if chat GPT

9
00:00:16,980 --> 00:00:18,840
could replace me and talked about some

10
00:00:18,840 --> 00:00:20,400
of the fundamental limitations with its

11
00:00:20,400 --> 00:00:23,400
architecture and why it works well for

12
00:00:23,400 --> 00:00:25,859
certain things and what those tasks

13
00:00:25,859 --> 00:00:27,900
actually are but also why it doesn't

14
00:00:27,900 --> 00:00:31,019
work well for certain tasks and in this

15
00:00:31,019 --> 00:00:32,579
video I want to dive a little bit deeper

16
00:00:32,579 --> 00:00:36,600
into the why aspect specifically talking

17
00:00:36,600 --> 00:00:39,300
about what the evolution was of gbt over

18
00:00:39,300 --> 00:00:41,160
the past couple of years to get to where

19
00:00:41,160 --> 00:00:43,140
it is today and then in the next video

20
00:00:43,140 --> 00:00:44,760
you're going to get to see how you can

21
00:00:44,760 --> 00:00:47,040
actually build your own version and then

22
00:00:47,040 --> 00:00:49,760
deploy it as a Discord bot

23
00:00:49,760 --> 00:00:52,620
specifically a micro version of chat gbt

24
00:00:52,620 --> 00:00:53,879
that doesn't of course reach the levels

25
00:00:53,879 --> 00:00:56,460
of quality that openai does because we

26
00:00:56,460 --> 00:00:58,199
don't have infinite money to throw but

27
00:00:58,199 --> 00:00:59,699
for relatively cheap you can get a model

28
00:00:59,699 --> 00:01:02,340
that forms surprisingly well especially

29
00:01:02,340 --> 00:01:04,199
considering how well these models would

30
00:01:04,199 --> 00:01:05,760
have performed just a couple of months

31
00:01:05,760 --> 00:01:07,740
ago and it'll all be permissively

32
00:01:07,740 --> 00:01:09,600
licensed something that you could

33
00:01:09,600 --> 00:01:10,619
theoretically use for your own

34
00:01:10,619 --> 00:01:12,780
applications without having any

35
00:01:12,780 --> 00:01:15,540
proprietary data or any data that comes

36
00:01:15,540 --> 00:01:18,119
from a legal gray area like the data

37
00:01:18,119 --> 00:01:21,000
sets that come from gbt itself let's

38
00:01:21,000 --> 00:01:23,040
dive into it let's start off by taking a

39
00:01:23,040 --> 00:01:24,720
look at a small example of some of the

40
00:01:24,720 --> 00:01:26,640
content that your Bot will be able to

41
00:01:26,640 --> 00:01:28,500
generate by the end of these two videos

42
00:01:28,500 --> 00:01:30,060
once you've actually gone through the

43
00:01:30,060 --> 00:01:31,740
process and spent the thirty dollars to

44
00:01:31,740 --> 00:01:34,020
actually train this model you'll be able

45
00:01:34,020 --> 00:01:37,439
to ask it questions like what is TLS and

46
00:01:37,439 --> 00:01:39,720
get answers like this super detailed and

47
00:01:39,720 --> 00:01:41,820
written out very well potentially even

48
00:01:41,820 --> 00:01:43,560
be able to ask questions like to write

49
00:01:43,560 --> 00:01:46,200
simple Python scripts that do things or

50
00:01:46,200 --> 00:01:48,240
explain the concept of ownership in Rust

51
00:01:48,240 --> 00:01:50,340
and so as you can tell the bot is

52
00:01:50,340 --> 00:01:53,280
powerful and it's only a three billion

53
00:01:53,280 --> 00:01:55,079
parameter model

54
00:01:55,079 --> 00:01:58,140
saying that feels wrong because it's

55
00:01:58,140 --> 00:02:00,659
three billion parameters up until once

56
00:02:00,659 --> 00:02:03,540
again relatively recently that was a lot

57
00:02:03,540 --> 00:02:06,000
of parameters for something that I'd be

58
00:02:06,000 --> 00:02:08,280
running locally as a Discord bot but

59
00:02:08,280 --> 00:02:10,979
these days that's actually not that much

60
00:02:10,979 --> 00:02:14,459
that is just a micro version of gbt yet

61
00:02:14,459 --> 00:02:17,099
it is a large language model

62
00:02:17,099 --> 00:02:19,680
now GPT itself stands for generative

63
00:02:19,680 --> 00:02:21,420
pre-trained Transformer I've already

64
00:02:21,420 --> 00:02:22,860
talked about the Transformer

65
00:02:22,860 --> 00:02:24,300
architecture and what makes it special

66
00:02:24,300 --> 00:02:26,459
compared to previous iterations of the

67
00:02:26,459 --> 00:02:28,680
kinds of networks we used to use for uh

68
00:02:28,680 --> 00:02:30,000
different tasks like natural language

69
00:02:30,000 --> 00:02:31,800
understanding and processing and things

70
00:02:31,800 --> 00:02:33,660
like this in a previous video that will

71
00:02:33,660 --> 00:02:36,900
be linked in the description below but

72
00:02:36,900 --> 00:02:40,140
the primary goal of the GPT architecture

73
00:02:40,140 --> 00:02:43,260
was to train transformer models that are

74
00:02:43,260 --> 00:02:45,420
good at just modeling language and

75
00:02:45,420 --> 00:02:48,060
generating language and the way that

76
00:02:48,060 --> 00:02:49,980
they would do it is the traditional Auto

77
00:02:49,980 --> 00:02:52,560
regressive approach so effectively take

78
00:02:52,560 --> 00:02:55,260
a bunch of content from the internet uh

79
00:02:55,260 --> 00:02:58,140
take it tokenize it into words create

80
00:02:58,140 --> 00:03:00,239
word embeddings out of these words feed

81
00:03:00,239 --> 00:03:02,280
them into a neural network one token at

82
00:03:02,280 --> 00:03:04,800
a time and for every token attempt to

83
00:03:04,800 --> 00:03:06,900
predict the next token that is the

84
00:03:06,900 --> 00:03:08,640
objective that all of these models have

85
00:03:08,640 --> 00:03:11,159
been trained with and we sort of then

86
00:03:11,159 --> 00:03:14,640
abuse this objective by then taking that

87
00:03:14,640 --> 00:03:18,480
final next token prediction and to just

88
00:03:18,480 --> 00:03:20,879
take sampling a token from it and

89
00:03:20,879 --> 00:03:23,040
feeding it back into the network as if

90
00:03:23,040 --> 00:03:25,019
that was actually the next token in the

91
00:03:25,019 --> 00:03:27,239
sequence and by using the architecture

92
00:03:27,239 --> 00:03:30,060
this way we're able to sort of I guess

93
00:03:30,060 --> 00:03:32,819
misuse it to generate language the

94
00:03:32,819 --> 00:03:34,319
reason I say misuse is because

95
00:03:34,319 --> 00:03:36,599
fundamentally the architecture and its

96
00:03:36,599 --> 00:03:38,640
training objective was never really

97
00:03:38,640 --> 00:03:41,459
meant for text generation it was really

98
00:03:41,459 --> 00:03:43,260
meant to try and model the output

99
00:03:43,260 --> 00:03:46,560
distribution of the kind of text that is

100
00:03:46,560 --> 00:03:48,780
seen on the internet so given a series

101
00:03:48,780 --> 00:03:51,120
of tokens predicting the distribution of

102
00:03:51,120 --> 00:03:52,799
probabilities of what the next possible

103
00:03:52,799 --> 00:03:54,900
you know token would be is something

104
00:03:54,900 --> 00:03:57,659
that these models do very well they are

105
00:03:57,659 --> 00:04:00,360
surprisingly accurate at it and the

106
00:04:00,360 --> 00:04:02,099
thing is theoretically if they are

107
00:04:02,099 --> 00:04:04,620
literally perfect at it then the output

108
00:04:04,620 --> 00:04:06,000
distribution would be the same as the

109
00:04:06,000 --> 00:04:07,739
input distribution and you'd just be

110
00:04:07,739 --> 00:04:09,299
able to sample those tokens feed it back

111
00:04:09,299 --> 00:04:11,220
into the model and actually generate

112
00:04:11,220 --> 00:04:12,720
text and you know this would be

113
00:04:12,720 --> 00:04:14,819
perfectly fine but the problem is that

114
00:04:14,819 --> 00:04:16,979
the output distribution is never perfect

115
00:04:16,979 --> 00:04:20,220
so that's why through gpt1 and gpt2 and

116
00:04:20,220 --> 00:04:22,680
even the beginning of gbt3 you know they

117
00:04:22,680 --> 00:04:24,120
progressively got more and more

118
00:04:24,120 --> 00:04:26,100
impressive you might remember you know

119
00:04:26,100 --> 00:04:28,440
like the Unicorn generation from gpt2

120
00:04:28,440 --> 00:04:30,840
and the arguments against recycling from

121
00:04:30,840 --> 00:04:33,479
gbd2 and then eventually gpt3 generating

122
00:04:33,479 --> 00:04:35,160
even higher quality versions more

123
00:04:35,160 --> 00:04:37,199
reliably right and this is the reason

124
00:04:37,199 --> 00:04:38,940
they got better and better is because we

125
00:04:38,940 --> 00:04:41,040
made the models bigger and bigger gbt2

126
00:04:41,040 --> 00:04:43,320
was even bigger gpt3 was bigger still

127
00:04:43,320 --> 00:04:46,919
reaching even 175 billion parameters at

128
00:04:46,919 --> 00:04:49,440
its peak right for context a lot of

129
00:04:49,440 --> 00:04:51,180
other natural language models are in the

130
00:04:51,180 --> 00:04:53,280
hundreds of millions to maybe a billion

131
00:04:53,280 --> 00:04:55,800
or two before this and then gpt3 just

132
00:04:55,800 --> 00:04:57,660
blows them all out of the water right

133
00:04:57,660 --> 00:04:59,580
and so because the models are really big

134
00:04:59,580 --> 00:05:02,340
big and trained on lots of data it's

135
00:05:02,340 --> 00:05:04,259
possible for us to train them to have

136
00:05:04,259 --> 00:05:06,780
really good output distributions but

137
00:05:06,780 --> 00:05:08,280
these really good output distributions

138
00:05:08,280 --> 00:05:11,220
are still imperfect and that imperfect

139
00:05:11,220 --> 00:05:12,900
nature of the distribution makes it so

140
00:05:12,900 --> 00:05:14,280
that when we use them for generating

141
00:05:14,280 --> 00:05:17,160
text we end up sampling tokens and then

142
00:05:17,160 --> 00:05:19,080
feeding them back into a network that's

143
00:05:19,080 --> 00:05:21,300
not used to seeing tokens that are

144
00:05:21,300 --> 00:05:23,940
generated that way that is a major

145
00:05:23,940 --> 00:05:26,639
problem right so we end up now with two

146
00:05:26,639 --> 00:05:29,039
issues one of them is that we are now

147
00:05:29,039 --> 00:05:30,960
misusing the architecture by feeding

148
00:05:30,960 --> 00:05:33,539
data sampling data from the architecture

149
00:05:33,539 --> 00:05:35,400
feeding it back into itself but

150
00:05:35,400 --> 00:05:37,440
fundamentally during training the

151
00:05:37,440 --> 00:05:39,660
network has never seen that output data

152
00:05:39,660 --> 00:05:42,120
and on the other hand we're really just

153
00:05:42,120 --> 00:05:44,699
training it on arbitrary internet data

154
00:05:44,699 --> 00:05:46,380
and the reason that's a problem is

155
00:05:46,380 --> 00:05:49,139
because we want to use this for language

156
00:05:49,139 --> 00:05:52,020
generation at a scale where it's easy to

157
00:05:52,020 --> 00:05:54,479
do right so for example traditionally

158
00:05:54,479 --> 00:05:56,280
with GPT two and three if you were to

159
00:05:56,280 --> 00:05:58,039
prompt the model with something like

160
00:05:58,039 --> 00:06:02,340
write me a story about

161
00:06:02,340 --> 00:06:05,699
um about a man who uh who who loved the

162
00:06:05,699 --> 00:06:08,880
code right random random story or if you

163
00:06:08,880 --> 00:06:10,620
for example gave it an article and you

164
00:06:10,620 --> 00:06:12,539
said summarize this article into a

165
00:06:12,539 --> 00:06:14,520
paragraph the language model wouldn't

166
00:06:14,520 --> 00:06:16,560
actually do it very well in fact it

167
00:06:16,560 --> 00:06:17,940
would generate something relatively

168
00:06:17,940 --> 00:06:20,060
nonsensical compared to what you asked

169
00:06:20,060 --> 00:06:23,100
but considering the training objective

170
00:06:23,100 --> 00:06:25,500
it would make complete sense because on

171
00:06:25,500 --> 00:06:27,840
the internet you never just see tokens

172
00:06:27,840 --> 00:06:30,240
of here's an article and then an article

173
00:06:30,240 --> 00:06:31,979
please summarize the article and then a

174
00:06:31,979 --> 00:06:33,900
great summary you never see that on the

175
00:06:33,900 --> 00:06:36,000
internet that's not part of its training

176
00:06:36,000 --> 00:06:38,039
distribution and therefore it's not

177
00:06:38,039 --> 00:06:40,259
something that it's good at if you asked

178
00:06:40,259 --> 00:06:41,819
it to summarize the article it might

179
00:06:41,819 --> 00:06:43,680
just you know continue on generating

180
00:06:43,680 --> 00:06:45,419
content as if it were a web page where

181
00:06:45,419 --> 00:06:47,520
it saw that sentence and that wouldn't

182
00:06:47,520 --> 00:06:50,759
necessarily be a summary and so the way

183
00:06:50,759 --> 00:06:52,500
to get around this and the way to make

184
00:06:52,500 --> 00:06:54,180
it so the model's better at actually

185
00:06:54,180 --> 00:06:56,280
following instructions is to then

186
00:06:56,280 --> 00:06:59,100
further tune the model now that it has a

187
00:06:59,100 --> 00:07:00,960
baseline understanding of what language

188
00:07:00,960 --> 00:07:03,539
is to be good at instruction following

189
00:07:03,539 --> 00:07:07,080
and that's exactly what openai does they

190
00:07:07,080 --> 00:07:09,060
then perform what's called supervised

191
00:07:09,060 --> 00:07:11,160
fine-tuning on an instruction following

192
00:07:11,160 --> 00:07:13,740
data set they don't fundamentally change

193
00:07:13,740 --> 00:07:15,539
the nature of the architecture at all

194
00:07:15,539 --> 00:07:17,819
it's still next token prediction but

195
00:07:17,819 --> 00:07:19,979
they then fine-tune the model on a data

196
00:07:19,979 --> 00:07:23,099
set of instruction response Pairs and

197
00:07:23,099 --> 00:07:25,259
these pairs enable a model to understand

198
00:07:25,259 --> 00:07:27,539
you know given a prompt of a certain

199
00:07:27,539 --> 00:07:30,060
instruction the completion is supposed

200
00:07:30,060 --> 00:07:31,800
to contain tokens that quote unquote

201
00:07:31,800 --> 00:07:34,919
follow that instruction this supervised

202
00:07:34,919 --> 00:07:36,960
fine tuning brings the model into a

203
00:07:36,960 --> 00:07:40,800
really good State and this plus one

204
00:07:40,800 --> 00:07:43,860
other trick enables the instruct series

205
00:07:43,860 --> 00:07:46,319
of models from open AI these are the

206
00:07:46,319 --> 00:07:47,819
models that you are probably most

207
00:07:47,819 --> 00:07:50,280
familiar with since before chat gbt

208
00:07:50,280 --> 00:07:51,960
right before chat gbt was a thing the

209
00:07:51,960 --> 00:07:54,000
instruct models were incredible you can

210
00:07:54,000 --> 00:07:55,380
provide them instructions in natural

211
00:07:55,380 --> 00:07:57,539
language and they respond by attempting

212
00:07:57,539 --> 00:07:59,940
to complete said instruction or complete

213
00:07:59,940 --> 00:08:01,500
said task

214
00:08:01,500 --> 00:08:04,680
and these instruct models uh were

215
00:08:04,680 --> 00:08:07,500
insanely powerful not just because of

216
00:08:07,500 --> 00:08:09,599
that instruction tuning making it

217
00:08:09,599 --> 00:08:11,880
infinitely easier to actually use the

218
00:08:11,880 --> 00:08:14,099
models but also because of one other

219
00:08:14,099 --> 00:08:16,020
piece of secret sauce and that secret

220
00:08:16,020 --> 00:08:18,360
sauce is reinforcement learning from

221
00:08:18,360 --> 00:08:19,819
Human feedback

222
00:08:19,819 --> 00:08:23,520
rlhf as it's as it's uh short form is

223
00:08:23,520 --> 00:08:25,560
called now this is something that there

224
00:08:25,560 --> 00:08:28,139
is a lot of hype for right around and

225
00:08:28,139 --> 00:08:29,460
lots of people give a lot of different

226
00:08:29,460 --> 00:08:32,880
descriptions as to what rlhf is and I

227
00:08:32,880 --> 00:08:34,740
think that there's actually a pretty

228
00:08:34,740 --> 00:08:37,320
succinct way of trying to summarize what

229
00:08:37,320 --> 00:08:40,140
rlhf does and I haven't seen uh

230
00:08:40,140 --> 00:08:41,279
reinforcement learning from Human

231
00:08:41,279 --> 00:08:43,560
feedback summarized in exactly this way

232
00:08:43,560 --> 00:08:46,140
before but it's sort of my intuitive

233
00:08:46,140 --> 00:08:49,680
model as to why it works so well you see

234
00:08:49,680 --> 00:08:52,980
rlhf brings instruct models from G from

235
00:08:52,980 --> 00:08:55,620
openai and Chachi BT to the next level

236
00:08:55,620 --> 00:08:58,380
it makes them truly valuable and the way

237
00:08:58,380 --> 00:09:00,899
it does so is by training the model not

238
00:09:00,899 --> 00:09:03,420
just on next token prediction from an

239
00:09:03,420 --> 00:09:06,360
existing data set but also on human

240
00:09:06,360 --> 00:09:09,779
feedback of the model's own Generations

241
00:09:09,779 --> 00:09:12,240
this does introduce a bit of a challenge

242
00:09:12,240 --> 00:09:15,480
though and the challenge is that well

243
00:09:15,480 --> 00:09:17,519
the models are trained currently end to

244
00:09:17,519 --> 00:09:19,500
end right we have a data set we have

245
00:09:19,500 --> 00:09:21,240
tokens so we feed them in and we make

246
00:09:21,240 --> 00:09:23,160
them predict the next token and because

247
00:09:23,160 --> 00:09:25,500
it's a relatively simple objective the

248
00:09:25,500 --> 00:09:27,120
models can be trained with gradient

249
00:09:27,120 --> 00:09:29,100
descent this gradient descent has a

250
00:09:29,100 --> 00:09:31,019
super simple error function classic

251
00:09:31,019 --> 00:09:33,839
classification across time steps

252
00:09:33,839 --> 00:09:36,779
the problem now though is that we are

253
00:09:36,779 --> 00:09:39,120
trying to introduce human feedback into

254
00:09:39,120 --> 00:09:41,339
the equation the way we're training with

255
00:09:41,339 --> 00:09:42,480
reinforcement learning with human

256
00:09:42,480 --> 00:09:45,060
feedback or or through human feedback

257
00:09:45,060 --> 00:09:47,640
the way we train is by quite literally

258
00:09:47,640 --> 00:09:50,220
giving humans Generations from the model

259
00:09:50,220 --> 00:09:52,320
say an instruction and what the model

260
00:09:52,320 --> 00:09:55,320
did and asking humans to either rate the

261
00:09:55,320 --> 00:09:57,600
quality of the output or choose between

262
00:09:57,600 --> 00:09:59,459
two different outputs to determine which

263
00:09:59,459 --> 00:10:02,760
one is better and humans if you if you

264
00:10:02,760 --> 00:10:03,779
didn't already know aren't

265
00:10:03,779 --> 00:10:06,120
differentiable we can't calculate the

266
00:10:06,120 --> 00:10:07,980
derivative of a function or a gradient

267
00:10:07,980 --> 00:10:10,920
through a human and so this introduces a

268
00:10:10,920 --> 00:10:13,620
challenge how do we now tune GPT how do

269
00:10:13,620 --> 00:10:16,800
we train GPT the model the machine

270
00:10:16,800 --> 00:10:18,120
learning model that's meant to have

271
00:10:18,120 --> 00:10:21,540
gradients if we introduce a human into

272
00:10:21,540 --> 00:10:23,760
the error function or into the reward in

273
00:10:23,760 --> 00:10:26,160
this case where we can't actually

274
00:10:26,160 --> 00:10:28,019
calculate the gradient through the

275
00:10:28,019 --> 00:10:30,660
function anymore well the solution is

276
00:10:30,660 --> 00:10:33,540
twofold on one hand we don't really want

277
00:10:33,540 --> 00:10:36,300
to use gradients the entire Point here

278
00:10:36,300 --> 00:10:38,640
is that we want to learn from overall

279
00:10:38,640 --> 00:10:42,839
feedback the other point is that well

280
00:10:42,839 --> 00:10:44,880
how do we even scale the function then

281
00:10:44,880 --> 00:10:47,339
right because we want to train on tons

282
00:10:47,339 --> 00:10:49,380
of data we want to train on tons of

283
00:10:49,380 --> 00:10:52,740
rewards but there is a limited amount of

284
00:10:52,740 --> 00:10:54,660
humans that we can throw into a room in

285
00:10:54,660 --> 00:10:56,579
a limited amount of money we can provide

286
00:10:56,579 --> 00:10:59,519
to them and time they can spend to

287
00:10:59,519 --> 00:11:01,560
continuously rate the output of these

288
00:11:01,560 --> 00:11:03,240
models so this introduces those two

289
00:11:03,240 --> 00:11:05,040
challenges right we don't want to use

290
00:11:05,040 --> 00:11:06,420
gradients we want to learn off of

291
00:11:06,420 --> 00:11:09,000
feedback but on the other hand we can't

292
00:11:09,000 --> 00:11:11,459
scale humans infinitely and apply them

293
00:11:11,459 --> 00:11:14,279
to keep rating model outputs as it

294
00:11:14,279 --> 00:11:16,019
learns so

295
00:11:16,019 --> 00:11:18,120
the solution

296
00:11:18,120 --> 00:11:21,180
what what do you do when you can't scale

297
00:11:21,180 --> 00:11:23,040
a bunch of humans and they need to do

298
00:11:23,040 --> 00:11:25,800
something that traditionally only humans

299
00:11:25,800 --> 00:11:27,660
can do well you throw machine learning

300
00:11:27,660 --> 00:11:30,180
at the problem and so open AI through

301
00:11:30,180 --> 00:11:31,740
machine learning at the problem of how

302
00:11:31,740 --> 00:11:33,540
to train a machine learning algorithm

303
00:11:33,540 --> 00:11:35,279
and what they do is they train what's

304
00:11:35,279 --> 00:11:38,040
called a reward model so they will take

305
00:11:38,040 --> 00:11:40,560
a bunch of output completions from chat

306
00:11:40,560 --> 00:11:43,680
gbt and even just human written data as

307
00:11:43,680 --> 00:11:45,720
well just regardless of what it is

308
00:11:45,720 --> 00:11:48,540
and to take these ratings take these

309
00:11:48,540 --> 00:11:51,000
preference values this data set of

310
00:11:51,000 --> 00:11:52,920
people saying hey this output was better

311
00:11:52,920 --> 00:11:54,240
than this output or this one was worse

312
00:11:54,240 --> 00:11:56,220
than this other one all right take all

313
00:11:56,220 --> 00:11:59,279
of this data and train a reward model to

314
00:11:59,279 --> 00:12:02,760
predict human preferences right so take

315
00:12:02,760 --> 00:12:05,040
whatever open AI generates whatever gbt

316
00:12:05,040 --> 00:12:07,620
generates and train a model to take that

317
00:12:07,620 --> 00:12:10,680
generation and predict a quality score

318
00:12:10,680 --> 00:12:12,000
for that output

319
00:12:12,000 --> 00:12:13,440
so think about what we've just done

320
00:12:13,440 --> 00:12:15,480
we've removed the human from the

321
00:12:15,480 --> 00:12:17,880
equation we no longer need to

322
00:12:17,880 --> 00:12:20,220
necessarily use a human to determine

323
00:12:20,220 --> 00:12:22,079
which of two outputs is better and if

324
00:12:22,079 --> 00:12:24,779
you train a very strong reward model

325
00:12:24,779 --> 00:12:26,660
which itself could be based on

326
00:12:26,660 --> 00:12:29,040
theoretically the same language model or

327
00:12:29,040 --> 00:12:32,940
a similar language model as gbt then you

328
00:12:32,940 --> 00:12:35,399
end up in a scenario where you now have

329
00:12:35,399 --> 00:12:38,519
a super strong model that can do that

330
00:12:38,519 --> 00:12:41,640
human annotation of which generation is

331
00:12:41,640 --> 00:12:44,700
better and because it's scalable you can

332
00:12:44,700 --> 00:12:48,360
now train your base model to attempt to

333
00:12:48,360 --> 00:12:51,120
optimize it to produce higher and higher

334
00:12:51,120 --> 00:12:54,360
Rewards or quality scores against this

335
00:12:54,360 --> 00:12:57,000
reward model and we do it specifically

336
00:12:57,000 --> 00:13:00,240
using reinforcement learning and the

337
00:13:00,240 --> 00:13:02,040
reason reinforcement learning is used is

338
00:13:02,040 --> 00:13:04,320
because even though we have now turned

339
00:13:04,320 --> 00:13:06,120
this into a machine learning task where

340
00:13:06,120 --> 00:13:07,620
the human is removed from the equation

341
00:13:07,620 --> 00:13:10,680
the problem is there is still a break in

342
00:13:10,680 --> 00:13:13,740
differentiability you still cannot fully

343
00:13:13,740 --> 00:13:16,620
at least truly optimize this end to end

344
00:13:16,620 --> 00:13:19,320
and the reason you cannot do that is

345
00:13:19,320 --> 00:13:22,920
because the output of GPT is still a

346
00:13:22,920 --> 00:13:25,200
language model and this brings me to the

347
00:13:25,200 --> 00:13:28,620
final real problem you see gbt being a

348
00:13:28,620 --> 00:13:30,480
language model fundamentally is

349
00:13:30,480 --> 00:13:32,940
optimized for next token prediction even

350
00:13:32,940 --> 00:13:36,240
after instruct tuning even after rlhf it

351
00:13:36,240 --> 00:13:39,240
was it's only really tuned for that sort

352
00:13:39,240 --> 00:13:41,279
of next token prediction

353
00:13:41,279 --> 00:13:43,139
outputting a distribution of

354
00:13:43,139 --> 00:13:45,180
probabilities that way you see the thing

355
00:13:45,180 --> 00:13:48,180
about this output distribution is that

356
00:13:48,180 --> 00:13:50,700
not only is the model not used to having

357
00:13:50,700 --> 00:13:53,519
input where the distribution is its own

358
00:13:53,519 --> 00:13:56,040
output because it's only ever seeing a

359
00:13:56,040 --> 00:13:58,860
real text as input the other issue with

360
00:13:58,860 --> 00:14:01,440
it is that because the outputs are

361
00:14:01,440 --> 00:14:03,720
probabilities when you try and distill

362
00:14:03,720 --> 00:14:05,940
those probabilities down into an actual

363
00:14:05,940 --> 00:14:07,860
token you've broken differentiability

364
00:14:07,860 --> 00:14:11,600
right so if gbt were to see the sentence

365
00:14:11,600 --> 00:14:15,000
my name is right and if let's just say

366
00:14:15,000 --> 00:14:17,100
for some reason you know the next

367
00:14:17,100 --> 00:14:19,560
highest probability token from gbt was

368
00:14:19,560 --> 00:14:22,200
tan May that doesn't mean gpt's

369
00:14:22,200 --> 00:14:25,079
generating the token tan may all that's

370
00:14:25,079 --> 00:14:28,260
saying is that the highest probability

371
00:14:28,260 --> 00:14:31,079
token to come next should be tanning we

372
00:14:31,079 --> 00:14:32,940
then abuse that output to then say okay

373
00:14:32,940 --> 00:14:34,980
we're going to choose tanway as the next

374
00:14:34,980 --> 00:14:36,959
most uh as the next token in the

375
00:14:36,959 --> 00:14:37,800
sequence

376
00:14:37,800 --> 00:14:40,500
and because we have to choose a token

377
00:14:40,500 --> 00:14:42,420
and turn it from a continuous

378
00:14:42,420 --> 00:14:45,240
probability space into a discrete chosen

379
00:14:45,240 --> 00:14:47,220
token differentiability is broken

380
00:14:47,220 --> 00:14:49,620
meaning that when we feed this these

381
00:14:49,620 --> 00:14:51,720
series of tokens the sentence my name is

382
00:14:51,720 --> 00:14:53,940
tanmay into the reward model for it to

383
00:14:53,940 --> 00:14:56,459
give us a reward there is no way for us

384
00:14:56,459 --> 00:14:59,279
to go gradients of the reward back to

385
00:14:59,279 --> 00:15:02,339
how to update the underlying gbt model

386
00:15:02,339 --> 00:15:05,160
this is a problem but it's solved using

387
00:15:05,160 --> 00:15:07,380
reinforcement learning reinforcement

388
00:15:07,380 --> 00:15:09,779
learning you may know is what we use to

389
00:15:09,779 --> 00:15:12,600
train agents in an environment in an

390
00:15:12,600 --> 00:15:14,699
environment where we may not have a

391
00:15:14,699 --> 00:15:16,980
clearly differentiable just end-to-end

392
00:15:16,980 --> 00:15:20,160
optimization you know solution

393
00:15:20,160 --> 00:15:23,040
and so reinforcement learning being used

394
00:15:23,040 --> 00:15:24,839
in all sorts of different techniques to

395
00:15:24,839 --> 00:15:26,880
for example play Atari games and so on

396
00:15:26,880 --> 00:15:29,519
is super advantageous here because now

397
00:15:29,519 --> 00:15:33,480
what we can do is we can train GPT using

398
00:15:33,480 --> 00:15:37,740
rewards predicted by a reward model but

399
00:15:37,740 --> 00:15:40,860
we don't actually need to Output at the

400
00:15:40,860 --> 00:15:44,100
individual token probability level gbt

401
00:15:44,100 --> 00:15:47,399
is no longer being optimized at the

402
00:15:47,399 --> 00:15:48,959
individual just what should the next

403
00:15:48,959 --> 00:15:51,899
token be level GPT is being optimized to

404
00:15:51,899 --> 00:15:54,260
really just improve its probabilities

405
00:15:54,260 --> 00:15:58,500
across time steps such that the discrete

406
00:15:58,500 --> 00:16:01,560
tokens we end up choosing result in

407
00:16:01,560 --> 00:16:04,740
higher outputs at the reward model level

408
00:16:04,740 --> 00:16:06,899
and because we've now very subtly

409
00:16:06,899 --> 00:16:09,000
shifted the objective away from next

410
00:16:09,000 --> 00:16:11,579
token prediction and into a space where

411
00:16:11,579 --> 00:16:13,620
a we're optimizing across all

412
00:16:13,620 --> 00:16:15,420
probabilities across all time steps

413
00:16:15,420 --> 00:16:17,100
right we're no longer just optimizing

414
00:16:17,100 --> 00:16:19,680
for an individual step at a time and

415
00:16:19,680 --> 00:16:21,779
because of the fact that during the

416
00:16:21,779 --> 00:16:23,940
generation it was seeing its own output

417
00:16:23,940 --> 00:16:26,220
distribution because of those two

418
00:16:26,220 --> 00:16:29,100
factors we end up with a model that

419
00:16:29,100 --> 00:16:31,560
learns a lot better both how to be used

420
00:16:31,560 --> 00:16:34,440
for text generation but also how to

421
00:16:34,440 --> 00:16:36,540
align to the preferences of a reward

422
00:16:36,540 --> 00:16:39,360
model which themselves elves are derived

423
00:16:39,360 --> 00:16:42,000
from preferences from humans things like

424
00:16:42,000 --> 00:16:43,920
safety concerns making sure that the

425
00:16:43,920 --> 00:16:45,779
model doesn't generate toxic content

426
00:16:45,779 --> 00:16:47,699
making sure that the model generates

427
00:16:47,699 --> 00:16:49,980
accurate content or that it follows the

428
00:16:49,980 --> 00:16:51,120
instructions that it was told to follow

429
00:16:51,120 --> 00:16:52,860
and doesn't repeat itself and so on and

430
00:16:52,860 --> 00:16:55,680
so forth those two factors make it so

431
00:16:55,680 --> 00:16:58,800
that instruct GPT was already so good

432
00:16:58,800 --> 00:17:01,079
and Chachi BT just brings that to the

433
00:17:01,079 --> 00:17:03,540
next level chat gbt is quite literally

434
00:17:03,540 --> 00:17:06,120
just another round of supervised

435
00:17:06,120 --> 00:17:08,280
fine-tuning on top of a chat data set

436
00:17:08,280 --> 00:17:11,160
and then reinforcement learning from

437
00:17:11,160 --> 00:17:13,260
Human feedback to make it that the model

438
00:17:13,260 --> 00:17:15,780
is both used to its own input

439
00:17:15,780 --> 00:17:17,939
distribution being its output

440
00:17:17,939 --> 00:17:20,699
distribution as well as optimizing

441
00:17:20,699 --> 00:17:23,339
against something that is not just a

442
00:17:23,339 --> 00:17:25,860
next token prediction objective that's

443
00:17:25,860 --> 00:17:28,679
why rlhf works so well that's why I've

444
00:17:28,679 --> 00:17:30,000
been interested in it for quite

445
00:17:30,000 --> 00:17:33,000
literally years even before instruct gbt

446
00:17:33,000 --> 00:17:34,740
came out if you remember my project for

447
00:17:34,740 --> 00:17:36,900
say generating music lyrics you might

448
00:17:36,900 --> 00:17:38,220
know that I've been a subscribe fiber of

449
00:17:38,220 --> 00:17:40,980
the TRL GitHub repo which which aims to

450
00:17:40,980 --> 00:17:42,960
do reinforcement learning for

451
00:17:42,960 --> 00:17:45,780
Transformers and so I've been I've been

452
00:17:45,780 --> 00:17:47,039
looking into these sorts of techniques

453
00:17:47,039 --> 00:17:48,840
for multiple years now

454
00:17:48,840 --> 00:17:51,780
since around like end of 2019 and it's

455
00:17:51,780 --> 00:17:53,400
so exciting to finally get to see

456
00:17:53,400 --> 00:17:55,440
reinforcement learning used at this

457
00:17:55,440 --> 00:17:57,600
scale to enable such incredible

458
00:17:57,600 --> 00:18:00,299
applications rlhf is traditionally

459
00:18:00,299 --> 00:18:02,640
expensive and difficult to do it's

460
00:18:02,640 --> 00:18:04,620
getting more accessible but for the sake

461
00:18:04,620 --> 00:18:06,660
of this video we're going to see how you

462
00:18:06,660 --> 00:18:08,940
can do the first stage of what openai

463
00:18:08,940 --> 00:18:12,120
did the supervised fine-tuning on top of

464
00:18:12,120 --> 00:18:15,240
a chat data set to build a model that is

465
00:18:15,240 --> 00:18:17,340
capable of conversing with you but

466
00:18:17,340 --> 00:18:19,320
clearly doesn't necessarily have that

467
00:18:19,320 --> 00:18:22,559
reward based feedback to help it be sort

468
00:18:22,559 --> 00:18:25,200
of at that next level of quality so

469
00:18:25,200 --> 00:18:27,000
let's go ahead and in the next video

470
00:18:27,000 --> 00:18:29,160
take a look at how you can actually

471
00:18:29,160 --> 00:18:32,280
download the both open assistant and

472
00:18:32,280 --> 00:18:35,039
Dolly data set from databricks both of

473
00:18:35,039 --> 00:18:37,020
these data sets are going to be used to

474
00:18:37,020 --> 00:18:39,799
train the Replay code model so replit

475
00:18:39,799 --> 00:18:42,660
released a 3 billion parameter language

476
00:18:42,660 --> 00:18:44,340
model trained on top of something like

477
00:18:44,340 --> 00:18:47,039
half a trillion tokens of code for three

478
00:18:47,039 --> 00:18:49,620
epochs it is a really good code model

479
00:18:49,620 --> 00:18:52,980
for its size and we are going to take

480
00:18:52,980 --> 00:18:55,020
both the um

481
00:18:55,020 --> 00:18:57,720
open assistant and Dolly data sets we're

482
00:18:57,720 --> 00:19:00,000
going to reformat them to fit what we

483
00:19:00,000 --> 00:19:02,460
need and train this model to be really

484
00:19:02,460 --> 00:19:04,740
good at next token prediction on top of

485
00:19:04,740 --> 00:19:06,120
those two data sets and in a future

486
00:19:06,120 --> 00:19:08,220
video we will also take a look at how

487
00:19:08,220 --> 00:19:09,600
you can potentially reinforcement

488
00:19:09,600 --> 00:19:12,059
learning based tune these models to be

489
00:19:12,059 --> 00:19:15,539
good at following human preferences and

490
00:19:15,539 --> 00:19:18,120
so now without any further Ado let's go

491
00:19:18,120 --> 00:19:19,919
ahead and take a look at how you can

492
00:19:19,919 --> 00:19:22,679
train those models in the next video I

493
00:19:22,679 --> 00:19:25,280
will see you there
